,QuestionId,Title,Body,DocRelated,Issue Type
0,34589335,How does the distorted_inputs() function in the TensorFlow CIFAR-10 example tutorial get 128 images per batch?,"I was going through the CIFAR-10 example at TensorFlow getting started guide for CNN Now in the train function in cifar10_train.py we get images as In the distorted_inputs() function we generate the filenames in a queue and then read a single record as When I add debugging code, the read_input variable contains only 1 record with an image and its height, width, and label name. The example then applies some distortion to the read image/record and then passes it to the _generate_image_and_label_batch() function. This function then returns a 4D Tensor of shape [batch_size, 32, 32, 3] where batch_size = 128. The above function utilizes the tf.train.shuffle_batch() function when returns the batch. My question is where do the extra records come from in the tf.train.shuffle_batch() function? We are not passing it any filename or reader object. Can someone shed some light on how we go from 1 record to 128 records? I looked into the documentation but didn't understand.",1,Documentation Replication on Other Examples
1,34619177,What does tf.nn.conv2d do in tensorflow?,"I was looking at the docs of tensorflow about tf.nn.conv2d here. But I can't understand what it does or what it is trying to achieve. It says on the docs, Now what does that do? Is that element-wise multiplication or just plain matrix multiplication? I also could not understand the other two points mentioned in the docs. I have written them below : It would be really helpful if anyone could give an example, a piece of code (extremely helpful) maybe and explain what is going on there and why the operation is like this. I've tried coding a small portion and printing out the shape of the operation. Still, I can't understand. I tried something like this: I understand bits and pieces of convolutional neural networks. I studied them here. But the implementation on tensorflow is not what I expected. So it raised the question. EDIT: So, I implemented a much simpler code. But I can't figure out what's going on. I mean how the results are like this. It would be extremely helpful if anyone could tell me what process yields this output. output",1,Documentation Replication on Other Examples
2,34642595,Tensorflow Strides Argument,"I am trying to understand the strides argument in tf.nn.avg_pool, tf.nn.max_pool, tf.nn.conv2d. The documentation repeatedly says My questions are: Sadly the examples in the docs for reshape using -1 don't translate too well to this scenario.",1,Documentation Ambiguity
3,34931121,Can cond support TF ops with side effects?,"The (source code) documentation for tf.cond is unclear on whether the functions to be performed when the predicate is evaluated can have side effects or not. I've done some tests but I'm getting conflicting results. For example the code below does not work: I.e. no matter what value the predicate evaluates to, both functions are getting run, and so the net result is a subtraction of 1. On the other hand, this code snippet does work, where the only difference is that I add new ops to the graph every time my_op is called: Not sure why creating new ops every time works while the other case doesn't, but I'd obviously rather not be adding nodes as the graph will eventually become too big.",1,Documentation Replicability
4,35689547,How to process single training file in parallel,"I have a file train.csv that contains paths to images and their labels. ie: After going through the reading data tutorial I came up with some code to go through each image, resize it and apply distortions: The problem now is that I'm confused how to do these operations across the file in parallel. The documentation suggests either using tf.train.batch_join or tf.train.batch with num_threads=N. I first tried following the example code using tf.train.batch_join but this seems to be intended for processing multiple files in parallel. In my case however I just have 1 file. I also tried setting tf.train.batch([example, label], batch_size, num_threads=8) but its not clear to me if this is doing the right thing (although I can see more cpu cores in use) Here is my code for executing the graph: Whats the best way to process this file in parallel?",1,Documentation Replication on Other Examples
5,36223157,Set weight and bias tensors of tensorflow conv2d operation,"I have been given a trained neural network in torch and I need to rebuild it exactly in tensorflow. I believe I have correctly defined the network's architecture in tensorflow but I am having trouble transferring the weight and bias tensors. Using a third party package, I converted all the weight and bias tensors from the torch network to numpy arrays then wrote them to disk. I can load them back into my python program but I cannot figure out a way to assign them to the corresponding layers in my tensorflow network. For instance, I have a convolution layer defined in tensorflow as According to the tensorflow documentation, the tf.nn.conv2d operation uses the shape defined in the kernel_1 variable to construct the weight tensor. However, I cannot figure out how to access that weight tensor to set it to the weight array I have loaded from file. Is it possible to explicitly set the weight tensor? And if so, how? (The same question applies to bias tensor.)",1,Lack of Alternative Solutions/Documentation
6,36570729,tf.IndexedSlicesValue when returned from tf.gradients(),"I'm having the following problem, I have four embedding matrices and want to get the gradients of my loss function with respect to those matrices. When I run the session to return the values for the gradients, two of those returned objects are of type tensorflow.python.framework.ops.IndexedSlicesValue, the other two are numpy arrays. Now for the numpy arrays, their shape corresponds to the shape of their corresponding embedding matrix, but I'm having problems with the IndexedSlicesValue objects. If I call .values on one of those objects, I get an array whose shape does not match that of the gradient, the shape of the embedding matrix is [22,30], but calling .values on the IndexedSlicesValue object I get an array with shape [4200,30] ( The shape of my input tensor had dimensions of [30,20,7], the product of those dimensions equals 4200, not sure if this is relevant). The IndexedSlicesValue object has an attribute called dense_shape, which is an array that holds the dimensions the gradient should have, i.e. array([22,30]) is value returned by .dense_shape. I don't really understand the docs here: https://www.tensorflow.org/versions/r0.7/api_docs/python/state_ops.html#IndexedSlices It says: So this array of shape (4200,30) is extracted from an array corresponding to an even larger, dense tensor? What exactly is the gradient in this IndexedSlicesValue object and why does tensorflow automatically use this type for some gradients returned by tf.gradients()? Here is my code:",1,Documentation Replication on Other Examples
7,36631868,Tensorflow: Noise contrastive estimation language model,"I want to change the loss function in the ptb_word_lm.py example to tf.nn.nce_loss. Looking at the tf.nn.nce_loss implementation: I think But I do not know what are the first two parameters, weights and biases. How could I adapt tf.nn.nce_loss to language model? Thanks. @Aaron: Thanks, I have tried the following: According to the document at here: So, My PTBModel model looks like However, I got an error Did I miss anything here? Thanks again.",1,Documentation Replication on Other Examples
8,37044006,Tensorflow conditional throwing value error,I am trying to use conditionals with tensorflow and I am getting the error: Below is the code I use that is throwing the error. It is saying the error is in the conditional Stack trace: I have tried changing the WORKING to be an array instead of a scalar. I believe that the problem is that tf.equal is returning an int32 instead of the bool that it is supposed to return according to the documentation,1,Documentation Ambiguity
9,37376861,what does the tf.nn.lrn() method do?,Here is the code-snipped from the cifar10-tutorial. It's from the cifar10.py. What does the tf.nn.lrn-Method do? I can't find a definition in the API Documentation on https://www.tensorflow.org/versions/r0.8/api_docs/python/index.html,1,Lack of Alternative Solutions/Documentation
10,38033079,Tensorflow understanding tf.train.shuffle_batch,"I have a single file of training data, about 100K rows, and I'm running a straightforward tf.train.GradientDescentOptimizer on each training step. The setup is essentially taken directly from Tensorflow's MNIST example. Code reproduced below: Given that I'm reading training data from a file, I'm using tf.train.string_input_producer and tf.decode_csv to read rows from the csv, and then tf.train.shuffle_batch to create batches that I then train on. I'm confused as to what my parameters should be for tf.train.shuffle_batch. I read Tensorflow's documentation, and yet I'm still not sure what the ""optimal"" batch_size, capacity, and min_after_dequeue values are. Can anyone help shed some light on how I go about choosing proper values for these parameters, or link me to a resource where I can learn more? Thanks-- Here's the API link: https://www.tensorflow.org/versions/r0.9/api_docs/python/io_ops.html#shuffle_batch",1,Documentation Ambiguity
11,38111170,How is the input tensor for TensorFlow's tf.nn.dynamic_rnn operator structured?,"I am trying to write a language model using word embeddings and recursive neural networks in TensorFlow 0.9.0 using the tf.nn.dynamic_rnn graph operation, but I don't understand how the input tensor is structured. Let's say I have a corpus of n words. I embed each word in a vector of length e, and I want my RNN to unroll to t time steps. Assuming I use the default time_major = False parameter, what shape would my input tensor [batch_size, max_time, input_size] have? Maybe a specific tiny example will make this question clearer. Say I have a corpus consisting of n=8 words that looks like this. Say I embed it in a vector of size e=3 with the embeddings 1 -&gt; [10, 10, 10], 2 -&gt; [20, 20, 20], and 3 -&gt; [30, 30, 30], what would my input tensor look like? I've read the TensorFlow Recurrent Neural Network tutorial, but that doesn't use tf.nn.dynamic_rnn. I've also read the documentation for tf.nn.dynamic_rnn, but find it confusing. In particular I'm not sure what ""max_time"" and ""input_size"" mean here. Can anyone give the shape of the input tensor in terms of n, t, and e, and/or an example of what that tensor would look like initialized with data from the small corpus I describe? TensorFlow 0.9.0, Python 3.5.1, OS X 10.11.5",1,Documentation Replicability
12,38114534,Basic 1d convolution in tensorflow,"OK, I'd like to do a 1-dimensional convolution of time series data in Tensorflow. This is apparently supported using tf.nn.conv2d, according to these tickets, and the manual. the only requirement is to set strides=[1,1,1,1]. Sounds simple! However, I cannot work out how to do this in even a very minimal test case. What am I doing wrong? Let's set this up. OK, now generate a basic convolution test on two small arrays. I will make it easy by using a batch size of 1, and since time series are 1-dimensional, I will have an ""image height"" of 1. And since it's a univariate time series, clearly the number of ""channels"" is also 1, so this will be simple, right? BOOM. Error. OK, For a start, I don't understand how this should happen with any dimension, since I've specified that I'm padding the arguments in the convolution OP. but fine, maybe there are limits to that. I must have got the documentation confused and set up this convolution on the wrong axes of the tensor. I'll try all possible permutations: Result: Hmm. OK, it looks like there are two problems now. Firstly, the ValueError is about applying the filter along the wrong axis, I guess, although there are two forms. But then the axes along which I can apply the filter are confusing too - notice that it actually constructs the graph with input shape (5, 1, 1, 1) and filter shape (1, 1, 1, 3). AFAICT from the documentation, this should be a filter that looks at on example from the batch, one ""pixel"" and one ""channel"" and outputs 3 ""channels"". Why does that one work, then, when others do not? Anyway, sometimes it does not fail while constructing the graph. Sometime it constructs the graph; then we get the tensorflow.python.framework.errors.InvalidArgumentError. From some confusing github tickets I gather this is probably due to the fact that I'm running on CPU instead of GPU, or vice versa the fact that the convolution Op is only defined for 32 bit floats, not 64 bit floats. If anyone could throw some light on which axes I should be aligning what on, in order to convolve a time series with a kernel, I'd be very grateful.",1,Documentation Ambiguity
13,38641887,How to save a trained tensorflow model for later use for application?,"I am a bit of a beginner with tensorflow so please excuse if this is a stupid question and the answer is obvious. I have created a Tensorflow graph where starting with placeholders for X and y I have optimized some tensors which represent my model. Part of the graph is something where a vector of predictions can be calculated, e.g. for linear regression something like After training has been completed I have acceptable values for w and d and now I want to save my model for later. Then, in a different python session I want to restore the model so that I can again run for some different data and get back the y-values. I want this to work in a way where the graph for calculating the y-values from the placeholders is also stored and restored - as long as the placeholders get fed the correct data, this should work transparently without the user (the one who applies the model) needing to know what the graph looks like). As far as I understand tf.train.Saver().save(..) only saves the variables but I also want to save the graph. I think that tf.train.export_meta_graph could be relevant here but I do not understand how to use it correctly, the documentation is a bit cryptic to me and the examples do not even use export_meta_graph anywhere.",1,Inadequate Examples
14,38893526,What's the meaning of tf.nn.embedding_lookup_sparse in TensorFlow?,"We spend a lot of time in reading the API document of tf.nn.embedding_lookup_sparse. The meaning of embedding_lookup_sparse is confusing and it seems quite different from embedding_lookup. Here's what I think and please correct me if I'm wrong. The example of wide and deep model uses contrib.layers APIs and call embedding_lookup_sparse for sparse feature colume. If it gets the SparseTensor(for example, country, which is sparse), it creates the embedding which is actually for one-host encoding. Then call to_weights_sum to return the result of embedding_lookup_sparse as prediction and the embedding as variable. The the result of embedding_lookup_sparse add bias and become the logits for loss function and training operation. That means the embedding_lookup_sparse do something like w * x(part of y = w * x + b) for dense tensor. Maybe for one-hot encoding or SparseTensor, the weight from embedding_lookup_sparse is actually the value of w * x because the look-up data is always 1 and no need to add other 0s. What I said is also confusing. Can anyone help to explain this in detail?",1,Documentation Replication on Other Examples
15,38962308,Unclear behavior for sampler in Tensorflow,"For the samplers implemented in tensorflow, e.g. tf.nn.fixed_unigram_candidate_sampler. The behavior is not well-defined in the document. For instance, I would expect the labels specified in true_classes will be excluded from the sampling pool, and the sampling will be conducted for each batch. But according to my experiments, neither of above is true. Consider the following code: The output can be 3, which actually belongs to the set of true classes. - Also, the output has the dimension [1], which basically means that the sampling is only conducted once, not for each batch. Can someone help to clarify this?",1,Documentation Ambiguity
16,39133312,Why does setting an initialization value prevent placing a variable on a GPU in TensorFlow?,"I get an exception when I try to run the following very simple TensorFlow code, although I virtually copied it from the documentation: The exception is: If I change the variable's initial value to tf.zeros([1]) instead, everything works fine: Any idea what's going on?",1,Documentation Replicability
17,39210093,regarding the correct way to understand the result of tf.pad,"When reading the document for tf.pad, I feel quite confusing about the example given in the tutorial. For instance, padding is [[1,1,],[2,2]], how does it cause the resulting tensor has the shape as shown in the figure. Besides, what's the mechanism to generate those padded values, e.g., the ones marked in red circle. It is not very clear how to connect the explanation with the example.",1,Lack of Alternative Solutions/Documentation
18,39211332,Custom initializer for get_variable,"How can one specify a custom initializer as the third argument for tf.get_variable()? Specifically, I have a variable y which I want to initialize using another (already initialized) variable x. This is easy to do using tf.Variable(), just say, y = tf.Variable(x.initialized_value()). But I couldn't find an analog in the documentation for tf.get_variable().",1,Lack of Alternative Solutions/Documentation
19,39450992,Tensorflow slim how to specify batch size during training,"I'm trying to use slim interface to create and train a convolutional neural network, but I couldn't figure out how to specify the batch size for training. During the training my net crashes because of ""Out of Memory"" on my graphic card. So I think that should be a way to handle this condition... Do I have to split the data and the labels in batches and then explicitly loop or the slim.learning.train is taking care of it? In the code I paste train_data are all the data in my training set (numpy array)..and the model definition is not included here I had a quick loop to the sources but no luck so far... Any hints suggestions? Edit: I re-read the documentation...and I found this example But It's not clear at all how to feed image and label to be passed to tf.train.batch... as MyPascalVocDataLoader function is not specified... In my case my data set are loaded from a sqlite database and I have training data and labels as numpy array....still confused. Of course I tried to pass my numpy arrays (converted to constant tensor) to the tf.train.batch like this But seems not the right path to follow... it seems that the train.batch wants only one element from my data set...(how to pass this? it does not make sense to me to pass only train_data[0] and train_labels[0])",1,Documentation Ambiguity
20,39627140,Why would the naive definition of moving average cause unnecessary locking in TensorFlow?,"The TensorFlow docs for tf.train.ExponentialMovingAverage say, Why does the first formula permit more concurrency than the second formula? How can I know if my own code is incurring unnecessary locking because of some subtle locking issue?",1,Documentation Replicability
21,39681026,Tensorflow: How to pass output from previous time-step as input to next timestep,"It is a duplicate of this question How can I feed last output y(t-1) as input for generating y(t) in tensorflow RNN? I want to pass the output of RNN at time-step T as the input at time-step T+1. input_RNN(T+1) = output_RNN(T) As per the documentation, the tf.nn.rnn as well as tf.nn.dynamic_rnn functions explicitly take the complete input to all time-steps. I checked the seq2seq example at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py It uses a loop and calls the cell(input,state) function. The cell can be lstm or gru or any other rnn cell. I checked the documentation to find the data type and shape of the arguments to cell(), but I found only the contructor of the form cell(num_neurons). I would like to know the correct way of passing output to input. I don't want to use other libraries/wrappers like keras built over tensorflow. Any suggestions?",1,Documentation Completeness
22,40195549,tf.rank function in Tensorflow,"I ma trying to understand tf.rank function in tensorflow. From the documentation here, I understood that rank should return the number of distinct elements in the tensor. Here x and weights are 2 distinct 2*2 tensors with 4 distinct elemnts in each of them. However, rank() function outputs are: Also, for the tensor x, I used tf.constant() with dtype = float to convert ndarray into float32 tensor but the rank() still outputs as int32. How should I interpret the output.",1,Documentation Replication on Other Examples
23,40394910,What do classes tf.train.Coordinator and class tf.train.QueueRunner do in tensorflow?,"I understand that both classes deal with threads. According to the documentation, tf.train.Coordinator coordinates the termination of a set of threads and tf.train.QueueRunner holds a list of enqueue operations for a queue, each to be run in a thread. However, what is their role in simple words? When are they necessary during the training?",1,Documentation Replicability
24,40451974,"Tensorflow, restore variables in a specific device","Maybe my question is a bit naive, but I really didn't find anything in the tensorflow documentation. I have a trained tensorflow model where the variables of it was placed in the GPU. Now I would like to restore this model and test it using the CPU. If I do this via 'tf.train.Saver.restore` as in the example: saver = tf.train.import_meta_graph(""/tmp/graph.meta"") saver.restore(session, ""/tmp/model.ckp"") I have the following excpetion: InvalidArgumentError: Cannot assign a device to node 'b_fc8/b_fc8/Adam_1': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0 How can I make restore these variables in the CPU? Thanks",1,Documentation Replication on Other Examples
25,40846881,"How do I finde the Code of ""tf.nn.dynamic_rnn"" in the tensorflow repository?","I'm trying to understand the structure and the coding of tensorflow. While going through this tutorial ""https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/"" I have searched for the code that is used in the functions. For example the line uses the function GRUCell, which I can find in the file ""rnn_cell.py"" in the tensorflow repository. Furthermore the GRUCell is wrapped by a function called ""tf.nn.dynamic_rnn"" as follows: Unfortunately I am not able to find the code for this function. Where do I find it? Everything I find is this documentation: https://github.com/tensorflow/tensorflow/blob/eb56a8af24695bf8258addf28b0c53fbabff72e6/tensorflow/g3doc/api_docs/python/functions_and_classes/shard8/tf.nn.dynamic_rnn.md Thanks for helping!",1,Lack of Alternative Solutions/Documentation
26,40879504,How to apply Drop Out in Tensorflow to improve the accuracy of neural network?,"Drop-Out is regularization techniques. And I want to apply it to notMNIST data to reduce over-fitting to finish my Udacity Deep Learning Course Assignment.I have read the docs of tensorflow on how to call the tf.nn.dropout. And here is my code The tf.nn.dropout is called in function model(), but after I applied the DropOut technique to the neural network, the accuracy did seem any change, here is the result: How can I apply DropOut by Tensorflow to improve the accuracy of the network? Thank you!",1,Documentation Replication on Other Examples
27,41125183,Tensorflow: split_v with variable num_splits,"I am wondering if the same holds for tf.split_v() as tf.split(). According to the documentation split_v also accepts a Tensor as second argument. However, when I try this code The error is Is this possible or not?",1,Documentation Replicability
28,41283115,"Tensorflow, difference between tf.nn.softmax_cross_entropy_with_logits and tf.nn.sparse_softmax_cross_entropy_with_logits","I have read the docs of both functions, but as far as I know, for function tf.nn.softmax_cross_entropy_with_logits(logits, labels, dim=-1, name=None), the result is the cross entropy loss, in which the dimensions of logits and labels are the same. But, for function tf.nn.sparse_softmax_cross_entropy_with_logits, the dimensions of logits and labels are not the same? Could you give a more detail example of tf.nn.sparse_softmax_cross_entropy_with_logits?",1,Requesting (Additional) Documentation/Examples
29,41308515,Force copy of tensor when enqueuing,"first, I'm not sure if the title is very good, but it was the best I could come up with given my understanding of the situation. The background is that I'm trying to understand how queues work in tensorflow and ran into the following issue which puzzled me. I have a variable n, which I enqueue to a tf.FIFOQueue, and then I increment the variable. This is repeated several times, and one would expect a result similar to 0, 1, 2, ... However, when emptying the queue all values are the same. More precisely, the code is as follows: Which I expect would print: Instead I get the following result: It seems like I'm pushing some pointer to n to the queue, instead of the actual value, which is what I want. However, I don't really have any actual understanding of tensorflow internals, so maybe something else is going on? I tried changing to since answers to How can I copy a variable in tensorflow and In TensorFlow, what is tf.identity used for? gives me the impression that it might help, but it does not change the result. I also tried adding a tf.control_dependencies(), but again, all values are the same when dequeueing. Edit: The output above is from running the code on a computer with a single CPU, when trying to see if there was some difference between different versions of tensorflow, I noticed if I run the code on a computer with CPU and GPU I get the ""expected"" result. Indeed, if I run with CUDA_VISIBLE_DEVICES="""" I get the result above, and with CUDA_VISIBLE_DEVICES=""0"" I get the ""expected"" result.",1,Documentation Ambiguity
30,41353079,Tensorflow image.central_crop (mis)behavior,In the Tensorflow documentation for tf.image.central_crop function: Consider the following code: Original image size is 456x450 Crop size is 228x226 Which gives area ratio of: Not 0.5 as I expected. Can someone help to clarify this?,1,Documentation Ambiguity
31,41437483,How does tf.nn.conv2d calculate its values?,"I've looked at the documentation for tf.nn.conv2d but it didn't really help much. So I tried to multiply the first 4 values of my input array that form a square (0, 1, 2, 2.5) with the first column of the filter_weights array (0.19041163, -0.36705261, 0.69018674, 1.7655524). But regardless of how I multiply these values I'm not getting 1.13938534, I don't know what I'm doing wrong. Below I have the code that I used. Given an array: And weights: Which prints: How does: Give:",1,Lack of Alternative Solutions/Documentation
32,41467115,"Using word2vec pretrained vectors, how to generate ids of a sentence as input to tf.nn.embedding_lookup function in tensorflow?","To extract the embedding representations of input data, the tensorflow documentation says we can use the following: Accdg to the TF documentation, the 2nd parameter of the function tf.nn.embedding_lookup is a tensor of ids: My question is: Given a sentence, say, how can I represent and transform it into ids? In the code below, how can I transform my sentence into input_data.",1,Documentation Replication on Other Examples
33,41602374,tf.zeros doesn't return a 1D tensor?,"I'm trying to duplicate a tensor across a new axis, like this: However I'm getting this error: In the documentation it says the way I wrote tf.constant is supposed to have it return a 1D tensor but when I checked its shape with get_shape(), it has (5,) as its shape. I tried reshaping it but nothing changed. Why am I getting this error? Thanks.",1,Documentation Replicability
34,41673889,TensorFlow: does tf.train.batch automatically load the next batch when the batch has finished training?,"For instance, after I have created my operations, fed the batch data through the operation and run the operation, does tf.train.batch automatically feed in another batch of data to the session? I ask this because tf.train.batch has an attribute of allow_smaller_final_batch which makes it possible for the final batch to be loaded as a size lesser than the indicated batch size. Does this mean even without a loop, the next batch could be automatically fed? From the tutorial codes I am rather confused. When I load a single batch, I get literally a single batch size of shape [batch_size, height, width, num_channels], but the documentation says it Creates batches of tensors in tensors. Also, when I read the tutorial code in the tf-slim walkthrough tutorial, where there is a function called load_batch, there are only 3 tensors returned: images, images_raw, labels. Where are 'batches' of data as explained in the documentation? Thank you for your help.",1,Lack of Alternative Solutions/Documentation
35,41780655,What is the difference between tf.group and tf.control_dependencies?,"Aside from tf.control_dependencies being a context manager (i.e. used with Python with), what's the difference between tf.group and tf.control_dependencies? When should which be used? Is it that tf.group doesn't have any particular order of operations? I'd assume tf.group([op_1, op_2, op_3]) executes ops in the list's order, but maybe that's not the case? The docstring doesn't specify a behaviour.",1,Documentation Replicability
36,41789133,What are c_state and m_state in Tensorflow LSTM?,Tensorflow r0.12's documentation for tf.nn.rnn_cell.LSTMCell describes this as the init: where state is as follows: What aare c_state and m_state and how do they fit into LSTMs? I cannot find reference to them anywhere in the documentation. Here is a link to that page in the documentation.,1,Lack of Alternative Solutions/Documentation
37,41941940,TensorFlow: Understanding the `collections` argument in tf.summary.scalar,"I am working with TensorBoard, specifically tf.summary.scalar. In the documentation it has an arugment collections=None, which is described as: I don't understand this description, and what collections is used for. Can someone please explain this to me, and perhaps point me towards a good example use-case?",1,Documentation Ambiguity
38,42022950,Which seeds have to be set where to realize 100% reproducibility of training results in tensorflow?,"In a general tensorflow setup like Where construct_model() contains the model definition including random initialization of weights (tf.truncated_normal) and train_model(sess) executes the training of the model - Which seeds do I have to set where to ensure 100% reproducibility between repeated runs of the code snippet above? The documentation for tf.random.set_random_seed may be concise, but left me a bit confused. I tried: But got different results each time.",1,Documentation Replicability
39,42133661,Tensorflow - LSTM state reuse within batch,"I am working on a Tensorflow NN which uses an LSTM to track a parameter (time series data regression problem). A batch of training data contains a batch_size of consecutive observations. I would like to use the LSTM state as input to the next sample. So, if I have a batch of data observations, I would like to feed the state of the first observation as input to the second observation and so on. Below I define the lstm state as a tensor of size = batch_size. I would like to reuse the state within a batch: In the API there is a tf.nn.state_saving_rnn but the documentation is kinda vague. My question: How to reuse curr_state within a training batch.",1,Documentation Ambiguity
40,42333101,Predicting Next Word of LSTM Model from Tensorflow Example,"My buddy and I are trying to utilize the trained model from the LSTM tensorflow example here. We've been able to train our model, save the model, and then import the model. We've just used tensorflow's Supervisor. It was in the tutorial, but you can read more about it here. It's weird because there's not a whole lot of clear documentation for this. I understand that tensorflow is an API that's going through a lot of changes and adaptations right now, but it's hard to find clear answers. For example, we want to use tf.train.Saver(), but we aren't sure if there is anything comparable to tf.train.Supervisor()'s managed_session. More to the point, however, we just want to use our model. We want to be able to map a string using tensorflow.models.rnn.ptb.reader. We're not sure how to do this. We pass in a string, and we want to do a simple prediction in terms of like predicting the next word in a string. So, something similar to this: But again, my buddy and I are pretty new to this, so we're not sure about where to go. I know this is probably too broad of a question for stack, but we've been pouring over the documentation and haven't been able to make much progress. ANY help would be appreciated so much! We've already found these other Stack links. Check them out here and here. We are not sure how to associate the logits probability list with any meaningful words.",1,Documentation Replication on Other Examples
41,42334855,state output from tf.nn.dynamic_rnn operation,"For this code snippet: I was expected the last time index from rnn_out to be equal to state. Or, perhaps the tanh of the state. But this isn't what I am seeing - they don't match. In the context of this RNN recurrence relation, what value does state contain? h(t) = tanh[b + Wh(t-1) + Ux(t)] The answer here, implies the last time index of rnn_out and state should be equal (but they are not): for the tf.nn.rnn_cell.BasicRNN,what's the difference between the state and output The TF documentation isn't clear to me on this point.",1,Documentation Ambiguity
42,42338981,It seems inconsistent the ways tensorflow allows me to specify variable length dimension,"I'm a novice to tensorflow. I was practicing coding with this tutorial code. Most of all the code made sense to me but at some points I got stuck. With tf.placholder function I had to specify variable length dimesion with None. But with tf.reshape I had to use -1, not None. In documentation for the two functions, both of the pertaining arguments have the name shape. So I am feeling lost here. Do they really have different meanings? Or is it just a small design mistake of the tensorflow developers?",1,Documentation Ambiguity
43,42437115,"Tensorflow: Replacement for tf.nn.rnn_cell._linear(input, size, 0, scope)","I am trying to get the SequenceGAN (https://github.com/LantaoYu/SeqGAN) from https://arxiv.org/pdf/1609.05473.pdf to run. After fixing the obvious errors, like replacing pack with stack, it still doesn't run, since the highway-network part requires the tf.nn.rnn_cell._linear function: the tf.nn.rnn_cell._linear function doesn't appear to be there anymore in Tensorflow 1.0 or 0.12, and I have no clue what to replace it with. I can't find any new implementations of this, or any information on tensorflow's github or (unfortunately very sparse) documentation. Does anybody know the new pendant of the function? Thanks a lot in advance!",1,Requesting (Additional) Documentation/Examples
44,42675391,tf.nn.sigmoid_cross_entropy_with_logits companies about arguments from documentation,"So I have the following model that I am wanting to test out an idea with. I am particularly interested in tf.nn.sigmoid_cross_entropy_with_logits() because my labels are not mutually exclusive. However, I am getting the following error repeatedly, which seems to be contradicting the tensor flow documentation. Please help!!",1,Documentation Replicability
45,42695305,Accessing row in a 2-D tensor,"I have the following code of an incredibly simple neural network (this code is actually an adaptation for an easy question): Basically, given an input x, compare it to the weights w and choose the node BMU with the minimum differences. I have several problems with that code: 1. sometimes it works without errors sometimes it raises an exception. When it DOES NOT work, the output is this: The full stack follows: When it works bmu is wrong, it should be 1, but the slice is correct. Sometimes I get this: bmu is 1, but slice is empty. 2. When I switch to the GPU, I have an exception telling me I cannot use bmu for indexing. Starting with with tf.device(""gpu:0""):, I get this: The full stack trace follows: I cannot understand what's happening: I have an idea, but cannot find any reference in the documentation or anywhere else. May be I use the wrong keywords. Is there anyone who can help me?",1,Lack of Alternative Solutions/Documentation
46,42754259,Sampled softmax loss over variable sequence batches?,"Background info: I'm working on sequence-to-sequence models, and right now my model accepts variable-length input tensors (not lists) with input shapes corresponding to [batch size, sequence length]. However, in my implementation, sequence length is unspecified (set to None) to allow for variable length inputs. Specifically, input sequence batches are padded only to the length of the longest sequence in that batch. This has sped up my training time considerably, so I'd prefer to keep it this way, as opposed to going back to bucketed models and/or padded all sequences in the training data to the same length. I'm using TensorFlow 1.0.0. Problem: I'm currently using the following to compute the loss (which runs just fine). where vocab size is typically about 40,000. I'd like to use a sampled softmax, but I've ran into an issue that's due to the unspecified nature of the input shape. According to the documentation for tf.nn.sampled_softmax_loss, it requires the inputs to be fed separately for each timestep. However, I can't call, for example, since the axis is unknown beforehand.Does anyone know how I might go about implementing this? One would assume that since both dynamic_rnn and tf.losses.sparse_softmax_cross_entropy seem to have no issue doing this, that a workaround could be implemented with the sampled softmax loss somehow. After digging around in the source code and even models repository, I've come up empty handed. Any help/suggestions would be greatly appreciated.",1,Documentation Replicability
47,42773379,tf.nn.relu vs. tf.contrib.layers.relu?,"I see this ""tf.nn.relu"" documented here: https://www.tensorflow.org/api_docs/python/tf/nn/relu But then I also see usage of tf.contrib.layers.relu on this page in ""model_fn"": https://www.tensorflow.org/extend/estimators It seems like the latter isn't described like the first one in an API-like fashion, but only presented in use. Why is this? Are the docs out of date? Why have two - is one old and no longer supported/going to be removed?",1,Documentation Ambiguity
48,42785026,tf.nn.conv2d vs tf.layers.conv2d,"Is there any advantage in using tf.nn.* over tf.layers.*? Most of the examples in the doc use tf.nn.conv2d, for instance, but it is not clear why they do so.",1,Lack of Alternative Solutions/Documentation
49,42933599,Slice a tensor in half in tensorflow,"I have a tensor of shape (32, 32, 32, 1) and I want to slice it into two tensors, along the first dimension, containing the first and second halves like so I am trying to use tf.slice but I don't know how to use the begin and end indices, and the documentation is anything but clear.",1,Documentation Ambiguity
50,43175272,check if tensorflow placeholder is filled,"Suppose I have two placeholder quantities in tensorflow: placeholder_1 and placeholder_2. Essentially I would like the following computational functionality: ""if placeholder_1 is defined (ie is given a value in the feed_dict of sess.run()), compute X as f(placeholder_1), otherwise, compute X as g(placeholder_2)."" Think of X as being a hidden layer in a neural network that can optionally be computed in these two different ways. Eventually I would use X to produce an output, and I'd like to backpropagate error to the parameters of f or g depending on which placeholder I used. One could accomplish this using the tf.where(condition, x, y) function if there was a way to make the condition ""placeholder_1 has a value"", but after looking through the tensorflow documentation on booleans and asserts I couldn't find anything that looked applicable. Any ideas? I have a vague idea of how I could accomplish this basically by copying part of the network, sharing parameters and syncing the networks after updates, but I'm hoping for a cleaner way to do it.",1,Lack of Alternative Solutions/Documentation
51,43367697,Batching and shuffling padded tf.train.SequenceExample,"I have some training example of a sequence-to-sequence scenario which are stored as tf.train.SequenceExample in one (or more) file(s) written TFRecordWriter. I would like to read, decode them and feed shuffled batches of them into my network. I have been struggling with the documentation and some tutorials found here and there but I could not make anything out of such stuff. I am working on a self-contained example, here below. Can anyone suggest me how to proceed? Thanks in advance! P.S. as a side request: any pointer about resources to better understand the input pipeline APIs of TensorFlow is appreciated.",1,Requesting (Additional) Documentation/Examples
52,43422949,CTC Loss InvalidArgumentError: sequence_length(b) <= time,"I am running into this error while trying to use tf.nn.ctc_loss through keras (ctc_batch_cost): According to the documentation for tf.nn.ctc_loss, Input requirements are: I am having a hard time understanding what this means-- what is b and what is sequence_length(b)?",1,Documentation Ambiguity
53,43460838,tensorflow tfrecord storage for large datasets,"I'm trying to understand the ""proper"" method of storage for large datasets for tensorflow ingestion. The documentation seems relatively clear that no matter what, tfrecord files are preferred. Large is a subjective measure, but the examples below are randomly generated regression datasets from sklearn.datasets.make_regression() of 10,000 rows and between 1 and 5,000 features, all float64. I've experimented with two different methods of writing tfrecord files with dramatically different performance. For numpy arrays, X, y (X.shape=(10000, n_features), y.shape=(10000,) I construct a tf.train.Example in the way that tensorflow developers seem to prefer, at least judging by tensorflow example code at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py. For each observation or row in X, I create a dictionary keyed with feature names (f_0, f_1, ...) whose values are tf.train.Feature objects with the feature's observation data as a single element of its float_list. I construct a dictionary with one feature (really two counting the label) whose value is a tf.train.Feature with the entire feature row in as its float_list As the number of features in the dataset grows, the second option gets considerably faster than the first, as shown in the following graph. Note the log scale 10,000 rows: It makes intuitive sense to me that creating 5,000 tf.train.Feature objects is significantly slower than creating one object with a float_list of 5,000 elements, but it's not clear that this is the ""intended"" method for feeding large numbers of features into a tensorflow model. Is there something inherently wrong with doing this the faster way?",1,Documentation Replication on Other Examples
54,43792961,Understanding the while loop in Tensorflow,"I am using the Python API for Tensorflow. I am trying to implement the Rosenbrock function given below without the use of a Python loop: My current implementation is as follows: I have tried implementing the summation in a tf.while_loop(); however, I found the API somewhat unintuitive when it comes to using an index integer that is meant to remain separate from the data. The example given in the documentation uses the data as the index (or vice-versa):",1,Documentation Replication on Other Examples
55,43827792,How do I use strided_slice to select all the element in tensorflow?,"I read the examples in document: It seems like that I can not simply use input[:,:] to select all the element, instead I have to use the syntax like input[:-1, :-1]. However in this way input[:-1, :-1] , I will miss the last row or last column. What should I do? I take an example: output: I read a lot of material and I found that I can use tf.shape(ph),let see: out: However, if I want to get the result like this: What can I do?",1,Documentation Replicability
56,43885770,Clarification of tf.name_scope in TensorFlow documentation,"The TensorFlow documentation mentions the following for tf.name_scope What is the meaning of given values are from the same graph, makes that graph the default graph ? Same graph refers to which graph ? Also, what is the use of values parameter in tf.name_scope ?",1,Documentation Ambiguity
57,43916019,Control dependencies and order of evaluation,"Please consider the following code: The output is: My question is: why does f(1).eval() return 0 even if there is a control dependency on the whileOp that modifies the returned variable acc? After reading the documentation, I was expecting whileOp to be evaluated before returning acc. How should I write the function f(.) in order to force the evaluation of whileOp? In f(.), if I return tf.identity(acc) instead of acc, it works as I expect.",1,Inadequate Examples
58,44093698,How does Tensorflow Batch Normalization work?,"I'm using tensorflow batch normalization in my deep neural network successfully. I'm doing it the following way: And it works fine both for training and testing phases. However I encounter problems when I try to use the computed neural network parameters in my another project, where I need to compute all the matrix multiplications and stuff by myself. The problem is that I can't reproduce the behavior of the tf.nn.batch_normalization function: According to the formula on the page https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/nn/batch_normalization: But as we can see, Which differs from the value 0.30664611, computed by Tensorflow itself. So what am I doing wrong here and why I can't just calculate batch normalized value myself? Thanks in advance!",1,Documentation Replication on Other Examples
59,44123088,How tf.nn.softmax_cross_entropy_with_logits can compute softmax cross entropy in tensorflow?,"tf.nn.softmax_cross_entropy_with_logits, Documentation says that it computes softmax cross entropy between logits and labels what does it mean? Is it not applying cross entropy loss function formula on it? Why documentation says that it computes sofmax cross entropy?",1,Documentation Ambiguity
60,44162432,Analysis of the output from tf.nn.dynamic_rnn tensorflow function,"I am not able to understand the output from tf.nn.dynamic_rnn tensorflow function. The document just tells about the size of the output, but it doesn't tell what does each row/column means. From the documentation: The outputs tensor is a 3-D matrix but what does each row/column represent?",1,Documentation Completeness
61,44206534,Why is tf.transpose so important in a RNN?,I've been reading the docs to learn TensorFlow and have been struggling on when to use the following functions and their purpose. My guess so far is that: tf.split() is used because inputs must be a sequence. tf.reshape() is used to make the shapes compatible (Incorrect shapes tends to be a common problem / mistake for me). I used numpy for this before. I'll probably stick to tf.reshape() now. I am not sure if there is a difference between the two. tf.transpose() swaps the rows and columns from my understanding. If I don't use tf.transpose() my loss doesn't go down. If the parameter values are incorrect the loss doesn't go down. So the purpose of me using tf.transpose() is so that my loss goes down and my predictions become more accurate. This bothers me tremendously because I'm using tf.transpose() because I have to and have no understanding why it's such an important factor. I'm assuming if it's not used correctly the inputs and labels can be in the wrong position. Making it impossible for the model to learn. If this is true how can I go about using tf.transpose() so that I am not so reliant on figuring out the parameter values via trial and error?,1,Documentation Replicability
62,44244763,TensorFlow tf.group ignoring dependencies?,"Following on from an earlier question, it seems tf.group is indeed ignoring dependencies. Here's a simple stand-alone example (I have run it on Python 2.7 with TensorFlow 1.1): Expected output: Actual output (different each time the code is run): There's nothing in the tf.group documentation to indicate why dependencies are ignored. Is there an alternative to tf.group that does consider dependencies? Switching to use tf.control_dependencies instead of tensorflow.python.ops.control_flow_ops.with_dependencies doesn't help:",1,Documentation Ambiguity
63,44357675,Documentation on how to use tf.estimator in TensorFlow,I understand that we can write custom models and encapsulate it using tf.estimator. But I just can't seem to find any documentation with an example. I know that you have to define your model inside a 'model_fn' but what exactly should I return from this function. Also am I supposed to put the the loss and the training step within the 'model_fn' or just the network. How should I modify the code give below to make it work with tf.estimator. Would really appreciate some help.,1,Documentation Replicability
64,44415901,tensorflow using tf.train.string_input_producer,"I'm using tf.train.string_input_producer to read data from tfRecord file. I suppose it create a queue and pipeline and the data will automatically loaded and feed into my model. However, it stuck at the first batch, and show this exception: my tfrecord was made by tf.train.SequenceExample, instead of tf.train.Example, which don't have clear documentation in the official guide. here is code snapshot to reproduce my problem. (I believe my problem come from the queue initializing or sth. because it seems that the whole pipeline is hang up)",1,Lack of Alternative Solutions/Documentation
65,44415901,tensorflow using tf.train.string_input_producer,"I'm using tf.train.string_input_producer to read data from tfRecord file. I suppose it create a queue and pipeline and the data will automatically loaded and feed into my model. However, it stuck at the first batch, and show this exception: my tfrecord was made by tf.train.SequenceExample, instead of tf.train.Example, which don't have clear documentation in the official guide. here is code snapshot to reproduce my problem. (I believe my problem come from the queue initializing or sth. because it seems that the whole pipeline is hang up)",1,Documentation Replicability
66,44478812,What kind of calculation does tf.nn.dynamic_rnn do with its input parameters?,"What kind of calculation does tf.nn.dynamic_rnn perform? How does it use the parameters cell and inputs (to create the result)? I have looked up in the documentation, but I have not found an explanation.",1,Documentation Completeness
67,44526763,How to perform tf.image.per_image_standardization on a batch of images in tensorflow,"I would like to know how to perform image whitening on a batch of images. According to the documentation in https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization, it is said that tf.image.per_image_standardization takes as input a 3D tensor, that is an image, of shape: [height, width, channels]. Is it a missing feature or there is a different method? Any help is much appreciated.",1,Documentation Replicability
68,44640357,Does Tensorflow's tf.while_loop automatically capture dependencies when executing in parallel?,"I am interested in implementing a Recursive Neural Network in Tensorflow, like what has been done in How can I implement a recursive neural network in TensorFlow?. However, in his implementation, the parallel_iterations of the tf.while_loop statement was fixed to be 1. I fear that this might be too slow. Since the tree I am going to feed into tensorflow have parts that are not dependent on each other, I would hope that I could set parallel_iterations to a higher value. However, it is inevitable that there are some dependencies required in the tree I feed in as input to tensorflow, and I am afraid that setting it to higher value may break the dependency property. So my question is, had Tensorflow's tf.while_loop automatically captured dependencies already, in order to only use paralleism on placed that are not dependent on each other? The tensorflow documentation says the following: But I am not sure what they mean by ""correct programs"".",1,Documentation Ambiguity
69,44690363,How to use tf.train.ExponentialMovingAverage in Android/IOS,"I use freeze_graph to export my model to a file named ""frozen.pb"". But Found that the accuracy of predictions on frozen.pb is very bad. I know the problem maybe MovingAverage not included in frozen.pb. When I use model.ckpt files to restore model for evaluating, if I call tf.train.ExponentialMovingAverage(0.999) , then the accuracy is good as expected, else the accuracy is bad. So How To export a binary model which performance is the same as the one restored from checkpoint files? I want to use "".pb"" files in Android Devices. The official document doesn't mention this. Thanks!! Freeze Command: Evaluate Code:",1,Lack of Alternative Solutions/Documentation
70,44753916,How to slice a part of tensor?,"I want to slice [3.0 ,33.0].I have tried to access this slice by following code. I'm not so clear about tf.slice command. I'm not so clear about begin and size mentioned in documentaion about this command. Can someone please make it easy to understand.",1,Documentation Ambiguity
71,44871420,TensorFlow dynamic_rnn input for regression,"I'm stuck trying to convert an existing tensorflow sequence to sequence classifier to a regressor. Currently I'm stuck in handling the input for tf.nn.dynamic_rnn(). According to the documentation and other answers, input should be in the shape of (batch_size, sequence_length, input_size). However my input data has only two dimensions: (sequence_length, batch_size). The original solution uses tf.nn.embedding_lookup() as an intermediate step before feeding input to dynamic_rnn(). If I understand correctly, I believe I don't need this step since I'm working on a regression problem, not a classification problem. Do I need the embedding_lookup step? If so, why? If not, how can I fit my encoder_inputs directly into dynamic_rnn()? Below is a working minimalized example of the general idea: I have read similar questions here on stackoverflow but find my self still puzzled as to how to solve this. EDIT: I think I should clarify that the code above works well, however the real desired output should mimic a noisy signal (text to speech for example) which is why I think I need continuous output values instead of words or letters.",1,Documentation Replication on Other Examples
72,44939540,How to get tensorflow to do a convolution on a 2 x 2 matrix with a 1 x 2 kernel?,"I have the following matrix: and the following kernel: If I do a convolution with no padding and slide by 1 row, I should get the following answer: Because: Based the documentation of tf.nn.conv2d, I thought this code expresses what I just described above: But it produces this output: And I have no clue how that is computed. I've tried experimenting with different values for the strides padding parameter but still am not able to produce the result I expected.",1,Documentation Ambiguity
73,44946189,TypeError: __init__() got an unexpected keyword argument 'shape',"I am new to Tensorflow and I met an error while trying to run some sample codes. Running the code above gives the error: TypeError: __init__() got an unexpected keyword argument 'shape'. The comment below says that tf.zeros_initializer does not accept 'shape' argument according to the documentation. I tried and it says ValueError: Shape of a new variable (v) must be fully defined, but instead was . So, what kind of argument/expression should I use to define the shape without causing a type error? I cannot find how to solve it online. Please help. Thank you",1,Documentation Replication on Other Examples
74,44963306,Cannot printout concatenated tensor by tf.concat() (tensorflow 1.2.1 - gpu / py36),"Learning Tensorflow (Python bindings) since the last month. I've been reading the docs on tf.concat(), but cannot resolve the problem as shown below, so I'm asking for your help! What I want to do is to see the contents of the concatenated tensor. I tried Tensor.eval(). Output: tf.concat() supposed to return Tensor and looks like it does. But why aren't T.eval() and sess.run() not working?",1,Documentation Replication on Other Examples
75,45030619,Detecting out-of-bounds slicing with tf.slice like in numpy,"In tensorflow, I'm trying to use tf.slice, but as its documentation states, it requires the slice to fit in the input array. For instance, if you try to slice the first 5 positions of the tensor [1,2,3,4] it will crash. I want to have the same functionality we get with python lists or numpy arrays where slicing gets you the intersection of the original array and the slice you asked for. For instance if you ask for positions 2 to 6 of [1,2,3,4] you'll get [2,3,4]. How can I do that in tensorflow? Thanks!",1,Documentation Replication on Other Examples
76,45077445,How to use method recover_last_checkpoints of tf.train.Saver()?,"The documentation writes that a list of checkpoint paths should be passed to it, but how to get the list? By hard coding? No, it's a silly practice. By parsing the protocol buffer file (a file named as checkpoint in your model directory)? But tensorflow does not implement a parser, does it? So do I have to implement one by myself? Do you have a good practice to get the checkpoint paths list? I raise this question because these days I am troubled by one thing. As you know, a days-long training may crash for some reason, and I have to recover it from the latest checkpoint. Recovering training is easy, since I just need to write the following code: I can hard code latest_checkpoint, or somewhat wiser, use tf.train.latest_checkpoint(). However, a problem arises after I recover the training. Those old checkpoints files that are created before crash are left there. The Saver only manages the checkpoint files created in one run. I hope it could also manage the previously created checkpoints files so they would be automatically deleted, and I don't have to manually delete them every time. I think such repeating work is really silly. Then I find the recover_last_checkpoints method in class tf.train.Saver(), which allows Saver to manage old checkpoints. But it's not handy to use. So is there any good solution?",1,Lack of Alternative Solutions/Documentation
77,45090843,Does sequence_length help performance of dynamic_rnn?,"In Google's recent nmt tutorial, they say this: with this code: encoder_outputs, encoder_state = tf.nn.dynamic_rnn( encoder_cell, encoder_emb_inp, sequence_length=source_seqence_length, time_major=True) However, I was reading dynamic_rnn's documentation and it says: I'm just wondering if sequence_length really helps performance of dynamic_rnn, e.g. they do some kind of dynamic bucketing? If they do, is there any place where I can read more about it? Thanks a lot.",1,Requesting (Additional) Documentation/Examples
78,45151015,How does tf.gradients behave when passed a list of `ys` tensors?,"How exactly does tf.gradients behave when passed a list of tensors as its first argument? Take this very small example: If I compute the gradients of a single tensor, c, with respect to [a,b], I get the expected answer: According to the Tensorflow documentation, if you pass in a list of tensors as your first argument ys, tf.gradients will sum the gradients over that list, returning sum_over_ys(dy/dx) for each x in your second argument. So I would expect: to behave the same way as: Am I reading the docs wrong? When I test this code, I get the expected result [2, 3] for the second expression (explicitly summing a + b + c), but [2, 1] for the first. Where is this [2, 1] coming from?",1,Documentation Ambiguity
79,45229165,How can I serve the Faster RCNN with Resnet 101 model with tensorflow serving,"I am trying to serve the Faster RCNN with Resnet 101 model with tensorflow serving. I know I need to use tf.saved_model.builder.SavedModelBuilder to export the model definition as well as variables, then I need a script like inception_client.py provided by tensorflow_serving. while I am going through the examples and documentation and experimenting, I think someone may have done the same thing. So plase help if you have done the same or know how to get it done. Thanks in advance.",1,Documentation Replication on Other Examples
80,45247909,Tensorflow - How to get the gradients of the output w.r.t the model parameters,"I would like to know if it is possible to compute the gradients of the output of a model with respect to the model parameters. In other words I would like to compute dy / d theta. Here is a short example of what I mean: I have looked at the documentation of tf.gradients() and it states So I do understand that both args need to be a tensor. However, when I try model_parameters = tf.trainable_variables() model_parameters is a list of elements of type tensorflow.python.ops.variables.Variable Is there a way to get the parameters of the model as a tensor to use for differentiation?",1,Documentation Replicability
81,45373740,Tensorflow ReLU normalizes strangely,"in my opinion the rectified linear unit is supposed to execute the following function: However, this seems not to be the case with tf.nn.relu: The random matrix looks like this: And the output from the relu function like this: So, if I see it correctly, tf.nn.relu does some sort of normalization, right? If yes, why isn't it mentioned in the docs? Okay, I found out that the whole issue was related to my tensorflow installtion which seemed to be corrupt. On another machine, I did get the expected results. Thank you for the help and helpful comments.",1,Lack of Alternative Solutions/Documentation
82,45401311,What are channels in tf.nn.conv2D?,"I've looked through some great explanations on what different arguments of tf.nn.conv2D represent, but I still can't understand what exactly in_channels and out_channels represent. Could someone please clarify this for me?",1,Documentation Replicability
83,45428557,Tensorflow: How to make return value of tf.unique same size as input,"According to https://www.tensorflow.org/api_docs/python/tf/unique, tf.unique(x) returns a tuple (y, idx), The shape of y is (?, ) is not known during build time. Is there anyway I can pad y to match the input size x?. For example, I wanna make y = [1, 2, 4, 7, 8, 0, 0, 0, 0]",1,Inadequate Examples
84,45553038,How to compile custom ops in tensorflow without having to dynamically import them in python?,"I checked through tensorflow documentation and they seem to only give information about compiling a custom op through a bazel rule: Once bazel builds it, you get a zero_out.so file which you can import into python like below: Is there anyway you can link custom_ops during the bazel build of tensorflow so that you don't need to manually import custom ops through tf.load_op_library?",1,Documentation Completeness
85,45553280,TensorArray Initialization from another tensor,"What is the right way to initialize a tensorarray from another tensor in tensorflow. Suppose I have a tensor What is way to say that this tensorarray depends on T1? Looking at the documentation I cant figure out how to initialize this. Correct me if my understanding is wrong, T1 is a nested tensor and I want to loop over a dimension using tf.while_loop and hence I want to initialize the TensorArray with it.",1,Documentation Replication on Other Examples
86,45595419,Is it possible to have multiple conditions defined in tf.while_loop,"Is it possible to define to multiple conditions for termination of a tf.while_loop in tensorflow? For example depending on the two tensor values achieving two specific values. eg. i==2 and j==3 ? Also can I have several blocks of code in the body? In all the examples in the documentation, it seems that the body is more like a single statement returning a value or a tuple. I want to execute a set of several ""sequential"" statements in the body.",1,Inadequate Examples
87,45634450,What are the advantages of using tf.train.SequenceExample over tf.train.Example for variable length features?,"Recently I read this guide on undocumented featuers in TensorFlow, as I needed to pass variable length sequences as input. However, I found the protocol for tf.train.SequenceExample relatively confusing (especially due to lack of documentation), and managed to build an input pipe using tf.train.Example just fine instead. Are there any advantages to using tf.train.SequenceExample? Using the standard example protocol when there is a dedicated one for variable length sequences seems like a cheat, but does it bear any consequence?",1,Documentation Replicability
88,45705070,how to load and use a saved model on tensorflow?,"I have found 2 ways to save a model in Tensorflow: tf.train.Saver() and SavedModelBuilder. However, I can't find documentation on using the model after it being loaded the second way. Note: I want to use SavedModelBuilder way because I train the model in Python and will use it at serving time in another language (Go), and it seems that SavedModelBuilder is the only way in that case. This works great with tf.train.Saver() (first way): tf.saved_model.builder.SavedModelBuilder() is defined in the Readme but after loading the model with tf.saved_model.loader.load(sess, [], export_dir)), I can't find documentation on getting back at the nodes (see ""finalnode"" in the code above)",1,Lack of Alternative Solutions/Documentation
89,45774938,tensorflow: tf.split is given weird parameters,"Here is code(from here): I have issue understanding x = tf.split(0, n_chunks, x) , more specificaly third parameter(x-input). By documenation this should be axis...but that can't be, right? Isn't x one dimensional? I apologise if it's trivial, I'm beginner and can't sem to get it. Maybe it's just formality, but if it is I don't understand how it works...",1,Documentation Ambiguity
90,45784815,How best to implement a matrix mask operation in tensorflow?,"I had a case where I needed to fill some holes (missing data) in an image processing application in tensorflow. The 'holes' are easy to locate as they are zeros and the good data is not zeros. I wanted to fill the holes with random data. This is quite easy to do using python numpy but doing it in tensorflow requires some work. I came up with a solution and wanted to see if there is a better or more efficient way to do the same thing. I understand that tensorflow does not yet support the more advanced numpy type indexing yet but there is a function tf.gather_nd() that seems promising for this. However, I could not tell from the documentation how to us it for what I wanted to do. I would appreciate answers that improve on what I did or especially if someone can show me how to do it using tf.gather_nd(). Also, tf.boolean_mask() does not work for what I am trying to do because it does not allow you to use the output as an index. In python what I am trying to do: What I ended up doing in Tensorflow to achieve same thing (skipping filling the array steps)",1,Inadequate Examples
91,45879776,TensorFlow how to make results reproducible for `tf.nn.sampled_softmax_loss`,"I would like to get reproducible results for my tensorflow runs. The way I'm trying to make this happen is to set up the numpy and tensorflow seeds: As well as make sure that the weights of the neural network, that I initialized with tf.truncated_normal also use that seed: tf.truncated_normal(..., seed=rnd_seed) For reasons that are beyond the scope of this question, I'm using the sampled softmax loss function, tf.nn.sampled_softmax_loss, and unfortunately, I'm not able to control the stochasticity of this function with a random seed. By a look at the TensorFlow documentation of this function (https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss), I can see that parameter sampled_values should be the only parameter that affects randomization, but I'm not able to understand how to actually use a seed. [EDITED] This is (part of) my script",1,Documentation Replicability
92,45886201,Tensorflow: Can't use tf.case with input argument,"I need to create a variable epsilon_n that changes definition (and value) based on the current step. Since I have more than two cases, it seems that I can't use tf.cond . I am trying to use tf.case as follows: However, I keep getting this error message: I tried the following: Still I would the same error. The examples in Tensorflow documentation weigh in on cases where no input argument is passed to the callable functions. I couldn't find enough info about tf.case on the internet! Please any help?",1,Lack of Alternative Solutions/Documentation
93,45955241,How do I create padded batches in Tensorflow for tf.train.SequenceExample data using the DataSet API?,"For training an LSTM model in Tensorflow, I have structured my data into a tf.train.SequenceExample format and stored it into a TFRecord file. I would now like to use the new DataSet API to generate padded batches for training. In the documentation there is an example for using padded_batch, but for my data I can't figure out what the value of padded_shapes should be. For reading the TFrecord file into the batches I have written the following Python code: The code works well if I use dataset = dataset.batch(1) (no padding needed in that case), but when I use the padded_batch variant, I get the following error: Can you help me figuring out what I should pass for the padded_shapes parameter? (I know there is lots of example code using threading and queues for this, but I'd rather use the new DataSet API for this project)",1,Documentation Replication on Other Examples
94,46062649,tensorflow slim concurrent train and evaluation loops; single device,"I am interested in using the tensorflow slim library (tf.contrib.slim) to do evaluation of a model performance on a(n) (entire) test set periodically during training. The documentation is pretty clear that slim.evaluation.evaluation_loop is the way to go, and it looks promising. The issue is that I don't have a second gpu to spare, this model parameters take up an entire gpu's worth of memory, and I would like to do concurrent evaluation. For example, if I had 2 GPUs, I could run a python script that terminated with ""slim.learning.train()"" on the first gpu, and another that terminated with ""slim.evaluation.evaluation_loop()"" on the second gpu. Is there an approach that can manage 1 gpu's resources for both tasks? tf.train.Supervisor comes to mind, but I don't honestly know.",1,Documentation Replication on Other Examples
95,46139202,Tensorflow: TypeError with numpy_input_fn,I am coding a Convolutional Neural Network to classify images in TensorFlow but there is a problem: When I try to feed my NumPy array of flattened images (3 channels with RGB values from 0 to 255) to a tf.estimator.inputs.numpy_input_fn I get the following error: My numpy_imput_fn looks like this: In the documentation for the function it is said that x should be a dict of NumPy array:,1,Documentation Replicability
96,46298583,Tensorflow embeddings,"I know what embeddings are and how they are trained. Precisely, while referring to the tensorflow's documentation, I came across two different articles. I wish to know what exactly is the difference between them. link 1: Tensorflow | Vector Representations of words In the first tutorial, they have explicitly trained embeddings on a specific dataset. There is a distinct session run to train those embeddings. I can then later on save the learnt embeddings as a numpy object and use the tf.nn.embedding_lookup() function while training an LSTM network. link 2: Tensorflow | Embeddings In this second article however, I couldn't understand what is happening. This is given under the training embeddings sections. My doubt is: does the gather function train the embeddings automatically? I am not sure since this op ran very fast on my pc. Generally: What is the right way to convert words into vectors (link1 or link2) in tensorflow for training a seq2seq model? Also, how to train the embeddings for a seq2seq dataset, since the data is in the form of separate sequences for my task unlike (a continuous sequence of words refer: link 1 dataset)",1,Documentation Replication on Other Examples
97,46370159,Outputting batch/epoch training loss during `tf.train.MonitoredTrainingSession`,"I would like to output my loss with MonitoredTrainingSession every epoch or batch. Ideally I would love to get a flag that the epoch is ended or be able to provide a callback like in keras. I see that I can also do it by manually counting steps, but I want to use the tf functionality, which seems still poorly documented. From what I could find in their documentation, one can use tf.train.LoggingTensorHook to print the tensors every n steps. The problem however is that it prints with frequency different from what I request. When I run following with every_n_iter=4 I get output every 2nd iteration: I am getting output like: etc. That is it outputs every 2nd step, not every 4th. The documentation says: I am running it on one local machine. Why one ""local step"" equals two loop python iterations? Why two and not five? Looking at the Python source does not seem helping. Any Google folks aware of what it is doing?",1,Lack of Alternative Solutions/Documentation
98,46372554,When feeding a dictionary to a tensorflow function I get Why do I get TypeError: unhashable type: 'numpy.ndarray',I am working on a Tensor Flow Coursera Course and I dont understand why I am getting a type mismatch. This is the function I am defining: And when running this: I get this type error: I checked the Tensorflow documentation for tf.one_hot and there shouldn't be a problem with np.arrays. https://www.tensorflow.org/api_docs/python/tf/one_hot,1,Documentation Ambiguity
99,46381790,How does TensorFlow handle none shape?,"I'm trying to implement a simple computational graph framework and test it with simple neural network, mainly by learning from TensorFlow. Now I would want to be clear how does TensorFlow handle none shape tensors. In this example, X has shape [None, n_input], weights['h1'] has shape [n_input, n_hidden_1], and biases['b1'] has shape [n_hidden_1]. When it tries to do this: layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']), tf.matmul(x, weights['h1']) should have shape [None, n_hidden_1], and how exactly does TensorFlow add it with biases['b1']? Based on the documentation, tf.add only works when the 2 operands have the same shape. If we run with a batch of size 10, tf.matmul(x, weights['h1']) will have shape [10, n_hidden_1], and it shouldn't be able to be added with biases['b1'].",1,Documentation Replication on Other Examples
100,46418686,tf.nn.dynamic_rnn shape error in seq2seq,"I am attempting to write my own basic seq2seq classifier. Im doing this by using tf.nn.dynamic_rnn and the code is shown below. However, there seems to be a problem with the shape of the tensor I'm sending to tf.nn.dynamic_rnn. The reason I'm doing this is because tensorflow's documentation when it comes to seq2seq is very much all over the place. Running gives me the error: ValueError: Cannot feed value of shape (128, 10) for Tensor 'decoding/rnn/transpose:0', which has shape '(128, 10, 32)'. The graph is shown below:",1,Documentation Replicability
101,46484373,Return a tf.Variable from an Estimator,"I have an Tensorflow Estimator defined by a model function in the usual way. I want to determine which of my (zscore normalised) inputs are significant to the result, and which can be eliminated. I have altered the model to introduce two changes: (1) A new layer weight_layer which is randomly intialized and elementwise multiplied with input_layer. (2) A penalty sparsity which is added to the loss function to penalize the loss by the sum of the weights in weight_layer The trouble comes at prediction time, when I try to return the values of weight_layer, as follows: I get the following error: This seems odd, since although predictions[sparsity] is not a Tensor, it is a tf.Variable, and the tf.Variable documentation suggests I can treat a tf.Variable 'like a normal tf.Tensor'. How can I fix the above to return the weight_layer, or if I there is a more fundamental mistake, please recommend a way for me to determine which of my input variables are significant.",1,Documentation Replication on Other Examples
102,46658607,where is tf.nn.l2_loss defined?,According to this documentation https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss it says But when I go to tensorflow/python/ops/gen_nn_ops.py there is no l2_loss defined. I'm trying to see what would be the difference between using tf.nn.l2_loss(W) or just using tf.reduce_sum(tf.square(W)).,1,Documentation Ambiguity
103,46659101,Using `softmax_cross_entropy_with_logits()` with `seq2seq.sequence_loss()`,"I have a working RNN using the default softmax loss function for tf.contrib.seq2seq.sequence_loss() (which I'm assuming is tf.nn.softmax()) but would instead like to use tf.nn.softmax_cross_entropy_with_logits(). According to the seq2seq.sequence_loss documentation, one may use softmax_loss_function= to override the default loss function: Here is my code that works: My attempt to change the loss function is as follows (I've only indicated the code that is different): The line cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks, softmax_loss_function=loss) is now giving me ""TypeError: 'Tensor' object is not callable."" This is one of the most opaque errors I've seen Tensorflow produce and I haven't found much of anything in the way of explanation on the internet. Any help would be appreciated.",1,Documentation Replication on Other Examples
104,46752071,Feed a Tensor of SparseTensors to estimators,"To get started with TF, I wanted to learn a predictor of match outcomes for a game. There are three features: the 5 heros on team 0, the 5 heroes on team 1, and the map. The winner is the label, 0 or 1. I want to represent the teams and the maps as SparseTensors. Out of a possible 71 heroes, five will be selected. Likewise for maps, out of a possible 13, one will be selected. This fails on team_0s = tf.constant(list(map(lambda r: sparse_team(r.team_0), records))) It's very difficult to understand what tf wants me to return in my input_fn, because all of the examples I can find in the docs ultimately call out to a pandas or numpy helper function, and I'm not familiar with those frameworks. I thought that each dictionary value should be a Tensor containing all examples of a single feature. Each of my examples is a SparseTensor, and I want to simply embed them as their dense versions for the sake of the DNNClassifier. I'm sure my mental model is horribly broken right now, and I appreciate any help setting it straight. Error output:",1,Inadequate Examples
105,46759271,Image pixel value normalized for tf.image.decode_jpeg and tf.train.shuffle_batch?,"I am trying to use the tf.train.shuffle_batch function from tensorflow, then I need to first load the images using tf.image.decode_jpeg(or other similar functions to load png and jpg). But I just found out that the images are loaded as probability map, which means the max of the value of pixel is 1, and the min of the value of the pixel is 0. Below is my code updated from a github repo. I don't know why the values of pixels are normalized to [0,1], and I don't find related documentation on tensorflow. Could anyone help me? Thanks.",1,Documentation Replication on Other Examples
106,46885191,tf.nn.conv2d_transpose output_shape dynamic batch_size,"The documentation of tf.nn.conv2d_transpose says: The output_shape argument requires a 1D tensor specifying the shape of the tensor output by this op. Here, since my conv-net part has been built entirely on dynamic batch_length placeholders, I can't seem to device a workaround to the static batch_size requirement of the output_shape for this op. There are many discussions around the web for this, however, I couldn't find any solid solution to this issue. Most of them are hacky ones with a global_batch_size variable defined. I wish to know the best possible solution to this problem. This trained model is going be shipped as a deployed service.",1,Inadequate Examples
107,46900332,How to create custom metrics for use in Tensorflow's Estimator class?,"In Tensorflow's Creating Estimators in tf.estimator guide, the example used a metric that is already predefined in the tf.metrics module. Are there any resources that describe how to define a custom metric that can be used to evaluate an Estimator? I'd like to implement the F1 metric.",1,Documentation Replication on Other Examples
108,46976226,`tf.estimator.RunConfig` vs `tf.contrib.learn.RunConfig`,I am confused regarding whether I should be using tf.estimator.RunConfig or tf.contrib.learn.RunConfig to pass a RunConfig to an estimator. using tf.contrib.learn.RunConfig is straightforward: But tf.estimator.RunConfig has some odd syntax: Is there any reason to prefer one RunConfig over the other? The documentation is not clear on this.,1,Lack of Alternative Solutions/Documentation
109,47119604,"In tensorflow, does tf.summary record average values over multiple steps?","By default, RunConfig.save_summary_steps is 100 in tf.estimator.Estimator, so it saves summaries every 100 steps. At each time it saves a summary, does it just save the current summary value computed from the current step/minibatch? Or it saves the average summary values computed from the recent 100 steps/minibatches? I cannot find a clear description for this in the official documentation.",1,Lack of Alternative Solutions/Documentation
110,47205160,Tensorflow v1.4: Layer.input not supported in Eager mode,"I understand that Eager mode is a new alpha feature on the nightly builds and that it is not perfect yet, but I do not know if there are any tf.keras workarounds for this problem. The error Layer.input not supported in Eager mode. triggers on the block I do not know anything about keras or the keras tensorflow API and I was wondering if there was a way to avoid Layer.input with keras techniques so as to stay within Eager mode. Following a tutorial in the tf.Eager docs I have confirmed that model = tf.layers.Dense(1) works but I don't know how to add another layer. Any help is very much appreciated. EDIT As of tensorflow v1.10, keras is supported in eager mode.",1,Documentation Replication on Other Examples
111,47319390,Why does this TensorFlow code behave differently when inside a test case?,"I have a function (foo below) which is behaving differently when it's run directly vs when it is run inside a tf.test.TestCase. The code is supposed to create a dataset with elems [1..5] and shuffle it. Then it repeats 3 times: create an iterator from the data and use that to print the 5 elements. When run on its own it gives output where all the lists are shuffled e.g.: but when run inside a test case they are always the same, even between runs: I imagine it's something to do with how test cases handle random seeds but I can't see anything about that in the TensorFlow docs. Thanks for any help! I am running it with Python 3.6 and TensorFlow 1.4. No other modules should be needed.",1,Documentation Replication on Other Examples
112,47327256,Understanding Tensor Inputs & Transformations for use in an LSTM (dynamic RNN),"I am building an LSTM style neural network in Tensorflow and am having some difficulty understanding exactly what input is needed and the subsequent transformations made by tf.nn.dynamic_rnn before it is passed to the sparse_softmax_cross_entropy_with_logits layer. https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn The input function is sending a feature tensor in the form [batch_size, max_time] However the manual states that input tensors must be in the form [batch_size, max_time, ...] I have therefore expanded the input with a 1d tensor to take the form [batch_size, max_time, 1] At this point the input does not break upon running, but I don't understand exactly what we have done here and suspect it may be causing the problems when calculating loss (see below). This expanded tensor is then the 'features' tensor used in the code below This throws a value error at loss dimensions must be equal, but are [max_time, num_classes] and [batch_size] from https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/classification - A common use case is to have logits of shape [batch_size, num_classes] and labels of shape [batch_size]. But higher dimensions are supported. At some point in the process max_time and batch_size have been mixed up and I'm uncertain if its at input or during the LSTM. I'm grateful for any advice!",1,Lack of Alternative Solutions/Documentation
113,47380573,How to properly update variables in a while loop in TensorFlow?,"Can someone please explain (or point me to the relevant place in the documentation that I've missed) how to properly update a tf.Variable() in a tf.while_loop? I am trying to update variables in the loop that will store some information until the next iteration of the loop using the assign() method. However, this isn't doing anything. As the values of mu_tf and sigma_tf are being updated by the minimizer, while step_mu isn't, I am obviously doing something wrong, but I don't understand what it is. Specifically, I guess I should say that I know assign() does not do anything until it is executed when the graph is run, so I know that I can do sess.run(step_mu.assign(mu_tf)) and that will update step_mu, but I want to do this in the loop correctly. I don't understand how to add an assign operation to the body of the loop. A simplified working example of what I'm doing follows here:",1,Documentation Replicability
114,47389988,How to control GPU memory size with tf.estimator,I'm trying to control the size of GPU memory allocated for one tensorflow estimator tf.estimator.Estimator. The purpose is to only allocate half to run other tensorflow net on the same GPU. I found for the contrib version but not for the official. Someone knows if it's possible?,1,Documentation Replication on Other Examples
115,47485498,Tensorflow tf.nn.embedding_lookup,"is there a small neural network in tf.nn.embedding_lookup?? When I train some data, a value of the same index is changing. So is it trained also? while I'm training my model I checked the official embedding_lookup code but I can not see any tf.Variables for train embedding parameter. But when I print all tf.Variables then I can found a Variable which is within embedding scope Thank you.",1,Documentation Replicability
116,47568998,Tensorflow: Load data in multiple threads on cpu,"I have a python class SceneGenerator which has multiple member functions for preprocessing and a generator function generate_data(). The basic structure is like this: I used the class member function sceneGenerator.generate_data() in keras model.fit_generator() function to read the data from disk, preprocess it and yield it. In keras, this is done on multiple CPU threads, if the workers parameter of model.fit_generator() is set to something &gt; 1. I now want to use the same SceneGenerator class in tensorflow. My current approach is this: This, however, is slow and does not use multiple threads. I found the tf.data.Dataset api with some documentation, but I fail to implement the methods. Edit: Notice that I do not work with images so that the image loading mechanisms with file paths etc. do not work here. My SceneGenerator loads data from hdf5 files. But not complete datasets but - depending on the initialization parameters - only parts of a dataset. I would love to keep the generator function as it is and learn how this generator can be directly used as input for tensorflow and runs on multiple threads on the CPU. Rewriting the data from the hdf5 files to csv is not a good option because it duplicated lots of data. Edit 2:: I think something similar to this could help: parallelising tf.data.Dataset.from_generator",1,Documentation Replication on Other Examples
117,47644412,TensorFlow Dataset API Parsing Error,"I'm using the TensorFlow Dataset API to parse a CSV file and run a logistic regression. I'm following the example from the TF documentation here. The following code snippet shows how I am setting up the model: When calling lr.train(input_fn = lambda: input_fn(data_path, 1, 100)) (note: batch size is 100) I'm getting the error So I'm assuming this means one of the tf.feature_column.numeric_column calls is getting a scalar value which it doesn't like. However, I cannot figure out why this is the case. I've set batch_size to a positive integer and according to the documentation the shape of the NDarray resulting from tf.feature_column.numeric_column should be 1Xbatch_size by default. Can anyone explain why TensorFlow is returning this error? I'm sure this question has a simple answer that will make me feel stupid for not figuring it out, but after spending some time on this I'm still stumped.",1,Documentation Ambiguity
118,47665314,how can we get benefit from sharding the data to speed the training time?,"My main issue is : I have 204 GB training tfrecords for 2 million images, and 28GB for validation tf.records files, of 302900 images. it takes 8 hour to train one epoch and this will take 33 day for training. I want to speed that by using multiple threads and shards but I am little bit confused about couple of things. In tf.data.Dataset API there is shard function , So in the documentation they mentioned the following about shard function : So my question regarding the code above is when I try to makes d.shards of my data using shard function, if I set the number of shards (num_workers)to 10 , I will have 10 splits of my data , then should I set the num_reader in d.interleave function to 10 to guarantee that each reader take one split from the 10 split? and how I can control which split the function interleave will take? because if I set the shard_index (worker_index) in shard function to 1 it will give me the first split. Can anyone give me an idea how can I perform this distributed training using the above functions? then what about the num_parallel_call . should I set it to 10 as well? knowing that I have single tf.records file for training and another one for validation , I don't split the tf.records files into multiple files.",1,Documentation Replication on Other Examples
119,47814401,How does tf.layers.batch_normalization calculate mean and variance during test time? (test data has machine-generated samples),"I am trying to implement batch-normalization on my CNN that currently applies dropout. One problem is that I do not know how the mean and variance are calculated during test time. On the documentation it says that if training=False is set then the normalization is done with moving statistics. What does this mean? In addition, since my test data has lots of machine-generated samples I cannot use population mean and variance and just apply tf.nn.batch_normalization(). These samples are used to prevent hand labeling and are excluded when scoring my model",1,Documentation Completeness
120,47898147,Tensorflow Module Import error: AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell',"When attempting to pass my RNN call, I call tf.nn.rnn_cell and I receive the following error: Which is odd, because I'm sure I imported everything correctly: But looking at the docs, things have moved around between tensorflow versions. what would you all recommend to fix this?? Line, I'm getting the error against: Specifically: I'm using anaconda 3 to manage all of this so, the dependancies should all be taken care of. I have already tried working around a damn rank/shape error with Tensor shapes which took ages to resolve. Cheers in advance.",1,Documentation Replication on Other Examples
121,47947629,"Tensorflow: Keras, Estimators and custom input function","TF1.4 made Keras an integral part. When trying to create Estimators from Keras models with propratery input function (I.e., not using the tf.estimator.inputs.numpy_input_fn) things are not working as Tensorflow can not fuse the model with the Input function. I am using tf.keras.estimator.model_to_estimator and I get the following error message: I found some reference for this topic here (strangely enough its hidden in the TF docs in the master branch - compare to this) If you have the same issue - see my answer below. Might save you several hours.",1,Documentation Replicability
122,47984876,Tensorflow tf.map_fn parameters,"I'm attempting to structure my parameters so that they will work properly with tf.map_fn() but most of the example documentation only discusses arrays or tensors of the same shape as function arguments. Links include: Does tensorflow map_fn support taking more than one tensor? My specific example is this: I have some tensorflow function that expects [None, 2] and [x,y] as parameter tensor shapes. Tensor A is of shape [batch_size, x*y, 2] Tensor B is of shape [batch_size, x, y] From the tensorflow documentation: Since tensorsA and B only match in dimension 0, I cannot stack or concatenate them; I have also tried creating lambdaData as: All of the above result in varying dimension mismatch errors. I would follow the recommended use as per documentation of placing all of the data into a single tensor, but because of dimension mismatching between tensorA and tensorB I am unable to. Has anybody had any luck with tuples or lists of arguments for elems?",1,Lack of Alternative Solutions/Documentation
123,48033687,How to use tf.train.shuffle_batch to train NN?,"I've trained my neural built with tensorflow network and got some overfit I'd like to reduce. I hoped learning the model on batches could help ad I tried to test this idea. I found tf.train.shuffle_batch() and fought this may do the thing. So I tried and it didn't work. Tensorflow's documentation doesn't help. I've found one topic, but the example there only prints arrays out. It was promising to use it to learn NN but in my case istead of getting data divided to n-element batches I got them multiplied n-times in additional dimension. Here is the code sample: and here is the output: And the error list concludes with: The conlusion is not surprising when I fed the NN with 3D array but why do I get such a batch when I expect x:(15, 60) and y:(15, 1)? Why do I get x:(15, 165, 60) y:(15, 165, 1) and how to get useful batches? I'm using tensorflow-gpu but hope this should work as well, right?",1,Documentation Replication on Other Examples
124,48072635,Why and when does the tensor's shape information unspecific?,"I found piece of code like this: as Clarification on tf.Tensor.set_shape() said, set_shape can make the shape information more specific. But why data's shape information here is not specific? When does the tensor's information unspecific?",1,Documentation Replicability
125,48235239,The fit function from tf.contrib.learn.LinearRegressor asks to switch to tf.train.get_global_step,"I am trying to get a LinearRegressor to work and I get an error for which there doesn't seem to be much documentation about. When I do: I get the error: I am not sure how to proceed. I read from the docs: But I'm not sure to what I should change to, or how to switch to the global step. I tried using tf.estimator.LinearRegressor mainly because I'm out of ideas, and did something like this: But got no output at all.",1,Lack of Alternative Solutions/Documentation
126,48299597,How to efficiently shuffle a large tf.data.Dataset when using tf.estimator.train_and_evaluate?,"The tf.estimator.train_and_evaluate documentation makes it clear that the input dataset must be properly shuffled for the training to see all examples: In my application, I would like to uniformly sample examples from the full tf.data.Dataset with arbitrary evaluation frequency and shuffle()'s buffer size. Otherwise, the training can at most see the first: elements, effectively discarding the rest. Is there an efficient way to work around that without loading the complete dataset in the system memory? I considered sharding the dataset based on the buffer size, but if the evaluation does not occur frequently, it will iterate on the same shard multiple times (a repeat() closes the pipeline). Ideally, I would like to move to another shard after a complete iteration over the dataset, is that possible? Thanks for any pointers!",1,Documentation Replicability
127,48396599,"Canonical Tensorflow ""for loop""","What is the canonical way of running a Tensorflow ""for loop""? Specifically, suppose we have some body function which does NOT depend on the loop iteration, but must be run n times. One might think that a good method might be to run this inside of a tf.while_loop like this: In fact, that is precisely what the highest rated answer in this question suggests: How can I run a loop with a tensor as its range? (in tensorflow) However, the tf.while_loop docs specifically say If you put a counter in the body, then it seems that that condition is violated. So it seems that there must be a different way of setting up a ""for loop"". Furthermore, even if there is no explicit error, doing so seems like it will create a dependency between iterations meaning that I do not think they will run in parallel.",1,Lack of Alternative Solutions/Documentation
128,48427269,What's the efficient way to feed elements from Iterator (from tf.data.Dataset) into TensorFlow model?,"I'm using TensrFlow's new API for importing data via tf.data.Dataset and iterators. It is working fine, but I'm not sure if what I do is efficient. What I'm doing at the moment is evaluating an iterator's get_next() method, which gives me a bunch of elements like the actual image, its label, filename, etc. I then feed the image into my model using the feed_dict. I know that feed_dict is very slow, so am I losing benefits of Dataset and Iterators and having serialised dataset in TFRecords by evaluating the entries and feeding them into the graph via feed_dict? I haven't found any examples in TF's documentation which shows how one's expected to use Iterator's get_next() to feed elements into the model. Is it better to unpack get_next() and use the result directly in my graph?",1,Inadequate Examples
129,48445751,Keras: Constrained dictionary search with CTC decode,"I'm trying to constrain the CTC decoding to a specific (external) dictionary in Keras with a the tensorflow backend. In the tensorflow documentation for Keras' ctc_decode, it is written that when greedy=False a dictionary will be used. Here is the documentation for tf.nn.ctc_beam_search_decoder, which will be called by this option as far as I understand. Since there is no way to pass an external dictionary or language model (to constrain the search), I assume that with greedy=False it creates its own dictionary from the training data. Is this correct? Is there a way to constrain the search to a specific (external) dictionary?",1,Documentation Replication on Other Examples
130,48471926,In Tensorflow's Dataset API how do you map one element into multiple elements?,"In the tensorflow Dataset pipeline I'd like to define a custom map function which takes a single input element (data sample) and returns multiple elements (data samples). The code below is my attempt, along with the desired results. I could not follow the documentation on tf.data.Dataset().flat_map() well enough to understand if it was applicable here or not. Results: Desired results:",1,Documentation Replicability
131,48697799,Tensorflow feature column for variable list of values,"From the TensorFlow docs it's clear how to use tf.feature_column.categorical_column_with_vocabulary_list to create a feature column which takes as input some string and outputs a one-hot vector. For example Let's say ""kitchenware"" maps to [1,0,0] and ""electronics"" maps to [0,1,0]. My question is related to having a list of strings as a feature. For example, if the feature value was [""kitchenware"",""electronics""] then the desired output would be [1,1,0]. The input list length is not fixed but the output dimension is. The use case is a straight bag-of-words type model (obviously with a much larger vocabulary list!). What is the correct way to implement this?",1,Documentation Replication on Other Examples
132,48736753,"How do you load ""any"" model from disk into a TensorFlow Estimator without having the model_fn source code?","In Keras you can load a model that you had previously trained by using: trained_keras_model = tf.keras.models.load_model(model_name) Is there any equivalent method for doing this using TensorFlow estimator API? According to the documentation, I have to use: trained_estimator = tf.estimator.Estimator (model_fn,model_dir) I want to get the trained estimator using just the files in the model directory. To be more clear my idea was to load ""any"" model from disk without having the model_fn source code. Is it possible to do it this way? This feature is implemented in Keras so I am at a loss to understand why Estimator API cannot do this.",1,Documentation Replicability
133,48815906,Implement early stopping in tf.estimator.DNNRegressor using the available training hooks,I am new to tensorflow and want to implement early stopping in tf.estimator.DNNRegressor with available training hooksTraining Hooks for the MNIST dataset. The early stopping hook will stop training if the loss does not improve for some specified number of steps. Tensorflow documentaton only provides example for Logging hooks. Can someone write a code snippet for implementing it?,1,Requesting (Additional) Documentation/Examples
134,48914952,num_buckets as a parameter in a tensorflow feature column,Currently Tensorflow documentation define a categorical vocabulary column this way: However this suppose that we input manually the vocabulary list. In case of large dataset with many columns and many unique values I would like to automate the process this way: To do so I need to retrieve the parameter list_of_unique_values_in_the_column. Is there anyway to do that with Tensorflow? I know there is tf.unique that could return unique values in a tensor but I don't get how I could feed the column to it so it returns the right vocabulary list.,1,Documentation Replication on Other Examples
135,49066695,How to use tf.nn.raw_rnn function in Tensorflow?,"I am trying to implement LSTM based network where after hidden state computation we also apply linear + sigmoid transformation at each time step. I have found the official documentation and a nice article that describe tf.nn.raw_rnn function suitable for this task however I struggle to understand why it does not work in my particular case. So, let our input to LSTM be a minibatch of size [num_steps x batch_size x size], concretely [5, 32, 100]. Let LSTM have 200 hidden units. Then the output of the LSTM is [5, 32, 200] tensor which we can later use for loss computation. I assume the input [5, 32, 100] tensor is first unstacked into an array of [32, 100] tensors and then stacked back if we use tf.nn.dynamic_rnn with time_major=True in Tensorflow: In addition after each LSTM cell I need to perform linear + sigmoid transformation to squash each [32, 200] tensor into [32, 1] for example. Our tf.nn.dynamic_rnn won't work for that since it only accepts cells. We need to use tf.nn.raw_rnn API. So, here is my try: This unfortunately does not work. The loop_fn iterates only two times instead of num_steps times as I expected and its output is Tensor(""Train/Model/TensorArrayStack/TensorArrayGatherV3:0"", shape=(?, 32, 200), dtype=float32) not [5, 32, 1] as we intended. What am I missing here?",1,Documentation Replication on Other Examples
136,49155119,Using TensorFlow ``grad_loss / grad_ys`` parameter to add gradients,"I'm trying to use the grad_loss parameter in optimizer.minimize(loss, grad_loss=) to modify the network gradients with existing gradients. I followed the comments here: Use of grads_ys parameter in tf.gradients - TensorFlow and I would like to run a toy example, in which I recreate the default 1 values for grad_ys, as specified in the documentation. Here's the relevant code segment: The first part extracts gradients using compute_gradients. The last line computes gradients of the loss function loss_op but attempts to use 1-filled vectors for the grads. As far as I understand, this should behave similarly to funning minimize without the grad_loss parameter. Unfortunately, this fails since it expects grad_loss to be a Tensor (and have a dtype) and not a list. Looking into gradients_impl.py I see that the function expected grad_loss to be of the same dimension as loss (which in this case is a scalar). I would appreciate any assistance in this simple example - how do I add elements to the gradients this way? EDIT: I guess the question boils down to the definition of grad_loss: ""A Tensor holding the gradient computed for loss."" How do I generate such a tensor from a set of gradients obtained by compute_gradients? Thanks.",1,Documentation Replication on Other Examples
137,49201832,How to use TensorBoard and summary operations with the tf.layers module,"I have followed the TensorFlow Layers tutorial to create a CNN for MNIST digit classification using TensorFlow's tf.layers module. Now I'm trying to learn how to use TensorBoard from TensorBoard: Visualizing Learning. Perhaps this tutorial hasn't been updated recently, because it says its example code is a modification of that tutorial's and links to it, but the code is completely different: it manually defines a single-hidden-layer fully-connected network. The TensorBoard tutorial shows how to use tf.summary to attach summaries to a layer by creating operations on the layer's weights tensor, which is directly accessible because we manually defined the layer, and attaching tf.summary objects to those operations. To do this if I'm using tf.layers and its tutorial code, I believe I'd have to: Is that the best way to use TensorBoard with tf.layers, or is there a way that's more directly compatible with tf.layers and the functional interface? If so, is there an updated official TensorBoard tutorial? It would be nice if the documentation and tutorials were more unified.",1,Requesting (Additional) Documentation/Examples
138,49346599,tf.feature_column.input_layer returning wrong shape of tensor,"When I use tf.feature_column.input_layer, it seems to be returning a tensor with shape [number of features, batch size] when it should be returning the opposite - [batch size, number of features]. The code is: The documentation clearly states I am using my own custom estimator function (deep_q_model_test in the code above), and within that I have the first line as: And the shape shown in my print (and also after inspecting the tensor board) is: I am using the prebuilt pandas input function as well to be fed into it: tf.estimator.inputs.pandas_input_fn. The feature columns were built with it being: Also the neural net is actually trains and inspecting the tensorboard it does show the shape is flipped. The problem with this is when I run a batch size which is different, lets say for prediction where i want to only predict 1 observation, it wont work. EDITED Adding in the actual code for the model",1,Documentation Replication on Other Examples
139,49370940,One hot encoding characters,"Is there a possibilty to one-hot encode characters of a text in Tensorflow or Keras? Beside that, tf.keras.preprocessing.text.one_hot works really strange, since the response does not really seem one-hot encoded, since the following code: Lead to this result: Every time I run this program, the output is a different 3d vector, sometimes it is [1,1,1] or [2,1,1]. The documentation says, that unicity is not guaranteed, but this seems really senseless to me.",1,Documentation Ambiguity
140,49405794,Why tensor_summary doesn't work?,"I use tf.summary.tensor_summary in my code, following this: https://www.tensorflow.org/api_docs/python/tf/summary/tensor_summary But I didn't see anything new in tensorboard, and tensorboard also printed some warnings: How to make this work? Do I need install some plugin? I didn't find any docs on this. UPDATE: So I here is how I create my summary: Then I use a MonitoredTrainingSession, by default it will save the summary, and I can see my loss and wealth scalar summry, but not this wealth_tensor summary.",1,Lack of Alternative Solutions/Documentation
141,49418325,"Use ""tf.contrib.factorization.KMeansClustering""","Referring to this Link, (the Link) I try to practice using tf.contrib.factorization.KMeansClustering for clustering. The simple codes as follow works okay: My question is why would this ""input_fn"" code does the trick? If I change the code to this, it will run into an infinite loop. Why?? From the document (here), it seems that train() is expecting argument of input_fn, which is simply a A 'tf.data.Dataset' object , like Tensor(X). So, why do I have to do all these tricky things regarding lambda: tf.train.limit_epochs()? Can anyone who is familiar with the fundamental of tensorflow estimators help to explain? Many Thanks!",1,Inadequate Examples
142,49435335,Verify that keras GaussianNoise is enabled at train time when using inference with edward,"I would like to check if noise is truly added and used during training of my neural network. I therefore build my NN with keras like this: Then I use edward to execute inference: According to the documentation, the closest I get to this is through ed.MAP's run() and update() functions. Preferably, I would do something like this: Apparently the way I use GaussianNoise doesn't seem to add noise to my input since the following unittest fails: I also made sure that during the inference.update(...) the assertion assert tf.keras.backend.learning_phase() == 1 passes. Where could have something gone wrong here?",1,Documentation Replication on Other Examples
143,49472402,Tensorflow tf.nn.softmax() function performs much better than hand-written softmax,"I'm writing a simple logistic regression with tensorflow. I found out that when using tf.nn.softmax, the algorithm converges much quicker, and in the end the accuracy is higher. If switched to my own implementation of softmax, the network converges slower, and the end accuracy is not as good. Here's the code: Using my softmax: Using tensorflow's softmax: From the documentation, in theory tensorflow's softmax should be exact the same as I implemented, no? EDIT: I added a seed when initializing from normal distribution, now I can reproduce the accuracy results. When setting axis value in ""My softmax"" line, only axis=0 doesn't result in error. Setting axis=1 or axis=-1 both results in this error:",1,Documentation Replicability
144,49542954,What are tf.TensorArray objects?,"I am not able to get an understanding of tf.TensorArray objects. What are they and where are they needed? I have some (highly likely faulty) understanding - they are used in while_loops especially to write the information to the loop_state. If somehow we end up increasing the number of tensors or their dimensions across iterations it throws back an error. hence normal way to pass the loop_state across iterations collecting information from each iteration, which would be passing a list, would throw back an error. So we create tf.TensorArray objects and write the information to them at each iteration and pass these tf.TensorArray objects across loops and for some reason that way it is able to pass through I couldnt find any blog or documentation explaining the working of a tf.TensorArray objects and documentation doesn't help much either So, if this is not the best place to be asking this question, kindly direct me to nearest help.",1,Lack of Alternative Solutions/Documentation
145,49564318,Issue with fine-tuning inceptionv3 in slim tensorflow and tf record batches,"I am trying to fine-tune inceptionv3 model using slim tensorflow library. I am unable to understand certain things while writing the code for it. I tried to read source code (no proper documentation) and figured out few things and I am able to fine-tune it and save the check point. Here are the steps I followed 1. I created a tf.record for my training data which is fine, now I am reading the data using the below code. Now I am finetuning the model using slim and this is the code. Now I have few questions about the code, which I am quite unable to figure out. Once, the code reaches slim.learning.train I don't see anything printing however, it's training, I can see in the log. Now, 1. How do I give the number of epochs to the code? Right now it's running step by step with each step has batch_size = 64. 2. How do I make sure that in the code tf.train.shuffle_batch I am not repeating my images and I am training over the whole dataset? 3. How can I print the loss values while it's training?",1,Documentation Replication on Other Examples
146,49605330,Example of tf.feature_column.indicator_column,"Im reading tensorflows document about tf.feature_column.indicator_column. In this document, there is an example. My problem is the omitted(...) part of this code. I just want a complete, running, simple example. And I cant find a kind example including tf.Example and so on. Can anyone make this complete? Thank you for advance.",1,Documentation Completeness
147,49633383,tensorflow tf.nn.rnn_cell.BasicLSTMCell,"I'm trying to play with ""tf.nn.rnn_cell.BasicLSTMCell"" with TF on python. Reading here ""https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell"", in the ""init"" method, I see the param ""num_units"" with description ""int, The number of units in the LSTM cell."" But ... wait a moment ""number of UNITS""? Which type of units? The class is called ""...LSTMCell"" but from ""num_units"" it seem that we are speaking about a layer, not a single neuron. I'm confused. Any help is appreciated. TIA",1,Documentation Replicability
148,49662470,Tensorflow global_step TypeError,"I'm adding Tensorboard to an existing small Tensorflow project that I know works to practice working with Tensorboard but I get a type error that global_step must be a string or tensor, however I have assigned global_step to a tf.Variable(0, name='global_step', trainable=False) just like the documentation and every example I see online. Any idea of what I'm missing would be super appreciated. ---&gt; 70 [summary_merge, tf.train.global_step(sess, global_step_tensor) ,update_model, weights], feed_dict) TypeError: Fetch argument 0 has invalid type , must be a string or Tensor. (Can not convert a int into a Tensor or Operation.)",1,Lack of Alternative Solutions/Documentation
149,49686860,Side effect in tf.while_loop,"I am currently having a hard time trying to understand how tensorflow works, and I feel like the python interface is somehow obscure. I recently tried to run a simple print statement inside a tf.while_loop, and there are many things that remains unclear to me: Notice that if I initialize nb_iter with I got the following error: It get even worse when I try to use the 'i' index for indexing a tensor (example not shown here), I then get the following error Can someone point me to a documentation that explains how tf.while_loop works when used with tf.Variables, and if it possible to use side_effects (like print) inside the loop, as well as indexing tensor with the loop variable ? Thank you in advance for your help",1,Requesting (Additional) Documentation/Examples
150,49688134,TensorFlow session inside Keras custom loss function,"After going through some Stack questions and the Keras documentation, I manage to write some code trying to evaluate the gradient of the output of a neural network w.r.t its inputs, the purpose being a simple exercise of approximating a bivariate function (f(x,y) = x^2+y^2) using as loss the difference between analytical and automatic differentiation. Combining answers from two questions (Keras custom loss function: Accessing current input pattern and Getting gradient of model output w.r.t weights using Keras ), I came up with this: Which yields the error: TypeError: The value of a feed cannot be a tf.Tensor object. because of feed_dict={model.input:input_tensor}. I understand the error, I just don't know how to fix it. From what I gathered, I can't simply pass input data into the loss function, it must be a tensor. I realized Keras would 'understand' it when I call input_tensor. This all just leads me to think I'm doing things the wrong way, trying to evaluate the gradient like that. Would really appreciate some enlightenment.",1,Documentation Replicability
151,49701918,tf.layers.batch_normalization parameters,"I am not sure if it is only me who thinks that tensorflow documentation is a bit weak. I was planing to use the tf.nn.batch_normalization function to implement batch normalization but later recognized the tf.layers.batch_normalization function which seemingly should be the one to use for its simplicity. But the documentation is really poor if I may say it. I am trying to understand how to correctly use it but with the information provided on the Web page is it really not easy. I am hoping that maybe some other people have experience and help me (and possibly many others) to understand it.. Let me share the interface first: Q1) beta values are initialized to zero and gamma values are initialized to 1. But it does not say why. When batch normalization used, I understand that the ordinary bias parameter of the neural network becomes obsolete and beta parameter in the batch normalization step kind of does the same thing. From that angle, setting beta to zero is understandable. But why are gamma values initialized to 1? Is that really the most efficient way? Q2) I see a momentum parameter there as well. The documentation just says "" Momentum for the moving average."". I assume that this parameter is used when calculating the ""mean"" value for a certain mini batch in the corresponding hidden layer. With other words, the mean value used in batch normalization is NOT the mean of current mini batch, it is rather primarily the mean of the last 100 mini batches (since momentum = 0.99). But it is very unclear how this parameter affects the execution in testing, or if I am just validating my model on the dev set by calculating cost and accuracy. My assumption is that anytime I deal with test and dev sets, I set the parameter ""training"" to False so that momentum parameter becomes obsolete for that particular execution and the ""mean"" and ""variance"" values that were calculated during the training are used now instead of calculating new mean and variance values. It is how it should be if I am mistaken but I do not see anything in the documentation if it is the case. Could anyone confirm that my understanding correct? If not, I would really appreciate further explanation on this. Q3) I am having difficulties to give a meaning to the trainable parameter. I assume beta and gamma params are meant here. Why would they not be trainable? Q4) The ""reuse"" parameter. What is it really? Q5) adjustment parameter. Another mistery.. Q5) A kind of summary question.. Here is my overall assumption that needs confirmation and feedback.. Important params here are: - inputs - axis - momentum - center - scale - training And I assume that as long as the training=True when training, we are safe. And as long as training=False when validating dev set or test set or even when using the model in real life, we are safe too. Any feedback will really be appreciated. ADDENDUM: Confusion continues. Help! I am trying to use this function instead of implementing a batch normalizer manually. I have the following forward propagation function that loops through layers of the NN. The tf.layers.batch_normalization(..) function wants to have static dimensions but I do not have it in my case. Since I apply mini batches rather than training the entire train set each time before I run the optimizer, 1 dimension of the X appears to be unknown. If I write: I get: And when this is the case, when I run the whole program I get the following error below. I saw in some other threads that some people say that they could solve the problem by using tf.reshape function. I try it.. Forward prop goes fine but later on it crashes in the Adam Optimizer.. Here is what I get when I run the code above (without using tf.reshape): How do I solve this??? This is so hopeless.. ADDENDUM(2) I am adding more information: The following simply means that there are 5 units in input layer, 6 units in each hidden layer, and 2 units in output layer. Here is the updated version of forward prop function with tf.reshape When I do this, I can run the forward prop function. But it seems to be crashing in later execution. Here is the error that I get. (Note that I print out the shape of input X before and after reshaping in the forward prop function). Regarding the question why the shape of X is not static.. I don't know... HEre is how I setup the dataset. I have 2 csv files that include the train data. And I use the initializable iterator as following: During the training, I keep getting mini batches from the train set. Everything works fine when I disable batch normalization. If I enable batch norm, it requires static shape of the input X (mini batch). I reshape it but this time it crashes later in the execution as seen above. ADDENDUM(3) I guess I figured out where it crashes. It probably crashes when I run the optimizer after calculating the cost. First the sequence of commands: First forward prop, then compute cost, then run optimizer. First 2 seems to be working but not the optimizer. HEre is how I define the optimizer: I have the update_ops there to be able to update the moving averages. If I interpret it right, it is just crashing when it tries to update moving averages. I might be misinterpreting the error msg as well.. ADDENDUM(4) I tried to normalize based on the known dimension and it worked! But that's not the dimension I would like to normalize, which is now confusing. Let me elaborate: nr of units in input layer: 5 nr of units in layer 1 (first hidden layer): 6 so weight1 is (6, 5) matrix Assume that mini batch size is 7. Shape of A[0] (or X_mini_batch) in my case is: (7, 5), where 7 is the # training samples in mini batch, and 5 is the # units in input layer. When calculating Z[1]... Z[1] = weight1 * A[0].transpose ... then shape of Z[1] is (6, 7) matrix, where each column gives 6 features for each train sample. The question is then which column do we want to normalize in Z[1]? What makes sense to me is that you normalize each feature from all given train samples. This means that I need to normalize each row bcz I have different feature values for different train examples in each row. And since Z[1] has the shape (6, 7), if I set axis=0, it should refer to normalization in each row. And 7 is the unknown number in my case so it doesn't hurt. Based on this logic, it works! But I am totally puzzled if axis=0 really refers to each row here... Let me show another example about this axis issue, which has bothered me for a long time now.. The irrelevant from this topic code example: This gives the following output: When I set axis to 0, it is giving the average of each column. And if axis=1, it is giving the average of each row. (Note that cc.shape gives (2,3)) Now the million dollar question: In a 2 dimensional matrix, is axis 0 or 1 when I want to address each row? ADDENDUM(5) I guess I get it now correctly. Let me summarize my axis understanding here. Hopefully I am getting it right now... Here is the Z[1] matrix representation with the shape (6,7): t_ex : train example f: feature In this mini batch above, there are 7 train examples and each train ex has 6 features (since there are 6 units in layer 1). When we say ""tf.layers.batch_normalization(..,axis=0)"", we mean that the normalization has to be done per row for each feature to eliminate the high variance between - say - f1 values in the first row. With other words, we do NOT normalize f1,f2,f3,f4,f5,f6 with each other. We normalize f1:s with each other, and f2:s with each other, and so on..",1,Lack of Alternative Solutions/Documentation
152,49768997,Writing TFRecords in batches,"All documentations I found regarding TFRecords are generating tf.train.Example()s one by one, and writing them using Since I'm dealing with very big data, I know that I'll pay a high overhead price for writing examples separately Is there any way to write multiple tf.train.Example() to a TFRecord at once?",1,Documentation Replication on Other Examples
153,49899526,Tensorflow input pipeline where multiple rows correspond to a single observation?,"So I've just started using Tensorflow, and I'm struggling to properly understand input pipelines. The problem I'm working on is sequence classification. I'm trying to read in a CSV file with shape (100000, 4). First 3 columns are features, 4th column is the label. BUT - the data represents sequences of length 10 i.e. rows 1-10 are sequence 1, rows 11-20 are sequence 2 etc. This also means each label is repeated 10 times. So at some point in this input pipeline, I'll need to reshape my feature tensor like tf.reshape(features, [batch_size_, rows_per_ob, input_dim]). And only take every 10th row of my label tensor like label[::rows_per_ob] Another thing I should point out is that my actual dataset is in the billions of rows so I have to think about performance. I've put together the below code from documentation and other posts on here, but I don't think I fully understand this because I'm seeing the following error: There seems to be an out of range error. I also can't figure out what to do with these batches once I get them working. Initially, I thought I would reshape them then just feed them into ""feed_dict"", but then I read that this is really bad, and I should be using a tf.data.Dataset object. But I'm not sure how to feed these batches into a Dataset. I'm also not entirely sure when would be the optimal time in this process to reshape my data? And a final point of confusion - when you use an Iterator with a Dataset object, I see that we use the get_next() method. Does this mean that each element in the Dataset represent a full batch of data? And does this then mean that if we want to change the batch size, we need rebuild the entire Dataset object? I'm really struggling to fit all the pieces together. If anyone has any pointers for me, it would be very much appreciated! Thanks!",1,Documentation Replication on Other Examples
154,49959130,how to insert two or more label lists in the tf.estimator.inputs.numpy_input_fn?,"I am using the tf.estimator.inputs.numpy_input_fn to feed data in my model and train it in a similar way with the MNIST example. The only difference is that I need to insert two numpy lists of labels instead of one. I tried passing them in a dictionary like this: Then when I try to retrieve them in the model like so: I get the following error: I also tried to change the name of the variable ""labels"" to be ""targets"" according to tensorflow inputs.numpy_input_fn documentation: and I get this error: If you have any solution or suggestion to my problem, please let me know. Thanks a lot in advance. Antonios",1,Documentation Ambiguity
155,49987839,How to handle None in tf.clip_by_global_norm?,I have read in answers to this question here that tf.clip_by_global_norm() handles None values by simply ignoring them (comment by danijar in comments to the answer by @danijar) but when i try to apply it i seem to be doing something wrong as it throws ValueError: None values not supported. Can somebody please tell me what am i doing wrong or if tf.clip_by_global_norm() does not handle None gradients and i have to take care of them manually The official documentation seems to agree with @danijar's comments. see here,1,Documentation Replicability
156,49997294,Moving away from tf.contrib.learn: distributed training with dedicated evaluator process,"In TF 1.8's upcoming release, tf.contrib.learn.* will be deprecated. The tf.contrib.learn.Experiment class recommends switching to tf.estimator.train_and_evaluate instead, so I'm trying to port my code to that framework. What I want to do is set up distributed training on two machines' GPUs, plus a third CPU-only process that does continuous evaluation on a small validation set. Following the examples in the documentation of train_and_evaluate and the Distributed Tensorflow guide, I managed to set up the training half of my desired architecture, but I can't find a way to set up an estimator. So far, what I have looks as follows: This code works, although my understanding of it is still pretty limited. I get what the PS and workers do, but from the specification of chief I understand this should be the ""master"" worker that also logs summaries and saves checkpoints. What I'm missing now is the periodic evaluation... and I'm at a loss. From the train_and_evaluate codebase I see there's some ""evaluator"" support but I don't understand how to set it up properly.",1,Documentation Replication on Other Examples
157,50029121,How to use tf.layers classes instead of functions,"It seems that tf.Layer modules come in two flavours: functions and classes. I normally use the functions directly (e.g, tf.layers.dense) but I'd like to know how to use classes directly (tf.layers.Dense). I've started experimenting with the new eager execution mode in tensorflow and I think using classes are going to be useful there as well but I haven't seen good examples in the documentation. Is there any part of TF documentation that shows how these are used? I guess it would make sense to use them in a class where these layers are instantiated in the __init__ and then they're linked in the __call__ method when the inputs and dimensions are known? Are these tf.layer classes related to tf.keras.Model? Is there an equivalent wrapper class for using tf.layers? Update: for eager execution there's tfe.Network that must be inherited. There's an example here",1,Lack of Alternative Solutions/Documentation
158,50054453,Tensorflow shape inference static RNN compiler error,"I am working on OCR software optimized for phone camera images. Currently, each 300 x 1000 x 3 (RGB) image is reformatted as a 900 x 1000 numpy array. I have plans for a more complex model architecture, but for now I just want to get a baseline working. I want to get started by training a static RNN on the data that I've generated. Formally, I am feeding in n_t at each timestep t for T timesteps, where n_t is a 900-vector and T = 1000 (similar to reading the whole image left to right). Here is the Tensorflow code in which I create batches for training: The tf.nn.static_bidirectional_rnn documentation claims that the input must be a ""length T list of inputs, each a tensor of shape [batch_size, input_size], or a nested tuple of such elements."" So, I go through the following steps in order to get the data into the correct format. Without altering the batch any further, I make the following call: Note that I do not explicitly tell Tensorflow the dimensions of the matrices (this could be the problem). They all have the same dimensionality, yet I am getting the following bug: At which point in my stack should I be declaring the dimensions of my input? Because I am using a Dataset and hoping to get its batches directly to the RNN, I am not sure that the ""placeholder -&gt; feed_dict"" route makes sense. If that in fact is the method that makes the most sense, let me know what that looks like (I definitely do not know). Otherwise, let me know if you have any other insights to the problem. Thanks!",1,Documentation Replication on Other Examples
159,50078749,Tensorflow-hub Text-Module Preprocessing,"I'm playing around with the new Modules which are available on the tensorflow-hub (which I really like - thanks for that). Whats unclear to me, is the preprocessing which should take place when feeding a sentence. The module documentation says, that in the preprocessing step the inputj sentences gets splitted at the spaces. However, when I run the following program, I only get a single vector: How do I get the embeddings for each word, and what does the single vector represents? A fixed-dimensional representation of the sentence, the Unknown-Word embedding or something else? Thanks in advance! Edit: It seems the result is a combined embedding created with tf.nn.embedding_lookup_sparse. (Thanks for the confirmation @svsgoogle)",1,Documentation Replication on Other Examples
160,50149953,Using tf.keras within Tensorflow,What is the correct way of using the tf.keras API. Can tf.layers.* be directly replaced with tf.keras.layers(Similarly activations or loss functions)? Is it necessary to import tf.keras.backend and do set_learning_phase? This doesnt seem to be explained on the official TF docs but is mentioned in this relatively old blog post.,1,Lack of Alternative Solutions/Documentation
161,50203668,Using tf.custom_gradient in tensorflow r1.8,"Hello, I'm trying to make a custom_gradient op using the function of tf.custom_gradient. I made my test code based on the API explanation online. However, it seems there is a problem in the custom_gradient function. Thanks!",1,Documentation Ambiguity
162,50210594,the function of 'bounding_boxes' and 'min_object_covered' in tf.image.sample_distorted_bounding_box?,"How parameters 'bounding_boxes' and 'min_object_covered' control the generation of a single randomly distorted bounding box for an image in tf.image.sample_distorted_bounding_box? I have read the function in tensorflow api, but I still can not understand the proplem. Maybe I need a intuitive example.",1,Requesting (Additional) Documentation/Examples
163,50222149,How to scan through tensor not at dimension 0?,"The tensorflow document states that tf.scan scans on the list of tensors unpacked from elems on dimension 0. The simplest version of scan repeatedly applies the callable fn to a sequence of elements from first to last. The elements are made of the tensors unpacked from elems on dimension 0. My question is: How to scan on the list of tensors on other dimension instead of dimension 0? For example, I have a tensor, ref, defined as below. I want to scan through the ref[1,0], ref[1,1], ref[1,2] and apply a function to each of the, ,say add 1. That is to say, I want ref be after the operation Can I use tf.scan to do that? If yes, how? If not, any how to do in other way? Thanks.",1,Documentation Replication on Other Examples
164,50226274,how to explain the output of tf.rank in tensorflow,"I am new in tensorflow and have a question about tf.rank method. In the doc https://www.tensorflow.org/api_docs/python/tf/rank there is a simple example about the tf.rank: But when I run the code below: I get output like: Why can I get the output of ""3""?",1,Documentation Replication on Other Examples
165,50243230,Unable to understand tf.nn.raw_rnn,"In the official documentation of tf.nn.raw_rnn we have emit structure as the third output of loop_fn when the loop_fn is run for the first time. Later on the emit_structure is used to copy tf.zeros_like(emit_structure) to the minibatch entries that are finished by emit = tf.where(finished, tf.zeros_like(emit_structure), emit). my lack of understanding or lousy documentation on google's part is: emit structure is None so tf.where(finished, tf.zeros_like(emit_structure), emit) is going to throw a ValueError as tf.zeros_like(None) does so. Can somebody please fill in what I am missing here?",1,Documentation Replication on Other Examples
166,50246535,Tensorflow estimator input function: defining each feature or not?,"With x is a 120 x 4 feature matrix of Iris data (4 features) and y is a label, I can make an input function for tf.estimator like below then define the feature column like below: But, I found in the internet (I forget the source, still searching) that I also can define the input function like below. The difference with previous method is all four features now defined with only one key, ""x"". then define the feature column like below: I've run both method and both give almost same result. My question: I can't find any documentation that explain the difference between both method, because at a glance dict_x have different shape. Are they still treated equally at input layer on neural networks? I'm new using tf.estimator, Thank You My estimator code if needed:",1,Lack of Alternative Solutions/Documentation
167,50329855,How to use the Tensorflow Dataset Pipeline for Variable Length Inputs?,"I am training a Recurrent Neural Network in Tensorflow over a dataset of sequence of numbers of varying lengths and have been trying to use the tf.data API to create an efficient pipeline. However I can't seem to get this thing to work My data set is a NumPy array of shape [10000, ?, 32, 2] which is saved on my disk as a file in the .npy format. Here the ? denotes that elements have variable length in the second dimension. 10000 denotes the number of minibatches in the dataset and 32 denotes the size of a mini-batch. I am using np.load to open this data set and I am trying to create a tf.data.Dataset object using the from_tensor_slices method but it seems that this only works if all input Tensors have the same shape! I tried reading the docs but they have only given a very simple example. So the numpy files have been generated as follows - The code given below is my attempt to create a tf.data.Dataset object The error I get is ""TypeError: Expected binary or unicode string, got array([[[0.0875, 0. ], ..."" So I tried @mrry's answer and I am now able to created a Dataset object. However, I am not able to iterate through this dataset using iterators as said in the tutorial. This is what my code looks like now - The error I get is AttributeError: 'numpy.dtype' object has no attribute 'as_numpy_dtype'. I have no absolutely no clue what this means. This is the complete stack trace -",1,Documentation Replication on Other Examples
168,50383462,how to randomly initialize weights in tensorflow?,"in tensorflow, I learned from the tutorial that one would initialize the variables with something like sess.run(tf.global_variables_initializer()) however I found that every time I run this with the same input dataset, the loss value starts with the same value. I presume this is due to the fact that the initialization is always setting up the variables with the same values. (probably zero) I wish to randomize the values of weights. I've tried searching for this but tensorflow docs doesn't give a clear answer if the initialization is done with zero values by default or random values. How can I specify the initializaing to setup random values? update my network is first a bunch of CNNs and pooling layers like below: ``` conv1 = tf.layers.conv2d(inputs=input_layer, filters=32, kernel_size=[3,3], padding=""same"", activation=tf.nn.relu, name=""conv_chad_1"") ``` AFAIK, the weights are defined inside these predefined layers. How do I specify these layers to initialize their weight variables randomly??",1,Documentation Replication on Other Examples
169,50442156,Loading a model from tensorflow SavedModel onto mutliple GPUs,"Let's say someone hands me a TF SavedModel and I would like to replicate this model on the 4 GPUs I have on my machine so I can run inference in parallel on batches of data. Are there any good examples of how to do this? I can load a saved model in this way: ..but this would require that I have a handle to the session. For models that I have written myself, I would have access to the inference function and I could just call it and wrap it using with tf.device(), but in this case, I'm not sure how to extract the inference function out of a Saved Model. Should I load 4 separate sessions or is there a better way? Couldn't find much documentation on this, but apologies in advance if I missed something. Thanks!",1,Inadequate Examples
170,50454095,tf.gradients - dimensions of output,Here is my code: it returns: I am expecting to get 3 by 3 matrix or as per tf.gradients docs list of dim 3 with 3 elements for each list entry. What I am missing? UPDATE: I see in docs tf.gradients but why sum and how do I get all entries of Jacobian?,1,Lack of Alternative Solutions/Documentation
171,50457247,Tensorflow finding matching strings in tensor,"I'm trying to find variables that end in train_step from tf.report_uninitialized_variables(), but you can't iterate over tensors without eager execution. I get that you need to use tf.map_fn, but I do not understand it well enough. This is what I have:",1,Documentation Replicability
172,50500579,what is the difference between tf.nn.max_pool() and tf.layers.max_pooling2d(),"I am a beginner of tensorflow,i have met two methods about create max_pool layer these days. one is ""tf.nn.max_pool()"" and the other is ""tf.layers.max_pooling2d()"".i want to learn about its difference,and when to use them suitably.based on this ,i have read its official document and searched in google,it is not any help at all.i have searched it in stackoverflow , there is a similar answer(tf.nn.conv2d vs tf.layers.conv2dbut it didn't solve my problem.does any one can help me? thanks in advance.",1,Inadequate Examples
173,50514454,End of Sequence Error when using tf.estimator and tf.data,"I am using tf.estimator.train_and_evaluate and tf.data.Dataset to feed data to the estimator: Input Data function: Train Function: The training goes fine, but when it comes to evaluation I get this error: If I don't use Dataset.batch on evaluation dataset (by omitting the line dataset[name] = dataset[name].batch(batch_size) in data_fn) I get the same error but after a much longer time. I can only avoid this error if I don't batch the data and use steps=1 for evaluation, but does that perform the evaluation on the whole dataset? I don't understand what causes this error as the documentation suggests I should be able to evaluate on batches too. Note: I get the same error when using tf.estimator.evaluate on data batches.",1,Documentation Replication on Other Examples
174,50560013,"Tensorflow, multi-label confusion matrix","I am trying to figure out how to the generate a confusion matrix for a multi-label classification task using neural networks. I previously managed to calculate the accuracy using the function ""intersection"", since for that I did not care about any ordering. However, in order to calculate the confusion matrix, I do care about the indexing order of the predictions/labels. And since the labels have always the same value (1,1 or 0.5,0.5) there is no possible sorting according to higher/lower value. I wonder: 1) Is it possible to calculate a confusion matrix for the multi-label classification task? 2) How would that be implemented ? 3) How can you handle the case of failure in predicting both labels? Since it is not possible to know which confusion belongs to which prediction. 4) What is the logic behind the sorting of the function tf.nn.top_k() Below I show an example of the code that I was trying to use. I don't really get why the output of predicted_softmax is: I was expecting [5] [3] for the last two terms. There is no any logic to this output. In the documentation they don't specify anything about the ordering in the case that sorted = False thought, but I was expecting some consistent behavior. Thanks for any help!",1,Documentation Completeness
175,50606178,TensorFlow tf.data.Dataset and bucketing,"For an LSTM network, I've seen great improvements with bucketing. I've come across the bucketing section in the TensorFlow docs which (tf.contrib). Though in my network, I am using the tf.data.Dataset API, specifically I'm working with TFRecords, so my input pipeline looks something like this How can I incorporate the bucketing method into a the tf.data.Dataset pipeline? If it matters, in every record in the TFRecords file I have the sequence length saved as an integer.",1,Documentation Replicability
176,50724495,Train and validate using tensorflow estimator,"I have created a shallow NN using tf.estimator API. I would like to something similar to the hyperparameter search explained in here https://www.youtube.com/watch?time_continue=948&amp;v=eBbEDRsCmv4 at TensorFlow Dev Summit. I could not find any updated documentation about how can you do this. I have the following code (I will try to simplify as much as possible): Executing this code I can obtain the summary for the loss and observe it in Tensorboard. But imagine I want to obtain different curves. Let's say that I want to see how the loss evolves with the number of samples, so I would train two models with different sample size. Or two models with a different architecture... whatever. How can I get these two curves in Tensorboard?",1,Documentation Replication on Other Examples
177,50820781,quesion about the axis of tf.stack(),"I read the doc of tf.stack() on tensorflow stack . There is an example on the page: what I don't understand is the second example where axis=1. From the result it seems it converts the three inputs rows to columns first and then put them toghter along the axis=1, but I think the result should be can anyone help explain this? Thanks!",1,Inadequate Examples
178,50825446,Restoring a trained generator network in DCGAN,"I have a question regarding the saving and storing models in tensorflow. I know how to save a model with tf.train.Saver() and load it later through meta file. My problem is this: I have trained a variant of DCGAN (Deep Convolution GAN), now I want to use only generator network for other tasks. Unfortunately, I do not know how to get entire generator network such that if I feed it with a new vector z, it generates an output based on the trained parameters. All the example I found in the stackoverflow, or tensorflow documentation, just mention very simple operations with two numbers. This is not I want. I want to understand if you have trained a giant network, say with 50 layers, how to load it and feed it with new input and get the output without going into the different parameters and layers in the trained network. I want to load it as a blackbox.",1,Documentation Replication on Other Examples
179,50840759,"Incorrect name returned in Tensorflow causes ""Tensor which does not exist"" error while invoking get_tensor_by_name","As per the documentation TensorFlow would append ""_1"", ""_2"", and so on to the name in tf.Graph namespace, in order to make it unique. Here I define two convolutional operations. It is expected that the first one will be named as ""conv2d"" and second one ""conv2d_1"". But when I try to obtain the name of the second convolution it returns ""conv2d_2"". I causes error when I try to invoke get_tensor_by_name. Here is the code: I could not understand why conv_out2.name returns ""conv2d_2"" instead of ""conv2d_1""",1,Documentation Ambiguity
180,50967885,"tf.gradients, how can I understand `grad_ys` and use it?","In tf.gradients, there is a keyword argument grad_ys Why is grads_ys needed here? The docs here is implicit. Could you please give some specific purpose and code? And my example code for tf.gradients is",1,Documentation Replicability
181,50997477,Keras and Tensorflow: Saving non-sequential model weights,"(I'm using Tensorflow 1.8.0 ...) The documentation from Keras on how to save a model mentions no difference between saving a sequential model vs. one created from the functional API. But, all of the following blocks of code fail: or or or They raise a NotImplementedError. In the Keras module, the relevant lines are which shows up in .save and get_config (the latter is also called by to_json and to_yaml. The only thing that DOES work is the following in which case the weights are saved successfully and can be successfully loaded with net.load_weights. However, replacing the second line of the above blocks of code, net = tf.keras.models.Model(), with net = tf.keras.models.Sequential(), making net a sequential model, allows everything above to work. Is it really not possible to save the structure of a Keras model made with the functional API (using Model rather than Sequential)? Right now, can we only save weights?",1,Lack of Alternative Solutions/Documentation
182,51069173,What exactly qualifies as a 'Tensor' in TensorFlow?,"I am new to TensorFlow and just went through the eager execution tutorial and came across the tf.decode_csv function. Not knowing about it, I read the documentation. https://www.tensorflow.org/api_docs/python/tf/decode_csv I don't really understand it. The documentation says 'records: A Tensor of type string.' So, my question is: What qualifies as a 'Tensor'? I tried the following code: So the list dec_res contains tf.tensor objects. That seems reasonable to me. But is an ordinary string also a 'Tensor' according to the documentation? Then I tried something else with the tf.reshape function. In the documentation https://www.tensorflow.org/api_docs/python/tf/reshape it says that 'tensor: A Tensor.' So, l is supposed to be a tensor. But it is not of type tf.tensor but simply a python list. This is confusing. Then the documentation says But the type of l is list where the type of r is tensorflow.python.framework.ops.Tensor. So the types are not the same. Then I thought that TensorFlow is very generous with things being a tensor. So I tried: Now, the line in comments results in an error. So, can anyone help me to find out, what qualifies as a 'Tensor'? P.S.: I tried to read the source code of tf.reshape as given in the documentation But this file does not exist in the Github repo. Does anyone know how to read it?",1,Requesting (Additional) Documentation/Examples
183,51077930,tf.image.resize_bilinear()-when align_corners=False,"I am using Tensorflow 1.4.0 The Tensorflow tf.image.resize_bilinear() has an argument called 'align_corners' and I am confused with the behavior when we set it to be False. In the official document, it says: When I use tf.image.resize_bilinear() with align_corners=True in the following program: it outputs which corners are correctly aligned. However when I set the align_corners=False, I got the following weird outputs Is there anyone who understand why Tensorflow will use this weird implementation? I didn't find any explanation anywhere. Actually PyTorch's bilinear upsampling has the align_corner argument too, when you set it to True, it works well. But if you set it to False, it performs a differnet behaviour to Tensorflow's. I am totally confused with their implementations now (maybe just use align_corners=True will be fine).",1,Documentation Ambiguity
184,51248442,Behavior of the parameter 'throttle_secs' in tf.estimator.EvalSpec for use in tf.estimator.train_and_evaluate,"I am using tensorflow's train_and_eval function as in the example. Therefore i create an instance of tf.estimator.EvalSpec, according to According to its documentation the explanation of the parameter throttle_secs states that ""Of course, evaluation does not occur if no new checkpoints are available, hence, this is the minimum."" However, i observe a different behavior. If there is no new checkpoint and evaluation should be triggered according to the passed parameter a new checkpoint is created and evaluation is performed. Is this a bug or am i missing something here?",1,Documentation Replicability
185,51278422,Interpreting the FLOPs profile result of tensorflow,"I want to profile the FLOPs of a very simple neural network model, which is used to classify the MNIST dataset, and the batch size is 128. As I followed the official tutorials, I got the result of the following model, but I cannot understand some parts of the output. The images_iter and the labels_iter are the iterators of tf.data, which are similar to the placeholder. I used this code, which equals to scope -min_float_ops 1 -select float_ops -account_displayed_op_only in tfprof comments line tool, to profile the FLOPs and got the below result. My questions are I know it is discouraging to read a question so long, but a desperate boy who cannot find relating information from the official document needs your guys to help.",1,Lack of Alternative Solutions/Documentation
186,51392594,"`ValueError: Can not squeeze dim[1], expected a dimension of 1, got 10 for 'sparse_softmax_cross_entropy_loss' [Tensorflow]","I am rather new to Tensorflow, and has been trying to pick up the basics by reading through the guides and documentation on tensorflow.org I have learnt the basics of how to use the tf.data and tf.estimator APIs and is trying to get them to work together on a basic image model for MNIST. I am currently following these guides: https://www.tensorflow.org/tutorials/estimators/cnn https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py I've changed the original python script to be using Dataset.from_tensor_slices rather than numpy_input_fn but I am facing the error at the evaluation step. (though not at the training step) ValueError: Can not squeeze dim[1], expected a dimension of 1, got 10 for 'sparse_softmax_cross_entropy_loss/remove_squeezable_dimensions/Squeeze' (op: 'Squeeze') with input shapes: [1,10]. My code can be found in a python notebook here (only changed the input_fn): https://github.com/quanta0801/tf_scripts/blob/master/mnist/mnist_estimator_baseline.ipynb Thanks! PS: any additional links to excellent guides to using tf.data &amp; tf.estimators will be great too! Official documentation cycles between these, keras and the low level APIs which is not conducive.",1,Documentation Replication on Other Examples
187,51396366,TensorFlow with keras: Where is the ReLU layer in tf version 1.8?,"Update: Found it: The class is tf.keras.layers.Activation; needs to be called with argument activation='relu'.... Trying to access tf.keras.layers.ReLU gives the error: In the docs, version master has such a layer. Version 1.8 (and 1.9) only seems to have leaky relu, PReLU, and other derivatives. Right now I'm using ThresholdedReLU with theta of 0.0, I hope this results in a standard ReLU. But there must be a simple 'ReLU' layer as well? Where can I find keras' ReLU layer in tensorflow 1.8? I want a keras layer class, i.e., not tf.keras.backend.relu. It feels as if I'm overlooking something completely obvious. I haven't used keras before, so, sorry if this is a super stupid question.",1,Documentation Replication on Other Examples
188,51507788,What are inputs and outputs in tf.saved_model.simple_save?,"In tf.saved_model.simple_save, there are 4 params: I've been reading how to simple_save but I haven't been able to figure out what to put in these two parameters (inputs and outputs). I know the model must have input values so that it can be either trained or predict. So I don't know what these two parameters should contain and wether they should map variables inside the model or what... The docs aren't that great so any explaining would be much appreciated.",1,Requesting (Additional) Documentation/Examples
189,51586693,"Tensor has shape [?, 0] -- how to reshape to [?,]","When src has shape [?], tf.gather(src, tf.where(src != 0)) returns a tensor with shape [?, 0]. I'm not sure how a dimension can have size 0, and I'm especially unsure how to change the tensor back. I didn't find anything in the documentation to explain this, either. I tried to tf.transpose(tensor)[0], but the first dimension of the transposed tensor has size 0 and cannot be accessed! What's wrong?",1,Documentation Replication on Other Examples
190,51612489,tensorflow tf.edit_distance explanation required?,"How does tensorflow tf.edit_distance function works? How it compares string stored in two different sparse matrix equivalent of 2d or 3d dense matrix. Example given on tensorflow web page https://www.tensorflow.org/api_docs/python/tf/edit_distance is not so obvious. Please provide explanation using some other examples. Also this example is not clear. How output is of dimension [2,2]? What normalization is doing here?",1,Lack of Alternative Solutions/Documentation
191,51625529,How to use tf.data's initializable iterator and reinitializable interator and feed data to estimator api?,"All the official google tutorials use the one shot iterator for all the estimator api implementation, i couldnt find any documentation on how to use tf.data's initializable iterator and reinitializable interator instead of one shot iterator. Can someone kindly show me how to switch between train_data and test_data using tf.data's initializable iterator and reinitializable interator. We need to run a session to use feed dict and switch the dataset in the initializable iterator, its a low level api and its confusing how to use it part of estimator api architecture PS : I did find that google mentions ""Note: Currently, one-shot iterators are the only type that is easily usable with an Estimator."" But is there any work around within the community? or should we just stick with one shot iterator for some good reason",1,Lack of Alternative Solutions/Documentation
192,51687832,Probability Distribution for tf.nn.softmax_cross_entropy_with_logits_v2,"I am trying to understand the Tensorflow documentation better for tf.nn.softmax_cross_entropy_with_logits_v2(). In the documentation, it states: While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution. If they are not, the computation of the gradient will be incorrect. Does this mean that, for my labels, I shouldn't be simply using one-hot encoding, but should also account for the number of instances of each label? For example, if I have 2 classes, and there are 90 examples for class ""A"" and only 10 examples for class ""B"", should my label for a class A be [0.9, 0.1], instead of just [1, 0]? I hope this makes sense. Thanks!",1,Documentation Replication on Other Examples
193,51691199,How does tf.create_partitioned_variables work?,"I am trying to figure out how to use tf.create_partitioned_variables I am reading the documentation but I am having a hard time understanding. Could anyone explain how it works and give some examples of its usage? From what I understand I can use it to get a list of slices from a variable. I just dont understand how I get the slices ex: how would i get a list of [[1.],[3.]] from tf.Variable(np.array([[1.0],[3.0]]), dtype=tf.float32) or list of from",1,Inadequate Examples
194,51706848,How does tf.reshape() work internally ?,"I'm trying to understand how tf.reshape works. Let's have an example: Here I have a 2D tensor M_2D whose columns represent coefficients for the N0 embeddings of dimension N1. I want to create a 3D tensor where each column of M_2D is placed in the first dimension of M_3D, and columns are keep in the same order. My final goal is to create a 3D tensor of 2D embeddings, each weighted by the columns of M_2D. How can I be sure that reshape actually place each column in the new dimension of M_3D. Is it possible that it places the rows instead ? Is there somewhere in tensorflow documentation a clear explanation on the internal working process of tf.reshape, particularly when -1 is provided ?",1,Documentation Completeness
195,51776390,how to use tensorboard debugger with datalab which uses tf.estimator on google cloud platform,"When I start tensorboard via datalab it uses the google syntax which is described here. This document only mentions start, stop and list. However, there is a debugger pane which I can not use. This document describes how to use tensorboard debugger with a tf.estimator but it uses a different syntax. Is there someway to blend the two so the debugger is usable with datalab?",1,Documentation Replication on Other Examples
196,51806852,Can't save custom subclassed model,"Inspired by tf.keras.Model subclassing I created custom model. I can train it and get successfull results, but I can't save it. I use python3.6 with tensorflow v1.10 (or v1.9) Minimal complete code example here: Error message: I looked into the error line and found out that get_config method checks self._is_graph_network Do anybody deal with this problem? Thanks! Update 1: On the keras 2.2.2 (not tf.keras) Found comment (for model saving) file: keras/engine/network.py Function: get_config So, obviously it won't work... I wonder, why don't they point it out in the documentation (Like: ""Use subclassing without ability to save!"") Update 2: Found in keras documentation: So, there is no way to save model by using subclassing. It's possible to only use Model.save_weights()",1,Requesting (Additional) Documentation/Examples
197,51824310,Difference between Keras and tensorflow implementation of LSTM with dropout,"I was reviewing the documentation for the LSTM cell in tensorflow and Keras. In particular, I want to apply dropout as well. Here is what I have in Keras and would like to apply the same LSTM cell in tensorflow: Therefore, I know that I need to use tf.nn.rnn_cell.LSTMCell in tensorflow with num_units = num_units_2. Second, I need a DropoutWrapper as: Now, I want to apply dropout and recurrent_dropout similar to the Keras code. Therefore, I found that tensorflow's implementation of dropout will apply a different dropout mask at every time step unless variational_recurrent is set to True (Yet I'm not sure how variational_recurrent works in details). Additionally, I'm not sure if the LSTM in Keras apply different Mask at each time step as well. Second, I was confused about the difference between the output_keep_prob and the state_keep_prob as both mention: output_keep_prob: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added... Any help is much appreciated!!",1,Documentation Replication on Other Examples
198,51856041,Input dimension for tf.nn.in_top_k,"I am following the TF documentation with respect to in_top_k: https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k where it states that targets should be a vector of batch size Nevertheless, I'm continuously prompted with the following error: In my case, my predictions and targets inputs have the following shapes: From my understanding, there is still something not ok with my targets label, and despite using different combinations of tf.reshape or tf.squeeze I cannot seem to find where is the error. Is there any way to work around this issue?",1,Documentation Ambiguity
199,51858970,"tf.gradients() sums over ys, does it?","https://www.tensorflow.org/versions/r1.6/api_docs/python/tf/gradients In the documentation for tf.gradients(ys, xs) it states that I am confused about the summing part, I have read elsewhere that this sums the derivatives dy/dx across the batch for every x in the batch. However, whenever I use this I fail to see this happening. Take the following simple example: This gives the following output: This is the output I would expect, simply the derivative dy/dx for every element in the batch. I don't see any summing happening. I have seen in other examples that this operation is followed by dividing by the batch size to account for tf.gradients() summing the gradients over the batch (see here: https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html). Why is this necessary? I am using Tensorflow 1.6 and Python 3.",1,Documentation Ambiguity
200,51859776,lambda layer function definition without tf.keras.backend (Python Keras Package),"tf.keras.layers.Lambda documentation explains how a function can be defined in a lambda layer. That document provides the following function as an example, But according to that, tf.keras.backend must be used to conduct operations on the input Tensor object. Is there any way we can use default python packages and user-defined function to define the steps of a lambda function. If it's possible, please be kind enough to provide some examples.",1,Inadequate Examples
201,51883196,Tensorflow: tf.reverse_sequence - seq_dim and batch_dim,"I am trying to learn Tensorflow and was looking at tf.reverse_sequence. It has two parameters seq_dim and batch_dim. From the documentation given here I understand that setting batch_dim = 0 means we go through the the rows from top and setting seq_dim = 1 means we go through columns from left to right but what do these numbers mean? I can't understand from the documentation when I should set batch_dim = 1 or 2. I tried reverse_sequence on x = [[1,2,3], [4,5,6], [7,8,9]] and got [[3,2,1], [6,5,4], [9,8,7]] with batch_dim = 0 and seq_dim = 1. But I always get errors when I change seq_dim and batch_dim values from 1 and 0 respectively. Could someone explain the meaning of these values?",1,Lack of Alternative Solutions/Documentation
202,51971050,Graph optimizations on a tensorflow serveable created using tf.Estimator,"Context: I have a simple classifier based on tf.estimator.DNNClassifier that takes text and output probabilities over an intent tags. I am able to train an export the model to a serveable as well as serve the serveable using tensorflow serving. The problem is this servable is too big (around 1GB) and so I wanted to try some tensorflow graph transforms to try to reduce the size of the files being served. Problem: I understand how to take the saved_model.pb and use freeze_model.py to create a new .pb file that can be used to call transforms on. The result of these transforms (a .pb file as well) is not a servable and cannot be used with tensorflow serving. How can a developer go from: There's documentation that suggests that this is certainly possible, but its not at all intuitive from the docs as to how to do this. What I've Tried: My idea was to load the servable, extract the graph_def from the meta_graph_def, transform the graph_def and then try to recreate the servable. This seems to be the incorrect approach. Is there a way to successfully perform transforms (to reduce file size at inference) on a graph from an exported servable, and then recreate a servable with the transformed graph? Thanks. Update (2018-08-28): Found contrib.meta_graph_transform() which looks promising. Update (2018-12-03): A related github issue I opened that seems to be resolved in a detailed blog post which is listed at the end of the ticket.",1,Lack of Alternative Solutions/Documentation
203,51997426,TensorFlow: alternate between datasets of different output shapes,"I'm trying to use tf.Dataset for a 3D image CNN where the shape of the 3D image fed into it from the training set and the validation set are different (training: (64, 64, 64), validation: (176, 176, 160)). I didn't even know this was possible, but I'm recreating this network based on a paper, and using the classic feed_dict method the network indeed works. For performance reasons (and just to learn) I'm trying to switch the network to use tf.Dataset instead. I have two datasets and iterators built like the following: TensorFlow documentation has examples regarding switching between datasets using reinitializable_iterator or feedable_iterator, but they all switch between iterators of same output shape, which is not the case here. How should I switch between training set and validation set using tf.Dataset and tf.data.Iterator in my case then?",1,Lack of Alternative Solutions/Documentation
204,52035692,Tensorflow v1.10: store images as byte strings or per channel?,"It is known that, at the moment, TF's Record documentation leaves something to be desired. My question is in regards to what is optimal for storing: as a TF Record. Namely, this questions considers storing the sequence and class probabilities as channels vs as a byte string and whether or not the meta information should go in as features of a tf.train.Example or as the context of a tf.train.SequenceExample. (see questions at the bottom). For example, lets assume my looks sequence like this i.e. it is a 2 channel sequence of fixed length (in this example, 2) where the values can only be integer value. and that we have three classes for which we are trying to segment the sequence into where in effect both seq and cls_probs are numpy.arrays. The network only requires this information. However, I also have some meta data which I would like to keep with the sequence. e.g. Then I have several ways I could construct my tf.train.Example: where f'{variable}'.encode('utf-8') is the currently not suggested fb'&lt;string&gt;' (note: f-strings only work with python3.6+). This format is somewhat nice as each sequence channel is explicit. However it is also verbose and requires preprocessing when loaded to be feed into the network. or, I could dump my array to an string TF Records also accept another form: tf.train.SequenceExample. SequenceExample expects context features and an ordered list of unnamed features. So restructuring above's as channels example: likewise we can create the as string example: Here I gave a M.W.E. for how one could construct an example (ready to be exported to a TF Record) as both tf.train.Example and tf.train.SequenceExample. Further, I demonstrated both how to do this per channel or by dumping as a byte string. Both of these methods (as channels / as strings) include the meta information within the example. Thus my questions are: Does anyone know if there are any notable advantages / disadvantages for any of four these strategies? For those who would like to test this on larger less dummy like data, some functions for producing this code can be found below Lastly, I would like to point out this medium post which greatly elaborates on TF's docs.",1,Documentation Replication on Other Examples
205,52046902,cudnnLSTM won't restore into a cudnnCompatibleLSTM,"I'm trying to train an elementary network on a GPU machine (AWS p3x2, Volta) with TF 1.9 / 1.10. Not Keras -- TF only. Based on the [rather limited] documentation my aim is to train with cudnnLSTM cell, save a checkpoint, and then restore for inference on a CPU. Per that aim, I thought that cudnnCompatibleLSTM is the way to go as it is supposed to suck in the weights from the GPU-specific LSTM implementation. I get the following error, no matter what I try: Another related issue is that cudnnCompatibleLSTM and cudnnLSTM are not the same mathematically. I get different results for initialized cells. [initialized by some tf.constant() as initializer, no save/restore]. Seems that cudnnLSTM does depend on the random seed [dropout is zero], which means that there are some unique tensor/tensor initialization going on, separating it from cudnnCompatibleLSTM. Does anybody have a clue?",1,Documentation Replicability
206,52073782,Computations (such as tf.greater and tf.cond) on random value tensors not working as expected,"I am a tensorflow beginner. According to the documentation, tf.greater returns the truth value of (x&gt;y) element-wise My code is as below: The output I got is: x is bigger than y so it should return True and x+y should be 1.10271299 why is my expected output different than the actual output?",1,Documentation Replication on Other Examples
207,52234780,Reading from .tfrecord files using tf.data.Dataset,"I want to read the dataset generated by this code with the tf.data.Dataset api. The repo shows it was written like this: with (encoded byte-string, b'png', 32, 32, label) as parameters. So, to read the .tfrecord file, the data format would have to be: But it doesn't work. The dataset is empty after reading and generating an iterator with it raises OutOfRangeError: End of sequence. A short python script for reproduction can be found here. I'm struggling to find exact documentation or examples for this problem.",1,Documentation Replication on Other Examples
208,52254253,How does tf.layers.dense() interact with inputs of higher dim?,"In tensorflow layers.dense(inputs, units, activation) implements a Multi-Layer Perceptron layer with arbitrary activation function. Output = activation(matmul(input, weights) + bias) Typically input has shape=[batch_size, input_size] and might look like this: (units = 128 and activation = tf.nn.relu are chosen arbitrarily) I have not found any documentation on what would happen, if i fed higher dimensional input, e.g. because one might have time_steps resulting in a tensor of shape=[time_step, batch_size, input_size]. What one would want here is that the layer is applied to each single input_vector for each timestep for each element of the batch. To put it a bit differently, the internal matmul of layers.dense() should simply use broadcasting in numpy style. Is the behaviour i expect here what actually happens? I.e. is: applying the dense layer to each input of size input_size for each time_step for each element in batch_size? This should then result in a tensor(in dense_layer above) of shape=[time_step, batch_size, 128] I'm asking, as e.g. tf.matmul does not support broadcasting in the numpy style, so i'm not sure, how tensorflow handles these cases. Edit: This post is related, but does not finally answer my question",1,Lack of Alternative Solutions/Documentation
209,52319765,Swap a TensorFlow Dataset input pipeline with a placeholder after training,"I'm working with the new tf.data.Dataset API and I can't seem to figure out how to perform inference. Ultimately, I want to convert my model to a TensorRT graph and run it on the TX2, and all of the examples I have found assume you have a tf.placeholder for the input. Here is pseudocode for how I am training. The [...] is just meant to be a placeholder since I didn't actually run the code. Let's not debate the model, as it is just suppose to give an example: My question is how do I get this into TensorRT without having the input be a tf.placeholder? All of the example I can find use a tf.placeholder as the input. This example suggests that I can replace the iterator with a placeholder using the SavedModel class, but I cannot seem to find any documentation on how to accomplish that. Thanks! EDIT: Here is my solution thanks to the help below that will write out a UFF file that TensorRT can utilize. The biggest issues that I encountered was:",1,Lack of Alternative Solutions/Documentation
210,52473088,How tf.Variable maintains state of the graph?,I am trying to learn tensorflow. I am really confused with the usage of tf.Variable . I know that in machine learning we have to randomly assign weights to the filter. But this can be done with tf.truncated_normal function. Then what is the role of tf.Variable here? Documentation states that tf.Variable maintains the state of graph. What does it mean? If I omit tf.Variable result is same. So what is the role of tf.Variable? Can someone please help me to understand this?,1,Lack of Alternative Solutions/Documentation
211,52533156,Weight Initialization Tensorflow tf.estimator,Is there a way to adjust the weight initialization in the pre-built tf.estimator? I would like to use the method after Xavier (tf.contrib.layers.xavier_initializer) or from He. Which method is used by default? I couldn't figure it out from the documentation. I use the DNNRegressor.,1,Documentation Ambiguity
212,52572275,tensorflow: how to interleave columns of two tensors (e.g. using tf.scatter_nd)?,"I've read the tf.scatter_nd documentation and run the example code for 1D and 3D tensors... and now I'm trying to do it for a 2D tensor. I want to 'interleave' the columns of two tensors. For 1D tensors, one can do this via (aside: is there a better way to do this? I'm all ears.) This gives the output [ 1 2 10 3 4 11 5 6 12] Yay! that worked! Now lets' try to extend this to 2D. This gives the error ValueError: The outer 1 dimensions of indices.shape=[6,1] must match the outer 1 dimensions of updates.shape=[3,6]: Dimension 0 in both shapes must be equal, but are 6 and 3. Shapes are [6] and [3]. for 'ScatterNd_2' (op: 'ScatterNd') with input shapes: [6,1], [3,6], [2]. Seems like my indices is specifying row indices instead of column indices, and given the way that arrays are ""connected"" in numpy and tensorflow (i.e. row-major order), does that mean I need to explicitly specify every single pair of indices for every element in updates1? Or is there some kind of 'wildcard' specification I can use for the rows? (Note indices1 = tf.constant([[:,0], [:,1], [:,3], [:,4], [:,6], [:,7]]) gives syntax errors, as it probably should.) Would it be easier to just do a transpose, interleave the rows, then transpose back? Because I tried that... ...and got a much longer error message, that I don't feel like posting unless someone requests it. PS- I searched to make sure this isn't a duplicate -- I find it hard to imagine that someone else hasn't asked this before -- but turned up nothing.",1,Documentation Replication on Other Examples
213,52597523,How to load_weights to a Keras model from a Tensorflow checkpoint,"I have some python code to train a network using Tensorflow's TFRecords and Dataset APIs. I have built the network using tf.Keras.layers, this being arguably the easiest and fastest way. The handy function model_to_estimator() converts a Keras model to an estimator, which allows us to take advantage of the Dataset API nicely, and automatically save checkpoints to checkPointDirectory during training, and upon training completion. The estimator API presents some invaluable features, such as automatically distributing the workload over multiple GPUs, with, e.g. Now for big models and lots of data, it is often useful to execute predictions after training using some form of saved model. It seems that as of Tensorflow 1.10 (see https://github.com/tensorflow/tensorflow/issues/19295), a tf.keras.model object supports load_weights() from a Tensorflow checkpoint. This is mentioned briefly in the Tensorflow docs, but not the Keras docs, and I can't find anyone showing an example of this. After defining the model layers again in some new .py, I have tried but this gives a NotImplementedError: I would like to do as suggested by the Warning and use the 'object-based saver' instead, but I haven't found a way to do this via a RunConfig passed to estimator.train(). So is there a better way to get the saved weights back into an estimator for use in prediction? The github thread seems to suggest that this is already implemented (though based on the error, probably in a different way than I am attempting above). Has anyone successfully used load_weights() on a TF checkpoint? I haven't been able to find any tutorials/examples on how this can be done, so any help is appreciated.",1,Inadequate Examples
214,52711895,How to run define Tensorflow graph were all variables are in float16 instead instead of float32,"By default, the variables Tensorflow is in float32. To save memory, I'm trying to run in float16. In my graph, every place where I could define the datatype as float16, I did. However, I get an error when I run the code Here's my code below. And this is this is the error message The error comes from line tf.nn.sampled_softmax_loss. At first I thought perhaps tf.segment_mean may cast the output as a float32, so I tried casting averaged_embeds to float16 but I still get the same error. From the documentation, there doesn't seem to be a way to define any data types in sampled_softmax_loss https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss",1,Lack of Alternative Solutions/Documentation
215,52731151,Tesnorflow: How to provide your own `sampled_values` for tf.nn.sampled_softmax_loss?,"In tf.nn.sampled_softmax_loss, one of the optional inputs is to put your own samples values. I would like to provide my own samples values so that I can use float16 (half precision) variables. If sampled_values is left blank, Tensorflow will use log_uniform_candidate_sampler to get values, which can only return float32. Here are all the inputs. https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss This is the information they give for the sampled_values arg : I'm trying to figure out how to provide this tuple. What exactly are the sampled_candidates, true_expected_count, sampled_expected_count ? I know that it's sampling the weights and corresponding biases, so do I put them together in it's own tuple for sampled_candidates ? Also, am I putting the int for the place of the weight in the matrix, or am I putting the whole embedding itself? I've also looked at Tensorflow's math supplimental on negative sampling but I couldn't find any information for my issue https://www.tensorflow.org/extras/candidate_sampling.pdf In my search, I found this very similar question on a google forum https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/6IDJ-XAIb9M The Answer given is However, I still don't know how to input a value. I'm not sure what the datatypes should be, and if N is the int place in the embedding matrix, or the embedding itself. Also, I'm guessing N should be a list of values itself, the size of the number of negative labels we have to sample. I was wondering if I could get a example with some values. For example, for a negative sampling of 3, do I do something like this? sampled_values = ([4,29, 12], [1, 1, 1], [0, 0, 0]) Also, the documentation says that the tuple should be "" returned by a *_candidate_sampler function"" Does that mean I need to provide a function that returns the tuple, instead of the tuple itself?",1,Documentation Replication on Other Examples
216,52802359,Problem with tf.SparseTensor and tf.while_loop,"I face a problem when I try to change the shape of tf.SparseTensor inside a tf.while_loop. Let's say I have this sparse tensor: So, I want to take a slice from the first 3 rows. I know for that purpose I can use tf.sparse_slice but this is an example. In my real code, I gather multiple rows from the sparse Tensor which they are not serial. The code I wrote is this: which does't work for some reason when I run it. I get this: According to https://www.tensorflow.org/api_docs/python/tf/while_loop it says that: The shape_invariants argument allows the caller to specify a less specific shape invariant for each loop variable, which is needed if the shape varies between iterations. The tf.Tensor.set_shape function may also be used in the body function to indicate that the output loop variable has a particular shape. The shape invariant for SparseTensor and IndexedSlices are treated specially as follows: a) If a loop variable is a SparseTensor, the shape invariant must be TensorShape([r]) where r is the rank of the dense tensor represented by the sparse tensor. It means the shapes of the three tensors of the SparseTensor are ([None], [None, r], [r]). NOTE: The shape invariant here is the shape of the SparseTensor.dense_shape property. It must be the shape of a vector. What am I missing here?",1,Documentation Replication on Other Examples
217,52814880,Neuron freezing in Tensorflow,"I need to implement neurons freezing in CNN for a deep learning research, I tried to find any function in the Tensorflow docs, but I didn't find anything. How can I freeze specific neuron when I implemented the layers with tf.nn.conv2d?",1,Documentation Replicability
218,52878311,How to extract rows and columns from a 3D array in Tensorflow,"I wanted to do the following indexing operation on a TensorFlow tensor. What should be the equivalent operations in TensorFlow to get b and c as output? Although tf.gather_nd documentation has several examples but I could not generate equivalent indices tensor to get these results. I am not restricted to tf.gather_nd Any other suggestion to achieve the same operations on GPU will be helpful. old statement: c=a[:,idx], New statement: c=a[:,:,idx] What I wanted to achieve was re-ordering of columns as well.",1,Inadequate Examples
219,52956268,tf.data.Dataset: Map fails to split string,"I have a tf.data.Dataset that I've created like this: I want to split just the reviews (strings) on whitespace. When I do this: Python complains, telling me: I've looked at the docs and it's not obvious why Python thinks I've given two arguments...any ideas? Thanks!",1,Documentation Replicability
220,52976606,Global step not incrementing with batch norm and custom estimator,"I have a customer estimator that has several layers that look like the following in the model function: Because I'm using batch norm, the training op is set up like this: Where the optimizer is usually tf.train.AdamOptimizer. However, when I go to train the estimator the global step never increments (so training will run forever), and I get this: WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize. I am passing tf.train.get_global_step() to minimize, so I'm not sure why it never gets updated. My hunch is that it has something to do with the batch normalization because when I remove that or replace it with dropout, everything works fine (even when keeping the update ops lines that are required for batch normalization per the documentation). Anyone know what is going on? Happy to post more code if helpful.",1,Documentation Replication on Other Examples
221,53032922,TensorFlow while loop with condition dependent on body,"I want to have a while loop with the condition dependent on a tensor computed in the loop body, but I don't know how to accomplish this with tf.while_loop(). My input processing includes random cropping, but some crops can lead to low-quality examples and I want to discard those and try a new random crop until an example of sufficient quality is obtained. The inputs are cropped by and the condition is Going over the documentation and examples of tf.while_loop(cond, body, loop_vars, ...), what I understand is that both cond and body should take the same arguments given in loop_vars. I don't see how I can have cond depend on img_crop which would be calculated inside body, and isn't provided in loop_vars. I could equivalently compute cond using crop_begin_index without actually cropping, but it depends on the random values computed inside the loop, so I have the same problem. Is this indeed a limitation of TF looping? If not, how can I rewrite my code to use tf.while_loop()?",1,Documentation Replication on Other Examples
222,53053649,Tensorflow: Truncating a Tensor Seems to Have No Effect,"I'm using the tf.data.Dataset API and am trying to truncate a bunch of tensors to length 100. Here's what my dataset looks like: My reviews are just movie reviews (strings), so I perform some preprocessing and map that function on my dataset: However, my code fails with: On an input of length 240. So, it seems like my padding step calculates 100 - 240 = -140 and I get this error. Here's my question: how is this possible, given that I truncate to length 100 with: It seems clear that this line isn't having any effect, so I'm trying to understand why. The docs are very clear that this is acceptable syntactic sugar for tf.slice: Any ideas? Thanks!",1,Documentation Ambiguity
223,53079436,tensorflow Tf.cond giving unexpected output,"I seem to be having a misunderstanding on how tf.cond works. In the tensorflow documentation, it gives the following example: The result of the example, if x&lt;y is True is tf.add(x,z) else tf.square(y) Following this example, I am trying to build a small example with tf.cond and the result doesnt go along the lines mentioned in the documentation. in my example, deterministic_action = 4, random_action = 11, chose_random=False. The stochastic_action should be 4, instead it is 1. Where did the value 1 come from? here is the output:",1,Documentation Replication on Other Examples
224,53167302,Does tf.keras.layers.Conv2D as first layer in model truly need input_shape?,"According to the official document on tf.keras.layers.Conv2D, but actually without input_shape it does work in both graph execution and eager execution environment. In graph execution, works without any error and in eager execution, also does. Q1: Does tf.keras.layers.Conv2D as the first layer in a model truly need to specifying input_shape? Q2: If not, when is it needed and why is it mentioned so in the official document? UPDATE1: Tutorial on tf.keras says UPDATE2: git blame of docstring in TensorFlow source revealed that this document is copied from Keras API (which is not TensorFlow keras API).",1,Documentation Replication on Other Examples
225,53206900,Sound way of managing multiple sessions and graphs,"I'd like to manage multiple Keras models in multiple sessions. My application is constructed such that models can be live at the same time, in addition to creating, saving and loading them. What is the proper way of managing this situation? Currently one model is represented by an instance of a wrapper class. This is used in the training, saving, loading and prediction. One tf.Graph and tf.Session is created per instance, and they are used in every function requiring the actual model. Similar functions using the with statements are created for compiling the network, fitting, saving (weights to .h5 and model to JSON) and loading. So whenever the model is needed, the graph and session are brought to context. This resulted in a strange error (Q for further context), and I was left wondering, what is the standard way of dealing with this. I tried to release all possible resources before creating or loading a model, but it hasn't helped. This function is just a compilation of all possible routines scraped off the internet, and is purely guesswork. I've not found good documentation of a similar situation. So I'd very much appreciate any real insight into this. I might need to delete the old question, as it's quite all over the place. At the time of asking I had no idea what was going on. But it's there for now. Some specific questions have arisen. The underlying issue with the error has been finally (and somewhat embarassingly) resolved by updating all packages.",1,Lack of Alternative Solutions/Documentation
226,53272508,inception v3 using tf.data?,"I'm using a bit of code that is derived from inception v3 as distributed by the Google folks, but it's now complaining that the queue runners used to read the data are deprecated (tf.train.string_input_producer in image_processing.py, and similar). Apparently I'm supposed to switch to tf.data for this kind of stuff. Unfortunately, the documentation on tf.data isn't doing much to relieve my concern that I've got too much data to fit in memory, especially given that I want to batch it in a reusable way, etc. I'm confident that the tf.data stuff can do this; I just don't know how to do it. Can anyone point me to a full example of code that uses tf.data to deal with batches of data that won't all fit in memory? Ideally, it would simply be an updated version of the inception-v3 code, but I'd be happy to try and work with anything. Thanks!",1,Lack of Alternative Solutions/Documentation
227,53307954,TensorFlow Custom Estimator predict throwing value error,"Note: this question has an accompanying, documented Colab notebook. TensorFlow's documentation can, at times, leave a lot to be desired. Some of the older docs for lower level apis seem to have been expunged, and most newer documents point towards using higher level apis such as TensorFlow's subset of keras or estimators. This would not be so problematic if the higher level apis did not so often rely closely on their lower levels. Case in point, estimators (especially the input_fn when using TensorFlow Records). Over the following Stack Overflow posts: and with the gracious assistance of the TensorFlow / StackOverflow community, we have moved closer to doing what the TensorFlow ""Creating Custom Estimators"" guide has not, demonstrating how to make an estimator one might actually use in practice (rather than toy example) e.g. one which: While I still have many questions regarding this (from the best way to encode data into a TF Record, to what exactly the serving_input_fn expects), there is one question that stands out more prominently than the rest: How to predict with the custom estimator we just made? Under the documentation for predict, it states: (perhaps) Most likely, if one is using estimator.predict, they are using data in memory such as a dense tensor (because a held out test set would likely go through evaluate). So I, in the accompanying Colab, create a single dense example, wrap it up in a tf.data.Dataset, and call predict to get a ValueError. I would greatly appreciate it if someone could explain to me how I can:",1,Requesting (Additional) Documentation/Examples
228,53466500,Can't save and load a trained CNN model for binary image classification,"I have built a Binary Image classifier using Convolutional Neural Networks using TensorFlow.It is running fine, however, each time it takes too long to train from scratch. So, I want to save the trained model and load it next time. I can't seem to understand how to implement these guides in my program as shown in the TensorFlow documentation. Here's the full code: I have tried to use saver = tf.train.import_meta_graph('D:\\Project\\Final_Project\\chest_xray\\check_point-78.meta') to import the graph but I get this error Process finished with exit code 1",1,Documentation Replicability
229,53569622,Difference between tf.train.Checkpoint and tf.train.Saver,"I found there are different ways to save/restore models and variables in Tensorflow. These ways including: In tensorflow's documentations, I found some differences between them: How tf.train.Checkpoint can load graph without .meta file? or more generally What is the difference between tf.train.Saver and tf.train.Checkpoint?",1,Documentation Ambiguity
230,53572533,What is the second argument of TensorFlow's tf.data.filter() that I find no documentation of?,"I recently had a TypeError when using in The exact error was TypeError: lie_filter() takes 1 positional argument but 2 were given. Simply changing the function signature to lie_filter(line, x) made the error go away and the filtering appears to work as intended. However, it left me wondering what is this mysterious second argument. TensorFlow manual for tf.data.filter() only specifies one argument. There are also numerous examples by TensorFlow where filtering is done as per my attempt above. Take a look at, e.g., imports85.py. Printing the x inside lie_filter yields Tensor(""arg12:0"", shape=(), dtype=float32). What is the second argument and where can I find documentation about it? Thank you!",1,Documentation Completeness
231,53583456,What problem does a reinitializable iterator solve?,From the tf.data documentation: the following example was given: It is unclear what the benefit of this complexity is. Why not simply create 2 different iterators?,1,Documentation Ambiguity
232,53612973,TensorFlow Sigmoid Cross Entropy with Logits for 1D data,"Suppose we have some 1D data (e.g. time series), where all series have fixed length l: and we want to perform semantic segmentation, with n classes: then the output for a single example has shape [n, l] (i.e. the data_format is not ""channels_last"") and the batched output has shape [b, n, l], where b is the number of examples in the batch. These classes are independent, so it is my understanding that the use sigmoid cross entropy is applicable here as the loss rather than softmax cross entropy. I have a few small related questions in regards to the expected format for and use of tf.nn.sigmoid_cross_entropy_with_logits: This Colab, highlights my confusion and demonstrates that the data_format does in fact matter..., but the documentation does not explicitly state which is expected.",1,Lack of Alternative Solutions/Documentation
233,53677345,Passing >2GB data to tf.estimator,"I have x_train and y_train numpy arrays, each of &gt;2GB. I want to train model using the tf.estimator API, but I am getting the errors: I am passing the data using: The tf.data documentation mentions this error and provides solution using traditional TenforFlow API with placeholders. Unfortunately, I don't know how this could be translated into tf.estimator API?",1,Documentation Replication on Other Examples
234,53915078,"What are b, y, x and c which get flattened and returned along with the max-pooled features in tf.nn.max_pool_with_argmax?","I went through the documentation of tf.nn.max_pool_with_argmax where it is written The variables b, y, x and c haven't been explicitly defined hence I was having issues implementing this method. Can someone please provide the same.",1,Documentation Replicability
235,53919290,tensorflow sparse categorical cross entropy with logits,"I am a novice programmer trying to follow this guide. However, I ran across an issue. The guide says to define the loss function as: This gives me the following error: which I take to mean that from_logits is an argument not specified in the function, which is supported by the documentation, which that tf.keras.losses.sparse_categorical_crossentropy() has only two possible inputs. Is there a way to specify that logits are being used or is that even necesarry?",1,Documentation Replication on Other Examples
236,53922040,How does tf.keras.layers.Conv2DTranspose behave with stride and padding?,"While a convolution layer in TensorFlow has a complete description https://www.tensorflow.org/api_guides/python/nn#Convolution, transposed convolution does not have one. Although tf.keras.layers.Conv2DTranspose has a reference to https://arxiv.org/pdf/1603.07285.pdf, it is not complete. Is there any documentation that describes how tf.keras.layers.Conv2DTranspose behaves?",1,Documentation Completeness
237,53924692,Why can tf.random.truncated_normal get a shape that is not a vector even though it says it only receives shape of a vector?,"I am working with TensorFlow in Python. I read through the documentation of tf.random.truncated_normal that the input 'shape' gets 1-D tensor or python array, i.e. a vector (according to https://www.tensorflow.org/guide/tensors). However, with the example I'm using, 'shape' is a 4-D tensor. Or is it considered a vector? Perhaps I have problem with the definition of vectors and tensors?",1,Documentation Replicability
238,54096360,How to properly use tf.scatter_update for N-Dimensional Updating?,"I've been trying to make an N-Dimensional update using tf.scatter_update (after tf.scatter_nd was failing due to shape mismatch). In general, these will be used to create masks for filtering slices of an incoming tensor. Presumption is that input Tensor A is of shape (batch, i, j, k(depth)). I am only interested in modifying i,j values for all k, and for all b. MWE: Resulting in: I have tried this via Python Script, Python Notebook, and with/without Eager Execution. No luck. Input absolutely must be a tensor, as the idea is to sparsely update this tensor midway through a series of operations. Is there something fundamental I'm missing regarding tf.scatter_update? Would tf.scatter_nd be more suited? If so, what are the differences, specifically with indices for the updates. When referencing tf.scatter_update documentation, the examples are basic and utilise constants; I'm having difficulty applying this to a more realistic situation and problem.",1,Inadequate Examples
239,54183967,Using tf.map_fn with multiple GPUs,"I'm trying to extend my single-GPU TensorFlow code to multi-GPU. I have to work on 3 degrees of freedom and unfortunately I need to use tf.map_fn to parallelize over the 3rd one. I tried to use device placement as shown in the official documentation, but it looks like it is impossible to do it with tf.map_fn. Is there a way to run tf.map_fn on multiple GPUs? Here the error output: Here a simple code example to reproduce it:",1,Documentation Replicability
240,54524992,Tensorflow serving trained model saved with saved_model,"I find tf.saved_model documentation not clear, is there any valuable resources how to read trained model within other session?",1,Documentation Replication on Other Examples
241,54686895,Tensorflow dilation behave differently than morphological dilation,"As the following piece of code shows, the tensorflow tf.nn.dilation2D function doesn't behave as a conventional dilation operator. Returns the following tensor: I don't understand neither why it behaves like that, neither how I should use tf.nn.dilation2d to retrieve the expected output: Can someone enlighten the succinct documentation of tensorflow and give an explanation of what the the tf.nn.dilation2D function does ?",1,Documentation Ambiguity
242,54897832,Feeding large numpy arrays into TensorFlow estimators via tf.data.Dataset,"TensorFlow's tf.data.Dataset documentation on consuming numpy arrays states that in order to use numpy arrays in combination with the Dataset API, the arrays have to be small enough (&lt;2 GB in total) to be used as tensors, or they can be fed into the dataset via placeholders. However, if you use Dataset in conjunction with estimators (where placeholders are not available), the documentation does not provide a solution on working with large arrays without placeholders. Are there other options for passing placeholder values into estimators that can be used or is the solution to provide the data in tfrecord or csv format?",1,Inadequate Examples
243,54934603,"tensorflow documentation says ""WARNING: Avoid writing code which relies on the value of a Variable..."" what does it mean?",The tf.Variable documentation contains the following warning: I don't quite understand what time means and why the example above is broken. What does it mean that one cannot rely on the value of a Variable? Is it possible to have an example where the code above works not as expected?,1,Requesting (Additional) Documentation/Examples
244,54945641,"keras, model still setting expected input shape from training despite input_shape(None, ...)","I have a simple CNN model written in the tf.keras framework, which I wish to use with variable input size. According to this ""documentation"" I can use variable input size by setting input_shape=(None, None, n_channels), and I have used a GlobalMaxPooling2D layer before my dense layer to standardize the input to the dense layer. Yet when I train the model with one size of image and try to predict on a different size I get the error: This is the code used to define my model: So in essence my question is why is keras still defining an expected input shape, and is there any way to disable this implicit standardize_input_data that's going on?",1,Documentation Replicability
245,54966581,tf.boolean_mask not accepting the axis argument,"Here is my code: Error, I'm getting: The tf.boolean_mask() is not accepting the axis argument but is a valid argument as can be seen in the documentation: https://www.tensorflow.org/api_docs/python/tf/boolean_mask",1,Documentation Ambiguity
246,54985037,How to convert TF Tensor holding value into Tensor holding categorical values,I'm paring TFRecords which provide me a label as numerical value. But I need to convert this value into categorical vector while I'm reading proto records. How can I do that. Here is code snippet for reading the proto records: I know that there is tf.keras.utils.to_categorical function but it does not take a Tensor as an input.,1,Documentation Replication on Other Examples
247,54989442,"RNN in Tensorflow vs Keras, depreciation of tf.nn.dynamic_rnn()","My question is: Are the tf.nn.dynamic_rnn and keras.layers.RNN(cell) truly identical as stated in docs? I am planning on building an RNN, however, it seems that tf.nn.dynamic_rnn is depricated in favour of Keras. In particular, it states that: But I don't see how the APIs are equivalent, in the case of variable sequence lengths! In raw TF, we can specify a tensor of shape (batch_size, seq_lengths). This way, if our sequence is [0, 1, 2, 3, 4] and the longest sequence in the batch is of size 10, we can pad it with 0s and [0, 1, 2, 3, 4, 0, 0, 0, 0, 0], we can say seq_length=5 to process [0, 1, 2, 3, 4]. However, in Keras, this is not how it works! What we can do, is specify the mask_zero=True in previous Layers, e.g. the Embedding Layer. This will also mask the 1st zero! I can go around it by adding ones to the whole vector, but then thats extra preprocessing that I need to do after processing using tft.compute_vocabulary(), which maps vocabulary words to 0 indexed vector.",1,Documentation Replication on Other Examples
248,55005915,How to resize image to put into tf.train.Example,"I have an image (JPEG or PNG) as a byte buffer (read from the internet), and this is the way I was putting it in a tf.train.Example before: However, for my usecase, the images are too big, so I'd like to resize them either before I put them in the tf.train.Example or just after (whichever is easiest). Here's what I'm trying: I suspect this is valid right up until I actually try to put it in the tf.train.Example, at which point it tells me TypeError: &lt;tf.Tensor 'EncodeJpeg:0' shape=() dtype=string&gt; has type Tensor, but expected one of: bytes. I've tried figuring out how to get the Tensor into a BytesList or something like it, but I haven't been able to find any documentation for this. I suspect there may be a better way to approach the entire process however. How can I do this the right way?",1,Lack of Alternative Solutions/Documentation
249,55044905,"Tensorflow low level api, batch normalization problem","The tf.layers.batch_normalization documentation says it will be removed in a future version, and should be replaced by tf.keras.layers.BatchNormalization, but i cannot find a way to replace the functionality using tensorflow low level api. which outputs: If we instead use keras as suggested in the documentation we get an empty output: Since UPDATE_OPS is empty, the model is unable to update the batch normalization moving_avg_mean and moving_avg_variance during training using keras (resulting in a much higer test error). Any suggestion how to solve this is greatly appreciated! The example above is taken from an older post of how to use tf.layers.batch_normalization",1,Documentation Replication on Other Examples
250,55094952,Understanding Tensorflow control dependencies,"I am trying to gain a stronger grasp of TensorFlow. I came across the concept of control dependencies. I understand that the order of ops as specified by us is not really relevant to Tensorflow during execution. In order to optimise the speed of execution TensorFlow decides its own order of calculating nodes. But we can customise order of execution by using tf.control_dependencies. I am not able to understand the use cases of the function. Can anyone direct me to some resource(other than the documentation) or explain the working of this function? An example: The output of the code is 8. So I infer that since z=x+y,the assign node has not been evaluated(right?). But doesn't this mean that the result of tensorflow may be erroneous? This means we need to create new nodes during every operation to force TensorFlow to calculate all the nodes leading up to the result. But in say training a neural network with 10000 steps if each step creates a new set of 1000 weights/parameters won't the space complexity explode?",1,Inadequate Examples
251,55109696,TensorFlow - Difference between tf.keras.layers.Layer vs tf.keras.Model,"Reading through the documentation of implementing custom layers with tf.keras, they specify two options to inherit from, tf.keras.Layer and tf.keras.Model. Under the context of creating custom layers, I'm asking myself what is the difference between these two? Technically what is different? If I were to implement the transformer encoder for example, which one would be more suitable? (assuming the transformer is a only a ""layer"" in my full model)",1,Documentation Replication on Other Examples
252,55141486,Unable to see keras model graph in Tensorboard when using TensorFlow 2.0 Alpha,"I am trying custom training on TensorFlow 2.0 alpha and at the same time I am trying to add some metrics and my training graph to TensorBoard. Consider the following contrived example This, does not show the model graph properly, on doing In the graphs tab I am seeing I am getting the graph when I am training through model.fit or estimator. For example, here is the graphs section when I use model_to_estimator to convert a model The guide article does not track metrics through tensorboard, and I did not find any sections on the new workflow for custom adding and tracking of metrics in TensorBoard on alpha (https://www.tensorflow.org/alpha). My contrived implementation is based on the API documentation of tf.summary (https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/summary)",1,Documentation Replication on Other Examples
253,55168906,Tensorflow - tf.nn.weighted_cross_entropy_with_logits - logits and targets must have the same shape,"I've just started using tensorflow for a project I'm working on. The program aims to be a binary classifier with input being 12 features. The output is either normal patient or patient with a disease. The prevalence of the disease is quite low and so my dataset is very imbalanced, with 502 examples of normal controls and only 38 diseased patients. For this reason, I'm trying to use tf.nn.weighted_cross_entropy_with_logits as my cost function. The code is based on the iris custom estimator from the official tensorflow documentation, and works with tf.losses.sparse_softmax_cross_entropy as the cost function. However, when I change to weighted_cross_entropy_with_logits, I get a shape error and I'm not sure how to fix this. I have searched and similar problems have been solved by just reshaping the labels - I have tried to do this unsuccessfully (and don't understand why tf.losses.sparse_softmax_cross_entropy works fine and the weighted version does not). My full code is here https://gist.github.com/revacious/83142573700c17b8d26a4a1b84b0dff7 Thanks!",1,Documentation Replication on Other Examples
254,55176818,How to support masking in custom tf.keras.layers.Layer,"I'm implementing a custom tf.keras.layers.Layer that needs to support masking. Consider the following scenario Now per the documentation I wonder, what does that mean? Looking through TensorFlow's custom layers guide and the tf.keras.layer.Layer documentation it is not clear what should be done to support masking",1,Lack of Alternative Solutions/Documentation
255,55300544,TF 2.0 @tf.function example,"In the tensorflow documentation at the autograph section we have the following code snippet I have a small question concerning the step variable, it's an integer and not a tensor, autograph supports built-in python type such as integer. Therefore the tf.equal(step%10,0) could be changed to simply step%10 == 0 right ?",1,Documentation Replication on Other Examples
256,55347304,Error when applying sample/class weights to fit generator,"I am using a tf.keras.Model fit_generator (https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit_generator) to feed batches of data to a model. According to TensorFlow Documentation, the fit generator should be able to accept size 2 (inputs, targets) or 3 (inputs, targets, sample_weights) tuple. We have the size 2 working, but we have unbalanced classes, so I have determined sample weights. When the fit generator returns a size 3 tuple, I get the error: tensorflow.python.framework.errors_impl.InvalidArgumentError: Can not squeeze dim[0], expected a dimension of 1, got [batch_size]"" I am using tensorflow 1.12 Loss Function is tf.losses.softmax_cross_entropy",1,Documentation Replication on Other Examples
257,55363728,How to feed .h5 files in tf.data pipeline in tensorflow model,"I'm trying to optimize the input pipeline for .h5 data with tf.data. But I encountered a TypeError: expected str, bytes or os.PathLike object, not Tensor. I did a research but can't find anything about converting a tensor of string to string. This simplified code is executable and return the same error: Apparently the fname is a tensor of string but the positional argument waits for only a string. I can't find any documentation on this. And the answer of another post doesn't solve this problem. In my case, I work only with h5 where one h5 store one batch. Update Solution: Thanks to the comment of @kvish, the part of loading .h5 file is solved. The code is upgraded with a simple conv layer, the placeholders have been taken. Each .h5 is one batch. I want to prefetch in parallele multiple batches(h5py doesn't support multithread reading so I write batches into multiple files). One can copy-paste-and-launch: Somehow there will be another cudnn issue which isn't related to this post. tensorflow-cpu v1.12: work fine tensorflow-gpu v1.12: runtime issue happens",1,Lack of Alternative Solutions/Documentation
258,55379830,How do I resize image with unknown size in Tensorflow(tf.shape(input) method doesn't work),"According to this post, one can use tf.shape() to resize image with unknown size like placeholder. But the method doesn't seem to work for me. I have some simple code that looks like: Basically, my code does the following: Given an input 1D data x, the program tries to stretch or compress the sequence by some random factor and return the tuned sequence. Since I didn't find any Tensorflow function that directly performs this operation, I use tf.resize by treating the data as 1xD image where D is the length of the signal. But I got an error: So it seems like tf.shape(x) returns a Tensor rather than integer values that specify the shape of the tensor(verified by Tensorflow document). How can I solve this?",1,Documentation Replication on Other Examples
259,55422537,Testing TF serving model fails with bytes as strings and strings as bytes confusion,"I'm having a problem serving my text classification model on Tensorflow 1.12. I'm using tf.estimator.inputs.pandas_input_fn to read in my data, and tf.estimator.DNNClassifier to train/evaluate. I'd then like to serve my model. (Apologies in advance, it's tough to provide a full working example here, but it's very much like the example TF provides at https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier ) I'm currently saving my model with ... This actually fails to run with the error: I tried to save a second way doing: This actually works, until I try testing it with the saved_model_cli. Some output for saved_model_cli show --all --dir TEST_SERVING/1553879255/: But now I can't seem to test it. Ok, lets turn it into a bytes object by changing to b[""What is going on""] and b[""Help me""]... Any ideas/thoughts?? Thanks!",1,Documentation Replication on Other Examples
260,55425811,Implementing Intersection over Union Loss Using Tensorflow,"This may be more of a Tensorflow gradient question. I have been attempting to implement Intersection over Union (IoU) as losses and have been running into some problems. To the point, here is the snippet of my code that computes the IoU: It works as predicted. However, the issue that I am having is the losses do not decrease. The model does train, though the results are less than ideal so I am wondering if I am implementing it correctly. Do I have to compute the gradients myself? I can compute the gradients for this IoU loss derived by this paper using tf.gradients(), though I am not sure how to incorporate that with the tf.train.AdamOptimizer(). Reading the documentation, I feel like compute_gradients and apply_gradients are the commands that I need to use, but I can't find any examples on how to use them. My understanding is that the Tensorflow graph should be able to come up with the gradient itself via chain rule. So is a custom gradient even necessary in this problem? If the custom gradient is not necessary then I may just have an ill-posed problem and need to adjust some hyperparameters. Note: I have tried Tensorflow's implementation of the IoU, tf.metrics.mean_iou(), but it spits out inf every time so I have abandoned that.",1,Documentation Replication on Other Examples
261,55560676,How to use tf.while_loop with eager execution?,"In the documentation, the body of a tf.while_loop needs to be a python callable. works but throws a ValueError: Attempt to convert a value (None) with an unsupported type() to a Tensor In 2.0, eager execution is default, I wonder what's the problem?!",1,Documentation Replicability
262,55573670,Unexpected output for tf.nn.sparse_softmax_cross_entropy_with_logits,"The TensorFlow documentation for tf.nn.sparse_softmax_cross_entropy_with_logits explicitly declares that I should not apply softmax to the inputs of this op: However if I use cross entropy without softmax it gives me unexpected results. According to CS231n course the expected loss value is around 2.3 for CIFAR-10: However without softmax I get much bigger values, for example 108.91984. What exactly am I doing wrong with sparse_softmax_cross_entropy_with_logits? The TF code is shown below.",1,Documentation Replication on Other Examples
263,55703097,Training while loop in Tensorflow,"I've attempted converting a Python-side training loop to Tensorflow to (hypothetically) make the code run faster - not having to pass control over to cpu constantly. However, I can't manage using tf.while_loop. Here's the code that works: Now, if I create an update function to pass tf.while_loop, an error is thrown. I don't quite understand what is happening even after reading the documentation. weights is a Variable after all. What could be done to correctly make the training loop?",1,Documentation Replication on Other Examples
264,55711355,How to restore dangling tf.py_func within the tf.data.Dataset() with tf.saved_model API?,"After doing a research for restoring the tf.py_func() when using saved_model API in vain, I couldn't find other information than documented in tensorflow: Two save/load snippets help to illustrate the situation. Save part: Load part: Error: It's well known that the function wrapped by the tf.py_func() isn't saved with the model. Does anybody has a solution to restore this by using the small hint given by the tf doc applying tf.train.Server",1,Lack of Alternative Solutions/Documentation
265,55718702,How to correctly train with tf.keras.layers.BatchNormalization: Is there still a tf.GraphKeys.UPDATE_OPS dependency?,"My goal is how to correctly train with batch normalizations layers in TensorFlow (TensorFlow version 1.13.1 for Python in Graph Mode) using the recommended tf.keras.layers.BatchNormalization class (https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization). An older recommended approach was to use tf.layers.batch_normalization. The documentation (https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization) indicates that it is currently deprecating instead in favor of tf.keras.layers.BatchNormalization. While using the older class, the documentation indicates we must explicitly add dependency on the mean and variance update operations, which would otherwise be dangling nodes outside from any dependencies in training operations: My question: Is this explicit dependence on UPDATE_OPS still needed when training batch norms in TF 1.13 with tf.keras.layers.BatchNormalization? I don't see this mentioned in the documentation, however, I would be much more comfortable if someone knew for sure (and even better if can point to official documentation or code) that these operation dependencies are implicitly taken care of.",1,Lack of Alternative Solutions/Documentation
266,55778682,fix/freeze individual kernel weights in a convolutional operation,"I'm trying out a workaround for fixing individual kernel weights in a convolutional operation in TensorFlow using Python 3.7. I do it by creating A 1 in the ""mask"" tensor indicates that I want to fix/freeze that specific weight during training, i.e. not update it in the backward pass. Now, this workaround works perfectly fine when applied to a fully connected layer but fails when applied to a convolutional layer and I can't figure out why or how to make it work. Something seems to be happening in the tf.nn.conv2d() function call (see code example below) and according to the documentation this is what they do: But since I use weights_frozen which is a tensor and depends on the trainable variable, non-trainable variable and mask_weights it should get zero-valued gradients on the positions where I have a 1 in the mask_weights tensor. As mentioned, I expect to get zero-valued gradients on the positions where I have a 1 in the mask_weights tensor, but instead they are non-zero and therefore those weights are being trained, which is not the behavior I'm trying to achieve.",1,Documentation Ambiguity
267,55788007,Unexpected results when using tfrecords loaded using tf.data.Dataset.list_files() with shuffle argument,"I'm hoping to get clarification on how the shuffle argument in tf.data.Dataset.list_files() works. The documentation states that when shuffle=True, the filenames will be shuffled randomly. I've made model predictions using a tfrecords dataset that has been loaded using tf.data.Dataset.list_files(), and I would've expected the accuracy metric to be the same no matter the order of the files (i.e. whether shuffle is True or False), but am seeing otherwise. Is this expected behavior or is there something wrong with my code or intepretation? I have reproducible example code below. Oddly, as long as tf.random.set_random_seed() is set initially (and it seems it doesn't even matter what seed value is set), then the predictions results are the same no matter whether shuffle is True or False in list_files(). tensorflow==1.13.1, keras==2.2.4 Thanks for any clarifications! Edit: re-thinking it through and wondering if Y = [y[0] for _ in range(steps) for y in sess.run(Y)] is a separate and independent call? Split dataset into multiple tfrecords files so we can reload it with list_files() later: At this point, I exit (ipython) and restart again:",1,Documentation Replication on Other Examples
268,55868686,how to use eager execution to save and restore in TensorFlow,"We always use tf.train.Saver() to save and restore weights, like in this example. But how to use eager execution to save? how to change the following example? Another question, is it a good idea to use eager? I found tf.contrib.eager.Saver here, but it says, What does it mean?",1,Documentation Replication on Other Examples
269,55904359,TypeError computing gradients with GradientTape.gradient,"Hello, I'm currently trying to compute gradients in Tensorflow 1.13.1 and using the GradientTape class as explained in the official documentation , but I am getting a TypeError: Fetch argument None has invalid type &lt;class 'NoneType'&gt;. Below, I will include two simple cases where I get this error, using only out-of-the-box Tensorflow function, the first one being the simpler minimal working example, and the second one that I actually need to solve/get a work-around. For completeness, I am using Python 3.6.8. In this code, f1 and f2 are computed in different ways, but give the same array. However, when trying to compute the gradients associated with them, the first line one gives the following error, whereas the second line works flawlessly. I report below the stack trace of the error Please note that I also tried computing only one gradient at a time, i.e with persistent=False, and got the same results. Below, I will include also the minimal working example to reproduce the same error I got, but trying to resolve the problem I am actually working on. In this code, I'm using a RNN to compute an output w.r.t some inputs, and I need to compute the jacobian of this output w.r.t the inputs. Regarding the stack trace, I get the same error but with less lines (there are one for_fetch, &lt;listcomp&gt; and __init less in this stack trace). For completeness, I still include it below I feel like there is a bug with some Tensorflow function that gets me the error, however I am not sure. At the end, what interest me is getting a tensor containing the jacobian of the output of my network w.r.t to the inputs. How can I achieve that using other tools, or correcting my code ? EDIT: Ok, so I took into account the comments by danyfang, and tried to look into the issue raised on Github he quoted about tf.gradients returning None instead of 0 due to some implementation design in low-level Tensorflow. Therefore, I tried to create a simple case where I am sure that gradient are different from 0, by computing tf.matmul(tf.transpose(x), x). I am posting below a MWE. This shows (at least in my opinion) that the error arises not because of the behavior reported by this issue, but another thing due to lower level implementation.",1,Documentation Replication on Other Examples
270,55909188,How can I apply a TensorFlow 2D Convolution (tf.nn.conv2d) to a single (non-batch) 2D image?,"I would like to use the function tf.nn.conv2d() on a single image example, but the TensorFlow documentation seems to only mention applying this transformation to a batch of images. The docs mention that the input image must be of shape [batch, in_height, in_width, in_channels] and the kernel must be of shape [filter_height, filter_width, in_channels, out_channels]. However, what is the most straightforward way to achieve 2D convolution with input shape [in_height, in_width, in_channels]? Here is an example of the current approach, where img has shape (height, width, channels): I am reshaping the input as follows: [in_height, in_width, in_channels]-&gt;[1, in_height, in_width, in_channels]-&gt;[in_height, in_width, in_channels] This feels like an unnecessary and costly operation when I am only interested in transforming one example. Is there a simple/standard way to do this that doesn't involve reshaping?",1,Documentation Replication on Other Examples
271,55916743,How to get gradients with respect to input and change input (rather than trainable vars) to minimize loss in TF 2?,"I want to use a trained model to change the input so it minimizes the loss (rather than changing the trainable variables) a la Deep Dreaming in Tensorflow 2.0 but I am not having success. Say I have a basic NN as the one in the docs Which I train using a simple tf.GradientTape function What's the idiomatic way to create a function that will instead calculate and apply the gradients to the input - images. I assumed it will be as simple as However, that doesn't work.",1,Documentation Replication on Other Examples
272,55936016,TensorFlow 2.0 clip_by_value with dynamic bounds,"It is not clear from the TensorFlow documentation whether I can have dynamic range constraints imposed on a tf.Variable via tf.clip_by_value. From my testing it doesn't seem to work, but I would like to be sure, and if it isn't then I also would like to know how to achieve this (there are certain parts of my parameter space that cause NaNs in my loss function, and these constraints can only be described in terms of combinations of parameters). Here is my test scenario: Output: The constraint on eta should enforce that x + eta is positive, but already we see negative values appearing in the first two loops. Or is this perhaps an issue of the order of updating of variables? For example in the minimise loop I guess it is important that the x variables get updated first so that the constraint on the eta variables gets calculated correctly? I guess there is no guarantee of that, and I need to tell TensorFlow to do this? Edit: I attempted to manually clip the variables in the optimisation loop, like so: and it seems to kind of work in this case, but in other test cases it seems to make the minimiser go haywire. I guess because it fights with the minimiser about what values the variables should have. So I'm not sure that this is a good solution. Edit 2: Well for my specific case I realised that I could solve my problem with math, i.e. by a change of variables I can make the problem not require any explicit constraints. Which is probably the best thing to do if it is possible. But it won't always be, so I am still curious how to do it with constraints.",1,Documentation Replication on Other Examples
273,55986982,What is the way to use Tensor flow 2.0 object in open cv2 python and why is it so circuitous?,"I load an image using tensor flow api (2.0) like so : Now that I have this object, I want to show this image, I can simply use matplotlib.pyplot, and this works. However attempting this with OpenCV2 is problematic from the start, most of the examples are from 1.0 with .eval() session based suggestion for numpy conversion. One way would be to first convert tensor flow object to numpy, here is the function to do that from API documentation : I dont understand why this does not works and I get a number of errors while all I want is to do something simple and then use some open cv2 functions like remap, resize etc.: Update 5/5/2018 : After searching more I found out that this has something to do with Tensorflow graph execution. I have a function This works nicely when called eagerly with .numpy() attribute, however when called like following code and when you try to inspect what real_image is and its type returns Please advice. Update 5/5/2018 : I decided to do a preprocessing of the data so I don't have to worry about the using any opencv functionality during the load time of the data. However during training time I still want to do some openCV operations. Now as per the suggestion of @giser_yugang I tried using py_function, I wrap opencv operations in py_function and call that function in a wrapper tf.function. This wrapper tf.function I call in train step. However the output I get from this wrapper function is like so : Then if I try to consume this tensor in the next train step operation I get a If I don't use this py_function wrapper in my train step and directly try the numpy operations using opencv I get another error I guess both ways you cant win !",1,Documentation Ambiguity
274,56047272,Explicit vs implicit type definition in TensorFlow,"I'm just beginning to learn TensorFlow. Quoting from the documentation: The second constant is implicitly typed as a float32. Is that based on the explicit typing of the first constant? And does that imply that the first dtype is required? tf.constant documentation would imply that it does not: But then it would be unnecessary to explicitly type the 3.0 constant above. I'm just looking for some clarification on this, since, like I said, I'm just starting out.",1,Documentation Replication on Other Examples
275,56166885,How to check evaluation auc after every epoch when using tf.estimator.EstimatorSpec?,"I defined my model using tf.estimator.EstimatorSpec. I know it has train, evaluation and prediction modes. But I want to check some metric scores such as auc after every epoch. Does this API support it like keras?",1,Inadequate Examples
276,56212366,TensorFlow tf.data processing dev set after each epoch,"I am using tf.data with reinitializable iterator to handle training and dev set data. For each epoch, I initialize the training data set. The official documentation has similar structure. I think this is not efficient especially if the training set is large. Some of the resources I found online has sess.run(train_init_op, feed_dict={X: X_train, Y: Y_train}) before the for loop to avoid this issue. But then we can't process the dev set after each epoch; we can only process it after we are done iterating over epochs epochs. Is there a way to efficiently process the dev set after each epoch?",1,Documentation Replication on Other Examples
277,56229730,Trouble with zero-padding inputs for Steered Convolution Layer,"I'm using Tensorflow's new graphics library to apply a steered convolution to a series of meshes. In many cases, you will have a series of meshes that are not the same size and you must zero-pad the smaller ones. According to the documentation, the ""sizes"" argument of the graph_conv.feature_steered_convolution_layer function takes in an int tensor consisting of the number of non-padded elements of each mesh. For some reason, when this argument is set something other than ""None"", I get a warning telling me that the sparse array used in the ""neighbors"" argument is being converted to a dense matrix. This causes my program to run absurdly slowly. The issue seems to be tied to the way that it calculates gradients. If the optimizer is commented out, the error does not come up. I read about a similar problem (link below) where the solution to the problem was to use tf.dynamic_partition rather than tf.gather. However, the tf.gather functions, in this case are located within the graph_convolution library. I attempted to make some edits in my copy of the library, but to no avail. How to deal with UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape When the above code is run, I get the following warning: I am starting to think that this might have more to do with the library itself than with this piece of code. I have referenced this posed in GitHub in case it requires updates (or additional documentation) to the library. https://github.com/tensorflow/graphics/issues/13",1,Requesting (Additional) Documentation/Examples
278,56231695,When should tf.losses.add_loss() be used in TensorFlow?,I cannot find an answer to this question in the TensorFlow documentation. I once read that one should add losses from tf.nn functions but it isn't necessary for functions from tf.losses. Therefore: When should I use tf.losses.add_loss()? Example: Thank yoou.,1,Lack of Alternative Solutions/Documentation
279,56284927,tf.keras equivalent code block written in tf.contrib.slim,"I'm trying to re-implement a research paper code in tf.keras, in init block it was written as: I didn't find a equivalent in tf.keras.layer.Conv2D arguments for normalizer_fn=slim.batch_norm. How to achieve this in keras? I tried: Is this a valid equivalent to the above tf.contrib.slim code. With limited documentation of tf.contrib.slim, I'm really confused.",1,Documentation Replication on Other Examples
280,56286350,tf.keras.metrics.SpecificityAtSensitivity num_thresholds interpretation,"I'm trying to get my head around tf.keras.metrics.SensitivityAtSpecificity. I'm fine with the concept of sensity and specificity in isolation, but I'm unsure how the two are related in this single metric. More specifically, I'm unsure how to interpret the num_thresholds argument. The example in documentation has num_thresholds=1. Setting num_thresholds greater than 1 with the same input data seems to always return a metric value of 1.0.",1,Documentation Replication on Other Examples
281,56344827,"in TF2, how do you save models/weights when not using the tf.keras API?","In the documentation it seems they focus on how to save and restore tf.keras.models, but i was wondering how do you save and restore models trained customly through some basic iteration loop? Now that there isnt a graph or a session, how do we save structure defined in a tf function that is customly built without using layer abstractions?",1,Lack of Alternative Solutions/Documentation
282,56386901,Example for tf. group_by_reducer?,Can someone show me an example of tf.data.experimental.group_by_reducer? I find the documentation tricky and couldn't understand fully. How can I use it for calculating average?,1,Documentation Replicability
283,56436701,tf.data.Dataset.window example from the documentation fails,I'm trying to use an example from the TF documentation for tf.data.Dataset.window and the example from the documentation is failing. Code derived from the documentation: Produces this error (trace removed): So iterator.get_next() is returning a VariantDataset rather than the usual tensor. TF Version: 1.13.1,1,Documentation Replication on Other Examples
284,56458133,Tensorflow error when adding writing summaries 'Tensor' object has no attribute 'value',"this is my code: It runs fine when the commented parts are commented out. However, I don't get why whenwriter.add_summary() is uncommented out I get the following error: It doesn't make sense to me. The docs for the FileWriter class clearly states that it takes in a list of String tensors, and the docs for the tf.summary.scalar says that it returns a tensor of type string with summary protocol, so it seems like it should work. Here's what I've tried, by people who seemed to have the same issue as me, but nothing worked: Tensorboard expects a string for summary, adding eval() didn't work. The value error is raised because you have to evaluate the summary node within a session. I am already running it in session. Merges all summaries in the default graph. Initially, I was using merging all summaries, but now I switched to tf.summary.merge. Coming from improperly connected tensors. I am using tf.matmul and tf.add here instead of regular operands. Another thing I tried was putting tensor objects into the second argument of tf.summary.scalar, something like replacing the commented part of the code above like this: But event that didn't seem to work. Why doesn't my above code work, while this code here works? I don't see a difference. Also, I am using Google Colab, which is why I'm importing tensorboardcolab.",1,Documentation Ambiguity
285,56491633,What is the difference between tf.scatter_add and tf.scatter_nd when indices is a matrix?,"Both tf.scatter_add and tf.scatter_nd allow indices to be a matrix. It is clear from the documentation of tf.scatter_nd that the last dimension of indices contains values that are used to index a tensor of shape shape. The other dimensions of indices define the number of elements/slices to be scattered. Suppose updates has a rank N. First k dimensions of indices (except the last dimension) should match with first k dimensions of updates. The last (N-k) dimensions of updates should match with the last (N-k) dimensions of shape. This implies that tf.scatter_nd can be used to perform an N-dimensional scatter. However, tf.scatter_add also takes matrices as indices. But, its not clear which dimensions of indices correspond to the number of scatters to be performed and how do these dimensions align with updates. Can someone provide a clear explanation possibly with examples?",1,Inadequate Examples
286,56552397,Custom metric: Using scikit learn's AucRoc Calculator with tf.keras,"I'm training a multilabel classifier using tf.keras and horovod that has 14 classes. AucRoc is used as the metric to evaluate the performance of the classifier. I want to be able to use scikit learn's AucRoc calculator as mentioned here: How to compute Receiving Operating Characteristic (ROC) and AUC in keras?. If I feed the tensors as is for the following function: I get an error that looks like this: I'm trying to convert tf tensors into a numpy array and then feed them to the roc_auc_score method like so: I get the following error: I've also tried tensorflow's https://www.tensorflow.org/api_docs/python/tf/metrics/auc like so: It works just fine. However, it gives me a single number for aucroc. I wonder what that number represents, is it an average aucroc value for all the 14 classes? or max aucscores of all the classes? or how does it get to a single number? 1) How do I fix the error with roc_auc_score? 2) What does that single number represent?",1,Lack of Alternative Solutions/Documentation
287,56553579,How to export Estimator's best model?,"I am training a simple CNN based on a Custom Estimator with TF Records. I am trying to export the best model in terms of validation loss during the train_and_evaluate phase. According to the documentation of the tf.estimator.BestExporter, I should feed a function that returns a ServingInputReceiver but after doing so, the train_and_evaluate phase crashes with a NotFoundError: model/m01/eval; No such file or directory. Seems like if the BestExporter does not permit saving the evaluation results as it would do without the exporter. I tried with different ServingInputReceiver but I keep getting the same error. As defined here: and here Here are my exporter and training procedure: This is a gist with the output. What's the correct way to define a ServingInputReceiver for the BestExporter?",1,Documentation Ambiguity
288,56606757,Tensorflow: output of multi-step decay function returns a TypeError,"We are trying to write a multi-step decay function in Tensorflow using tf.train.piecewise_constant() as suggested here. Tensorflow documentation here states that: ""When eager execution is enabled, this function returns a function which in turn returns the decayed learning rate Tensor"" However, when we tried running the code, it returned a TypeError. It returns the same error even when lr() is used. The code works as expected when we provide a constant learning rate. Is there something that we are missing?",1,Documentation Ambiguity
289,56635027,Feeding array (shape with rank 1) to TensorFlow tf.case,"Following this example from the tf.case documentation: I want to do the same, but allow to use a feed_dict as input, illustrated by this snipped: So, basically I want to feed three int-arrays of equal length and receive an array of int-values containing either 17, 23, or -1. Unfortunately, there code above gives and error: I understand, that tf.case requires boolean scalar tensor input values but is there any way to achieve what I want? I also tried tf.cond without success.",1,Documentation Replicability
290,56693863,Why does model.losses return regularization losses?,"I have met a snippet of code of tensorflow 2.0, which is used for calculating the loss. The total loss is composed of two parts: 1) regularization loss, 2) prediction loss. My question is why model.losses is regularization loss? model here is an instance of tf.keras.Model. I'm kind of confused by the tensorflow official API documentation. tf.keras.Model, it says Why could we get regularization loss via accessing losses property? Also, what is eager safe? If losses property is returning regularization loss, why is it named losses instead of regularization_loss?",1,Documentation Ambiguity
291,56754293,TensorflowServing on a trained native Keras model with a preprocessing function for the input,"My final goal is to use the What-If-Tool on tensorboard. In order to do that, I need to serve my Keras model on TensorflowServing, and the data in a TFRecordFile. So the data has to be transformed into tf.Examples. The tool is supposed to grab the network to run inference on the data. however, the network cannot handle tf.Examples as an input. So the served model needs to have a preprocessing function. According to the tensorflow documentation, one way is to create a tensorflow Estimator, and to use ""serving_input_receiver_fn"" to preprocess the data. This would have been perfect except for the case that I can't make an already trained native Keras model into an Estimator. It seems that the only way it to create it from a tf.keras model (and not a native keras model like I have), and to train it directly with the estimator. Another way would be to use the tf.saved_model.simple_save function, and then use TensorflowServing, but I did not find a way to preprocess the tf.Examples to make a correct input for the network. Since this is not working, I have no clue on how to resolve this. Edit: I tried to transform my native keras into a tf.keras model. My model is really big, so I build this function: However, this is not working because of Lambda layer. In the config layer, the function is now written in the form of: Hence, I gave up this method hoping something else would allow to pre-process the input of my serving model.",1,Documentation Replication on Other Examples
292,56802840,What exactly tensorflow.gather() does?,I saw code for triplet loss that contains the function tf.gather(). What this function does? I have gone through the tensorflow's official website for definition but still unable to get it.,1,Lack of Alternative Solutions/Documentation
293,56804123,Tensorflow-Lite : Exporting a GraphDef from tf.Session convert failed,"I want to convert my tensorflow Spectrogram session to .tflite file for using in Android. I try to follow the TFLite official example but it is failed during the convert procedure. Please give me some advice for fixing this error. Thanks a lots! I got the error message from Jupyter Notebook I want to make customize Spectrogram library by tf.signal.stft function, and save to .tflite file for using in Android device. I can't find anymore tutorial about TFLite converter, only the official document. Please help me to fix this error, thanks a lots !! ",1,Documentation Replication on Other Examples
294,56905939,Effective way to read images from a csv file and return a tf.data.Dataset object,I have a csv file that contains two columns: Each row in the csv corresponds to one item (sample). I want to create a tf.data pipeline that reads the file path and loads the numpy array and the label associated with it. How would I go about doing so so that I can return a tf.data.Dataset object? The documentation on the website is not very informative and I cannot figure out where to start from.,1,Lack of Alternative Solutions/Documentation
295,56939282,How do you feed a tf.data.Dataset dynamically in eager execution mode where initializable_iterator isn't available?,"What is the new approach (under eager execution) to feeding data through a dataset pipeline in a dynamic fashion, when we need to feed it sample by sample? I have a tf.data.Dataset which performs some preprocessing steps and reads data from a generator, drawing from a large dataset during training. Let's say that dataset is represented as: After training I want to produce various visualizations which require that I feed one sample at a time through the network for inference. I've now got this dataset preprocessing pipeline that I need to feed my raw sample through to be sized and shaped appropriately for the network input. This seems like a use case for the initializable iterator: This doesn't work with eager execution, you can't use the placeholder. The documentation examples all seem to assume a static input such as in the first example here. The only way I can think of doing this is with a queue and tf.data.Dataset.from_generator(...) which reads from the queue that I push to before predicting on the data. But this feels both hacky, and appears prone to deadlocks that I've yet to solve. TF 1.14.0",1,Documentation Replication on Other Examples
296,56969703,How to use `tf.scatter_nd` with multi-dimensional tensors,"I'm trying to create a new tensor (output) with the values of another tensor (updates) placed according to idx tensor. The shape of output should be [batch_size, 1, 4, 4] (like an image of 2x2 pixels and one channel) and update has shape [batch_size, 3]. I've read Tensorflow documentation (I'm working with gpu version 1.13.1) and found tf.scatter_nd should work for my problem. The issue is that I cannot make it work, I think I'm having problems understanding how I have to arange idx. Let's consider batch_size = 2, so what I'm doing is: I expect it to work, but it doesn't, it gives me this error: ValueError: The outer 3 dimensions of indices.shape=[2,1,3,2] must match the outer 3 dimensions of updates.shape=[2,3]: Shapes must be equal rank, but are 3 and 2 for 'ScatterNd_7' (op: 'ScatterNd') with input shapes: [2,1,3,2], [2,3], [4] I don't understand why it's expecting updates to have dimension 3. I thought idx has to make sense with output_shape (that's why I used expand_dims) and also with updates (specify the two indices for the three points), but it's obvious I'm missing something here. Any help would be appreciated.",1,Documentation Replication on Other Examples
297,56970612,Fitted values and weights in tensorflow (tesorflow DNNRegressor),I am using tensorflow version 2.0.0-beta1. I have created the input function to feed into tf.estimator.DNNRegressor. Below is the model that I am creating using DNNRegressor. Now I want to identify 1) Fitted values of my model on training data. 2) Weights associated with each variable in model(i.e. tf.estimator.DNNRegressor) I have search through the documentation of tensorflow and other sources but didn't get this information.,1,Lack of Alternative Solutions/Documentation
298,57014236,How to use the Embedding Projector in Tensorflow 2.0,"With the tf.contrib module gone from Tensorflow, and with tf.train.Saver() also gone, I cannot find a way to store a set of embeddings and their corresponding thumbnails, so that the Tensorboard Projector can read them. The Tensorboard documentation for Tensorflow 2.0 explains how to create plots and summaries, and how to use the summary tool in general, but nothing about the projector tool. Has anyone found how to store datasets for visualization? If possible, I would appreciate a (minimal) code example.",1,Documentation Replication on Other Examples
299,57083881,Tensorflow 2.0 Saving trained parameters to be restored in a new file,"I need to save trained variables of a TensorFlow 2.0 model using one of TF's built in functions like tf.train.Checkpoint or any other, and want to call them in a new file. I am not using tf.Keras.Sequantial and don't want to use something like model.save_weights() I have tried tf.train.Checkpoint to save variables, but not sure how to restore them. I used to work with tf.train.Saver() in TF 1.0 to save variables using sessions and restore them using tf.train.import_meta_graph and tf.train.latest_checkpoint. However, I haven't been able to find equivalent functionalities in TF 2.0 documentation so far. saver = tf.train.Checkpoint() saver.listed = [W, b_v, b_h] saver.mapped = {'W':saver.listed[0],'b_v':saver.listed[1], 'b_h':saver.listed[2]} save_path = saver.save('trained_parameters') restorer = tf.train.Checkpoint() restorer.restore('trained_parameters')",1,Documentation Replication on Other Examples
300,57120680,Deep copy of tensor in tensorflow python,"In some of my code, I have created a neural network using tensorflow and have access to a tensor representing that network's output. I want to make a copy of this tensor so that even if I train the neural network more, I can access the original value of the tensor. Following other answers and tensorflow documentation, I have tried the tf.identity() function, but it does not seem to be doing what I need. Some other links suggested the use of tf.tile(), but this did not help either. I do not wish to use sess.run(), evaluate the tensor, and store it elsewhere. Here is a toy example that describes what I need to do: The result of the above code is that t2 and t3 have the same value. What I want is for t3 to keep its value from being copied. Thanks in advance for your help.",1,Documentation Ambiguity
301,57134808,tf.keras.optimizers.Adam with tf.estimator model in Tensorflow 2.0.beta is crashing,"I am using Tensorflow 2.0.beta with Python 3.6.6 on Mac OS (nightly: tf-nightly-2.0-preview 2.0.0.dev20190721 but I never managed to have it working with compat module in Tensorflow 2.0). I am traying to migrate a tf.estimator model from Tensorflow 1.12 (fully working) to Tensorflow 2.0. Here is the code: predictions=predictions, loss=loss, train_op=train_op, export_outputs=predictions_output) If I keep the compat.v1 module it is working: If I try to use something without compat.v1 it is crashing: with the following error (I am running the code locally for the moment, not on GCP): Any idea how to fix that ? The error messages was changing over time since Tensorflow 2.0 alpha. I am also looking for a full working example of tf.estimator working with Tensorflow 2.0. I have issue to export the model as well. In the official documentation of Tensorflow 2.0 they only use in their example compat.v1 and don't export the model. All the online course on tf.estimator from GCP are using older version of Tensorflow (1.12 - 1.14).",1,Inadequate Examples
302,57140835,How to convert a tf.estimator to a keras model?,"In package tf.estimator, there's a lot of defined estimators. I want to use them in Keras. I checked TF docs, there's only one converting method that could convert keras. Model to tf. estimator, but no way to convert from estimator to Model. For example, if we want to convert the following estimator: How could it be converted into Keras Model?",1,Documentation Replicability
303,57170737,Cannot run tflite model on GPU (Jetson Nano) using Python,I have a quantized tflite model that I'd like to benchmark for inference on a Nvidia Jetson Nano. I use tf.lite.Interpreter() method for inference. The process doesn't seem to run on the GPU as the inference times on both CPU and GPU are the same. Is there any way to run a tflite model on GPU using Python? I tried to force GPU usage by setting tf.device() method but still doesn't work. The official documentation has something called delegates for GPU acceleration but I can't seem to find anything for Python.,1,Documentation Replication on Other Examples
304,57175343,Multiple inputs of keras model with tf.data.Dataset.from_generator in Tensorflow 2,"I am trying to implement a model in keras that will have multiple inputs: To feed that model, I want to write a generator to use with tf.data.Dataset.from_generator. From the docs of from_generator, its not clear to me how I should provide its parameters output_types, output_shapes. Can anyone help me with this?",1,Inadequate Examples
305,57279754,"What are the Tensorflow qint8, quint8, qint32, qint16, and quint16 datatypes?","I'm looking at the Tensorflow tf.nn.quantized_conv2d function and I'm wondering what exactly the qint8, etc. dataypes are, particularly if they are the datatypes used for the ""fake quantization nodes"" in tf.contrib.quantize or are actually stored using 8 bits (for qint8) in memory. I know that they are defined in tf.dtypes.DType, but that doesn't have any information about what they actually are.",1,Documentation Replication on Other Examples
306,57316557,"tf.keras.layers.pop() doesn't work, but tf.keras._layers.pop() does","I want to pop the last layer of the model. So I use the tf.keras.layers.pop(), but it doesn't work. When I use tf.keras._layers.pop(), it works. I don't find docs about this usage. Could someone help explain this?",1,Documentation Replication on Other Examples
307,57349824,"Recurrent neural network, time series prediction with newer Tensorflow 1.14","How to use new tf.keras API with recurrent neural network? I have checked the documentation but there is no example of such a situation. There is this great book Hands on machine learning from 2017. Since that year the API of tensorflow has evolved and I am trying to rewrite recurrent neural network for time series prediction with using version 1.14 code. The code from the book is using older tf.nn.dynamic_rnn and tf.nn.rnn_cell.BasicRNNCell: And this code works just fine (except that it throws warnings about deprecation left and right). I wanted to use tf.keras API as suggested in warning. My code is the same except: But this yields following exception: so I understand that the problematic line is After checking and comparing documentation for both cells https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn and https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN I can't find the culprit. What is the difference with these two cells? How to use tf.keras API with time series? Full old code: https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb Full ""my"" code:",1,Documentation Replication on Other Examples
308,57392510,TensorFlow simple example help - custom gradient,"How do you pass a custom gradient into a gradient optimization function in TensorFlow. I have illustrated what I am trying to do, with a simple example (trying to minimize z = 2x^2 + y^2 + 2). I have been looking at: https://www.tensorflow.org/api_docs/python/tf/train/Optimizer The problem seems to work if you pass in optimizer = tf.train.GradientDescentOptimizer(0.55) and train = optimizer.minimize(z) This code works: But I want to specify the gradient in the problem. aka I am trying to do this: How do you do this properly?",1,Documentation Replication on Other Examples
309,57403472,How do I add a new feature column to a tf.data.Dataset object?,"I am building an input pipeline for proprietary data using Tensorflow 2.0's data module and using the tf.data.Dataset object to store my features. Here is my issue - the data source is a CSV file that has only 3 columns, a label column and then two columns which just hold strings referring to JSON files where that data is stored. I have developed functions that access all the data I need, and am able to use Dataset's map function on the columns to get the data, but I don't see how I can add a new column to my tf.data.Dataset object to hold the new data. So if anyone could help with the following questions, it would really help: I have all the methods for taking the input as the elements from the columns and performing everything required to get the features for each element, I just don't understand how to get this data into the dataset. I could do ""hacky"" workarounds, using a Pandas Dataframe as a ""mediator"" or something along those lines, but I want to keep everything within the Tensorflow Dataset and pipeline process, for both performance gains and higher quality code. I have looked through the Tensorflow 2.0 documentation for the Dataset class (https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset), but haven't been able to find a method that can manipulate the structure of the object. Here is the function I use to load the original dataset: Then, I have methods which allow me to take a string input (column element) and return the actual feature data. And I am able to access the elements from the Dataset using a function like "".map"". But how do I add that as a column?",1,Inadequate Examples
310,57414387,Meaning of tf.keras.layers.LSTM parameters,"I am having trouble understanding some of the parameters of LSTM layers in the tf.keras.layers API. I am investigating using CuDNNLSTM layers instead of LSTM layers (to speed up training), but before I commit to CuDNN layers, I would like to have a full understanding of the parameters that I lose by using a CuDNNLSTM instead of a LSTM layer. I have read the docs, but they seem to assume some prior knowledge of LSTMs that I do not have. I have listed the pararameters that CuDNNLSTM does not have (that LSTM has) and interspersed with my questions about them, respectively. I've read a lot about LSTMs, and am at a point where I've decided to start training things, otherwise I won't absorb much more hypothetical knowledge. I've tried a lot of things in modeling, too, but the network I'm training is really simple so nothing seems to impact the results.",1,Documentation Replicability
311,57449484,What is trainable parameter in tensorflow?,tf.compat.v1.layers.batch_normalization takes trainable as an input. The documentation says: I think only scaling factor (gamma) and offset (beta) should be added to trainable variables and I am skeptical if even moving averages will get added to GraphKeys.TRAINABLE_VARIABLES. Can somebody tell me how trainable input is influencing the behavior of batch_normalization,1,Documentation Replicability
312,57460127,Tensorflow 2 creating custom dataset,"I am trying to build a custom dataset-loader, which laods ICDAR-Dataset. My frist step was to embed a dataset inside my loader as suggested also here in this post, but the problem is that you have to implement all the nice features that the tenfsoflow-2 class ""Dataset"" offers manually. My second try was to subclass the Dataset-Class, something like: But the problem is i did not find any documentation what dataset-class internally really does, the only implementation i found was this one. So question is does anybody know how to build a custom ""dataset"" in tf2 by subclassing tf.data.Dataset. By the way i also tried tensorflow_datasets, bit it does not really worked, shince it will downlaod the dataset, and split them manually which is in this is alreay seperated by train and test and also ICDAr can not be downlaoded without registration. The content of the ICDAR-Dataset is as following: Image: @https://rrc.cvc.uab.es/?ch=4 owns the copyrights of this image. Words and bounding boxes for the above image: Thanks does anyone know how to",1,Lack of Alternative Solutions/Documentation
313,57570041,"Tensorflow 2.0 ""future proof"" way of using tf.feature_columns in tf.keras subclass api?","I see there are some ways of using features columns wrapped in a tf.keras.layers.DenseFeatures layer: https://www.tensorflow.org/beta/tutorials/keras/feature_columns#create_a_feature_layer This suggests passing datasets into @tf.function wrapped ""def call"" methods? I don't see any mention (yet) of the interaction between tf.data.DataSet and tf.function. Are tf.data.DataSet considered tensors yet for the purpose of tf.function graph cache or will they be?",1,Documentation Ambiguity
314,57570385,"How to generate custom mini-batches using Tensorflow 2.0, such as those in the paper ""In defense of the triplet loss""?","I want to implement a custom mini-batch generator in Tensorflow 2.0 using tf.data.Dataset API. Concretely, I have image data, 100 classes with ~200 examples each. For each mini-batch, I want to randomly sample P classes, and K images from each class, for a total of P*K examples in a mini-batch (as described in the paper In Defense of the Triplet Loss for Person Re-Identification]). I've been searching through documentation for tf.data.Dataset, but can't seem to find the right method. I've looked into the from_generator method, but it doesn't seem suitable for this, since it generates a whole dataset from scratch as I understood. It seems to me that one way to do it would be to make a new class similar to BatchDataset which can be found in tf.data.Dataset source code, where I would somehow implement the logic, but I'm hoping for an easier solution to be honest.",1,Requesting (Additional) Documentation/Examples
315,57651287,How to swap tensor axes efficiently in tensorflow?,"I have to swap tensor's axes using tf.transpose to do the batch matrix multiplication (as the code shown below). tensor input_a: shape [10000, 10000] tensor input_b: shape [batch_size, 10000, 10] tensor output: shape [batch_size, 10000, 10] However, it seems very slow. I noticed this in the document page of tf.transpose: So, I think it might be the reason why my code run slowly? Is there any way to swap tensor's axes, or do the batch matrix multiplication efficiently?",1,Inadequate Examples
316,57717004,Tensorflow: Modern way to load large data,"I want to train a convolutional neural network (using tf.keras from Tensorflow version 1.13) using numpy arrays as input data. The training data (which I currently store in a single &gt;30GB '.npz' file) does not fit in RAM all at once. What is the best way to save and load large data-sets into a neural network for training? Since I didn't manage to find a good answer to this (surely ubiquitous?) problem, I'm hoping to hear one here. Thank you very much in advance for any help! Similar questions seem to have been asked many times (e.g. training-classifier-from-tfrecords-in-tensorflow, tensorflow-synchronize-readings-from-tfrecord, how-to-load-data-parallelly-in-tensorflow) but are several years old and usually contain no conclusive answer. My current understanding is that using TFRecord files is a good way to approach this problem. The most promising tutorial I found so far explaining how to use TFRecord files with keras is medium.com. Other helpful sources were machinelearninguru.com and medium.com_source2 and sources therin. The official tensorflow documentation and tutorials (on tf.data.Dataset, Importing Data, tf_records etc.) did not help me. In particular, several of the examples given there didn't work for me even without modifications. I'm assuming TFRecords are a good way to solve my problem but I'm having a hard time using them. Here is an example I made based on the tutorial medium.com. I stripped down the code as much as I could. The code creates a TFRecord file and starts fitting, then just gets stuck with no output or error messages. I don't know what the problem is or how I could try to fix it.",1,Documentation Replication on Other Examples
317,57719398,Unable to save model with tensorflow 2.0.0 beta1,"I have tried all the options described in the documentation but none of them allowed me to save my model in tensorflow 2.0.0 beta1. I've also tried to upgrade to the (also unstable) TF2-RC but that ruined even the code I had working in beta so I quickly rolled back for now to beta. See a minimal reproduction code below. What I have tried: 3. And this is where I am stuck now because it gives me no reasonable hint whatsoever. That's because I am NOT calling the save() function from a @tf.function, I'm already calling it from the outermost scope possible. In fact, I have no @tf.function at all in this minimal reproduction script below and still getting the same error. So I really have no idea how to save my model, I've tried every options and they all throw errors and provide no hints. The minimal reproduction example below works fine if you set save_model=False and it reproduces the error when save_model=True. It may seem unnecessary in this simplified auto-encoder code example to use a subclassed model but I have lots of custom functions added to it in my original VAE code that I need it for. Code:",1,Documentation Replicability
318,57813806,Apply feature columns without tf.Estimator (Tensorflow 2.0.0-rc0),"In the Tensorflow tf.Estimator and tf.feature_column docs it is well documented, how to use feature columns together with an Estimator e.g. in order to one-hot encode the categorical features in the dataset being used. However, I want to ""apply"" my feature columns directly to a tf.dataset which I create from a .csv file (with two columns: UserID, MovieID), without even defining a model or an Estimator. (Reason: I want to check what's happening exactly in my datapipeline, i.e. I'd like to be able to run a batch of samples through my the pipeline, and then see in the output how the features got encoded.) This is what I have tried so far: However the last line with the map call raises the following error: I'm not sure what this error means... I also tried to define a tf.keras model with only the feature_layer I defined, and then run .predict() on my dataset - instead of using ds.map(lambda x: feature_layer(x)): However, this results exactly in the same error as above. Does anybody have an idea what is going wrong? Is there maybe an easier way to achieve this?",1,Documentation Replication on Other Examples
319,57823210,How to combine tf.data.Dataset and tf.estimator.DNNRegressor properly,"I am currently learning to use tensorflow and have troubles getting started. I would like to use the newest API, namely estimator and dataset. But if I run the code presented below I get an Error. On the tensorflow page https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor I found, that ""The function should construct and return one of the following: * A tf.data.Dataset object: Outputs of Dataset object must be a tuple (features, labels) with same constraints as below."" I thought my code would provide that, but there seems to be a problem and I am out of ideas. I expect to run a training session on a deep neural network with 3 hidden layers a' 100 neurons in order to approximate the 'constant 1' function; instead I get the Error ""ValueError: features should be a dictionary of 'Tensor's. Given type: class, 'tensorflow.python.framework.ops.Tensor'",1,Documentation Replication on Other Examples
320,57872334,parallel inference in tensorflow (CPUs),"this is a really basic question, but I can't get the answer anywhere. Using tensorflow, can I do inference (tf.keras.Model.predict()) on multiple CPUs in parallel? I am using tf.data.Dataset to represent my data, but from the documentation it seems multiprocessing can be used only with generators or tf.keras.Sequence objects. Why is that? Am I supposed to somehow create an ordinary python generator from the Dataset or to manage the parallelism on my own with multiprocessing package? What would be the standard way of doing this? Thanks for any hints.",1,Documentation Replication on Other Examples
321,57921463,How tf.data.experimental.group_by_window() operates in Tensorflow 2.0,I am trying to understand the tf.data.experimental.group_by_window() method in Tensorflow 2 but I have some difficulties. For a reproducible example I use the one presented in the documentation:,1,Documentation Ambiguity
322,57929803,What is the proper way to convert Tracing Code using RunOptions to Tensorflow 2.0?,I'm having difficulty finding any documentation on how to migrate tracing code from 1.x to 2.0. In tensorflow 1.x you could do the following: How can you do a similar thing with a @tf.function call?,1,Inadequate Examples
323,57970717,Using pretrained convolutional network as a GAN discriminator,"I've pulled some code from TF2.0 documentation to generate images from a custom dataset. The code is here Since the documentation uses Keras i figured i might change the discriminator network to a pretrained network e.g InceptionV3, and only train the top layers. I've found this code (Fine-tune InceptionV3 on a new set of classes). I cant seem to figure out how to replace the the one with the other. I understand that im trying to replace Sequential mode with the Functional API. But i guess they are somehow interconnected. However, im not a frequent Keras user. My questions is: How do i replace a custom CNN in Sequential mode with a pretrained one from the Functional API to use as a discriminator? EDIT: I would be happy if anyone has examples of doing it with the GANEstimator instead as im more used to TF. Use the generator to generate a random image The current discriminator and helpers (Outputs tf.Tensor([[-0.0003378]], shape=(1, 1), dtype=float32)) The desired discriminator All imports EDIT: This was the discriminator i ended up with! Thanks to @pandrey",1,Requesting (Additional) Documentation/Examples
324,57995171,Need a clear simple approach to distributed learning in Tensorflow/Keras,"I have multiple machines, some with GPUs and some others not. I also have a keras model that works fine on a single machine but I want to train it in a distributed mode because I want to test it with a huge dataset and with a bigger number of layers. There is quite a lot of pages discussing the distribution strategy in tf.distribute, but at the same time there are a lot other pages showing how to do it with encapsulating keras model with an estimator, setting up the TF_CONFIG parameter and then call tf.estimator.train_and_evaluate. I used the second approach personally as it was more straightforward and am struggling to tune and debug it. It works anyway, but I am very confused what is the point in all of strategy-related stuff as I don't see any use them in the second approach, and the documentation is not helping to clear it. I also have some doubt if my file setting environment is correct: My understanding is that the PS server is going to hold the model parameters and the chief server is going to administer the whole training process, distributing data, and saving summaries and checkpoints. So I assume that: 0- I need only one chief server, at least one PS server, possibly some workers, and one evaluator. The data and parameter sharing and communication between all of these servers is done by system and I am not engaged in it. 1- The main python code for all machines should be exactly the same, except in TF_CONFIG definition that defines the task and index for that specific machine. 2- I should have one shared copy of data in a folder available to all chief and workers. 3- I should have one shared log directory accessible by all machines as defined in tf.estimator.RunConfig. 4- Having this setting then a piece of code such as below will do the job (assuming the model has been defined elsewhere and the read_datasets function returns features and labels for running the model): Although the above approach seems to work fine, I still have some difficulties understanding how the chief is partitioning the dataset among the workers and how to set the train_step and batch_size in this approach. Also I don't know how can I report accuracy and other metrics such as precision/recall/F1 in addition to loss when running the tf.estimator.evaluate without writing a custom model_fn for my encapsulated keras estimator.",1,Documentation Replication on Other Examples
325,58096095,How does tf.audio.decode_wav get its contents?,"I'm trying to pull some audio files into Tensorflow by using tf.audio.decode_wav. I can see someone is looking into providing more info in the docs, but does anyone have any examples of how this should work? Args: I'm guessing the contents is a tensor which has already been pulled from a file rather than a path?",1,Inadequate Examples
326,58112355,"What, exactly, is eager execution from a programming point of view?","I am trying to understand eager execution. Pages returned by Google describe what it does for you, and I'm ok with that. I am trying to understand it from the point of view of program code. Here is an example from this article. The article says this statement does something different depending on whether you are in eager mode or not. Without eager mode, print(a) gives: With eager mode, print(a) gives: Please could someone explain what these two return values are. If they are two different object types, a Tensor and a tf.Tensor, what is the difference between these objects? I have searched the TensorFlow documentation and can't see anything that addresses this distinction. Any pointers gratefully received. Thanks, Julian",1,Inadequate Examples
327,58126494,How to Translate CSV Data into TFRecord Files,"Currently I am working on a system that can take data from a CSV file and import it into a TFRecord file, However I have a few questions. For starters, I need to know what type a TFRecord file can take, when using CSV types are removed. Secondly, How can I convert data type:object into a type that a TFRecord can take? I have two columns (will post example below) of two objects types that are strings, How can I convert that data to the correct type for TFRecords? When importing Im hoping to append data from each row at a time into the TFRecord file, any advice or documentation would be great, I have been looking for some time at this problem and it seems there can only be ints,floats inputted into a TFRecord but what about a list/array of Integers? Thankyou for reading! Quick Note, I am using PANDAS to create a dataframe of the CSV file Some Example Code Im using Error Message: CSV Data Update: Got Two Columns of dating working using the following function... However when trying to include my two other column object types (What data looks like in both those columns) ""3,9,11,16,25,26,28,29,36,40,41,46,63,66,67,69,72,73,78,80"" I get an error, here is the function I tried for that This Error Appears: Should I try to Cast those two types outside the function and try combining it later into the TFRecord file alongside the tf.data from the make_csv_dataset function?",1,Requesting (Additional) Documentation/Examples
328,58162110,How to port a tf.Session to a tf.train.MonitoredSession call while allowing graph modifications,"The code I'm working on is this. The code uses tf.session call to take in a graph for object detection tasks. Link My aim here is to profile this code for Nvidia GPUs using the nvtx-plugins-tf to analyze the time taken for different ops. Link to docs The plugin library provides a function hook for a tf.train.MonitoredSession as given in their example code here. The code linked above uses tf.session along with a tf.config and when I try to modify the tf.session call to a tf.train.MonitoredSession call, I can't get my code to work and it fails with an error that graph can't be modified. I went through the tensorflow APIs and it turns out that tf.session doesn't support hook callbacks and tf.train.MonitoredSession doesn't support tf_config as a function argument. Any directions to go in would be appreciated. If there are ways in tensorflow to use hooks in conjunction with tf.session, that will also work for me.",1,Documentation Replication on Other Examples
329,58176215,Tensorflow difference between tf.stop_gradient and feed variables to optimizer?,"I'm trying to train a model in self-supervised learning. The flow chart is something like the following: Let's assume that N1 is already trained and we want to train just N2. This is my current implementation: In this way, I should be optimizing only N2. What makes me confused is the fact that if I were to use the following code I would obtain very different results (much better than the above): I wonder what is the problem with the first implementation. What is exactly the behaviour of tf.stop_gradient (the documentation is a bit poor)? How does this differ from the second approach? From a practical perspective in semi-supervised learning: what is the difference between the two? Which one is the correct approach? Thank you :) I added a possible solution to the problem in the comments below. I would still be happy to receive any feedback from more experienced users and to share some opinions on the best approach to structure a self-supervised learning problem in tensorflow. Bye, G.",1,Documentation Replication on Other Examples
330,58225926,Tensorflow Gradient Tape returning None,"I'm using TensorFlow 1.14.0 with Python(3.6.8). I'm trying to use tensorflow_probability's lbfgs optimizer implementation(documentation/example). If I run the example code provided in the documentation it works fine. I tried to follow the same procedure for my own code which uses the tf.GradientTape() approach for computing the objective function. When doing it that way, the gradients come back as None type. I'm not seeing why one is working, but the other is not. Edit: I realized that running eager execution using the gradients wouldn't work, so I adjusted the example to be able to be run with eager execution. Non-working example(using GradientTape) with eager execution",1,Documentation Replication on Other Examples
331,58338310,Predefined layers inside Custom layers,I want to use predefined layers from tf.keras.layers inside a custom layer. I want to create a custom layer that is a combination of dense and 1D Convolution layers. Is it possible to do something like that? I could not find an example in the tensorflow pages.,1,Lack of Alternative Solutions/Documentation
332,58384884,'tensorflow.python.keras.api._v1.keras.losses' has no attribute 'Reduction',"I am using Huber loss implementation in tf.keras in tensorflow 1.14.0 as follows: I am getting the error AttributeError: module 'tensorflow.python.keras.api._v1.keras.losses' has no attribute 'Reduction' I have tried using tf.losses.Reduction, tf.compat.v2.losses.Reduction nothing seems to work. Did tensorflow remove Reduction from tf.keras.losses, it is strange if they did so because their documentation still shows: https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/losses/Huber#args",1,Documentation Ambiguity
333,58412668,Hparams plugin with tf.keras (tensorflow 2.0),"I try to follow the example from the tensorflow docs and setup hyperparameter logging. It also mentions that, if you use tf.keras, you can just use the callback hp.KerasCallback(logdir, hparams). However, if I use the callback I don't get my metrics (only the outcome).",1,Documentation Ambiguity
334,58520594,"tf.Data.Dataset - On each Epoch, only train with a sub sample of the full dataset","I have an image dataset with a large imbalance of positive and negatives samples (many more negatives). I would like to create a tf.data.Dataset where each epoch it will train with all of the positive samples but only (ratio * len(positive) ) of the negative samples. I am currently using a datagen inherited from keras.util.Sequence to achieve this and using this subsampling policy is performing much better than training on all data. However reading the docs on Dataset, I cannot seem to find a way to do it, is it possible? In my existing data generator, I am doing this:",1,Lack of Alternative Solutions/Documentation
335,58630393,Does tf.keras.metrics.AUC work on multi-class problems?,"I have a multi-class classification problem and I want to measure AUC on training and test data. tf.keras has implemented AUC metric (tf.keras.metrics.AUC), but I'm not be able to see whether this metric could safely be used in multi-class problems. Even, the example ""Classification on imbalanced data"" on the official Web page is dedicated to a binary classification problem. I have implemented a CNN model that predicts six classes, having a softmax layer that gives the probabilities of all the classes. I used this metric as follows and the code was executed without any problem. However, sometimes I see some results that are quite strange for me. For example, the model reported an accuracy of 0.78333336 and AUC equal to 0.97327775, Is this possible? Can a model have a low accuracy and an AUC so high? I wonder that, although the code does not give any error, the AUC metric is computing wrong. Somebody may confirm me whether or not this metrics support multi-class classification problems?",1,Documentation Replication on Other Examples
336,58631390,What is the purpose of tf.compat?,"What's the purpose of tf.compat module? It looks like just the entire Tensorflow API is replicated inside this module. The documentation states So why there is a ""v1"" and a ""v2"" submodule? What are the compatibility problems address by tf.compat specifically?",1,Documentation Completeness
337,58699961,Problem of predicting with a loaded tensorflow estimator trained beforehand,"I had trained a tensorflow NN estimator to predict something in Python. And I saved the model in my Google drive via Google colab. Today, I loaded the model and it was a pretty hard work. I finally succeeded to load the estimator using tf.compat.v2.saved_model.load and .signature method. it seems WrappedFunction. This is my code till this step. But still I fail to put a test data to model to make a prediction. first, I tried: And it return an error Secondly, looked for some tensorflow document, I wrote this code This time it return I found dynamic/static shape of tensorflow. But I couldn't fully understand those concept and failed to reshaping. How can I get the result? Thanks.",1,Documentation Ambiguity
338,58730460,Freeze sublayers in tensorflow 2,"I have a model which is composed of custom layers. Each custom layer contains many tf.keras.layers. The problem is that if I want to freeze those layers after defining my model, the loop: only prints the ""outer"" custom layers and not those who exist inside. Is there any way to access the inner layers so I can freeze them? an example of a custom layer from the official tf docs:",1,Documentation Replication on Other Examples
339,58802573,Pre processing keras dataset using keras tokenizer,"I am trying to do some pre processing using the keras tokenizer on data I read using the following code: Now that I have the parsed example (output of _parse_example map function) I want to do some pre-processing on the text using tf.keras.preprocessing.text.Tokenizer method texts_to_sequences. However, texts_to_sequences expects an input of python strings and I get Tensors in the parsed_example. I can work around it by using py_func to wrap my code (see 'emb': tf.py_func.. in the code below), but then I will not be able to serialize my model (according to the py_func documentation). Looking for a way to do that (or a link to some similar example)",1,Lack of Alternative Solutions/Documentation
340,58818679,Why a model using tf.py_function can not be serialized?,According to the documentation of ty.py_function a model using it can't be serialized. Why is serialization not possible? I was looking for an explanation to why this is the case and alternatives to using tf.py_function but did not find helpful ones. In my specific case I want to use the Keras Tokenizer and its methods expect numpy arrays - so I am calling it using tf.py_function.,1,Lack of Alternative Solutions/Documentation
341,58822319,How to use tfa.seq2seq.BahdanauAttention with tf.keras functional API?,"I want to use tfa.seq2seq.BahdanauAttention with functional API of tf.keras. I have looked at the example given at tensorflow/nmt/attention_model.py. But I couldn't figure out how to use it with tf.keras's functional API. So I would like to use tfa.seq2seq.BahdanauAttention for a lipreading task, something like this: Thanks in advance.",1,Documentation Ambiguity
342,58842107,How do I update a model using a pre-release version of Tensorflow to run in a Google Colab instance?,"I'm trying to use the WikiReading dataset and model in a project and train it using a Google Colaboratory instance. For that purpose, I'm adapting the code to a Jupyter Notebook, which also uses a more recent version of Tensorflow than the provided baseline Bag of Words model did. The original paper and accompanying baseline model was published in August 2016, which predates Tensorflow 1.0.0. I've gone some way towards the process of updating the model to Tensorflow v1.x, but I appear to have hit a roadblock. From my (fairly limited) understanding, the last step in the model is to apply a softmax function on the results. In the original model, this was done using the following function call: This is then used in this statement for the optimization: I can't seem to find any documentation relating to the tf.contrib.learn.ops.softmax_classifier() function online. I'm assuming it takes in 4 tensors in some order, most likely something like the first holds the batch to classify, the second one holds a list of the answers to predict with the 3rd and 4th holding the embedding and biases for each answer. My problem is that I cannot find a function that maps neatly to that format with the same output and I'm not sure what transformations to apply to my tensors to get a similar result using something like tf.nn.softmax() without access to the documentation of tf.contrib.learn.ops.softmax_classifier(). How should I go about tackling this problem? Should I just rewrite the model_fcn()? The full Jupyter notebook can be accessed here I have found the source for the mentioned function here. The deprecation warning says this about updating this function: Now, I'm replicating the operations of this function as follows: but this gives me: This makes perfect sense when I look at the constant definitions: I guess my questions now are: As I try to update and train this model, I have grown more and more confused by the way it is defined. This is largely due to my inexperience with machine learning in general and TensorFlow in particular, but there are some things that don't make sense, at least to me. I have adjusted the ANSWER_NUM and ANSWER_DIM as I mention above and updated other functions and parameters (seen in the updated model_fn above) which gives me a valid data graph but the following error when trying to fit: This probably just requires a formatting step (that I'm not entirely sure of yet) before feeding running softmax_cross_entropy(). However, I also noticed that there is no definition of the hidden layers anywhere in the original model and HIDDEN_SIZE is unused. Is there an implicit definition of those layers somewhere in the model that I'm missing? At this point, I feel like it will be easier to just use a Keras functional model in TensorFlow 2.x to get as close as possible to the perceived original model.",1,Lack of Alternative Solutions/Documentation
343,58963793,ValueError: Shapes must be equal rank in assign_add(),"I am reading tf.Variable in Tensorflow r2.0 in TF2: And also how come the following returns false? My other question is that when we are in TF 2, we should not use tf.Session() anymore, correct? It seems we should never run session.run(), but the API document keys doing it with tf.compat.v1, etc. So why they are using it in TF2 docs? Any help would be appreciated. CS",1,Documentation Completeness
344,59056872,Why class name change after saving a keras model?,i wrote a basic keras model (tf.keras.__version = 2.2.4-tf) using tensorflow (2.0.0) : Results are Where can I find the difference between tensorflow.python.keras.saving.saved_model.load.Sequential and tensorflow.python.keras.engine.sequential.Sequential in tensorflow or keras documentation?,1,Lack of Alternative Solutions/Documentation
345,59074659,Best practice for allocating GPU and CPU resources in TensorFlow,"I'm wondering what is the correct way to set devices for creating/training a model in order to optimize resource usage for speedy training in TensorFlow with the Keras API? I have 1 CPU and 2 GPUs at my disposal. I was initially using a tf.device context to create my model and train on GPUs only, but then I saw in the TensorFlow documentation for tf.keras.utils.multi_gpu_model, they suggest explicitly instantiating the model on the CPU: I did this, and now when I train I see my CPU usage go way up with all 8 cores at about 70% usage each, and my GPU memory is maxed out. Would things go faster if the model were created on one of the GPUs? Even if I have just 1 GPU, is it still better to create model on CPU and use tf.device context to train the model on the GPU?",1,Documentation Replication on Other Examples
346,59142986,serving_input_receiver_fn() function without the deprecated tf.placeholder method in TF 2.0,"I have a functioning tf.estimator pipeline build in TF 1, but now I made the decision to move to TF 2.0, and I have problems in the end of my pipeline, when I want to save the model in the .pb format I'm using this high level estimator export_saved_model method: https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesRegressor#export_saved_model I have two numeric features, 'age' and 'time_spent' They're defined using tf.feature_column as such: After the model has been trained I turn the list of features into a dict using the method feature_column_make_parse_example_spec() and feed it to another method build_parsing_serving_input_receiver_fn() excactly as outlied on tensorflow's webpage, https://www.tensorflow.org/guide/saved_model under estimators. I then inspect the output using the CLI tools Resulting in the following: enter image description here Somehow Tensorflow squashes my two usefull numeric features into a useless string input crap called ""inputs"". In TF 1 this could be circumvented by creating a custom input_receiver_fn() function using some tf.placeholder method, and I'd get the correct output with two distinct numeric features. But tf.placeholder doesn't exist in TF 2, so now it's pretty useless. Sorry about the raging, but Tensorflow is horribly documented, and I'm really working with high level API's and it should just be straight out on the horse, but no. I'd really appreciate any help :)",1,Documentation Ambiguity
347,59174710,default tf.gradients in TensorFlow - total or partial derivatives?,"so I'm reading about tf.gradients() in the documentation (https://www.tensorflow.org/api_docs/python/tf/gradients) and I'm a bit confused. I've seen people stating that the results of tf.gradients() are This is also what I was thinking first. But then the documentation describes one optional arguments of this function as follows: So is it only possible to calculate the partial derivatives if I use 'stop_gradient' and otherwise the default values returned in a vector with len(xs) are total derivatives? Probably it's just my misunderstanding, it would be much appreciated if someone could elaborate on this a bit. Thanks a lot!",1,Requesting (Additional) Documentation/Examples
348,59177677,Custom aggregation for tf.GradientTape().gradient? (TF2.0),"As far as I know, the tf.gradients function provides option to choose the aggregation method for summarizing the gradients from multiple sources. https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/gradients However according to the Tensorflow API documentation, the tf.GradientTape().gradient method has no such option. https://www.tensorflow.org/api_docs/python/tf/GradientTape So my questions are as follows:",1,Documentation Ambiguity
349,59361689,Redundancies in tf.keras.backend and tensorflow libraries,"I have been working in TensorFlow for about a year now, and I am transitioning from TF 1.x to TF 2.0, and I am looking for some guidance on how to use the tf.keras.backend library in TF 2.0. I understand that the transition to TF 2.0 is supposed to remove a lot of redundancies in modeling and building graphs, since there were many ways to create equivalent layers in earlier TensorFlow versions (and I'm insanely grateful for that change!), but I'm getting stuck on understanding when to use tf.keras.backend, because the operations appear redundant with other TensorFlow libraries. I see that some of the functions in tf.keras.backend are redundant with other TensorFlow libraries. For instance, tf.keras.backend.abs and tf.math.abs are not aliases (or at least, they're not listed as aliases in the documentation), but both take the absolute value of a tensor. After examining the source code, it looks like tf.keras.backend.abs calls the tf.math.abs function, and so I really do not understand why they are not aliases. Other tf.keras.backend operations don't appear to be duplicated in TensorFlow libraries, but it looks like there are TensorFlow functions that can do equivalent things. For instance, tf.keras.backend.cast_to_floatx can be substituted with tf.dtypes.cast as long as you explicitly specify the dtype. I am wondering two things:",1,Documentation Replication on Other Examples
350,59497372,Is there an alternative to tf.py_function() for custom Python code?,"I have started using TensorFlow 2.0 and have a little uncertainty with regard to one aspect. Suppose I have this use case: while ingesting data with the tf.data.Dataset I want to apply some specific augmentation operations upon some images. However, the external libraries that I am using require that the image is a numpy array, not a tensor. When using tf.data.Dataset.from_tensor_slices(), the flowing data needs to be of type Tensor. Concrete example: The code above does not work yielding an I have read the documentation on TensorFlow 2.0 stating that if one wants to use an arbitrary python logic, one should use tf.py_function or only TensorFlow primitives according to: How to convert ""tensor"" to ""numpy"" array in tensorflow? My question is the following: Is there another way to use arbitrary python code in a function with a custom decorator/an easier way than to use tf.py_function? To me honestly it seems that there must be a more elegant way than passing to a tf.py_function, transforming to a numpy array, perform operations A,B,C,D and then retransform to a tensor and yield the result.",1,Lack of Alternative Solutions/Documentation
351,59531864,Why does TensorFlow calculate 2D convolutions when 1D convolution is called?,"In the documentation of tf.nn.conv1d, it is stated that I get that the operations are equivalent, but I am a bit confused about the implications of this implementation detail. Does the reshaping create some computational overhead? The 3D convolution has its own implementation, so why not the 1D convolution? Thanks for any explanation that helps me and others to understand this implementation detail of TensorFlow!",1,Documentation Replicability
352,59555206,keras to tf.keras Conversion: Dense layer dimensions not defined?,"So I've built a convnet using pure keras. It compiles and operates exactly as intended, but I need to convert it to use tf.keras so that I can make use of tfmot. Having read documentation, I attempted to convert it, only to get the following error: The last dimension of the inputs to Dense should be defined. Found None. Any idea what I'm doing wrong? Thanks! Original keras model: Converted tf.keras model: EDIT 1: I thought maybe I could get around the issue by saving the keras model after creation and loading it as a tf.keras model immediately before compilation / training. That throws the same error!",1,Documentation Replication on Other Examples
353,59607363,Tensorflow 2.0 dataset batching not working properly,"Tensorflow 2.0 dataset api's batch is not working as I expected it to work. I've made a dataset like this. This yields DatasetV1Adapter shapes: ((6,), ()), types: (tf.float32, tf.float32), and to this dataset I applied batch function from tf.data.Dataset. yields DatasetV1Adapter shapes: ((None, 6), (None,)), types: (tf.float32, tf.float32), and changing the batch size doesn't help at all. From official description of the batch, The way I thought this function would work, was to make [batch, 6], [batch,] but didn't work out well. I originally used pytorch, and started using TF 2.0 recently, and need some help on proper batching. Thanks in advance.",1,Documentation Replication on Other Examples
354,59743351,Tensorflow 2.0.0: AttributeError: 'TensorSliceDataset' object has no attribute 'as_numpy_iterator',"I am testing tensorflow tf.data.Dataset method as_numpy_iterator using tensorflow 2.0.0. According to the official documentation https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#as_numpy_iterator, this function allows directly inspecting the content of a tensorflow dataset. But when I try the given example: There occurs an error: AttributeError: 'TensorSliceDataset' object has no attribute 'as_numpy_iteractor'. I am wondering if this method is just newly added, beyond the support of tensorflow 2.0.0. If so, is there an alternative to checking the dataset content as the as_numpy_iterator()?",1,Documentation Replicability
355,59870349,"tf.data for keras model with multiple inputs, one ctc output, ValueError","I'm using tf.version '2.1.0-rc2' My model has 2 inputs and 1 output and I am struggling to feed it data. Example https://keras.io/examples/image_ocr/ uses a generator that output a dictionary of numpy arrays. and fit_generator but https://www.tensorflow.org/api_docs/python/tf/keras/Model indicates I believe td.data is the recommended way to feed data into tf.keras. So I would like to figure this out. First, a simplified dummy model My generator based on tf.data Instantiate a train and validation generator from the above Call fit with the td.data generators The error I also tried the dictionary approach ... with model.fit calling as recommended But this gives me the following error:",1,Documentation Replicability
356,59998335,Constantly update tf.cond based on bool value,"I am using tf.cond for controlling the flow of the Tensorflow graph. I went through the documentation and was able to implement tf.cond based branching successfully. But my concern is that while the graph is being loaded the value of the bool variable is checked and the branching decision is made at the initialization step itself. Any further changes in the bool is not tracked. Following is the MWE that better describes the problem: This prints only 32s. I tried with eager_execution too with the following code: Still the same result. So my question is how can I write code such that one part of the graph is chosen dynamically, based on the updates to the bool variable (if possible)? Thanks. I am using Tensorflow v1.14.",1,Documentation Replication on Other Examples
357,60013980,tf.nn.embedding_lookup_sparse 3D sparse tensor input,"I have an embedding matrix and there is a 3D sparse tensor which is used to get the embedding output, after reading the docs of tf.nn.embedding_lookup_sparse I found it only supports 2D sparse tensors, My example code here But the expected output is a matrix with a dimension of 3x2x4, not 3x4. Does tf.nn.embedding_lookup_sparse support this operation?",1,Documentation Replicability
358,60047705,Repeated use of GradientTape for multiple Jacobian calculations,"I am attempting to compute the Jacobian of a TensorFlow neural network's outputs with respect to its inputs. This is easily achieved with the tf.GradientTape.jacobian method. The trivial example provided in the TensorFlow documentation is as follows: This is fine if I want only want to compute the Jacobian of a single instance of the input tensor x. However, I need to repeatedly evaluate this Jacobian many, many times for various instances of x. For a non-trivial Jacobian calculation (e.g. for a deep convolutional neural network with non-linear activation functions), this is incredibly expensive to repeatedly rerun the GradientTape calculation and evaluate the jacobian method. I know from the TensorFlow documentation that the gradients (and hence the Jacobian) are computed via automatic differentiation. I have to imagine there is some internal storage of the analytical gradient of the network (computed by automatic differentiation) which is evaluated at the given inputs. My question: am I correct in assuming that TensorFlow builds and stores (at least parts of) the analytical gradients needed to compute the Jacobian? And if so, is there a way to save this analytical gradient and re-evaluate the Jacobian with new inputs without having to reconstruct it via the GradientTape method? A ""persistent"" GradientTape does not seem to solve this issue: it only allows for the repeated evaluation of a single GradientTape instance with respect to multiple internal arguments of the computation.",1,Documentation Replication on Other Examples
359,60143153,Is there a way in tensorflow to load batches of data each time?,"So I'm running tensorflow 2+ python in google colab. Each of my data file is a 3d image with shape [563, 563, 563, 1], so loading all of them throws a resource exhaustion error. I've spent days and hours searching for a way to load only a batch of my dataset as tensor and unloading/loading new batch each iteration. I'm guessing there might be a way using tf.data.Dataset.list_files, but I can't find the exact way. Is there any good suggestions on a way to do it or any documents I could try to read? I've read the tf.data document from tensorflow, but couldn't find the information I needed. Thank you! so this is the function I want to use to load my image and this was the way I was loading the dataset before, which exhausted the resource;",1,Requesting (Additional) Documentation/Examples
360,60213882,Using Tensorflow Interleave to Improve Performance,"I have an input pipe that is performing poorly with low CPU, GPU, and disk utilization. I've been reading the tensorflow ""Better performance with tf.data API"" doc and the Dataset docs, but I don't understand what's going on well enough to apply it to my situation. Here's my current setup: Should I interleave the whole pipe right at the end (POINT2), before the prefetch? Or interleave imgd and outd separately, after each FixedLengthRecordDataset (POINT1A, POINT1B), and parallelize the maps? (need to keep the imgd and outd synced up!) What's up with Dataset.range(rvalue)---seems it's necessary but not obvious what rvalue to use? Is there a better overall plan? Note that the datasets are very large and do not fit in RAM.",1,Documentation Replicability
361,60215970,What's the cleanest and most efficient way to pass two stereo images to a loss function in Keras?,"First off, why am I using Keras? I'm trying to stay as high level as possible, which doesn't mean I'm scared of low-level Tensorflow; I just want to see how far I can go while keeping my code as simple and readable as possible. I need my Keras model (custom-built using the Keras functional API) to read the left image from a stereo pair and minimize a loss function that needs to access both the right and left images. I want to store the data in a tf.data.Dataset. What I tried: So, is there anything I did not consider? I read the documentation and found nothing about what gets considered as y_pred and what as y_true, nor about how to convert a dataset into a tensor smartly and without loading it all in memory. My model is designed as such: And my dataset is built as such (in case 2, while in case 1 only the function read_stereo_pair_from_line() changes):",1,Lack of Alternative Solutions/Documentation
362,60311184,how to loop over a tensor object until a condition met,"I have a tensor like this: I want to loop through this tensor untill all elements get True. So I have another function, which will update this tensor, lets call it uniqueness. I looked at the documentation and got to know that I can do that using tf.while_loop. Although, I could not find any example working on boolean stuff. This is what I have done so far: It is obviously incorrect, but don't know how to use each element of masked_bad_col as a condition to continue looping through uniqueness function. Update 1 This is the method I am trying to call in the loop: And this is the way I called this method in the while_loop",1,Inadequate Examples
363,60314717,How does shuffle and batch work in tf.data.dataset?,I'm working on a large dataset with around 10million datapoints so I've decided to use tf.data.dataset api for fetching dataset. I've few doubts which isn't clear from tensorflow docs. I hope someone can address them. How does the shuffle work in my case? Because I have 10 million datapoints should I shuffle all 10 million (or) will 100k be enough? Will it have any performance impact choosing a large shuffle? Will the batch is considered only from shuffled dataset (or) the original dataset?,1,Documentation Replication on Other Examples
364,60384790,Difference - tf.gradients vs tf.keras.backend.gradients,"Being new to Tensorflow, I am trying to understand the difference between underlying functionality of tf.gradients and tf.keras.backend.gradients. The latter finds the gradient of input feature values w.r.t cost function. But I couldn't get a clear idea on the former whether it computes the gradient over cost function or output probabilities (For example, consider the case of binary classification using a simple feed forward network. Output probability here is referred to the Sigmoid activation outcome of final layer with single neuron. Cost is given by Binary cross entropy) I have referred the official documentation for tf.gradients, but it is short and vague (for me), and I did not get a clear picture - The documentation mentions it as just 'y' - is it cost or output probability? Why I need the gradients? To implement a basic gradient based feature attribution.",1,Requesting (Additional) Documentation/Examples
365,60398554,"Should we apply repeat, batch shuffle to tf.data.Dataset when passing it to fit function?","I still don't after having read documentation about tf.keras.Model.fit and tf.data.Dataset, when passing tf.data.Dataset to fit function, should I call repeat and batch on the dataset object or should I provide the batch_size and epochs arguments to fit instead? or both? Should I apply the same treatment to the validation set? And while I'm here, can I shuffle the dataset before the fit? (seems like it's an obvious yes) If so, before, after calling Dataset.batch and Dataset.repeat (if calling them)? Edit: When using batch_size argument, and without having called Dataset.batch(batch_size) previously, I am getting the following error: Thanks",1,Documentation Replication on Other Examples
366,60469970,How does tf.function compile a python function with autograph?,"How does tf.function compile a python function operating on tensors into a graph, especially wrt autograph? The docs don't go into detail Does it use the special methods called by conditionals (__bool__) and loops (__iter__) to 'trace' the function's implementation? For example could use the fact that the if results in Tensor.__bool__(...) and for _ in t results in Tensor.__iter__(...)",1,Documentation Replicability
367,60516977,Difficulties in understanding higher order derivatives for tf.custom_gradient(),"Based on the example as quoted in tensorflow's website here: https://www.tensorflow.org/api_docs/python/tf/custom_gradient There is a lack of details on why second_order_and_transpose(ddy) returns two objects. Based on the documentation of tf.custom_gradient, the grad_fn (i.e. second_order_and_transpose()) should return a list of Tensors which are the derivatives of dy w.r.t. unused_x. It is also not even clear why did they name it unused_x. Anyone has any idea on this example or in general create custom gradients for higher order derivatives?",1,Documentation Completeness
368,60525257,How to apply Non max suppression on batch of images in tensorflow 1.14?,"I have batch of cropped images from original image on which I have to perform object detection, I am trying to apply tensorflow NMS operation. I looked into tensorflow api docs, and found tf.image.combined_non_max_suppression(), but I am unable to understand it properly. The flow in my pipeline is of two step. For the first step, I use simple tf.image.non_max_suppression() followed by tf.gather(), but I am not able to understand, how to do it for second step. Please refer to code snippets below: But I got following error, when tried running above: How to solve it and apply NMS for batch of images in tensorflow ?",1,Documentation Replicability
369,60590333,Increasing each element of a tensor by the predecessor in Tensorflow 2.0,"I'm new to tensorflow 2.0, and haven't done much except designing and training some artificial neural networks from boilerplate code. I'm trying to solve an exercise for newcomers into the new tensorflow. I created some code, but it doesn't work. Below is the problem definition: Assuming we have tensor M of rational numbers in shape of (a, b, c) and scalar p  (0, 1) (memory factor), lets create a function that will return tensor N in shape of (a, b, c). Each element of N tensors moving along axis c should be increased by the value of predecessor multiplied by p. Assuming we have tensor: in shape of (1, 1, 4), we would like to get vector: Solution should be created in Tensorflow 2.0 and should be focused on delivering the shortest execution time on CPU. Created graph should allow to efficiently calculate derivative both on tensor M and value p. This is the code I created till now: But it throws a TypeError. I looked around documentation, I've seen functions like cumsum and polyeval, but I'm not sure they fit my needs. To my understanding, I need to write my own customer function annotated with @tf.function. I'm also not sure how to handle 3-dimension tensors properly according to the problem definition (adding the predecessor should happen on the last (""c"") axis). I've seen in documentation (here: https://www.tensorflow.org/tutorials/customization/performance) that there are ways to measure size of the produced graph. Although, I'm not sure how ""graph"" allows to efficiently calculate derivative both on tensor M and value p. ELI5 answers appreciated, or at least some materials I can read to educate myself better. Thanks a lot!",1,Lack of Alternative Solutions/Documentation
370,60639731,Tensorboard for custom training loop in Tensorflow 2,"I want to create a custom training loop in tensorflow 2 and use tensorboard for visualization. Here is an example I've created based on tensorflow documentation: I am accessing tensorboard with the following command on terminal: The code above produce summaries for losses and metrics. My question is: I've tried to use the recommended commands from tensorflow: tf.summary.trace_on() and tf.summary.trace_export(), but I haven't managed to plot the graph. Maybe I am using them wrong. I whould really appreciate any suggestion on how to do this.",1,Documentation Replication on Other Examples
371,60708695,"How can I make ""element wise"" comparsion inside of the tf.function?","I try to make my own activation function in TensorFlow 2 and the function looks like this: The problem is that it cant take as argument tf.constant([2.0, 3.0])because there is an issue with conditions. I have tried tf.math.qreater_equal(x, 0) which lead to same output also tf.cond(). I have had no luck with documentation examples either. It returns error: Thanks!",1,Documentation Ambiguity
372,60761888,Chaining custom Keras layers in functional style,"I want to build a model using tf.keras' functional API. My model is quite large, hence I would like to create custom layers by inheriting from tf.keras.layers.Layer. Below is my attempt, inspired by TensorFlow's documentation. This code crashes with the following error. What's wrong with my approach?",1,Documentation Replication on Other Examples
373,60782077,How do you use tensorflow ctc_batch_cost function with keras?,"I have been trying to implement a CTC loss function in keras for several days now. Unfortunately, I have yet to find a simple way to do this that fits well with keras. I found tensorflow's tf.keras.backend.ctc_batch_cost function but there is not much documentation on it. I am confused about a few things. First, what are the input_length and label_length parameters? I am trying to make a handwriting recognition model and my images are 32x128, my RNN has 32 time steps, and my character list has a length of 80. I have tried to use 32 for both parameters and this gives me the error below. Shouldn't the function already know the input_length and label_length from the shape of the first two parameters (y_true and y_pred)? Secondly, do I need to encode my training data? Is this all done automatically? I know tensorflow also has a function called tf.keras.backend.ctc_decode. Is this only used when making predictions? Error: tensorflow.python.framework.errors_impl.InvalidArgumentError: squeeze_dims[0] not in [0,0). for 'loss/softmax_loss/Squeeze' (op: 'Squeeze') with input shapes: [] Model: Here is the tensorflow documentation I was referencing: https://www.tensorflow.org/api_docs/python/tf/keras/backend/ctc_batch_cost",1,Documentation Ambiguity
374,60801403,NotFoundError: No registered 'PyFunc' OpKernel for 'CPU' devices compatible with node {{node PyFunc}} . Registered: <no registered kernels>,I am getting an error while trying to access data from a tf.data.Dataset object. The dataset object is built from a generator. Any help will be appreciated. I'm using TensorFlow 2 and trying to run the example from https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator The error is :,1,Documentation Replicability
375,60919434,String to one_hot tensor in Tensorflow,I have found in tensorflow doc the following function to compute and apply a vocabulary onto a string tensor but it was still using tf.session and I can't make it work with tf.function: I have been able to build such a function with direct use of the hash facilities. However I have had to use a hard-coded bucket_size/depth param. Any ideas?,1,Documentation Replicability
376,61059725,Why does tf.constant give a dtype error if we pass in a tensor?,"The following code gives the following error: Although from the documentation, setting dtype means that tf.constant is supposed to cast a to the specified data type. So I don't see why this should give a type error. I also know that: does not give an error. So actually, I'm mainly wondering about what's happening under the hood here.",1,Documentation Ambiguity
377,61136605,Why tf2 can't save a tf_function model as .pb file?,"I tried to saved a model like the official code of transformer on official website, but when i want to save the train_step graph or trace on it with tf.summary.trace_on ,it errors.the error is as I supposed it was some error on tensor operation and write an other demo to confirm my idea: the error occurs as supposed. But how can i fixed it? the positional_encoding requires and i have no idea about how to replace this operation.",1,Documentation Replicability
378,61175291,Why is optimizer.minimize not working if we pass loss as tf.constant?,"I simply have train = optimizer.minimize(loss = tf.constant(4,dtype=""float32"")) Line of code that i change before everything is working. Why it is giving error ? Because documentation say it can be tensor Here is Docs I really need this thing to work. In this particular example we can have other ways to solve it but i need it to work as my actual code can do this only Error is",1,Documentation Replication on Other Examples
379,61273445,Tensorflow MapDataset iterator fails,"I am trying to implement the method suggested by the tensorflow documentation over here (https://www.tensorflow.org/tutorials/load_data/images) to load images from local directory as a tensorflow dataset. Especially I am interested in loading using tf.data as a tf.data.Dataset object as it is suggested that the performance is better that way. I pretty much took the exact code from the documentation page and also made sure that the tensorflow version matches to the one in the documentation The problem happens when I try to iterate over the MapDataset object using take(). I get the following error and have no idea how to go about resolving this By some random coincidence I found that when CLASS_NAMES is set to None, the code runs and the lebel object of labeled_ds has a value 'False' See output below",1,Documentation Replication on Other Examples
380,61280184,Model with multiple outputs and custom loss function,"I'm trying to train a model that has multiple outputs and a custom loss function using keras, but I'm getting some error tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: iterating over ``tf.Tensor`` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function. It's hard to debug it because I'm doing model.compile and model.fit. I think it has something to do with how models are supposed to be defined when having multiple outputs, but I can't find good documentation on this. The guide specifies how to have models with multiple outputs suing the functional API, and has an example for this, but it doesn't clarify how custom loss functions should work when subclassing the Model API. My code is as follows: And where MLP is the following: I'm trying to minimize a custom loss function Finally, this is how I try to train it: Which results in the above error. Any pointers? In particular, the whole stack trace is So it's clearly related to the loss function. But the model's forward pass outputs a tuple, which I unpack in the loss function, so I don't know why is this an issue.",1,Lack of Alternative Solutions/Documentation
381,61293983,How does a tf.train.Int64LIst hold dtype unint64?,"From the tensorflow docs for TFRecords and tf.Example, it states on the topic of tf.train.Feature: How is it possible to store a uint64 dtype into a Int64List?",1,Documentation Replicability
382,61305781,Using Tensorflow embedded columns raises All feature_columns must be _FeatureColumn instances error,"I am new to tensorflow and I was trying to follow the official documentation where I came across tf.feature_column.categorical_column_with_vocabulary_list The code I tested is: However , when I run this sample code I get the following error : ValueError: All feature_columns must be _FeatureColumn instances. Given: [EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='colors', vocabulary_list=('X', 'R', 'G', 'B', 'Y'), dtype=tf.string, default_value=0, num_oov_buckets=0), dimension=3, combiner='mean', initializer=, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)] What I am doing wrong?",1,Documentation Replication on Other Examples
383,61355289,When will tf.print ACTUALLY WORK as expected (i.e. print the values of tensors and variables)?,"First of all, I am using TensorFlow 2.0 and I only care about this version or higher (and I am already caring too much for such a piece of software that only produces headaches). The TensorFlow documentation of tf.print says and then This is all very nice, but I still don't get where tf.print will ACTUALLY WORK (i.e. print the VALUES of variables and tensors) in my code. Of course, needless to say, I couldn't care less about the symbolic representations of tensors, variables or whatever. Whenever I try to use tf.print, I want to see the VALUES (real numbers, vectors or matrices). I've tried to use tf.print in multiple cases and in multiple places, e.g. Ok, so, as you can see, I have no idea where tf.print will do what I want, i.e. I want to see the values of tensors. If something is a tensor, it must have a value. Similarly for variables. So, when will tf.print ACTUALLY PRINT THE VALUES OF TENSORS? I am looking for answers that say e.g., ""tf.print will NEVER work"" or ""it will only work if you are dreaming"". Apart from the jokes and sarcasm, I am really looking for answers that tell me exactly in which places of my code or which stages of developing a model with TF tf.print will actually do what it is supposed to do. Please, don't tell me that tf.print will work when the input is a tensor!!",1,Documentation Ambiguity
384,61355474,Why does tf.executing_eagerly() return False in TensorFlow 2?,"Let me explain my set up. I am using TensorFlow 2.1, the Keras version shipped with TF, and TensorFlow Probability 0.9. I have a function get_model that creates (with the functional API) and returns a model using Keras and custom layers. In the __init__ method of these custom layers A, I call a method A.m, which executes the statement print(tf.executing_eagerly()), but it returns False. Why? To be more precise, this is roughly my setup The documentation of tf.executing_eagerly says But these cases are not my case, so tf.executing_eagerly() should return True in my case, but no. Why? Here's a simple complete example (in TF 2.1) that illustrates the problem. This example prints tf.executing_eagerly() = False. See the related Github issue.",1,Documentation Replicability
385,61428918,tensorflow2: keras: model.fit() callbacks and eager mode,"I am running Tensorflow 2.1 with keras API. I am following the following coding style: Now, I would like to save some intermediate layer tensor value as image summary (as a sample what is happening at n-th training step). In order to do this, I've implemented my own callback class. I've also learned how keras.callbacks.TensorBoard is implemented, since it can save layer weights as image summaries. I do the following in my on_epoch_end: Unfortunately, I am still getting issue related to eager/graph modes: Unfortunately, there is a little to no documentation on how to correctly combine keras callbacks and tf.summary.image. How could I overcome this issue? upd: tf_nightly-2.2.0.dev20200427 has the same behaviour.",1,Lack of Alternative Solutions/Documentation
386,61522019,Is it still necessary to implement `compute_output_shape()` when defining a custom tf.keras Layer?,"I have implemented a custom Layer in tf.keras, using TensorFlow 2.1.0. In the past, when using the stand-alone Keras, it was important to define the compute_output_shape(input_shape) method in any custom layer so that the computational graph could be created. Now, having moved to TF2, I found out that even if I remove that method from my custom implementation the layer still works as expected. Apparently, it works both in eager and graph mode. This is an example of what I mean: Is it safe to say that compute_output_shape() is not necessary anymore? Am I missing something important? In the documentation there's no explicit mention of removing compute_output_shape(), although none of the examples implements it explicitly. Thanks",1,Inadequate Examples
387,61526556,Serializing a tensor and writing to tfrecord from within a graph,"I would like to write tensorflow example records to a TFRecordWriter from inside an AutoGraph generated graph. The documentation for tensorflow 2.0 states the following: However, tf.io.serialize_tensor returns a tensor of byte-string. Creating an Example proto requires a bytes list, not a tensor. How do I write a tf.train.Example to a tf record from inside a graph? Code to reproduce: and the error",1,Documentation Replication on Other Examples
388,61720708,How do you save a Tensorflow dataset to a file?,"There are at least two more questions like this on SO but not a single one has been answered. I have a dataset of the form: and another of the form: I have looked and looked but I can't find the code to save these datasets to files that can be loaded later. The closest I got was this page in the TensorFlow docs, which suggests serializing the tensors using tf.io.serialize_tensor and then writing them to a file using tf.data.experimental.TFRecordWriter. However, when I tried this using the code: I get an error on the first line: How can I modify the above (or do something else) to accomplish my goal?",1,Documentation Replication on Other Examples
389,61743921,can we build object detection model using Tensorflow or it is only possible with the help f tf.keras,Is there any way to build object detection model using Tensorflow without any help of tf.keras module? From Tensorflow documentation I'm not able to find any example which helps to create model without Keras.,1,Lack of Alternative Solutions/Documentation
390,61767803,Tensorflow 1.x to Tensorflow 2.1.0,"I am trying to update code written in Tensorflow 1.x to code in Tensorflow 2.1.0. I have been converting codes using Tensorflow 2.1.0 documentation, and I had no problems until this code. Above code is Tensorflow 1.x version, and I think, according to Tensorflow 2.1.0 documentation, the properly updated code is Then, when I run I get the following error. So, I am guessing in Tensorflow 1.x version, the loss was passed as 'tensor' to tf.estimator.EstimatorSpec, but in Tensorflow 2.1.0, the loss has to be passed as scalar to tf.estimator.EstimatorSpec? Loss (the way it is defined here) in both Tensorflow 1.x and 2.1.0 is tensor if I remember it correctly. So, does anyone know how to convert tensor to scalar (which I don't think will be sufficient nor efficient in building the CNN model) or better yet, how to solve this dilemma? Or did I convert the original code the wrong way? I would very much appreciate if compat.v1. is not used unless absolutely necessary (i.e. no other way to use the code in Tensorflow 2.1.0 than compat.v1.)",1,Documentation Replication on Other Examples
391,61790621,List of tensors and just tensors,"I am updating codes from tensorflow 1.x to 2.1.0. I changed tensorflow 1.x code to tensorflow 2.1.0 code. But, when I run the updated code, I got the following error. So, I checked the tensorflow 2.1.0 document, and parameters for tf.keras.metrics.Accuracy.update_state() seem to be a list (in form of [ , , , ]). Then, I searched for a way to convert tensor to a list, which is After I run this code, it gives the following error. So, I tried to turn a list of Tensors into Tensors with tf.stack() did not work, as it gave the same initial TypeError saying 'y_pred' is missing at the updated code. torch.stack(), however, gave the following error. So, I am guessing torch.stack() only accepts a tuple, NOT a list. But, tf.stack() seems to accept a list, but it does not turn it into a Tensor? Are my labels and predict even a list of Tensors in the first place? If so, why would tf.stack() not turn them into Tensors? How can I correctly convert labels and predict so that they can be passed into tf.keras.metrics.Accuracy.update_state()? I would very much appreciate if not using compat.v1. unless absolutely necessary.",1,Documentation Replicability
392,61829273,Error trying to feed a tf.keras model with a tf.data.Dataset instead of tensors,"Why does the following tf2 tf.keras model 'work' when fitted with tensors but generates a ValueError when attempting to fit the same tensors in tf.data.Dataset.from_tensor_slices form? EDIT: Put another way, having developed/fitted/tested etc the model below using numpy arrays. How do those same numpy arrays need to be reshaped(?) so that they can be used to create a dataset with tf.data.Dataset.from_tensor_slices that works with the model? ValueError: If instead of using "".from_tensor_slices"" we use "".from_tensors"" to create X_ds and y_ds then, after zipping, all works well. However, the docs give me the impression "".from_tensors"" is memory heavy and not desirable. Also, I believe that the single element "".from_tensors"" dataset is simply providing the model with two 2D tensors whereas the from_tensor_slices version is a sequence of 1D elements.",1,Documentation Replication on Other Examples
393,61884176,Understanding tf.name_scope,"I am trying to understand tf.name_scope. The documentation mentions the following: ""This context manager pushes a name scope, which will make the name of all operations added within it have a prefix. For example, to define a new Python op called my_op: When executed, the Tensors a, b, c, will have names MyOp/a, MyOp/b, and MyOp/c."" My understanding is that the with block does not introduce a new local scope in Python. Under normal situation, the tensor variable a will also refer to the local parameter a of function my_op. How is the name prefixing with ""MyOp/"" implemented using Python context? In the source code link for tf.name_scope (https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/framework/ops.py#L6423-L6442) there is an invocation of but I could not find the semantics of context.context(). Most context manager discussion talk about enter and exit, but no mention of variable renaming with some prefix. Is this some introspective mechanism in Python that allows the manipulation of Python variable scopes? Many thanks for any insights.",1,Documentation Replicability
394,61885570,Reading a tfrecord: DecodeError: Error parsing message,I am using colab to run a tutorial on tensorflow ranking. It uses wget to fetch the tfrecord: I am using this code to try to look at the structure of the tfrecord: And I am getting: How to generally look at the structure of tfrecords instead? A second question: Where to find documentation on classes like tf.train.Example? I just find this empty page.,1,Requesting (Additional) Documentation/Examples
395,61946509,tf.estimator input_fn and eager mode,"I tried to use numpy inside cnn_model.evaluate(), but it gave AttributeError: 'Tensor' object has no attribute 'numpy'. I used numpy to calculate accuracy and mean squared error using tf.keras.metrics.Accuracy() and tf.keras.metrics.MeanSquaredError() inside cnn_model.evaluate() I googled it, and in tensorflow documentation, it said ""Calling methods of Estimator will work while eager execution is enabled. However, the model_fn and input_fn is not executed eagerly, Estimator will switch to graph mode before calling all user-provided functions (incl. hooks), so their code has to be compatible with graph mode execution."" So, I was wondering how I can update the current tf 1.x code to tf 2.1.0 code, while also using above information. My current code is: What I have tried so far is add tf.compat.v1.enable_eager_execution() to the 1) beginning of the code after all the imports, 2) next line right after importing tf, 3) line right before declaring eval_input_fn, 4) line right before calling eval_results, 5) inside CNN model definition. It all failed to turn on the eager mode. One other option that I found was remove @tf.function decorator, but I have no idea what that means and how to pass input_fn if @tf.function is removed.",1,Documentation Replication on Other Examples
396,61988657,Why does tensorflow.rank always return shape with null value,Being a beginner to TensorFlow I couldn't get why does tensorflow.rank always return shape with null value? This is what I am working on: and the output is So my question is what is this shape=() from tf.rank output? I couldn't get much from here - https://www.tensorflow.org/api_docs/python/tf/rank,1,Documentation Replicability
397,61994285,TensorFlow: Error using weighted_categorical_column,"I work on a binary classification problem containing a field STREET. In a first step I used the Tokenization to the get a word list (frequency of how often one word appears in the different datasets). Then I used this information to create two columns in my Dataframe describing the word and how often it was used: After I converted the Dataframe to a TensorFlow Dataset I have used the following code to add it to my future columns: As you can see I used the function tf.feature_column.weighted_categorical_column. Unfortunately I get the following error when I try to train my model: Furthermore I get the following warning: Now I have two questions: First: does it make sense to use this function for my described problem? Unfortunately, I couldnt find a detailed description how this function works (only this short documentations: https://www.tensorflow.org/api_docs/python/tf/feature_column/weighted_categorical_column) Second: How can I fix the described error?",1,Lack of Alternative Solutions/Documentation
398,62086633,Conditional branches using tf.function,"I am having problems calculating the gradient using the gradient tape when using a tf.function with conditional branches. Inside a gradient tape scope, I am trying to calculate the gradient of z w.r.t self.LMn. This works perfectly fine when I do not annotate the function with @tf.function. The error originates in a subclassed tf.keras.layers.Layer call function: The actual error is given as follows: More specifically, and I can assume that each of these outputs corresponds to the cases for some set of conditional branches outputs that are then fed into the tf.einsum function. I have read over all of the edge cases and precautions in the gradient tape documentation as this seems to the problem. Just a note that I am only performing conditional computation using hyperparameters (passed into tf.function as pythonic variables, such as basis_filters). There are also some conditional branches using (tensorflow ops) functions of these hyperparameters, is this allowed? or do I need to compute these values outside tf.function and pass these in as pythonic variables too? I know the question is not completely clear and I can provide any extra information if needed. It would very helpful to have some guidance on what to look for with this kind of problem. Thanks!",1,Documentation Replication on Other Examples
399,62211822,TF2 Keras - Feature Engineering in Keras saved model via Tensorflow Serving,"The Tensorflow 2 documentation for preprocessing / feature engineering over a Keras model seems to be quite confusing and isn't very friendly. Currently I have a simple Keras N-layer model with TF feature columns feeding as dense layer. For training I have CSV files read using tf.dataset API and I have written a feature engineering function that creates new features using dataset.map function. I can save the model easily using tf.keras.models.save_model method. However I am having trouble figuring out how to attach the feature_engineering steps in the serving function. Requirement: Now I want to take the same feature engineering function above and attach it to my serving function so that in JSON input via tensorflow_model_server the same feature engineering steps are applied. I know about the lambda Layer option in Keras but I want to do this via saved_model method but there are a lot of difficulties here. For Example, below code gives error: Error: The above error is because I have not provided InputSignature of my Keras model but I am not able to understand that I have 13 input fields, what is expected as input signature. So I wanted to know if anyone knows the shortest way of solving this out. This is a very basic requirement and Tensorflow seems to have kept this quite complicated for Keras Tensorflow model serving. GIST: https://colab.research.google.com/gist/rafiqhasan/6abe93ac454e942317005febef59a459/copy-of-dl-e2e-structured-mixed-data-tf-2-keras-estimator.ipynb EDIT: I fixed it, so TensorSpec has to be generated and passed for each feature and also model( ) has to be called in serving function.",1,Documentation Replication on Other Examples
400,62236460,How to set bounds and constraints on Tensorflow Variables (tf.Variable),"I am using Tensorflow to minimize a function. The function takes about 10 parameters. Every single parameter has bounds, e.g. a minimum and a maximum value the parameter is allowed to take. For example, the parameter x1 needs to be between 1 and 10. I also have a pair of parameters that need to have the following constraint x2 &gt; x3. In other words, x2 must always be bigger than x3. (In addition to this, x2 and x3 also have bounds, similarly to the example of x1 above.) I know that tf.Variable has a ""constraint"" argument, however I can't really find any examples or documentation on how to use this to achieve the bounds and constraints as mentioned above. Thank you!",1,Documentation Replicability
401,62249084,What is the numpy equivalent of TensorFlow Xavier initializer for CNN?,"I would like to re-create the Xavier initialization in NumPy (using basic functions) in the same way that TensorFlow2 does for CNN. Here is how I learned to do Xavier initialization in NumPy: This is the way I learned Xavier initialization for the logistic regression model. It seems that for Convolution Neural Network it should be different but I don't know how. I'm confused by the TensorFlow documentation when they explain the ""fan_in"" and ""fan_out"". I'm guessing this is where the problem is. Can somebody dumb it down for me, please? Much appreciate it! [UPDATE]: When I follow the tf.keras.initializers.GlorotUniform documentation I still don't come to the same results:",1,Documentation Replication on Other Examples
402,62284095,What are the parameters to tf.GradientTape()'s __exit__ function?,"According to the documentation for tf.GradientTape, its __exit__() method takes three positional arguments: typ, value, traceback. What exactly are these parameters? How does the with statement infer them? What values should I give them in the code below (where I'm not using a with statement):",1,Documentation Replicability
403,62476015,TF 2.2 Saved_model from keras with custom signatures and preprocessing,"I am trying to use the tf.saved_model.save after a training a Transformer Model in order to deploy it. My model has multiple inputs and outputs. If I am using the saved_model function for serving, I add some issue about the input shape with the first input, and the second input is not visible when I am using the saved_model_cli show function. I found a way to solve that issue by wrapping the main transformer block by a model module and then save the model. But, I see in the documentation there is another way which consist in using signatures to indicate the inputs/outputs during the tf.saved_model.save. To do that, we need to use the tf.Module class but I haven't understood how to use it in the case we have multiple inputs/outputs exactly (how to make understand the module that inputs tensor should be related to that inputs for the model?) . Does anyone know how to do that with the second method ? Morever can we do preprocessing of the data through the signature ?https://www.tensorflow.org/guide/saved_model",1,Documentation Replication on Other Examples
404,62611459,How to support mixed precision in custom Tensorflow layers?,"When developing my own custom layers for tf.keras: how am I supposed to support mixed precision? The documentation of mixed precision - a feature which is currently marked as experimental in Tensorflow 2.2 - only explains how to use it from a consumers perspective with predefined layers such as the tf.keras.layers.Dense one. I already tried to guess it myself and found two - maybe relevant - details: Am I supposed to use the above get_layer_policy-method and just cast my variables into compute_dtype within the call(...) method of my layer? (And pass variable_dtype in my layers build(...) method to add_weight(...) when creating variables?) For example, here is naive sample implementation of a standard dense neuron layer: Sure, nobody would implement such basic stuff themselves, that one is just for demonstration. But, would this be the way the Tensorflow team expects us to implement the call(...) methods of our custom layers?",1,Documentation Replication on Other Examples
405,62670041,batch_size in tf model.fit() vs. batch_size in tf.data.Dataset,"I have a large dataset that can fit in host memory. However, when I use tf.keras to train the model, it yields GPU out-of-memory problem. Then I look into tf.data.Dataset and want to use its batch() method to batch the training dataset so that it can execute the model.fit() in GPU. According to its documentation, an example is as follows: Is the BATCH_SIZE in dataset.from_tensor_slices().batch() the same as the batch_size in the tf.keras modelt.fit()? How should I choose BATCH_SIZE so that GPU has sufficient data to run efficiently and yet its memory is not overflown?",1,Documentation Replication on Other Examples
406,62752605,Loss function in tf.nn.sampled_softmax_loss,"I have a question regarding Tensorflow: Which loss function is used in tf.nn.sampled_softmax_loss? I believe it's cross-entropy, but it is not written on the official website. Can anyone confirm my guess?",1,Documentation Replicability
407,62877768,Input shape of tf.data.Dataset not accepted by model.fit(),"I would like to feed with data my model by applying a tf.data.Dataset. Having checked the documentation of TF 2.0 I found that the .fit() function (https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) accepts: So, I wrote the following minial proof of concept code: As mentioned in the second comment, the code that does not apply a tf.data.Dataset works fine. However, when applying the Dataset object I get the following error message: From my understanding of the documentation, the dataset I have constructed should be exactly the tuple object the fit method expects. I do not understand this error message. What am I doing wrong here?",1,Documentation Ambiguity
408,62956096,Is it possible to extract trained class names from tflite model?,"I have tried to search everywhere, tried everything in tflite_interpreter = tf.lite.Interpreter(model_path='model.tflite'), read tflite documentation but I cannot find the method to extract the class names from the model. Is it possible?",1,Lack of Alternative Solutions/Documentation
409,62962147,TensorFlow - Fashion MNIST Steps Per Epoch,"I'm working with the Kera's Fashion MNIST dataset. When I fit my model, I noticed to complete one epoch it would have to go through 1500 steps. I was looking at the docs for the fit function, but couldn't understand why the default steps were set to 1500. I understand when the steps_per_epoch is None the behavior is dependent on the data type of the dataset, but how can I check if the data type is a tensor or tf.data?",1,Documentation Replication on Other Examples
410,62994289,Saving TFrecords with TPU,"I'm trying to use tf.data.experimental.TFRecordWriter to save dataset on Google cloud bucket using TPU. The code from the example in documentation works: But I have dataset of tuples (string, int64), where first is jpg-encoded image and second is label. When I pass it to writer.write() method, it says: 'tuple' object has no attribute 'is_compatible_with'. I guess I have to pack image and label into tf.train.Example to make it work. I use the following code: But I get the following error: Although I didn't turn off eager mode and this code tf.constant([], dtype = float).numpy() works. Maybe TPU works not in eager mode? Ok, I changed .numpy() to .eval() in the code above. Then I get the foloowing error: What session does TPU use and how do I specify it? When I run the code below: I get an error: But I don't know how to get the current graph and pass it to tf.compat.v1.Session(). When I go another way and type: It says: Is it possible to use .eval() on TPU? Or how do I perform my task another way?",1,Documentation Replication on Other Examples
411,63004540,How to pad 1 dimensinal vector in tensorflow? Getting InvalidArgumentError: paddings must be a matrix with 2 columns with tf.pad,"I am trying to use tf.pad. Here is my attempt to pad the tensor to length 20, with values 10. I get this error message I am looking at the documentation https://www.tensorflow.org/api_docs/python/tf/pad But I am unable to figure out how to shape the pad value",1,Documentation Replicability
412,63020800,"Understanding Tensorflow Object-Detection API, kwargs for Checkpoint class, what is `_base_tower_layers_for_heads`?","Currently, I've been learning how to use Object-Detection API from Tensorflow. I follow a quick start tutorial for training with custom data with this notebook as suggested by them. In the effort to understanding each line of the code, I stumbled upon this snippet code in the ""Create Model and Restore Weight"" part. I don't really understand what are the keyword arguments that are available for the Checkpoint class in that particular snippet code. My question is; is there any documentation out there that shows the list of the keyword arguments? or at least explain what are _base_tower_layers_for_heads and_box_prediction_head? I've read the tf.train.Checkpoint documentation. It says that we can provide models or optimizers for the constructor's keyword argument. I am already familiar with this class to restore the weights to my model, however, I find it is alien to see _base_tower_layers_for_heads or _box_prediction_head for the keyword argument. I do know about 'heads' and different types of 'heads' in the object detection architecture and their relation to transfer learning, what I don't understand is in the context of their data structure. How do I know, these keyword arguments exist? and is there any other else? I would really appreciate it if somebody could give me insights or at least tell me where can I find documentation that I can read to understand it more.",1,Requesting (Additional) Documentation/Examples
413,63146831,What is the analytic interpretation for Tensorflow custom gradient?,"In the official tf.custom_gradient documentation it shows how to define custom gradients for log(1 + exp(x)) When y = log(1 + exp(x)), analytically the derivative comes out to be dy/dx = (1 - 1 / (1 + exp(x))). However in the code def grad says its dy * (1 - 1 / (1 + exp(x))). dy/dx = dy * (1 - 1 / (1 + exp(x))) is not a valid equation. While dx = dy * (1 - 1 / (1 + exp(x))) is wrong as it should be the reciprocal. What does the grad function equate to?",1,Documentation Replication on Other Examples
414,63158314,Tensorflow 2.3.0 - Warning: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version,"I've just updated to TF-2.3. In a model using tf.data.Dataset.from_tensor_slices as data source, I get the folowing warning: I didn't find the instructions on the documentation on how to use the updated methods. Training: Thanks in advance.",1,Lack of Alternative Solutions/Documentation
415,63379008,How to make a diagonal tensor and why doesn't Tensorflow linalg.tensor_diag do that?,"What I would consider a diagonal tensor is a tensor t of shape (d1, ..., dr) which is all zero except when the components are equal. So t[i,j,k,l] = 0 unless i == j == k == l. A function to create such a tensor should take in a shape (d1, ..., dr) and a vector [a1, ..., ak] of length min(d1, ..., dr), placing these values along the diagonal. I would like to do this in Tensorflow, and the most relevant function I could find was tf.linalg.tensor_diag, but it doesn't do what I want. For instance, the diagonal input is a tensor, and the output tensor always has twice the rank, and so it can never output tensors of odd rank. The documentation says ""Given a diagonal, this operation returns a tensor with the diagonal and everything else padded with zeros"", but I don't know how to square that with its actual behavior. My question is two parts: Here is an example output:",1,Documentation Ambiguity
416,63383594,How does Tensorflow build() work from tf.keras.layers.Layer,"I was wondering if anyone knew how the build() function works from the tf.keras.layers.Layer class under the hood. According to the documentation: so to me it seems like the class is behaving similar to this: I can't imagine build() would be called for ever __call__, but it is the only place where the input is passed in. Does anyone know how exactly this works under the hood?",1,Documentation Replication on Other Examples
417,63399368,Error with exporting TF2.2.0 model with tf.lookup.StaticHashTable for Serving,"I'm using StaticHashTable as in one Lambda layer after the output layer of my tf.keras model. It's quite simple actually: I've a text classification models and I'm adding a simple lambda layer that takes the model.output and convert the model_id to more general labels. I can save this version of model with model.save(... as H5 format..) without any issue, and can load it back and use it without any problem. Issue is, when I try to export my TF2.2.0 model for TF-Serving, I can't find how I can export it. Here is what I can do with TF1.X or with TF2.X + tf.compat.v1.disable_eager_execution() This will save my models with TF1.X format for serving and I can use it without any issue. Things is, I'm using LSTM layer and I want to use my model on GPU. By the documentation, if I disable the eager mode, I can't use the GPU-version of LSTM with TF2.2. And without going through above mentioned code, I can't save my model for serving wrt TF2.2 standard and StaticHashTables. Here is how I'm trying to export my TF2.2 model which is using StaticHashTables in final layer; and which is giving error as below: Error: Any suggestion or am I missing anything on exporting TF2.2 model which is using the StaticHashTables in final Lambda layer for TensorFlow Serving? More info here: https://github.com/tensorflow/serving/issues/1719 Thanks!",1,Documentation Replication on Other Examples
418,63482945,How does tf.nn.dilation2d compute gradient and learn its filters,"I want to understand how the tf.nn.dilation2d is working and how the ""filters"", which refers to a structural element are learned. The official documentation is available here : https://www.tensorflow.org/api_docs/python/tf/nn/dilation2d The documentation didn't reference a scientific paper, and I found a lot of different idea in scientific literature, about morphological filters in deep learnign. Here just some examples : I searched into the code (https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/ops/nn_ops.py#L327-L392), but the tf.nn.dilation2d only call gen_nn_ops.dilation2d I searched into gen_nn_ops.py (which I found inside my python lib folder, probably because it's generated from somewhere else) but I didn't understand what the code is doing. Thank you for your time.",1,Documentation Replicability
419,63600026,What are mixed layers in tf.keras.applications.InceptionV3?,"I am currently trying to understand the architecture of Inseption v3 as implemented in tf.keras.applications.InceptionV3. I am looking at the list of names in the model's layers: I understand how batch normalization, pooling and conv layers transform inputs, but deeper we have layers named mixed1, mixed2, ... and so on. I am trying to understand how they (mixed layers) are transforming their inputs. So far, I couldn't find any information about them. How does a mixed layer work? What does it do?",1,Lack of Alternative Solutions/Documentation
420,63715707,Does image_dataset_from_directory load all the images into memory at once?,"I'm new to machine learning, and I am trying to create an image classifier, I want to load the dataset, but I want to do it in a way such that it does not take up all of my memory. Reading the tensorflow documentation, it says that iteration of a dataset happens in streaming fashion, and I am wondering if tf.keras.preprocessing.image_dataset_from_directory will load the images at once or ""stream"" it a batch at a time. If not I was thinking of making a generator to read file names one at a time and load them when the batches are ready with keras.utils.Sequence.",1,Documentation Replication on Other Examples
421,63730066,How to use tf.data.Dataset with kedro?,"I am using tf.data.Dataset to prepare a streaming dataset which is used to train a tf.kears model. With kedro, is there a way to create a node and return the created tf.data.Dataset to use it in the next training node? The MemoryDataset will probably not work because tf.data.Dataset cannot be pickled (deepcopy isn't possible), see also this SO question. According to issue #91 the deep copy in MemoryDataset is done to avoid modifying the data by some other node. Can someone please elaborate a bit more on why/how this concurrent modification could happen? From the docs, there seems to be a copy_mode = ""assign"". Would it be possible to use this option in case the data is not picklable? Another solution (also mentioned in issue 91) is to use just a function to generate the streaming tf.data.Dataset inside the training node, without having the preceding dataset generation node. However, I am not sure what the drawbacks of this approach will be (if any). Would be greate if someone could give some examples. Also, I would like to avoid storing the complete output of the streaming dataset, for example using tfrecords or tf.data.experimental.save as these options would use a lot of disk storage. Is there a way to pass just the created tf.data.Dataset object to use it for the training node?",1,Inadequate Examples
422,63851431,How to Augment the Training Set using the tf.keras.utils.Sequence API?,"TensorFlow documentation have the following example that can illustrate how to create a batch generator to feed a training set in batches to a model when the training set is too large to fit in memory: My intention is to further increase the diversity of the training set by rotating each image 3x by 90. In each Epoch of the training process, the model would first be fed with the ""0 training set"" and next with the 90, 180 and 270 rotating sets, respectively. How can I modify the previous piece of code to perform this operation inside the CIFAR10Sequence() data generator? Please don't use tf.keras.preprocessing.image.ImageDataGenerator() so that the answer does not lose its generality for another type of similar problems that are of a different nature. NB: The idea would be to create the new data ""in real time"" as the model is fed instead of creating (in advance) and storing on disk a new and augmented training set bigger than the original one to be used later (also in batches) during the training process of the model. Thx in advance",1,Documentation Replication on Other Examples
423,63919438,TensorFlow keras model fit() parameters steps_per_epoch and epochs behavior on train set,"I'm using a tf.data dataset containing my training data consisting of (lets say) 100k images. I'm also using a tf.data dataset containing my validation set. Since an epoch of all 100k images takes quite long (in my case approximately one hour) before I get any feedback on performance on the validation set, I set the steps_per_epoch parameter in tf.keras.Model fit() to 10000. Using a batch size of 1 this results into having 10 validation scores when reaching 100k of images. In order to complete one epoch of 100k images of my entire training dataset, I set the epochs parameter to 10 However, I'm not sure if using steps_per_epoch and epochs this way has any other consequences. Is it correct to use these parameters in order to get more frequent feedback on performance? And also a more specific question, does it use all 100k images or does it use the same first 10k images of my training set at every 'epoch'? I already dug into the TensorFlow docs and read several different stack overflow questions, but I couldn't find anything conclusive to answer my own question. Hope you can help! Tensorflow version I'm using is 2.2.0.",1,Inadequate Examples
424,63958039,How do I interpret tf.keras.Model.predict() output?,"I am having trouble finding the documentation I need on this. To summarize the issue, I have trained a tf.keras model using two classes of images, labeled as '0' or '1'. I now want to use this model to predict whether new images are a '0' or '1'. My question is as follows: model.predict() returns a number between 1 and 0, but I can't seem to find what exactly this is. Is it correct to say that this is it's prediction (ie, closer to 1 means the image is likely a 1, and closer to 0 means the image is likely a 0)? Or is there something else going on here. I have included the code, and some output, below. In this case, is pred the probability the image is a 1, and 1 - pred the probability the image is a 0? Thanks for any and all help. Returns",1,Lack of Alternative Solutions/Documentation
425,63958998,Tensorflow/AI Cloud Platform: HyperTune trials failed to report the hyperparameter tuning metric,"I'm using the tf.estimator API with TensorFlow 2.1 on Google AI Platform to build a DNN Regressor. To use AI Platform Training hyperparameter tuning, I followed Google's docs. I used the following configuration parameters: config.yaml: And to add the metric to my summary, I used the following code for my DNNRegressor: According to Google's documentation, the add_metric function creates a new estimator with the metric specified, which is then used as the hyperparameter metric. However, the AI Platform Training service doesn't recognise this metric: Job details on AI Platform On running the code locally, the rmse metric does get outputted in the logs. So, how do I make the metric available to the Training job on AI Platform using Estimators? Additionally, there is an option of reporting the metrics through the cloudml-hypertune Python package. But it requires the value of the metric as one of the input arguments. How do I extract the metric from tf.estimator.train_and_evaluate function (since that's the function I use to train/evaluate my estimator) to input into the report_hyperparameter_tuning_metric function? ETA: Logs show no error. It says that the job completed successfully even though it fails.",1,Documentation Replication on Other Examples
426,64081367,Slicing a tensor with a tensor of indices and tf.gather,"I am trying to slice a tensor with a indices tensor. For this purpose I am trying to use tf.gather. However, I am having a hard time understanding the documentation and don't get it to work as I would expect it to: I have two tensors. An activations tensor with a shape of [1,240,4] and an ids tensor with the shape [1,1,120]. I want to slice the second dimension of the activations tensor with the indices provided in the third dimension of the ids tensor: I have given it the axis=1 option since that is the axis in the activations tensor I want to slice. However, this does not render the expected result and only gives me the following error: I have tried various combinations of the axis and batch_dims options, but to no avail so far and the documentation doesn't really help me on my path. Anybody care to explain the parameters in more detail or on the example above would be very helpful! Edit: The IDs are precomputed before runtime and come in through an input pipeline as such: They are then reshaped into the previous format: Afterwards they have the previously mentioned shape (batch_size = 1 and num_neighbours = 30 -&gt; [1,1,120]) and I want to use them to slice the activations tensor. Edit2: I would like the output to be [1,120,4]. (So I would like to gather the entries along the second dimension of the activations tensor in accordance with the IDs stored in my ids tensor.)",1,Documentation Ambiguity
427,64100466,How to use tf.Dataset in Keras model.fit without specifying targets?,"I want to use an AutoEncoder model with Keras functional API. Also I want to use tf.data.Dataset as an input pipeline for the model. However, there is limitation that I can pass the dataset to the keras.model.fit only with tuple (inputs, targets) accroding to the docs: So here is the question: can I pass the tf.data.Dataset without repeating inputs like that (inputs, inputs) and more like (inputs, None). And if I can't, will the repeated inputs double the GPU memory for my model?",1,Documentation Replication on Other Examples
428,64119612,Join ragged character tensor,"I have a ragged tensor of characters (copy/pastable code to reproduce): I want to join the rows as strings, but I would like to do it in the graph. I can accomplish this by evaluating the tensor: But as this is the output of my network (trained in graph mode) I would like to be able to express this operation in tensorflow so that it can be evaluated in validation steps. Two things I've tried that do not work: Frustratingly, the documentation for tf.strings.join mentions ragged tensors but does not give an example of them. What am I missing? It seems there should be an obvious solution to this.",1,Inadequate Examples
429,64326029,Load tensorflow images and create patches,"I am using image_dataset_from_directory to load a very large RGB imagery dataset from disk into a Dataset. For example, The Dataset has, say, 100000 images grouped into batches of size 32 yielding a tf.data.Dataset with spec (batch=32, width=256, height=256, channels=3) I would like to extract patches from the images to create a new tf.data.Dataset with image spatial dimensions of, say, 64x64. Therefore, I would like to create a new Dataset with 400000 patches still in batches of 32 with a tf.data.Dataset with spec (batch=32, width=64, height=64, channels=3) I've looked at the window method and the extract_patches function but it's not clear from the documentation how to use them to create a new Dataset I need to start training on the patches. The window seems to be geared toward 1D tensors and the extract_patches seems to work with arrays and not with Datasets. Any suggestions on how to accomplish this? UPDATE: Just to clarify my needs. I am trying to avoid manually creating the patches on disk. One, that would be untenable disk wise. Two, the patch size is not fixed. The experiments will be conducted over several patch sizes. So, I do not want to manually perform the patch creation either on disk or manually load the images in memory and perform the patching. I would prefer to have tensorflow handle the patch creation as part of the pipeline workflow to minimize disk and memory usage.",1,Documentation Replication on Other Examples
430,64356209,How does Model.fit() method's shuffle deals with Batches when using a tf.data.Dataset?,"I am using tensorflow 2. When using the Model.fit() method with a tf.data.Dataset, the argument 'batch_size' is ignored. Thus to train my model on batches, I have to first change my dataset of samples into a dataset of batches of samples by calling tf.data.Dataset.batch(batch_size). Then, after reading the documentation, I don't understand clearly how the .fit() method will shuffle my dataset at each epoch. Since my dataset is a dataset of batches, will it shuffle the batches among each other (the batches remain unchanged) ? Or will it shuffle all the samples and then regroup them into new batches (which is the desired behaviour) ? Thanks a lot for your help.",1,Documentation Replicability
431,64424397,Keras - Custom layer with multiple inputs,"I would like to implement a custom tf.keras layer called MyLayer which has three inputs and contains a sub layer which in turn has three inputs, like in the figure below: I assume that the right thing to do would be to create a MyLayer class that extends tf.keras.layers.Layer and implement the __init__, build and call methods, as mentioned in the official documentation. Now, the examples provided in the documentation are relative to pretty simple layers that are composed of several sublayers connected in a sequential manner, that is one after the other. For instance, the MLPBlock layer consists of 3 linear layers ordered sequentially. In general, however, sublayers are not ordered sequentially, but can form branches. This suggests that those layers could be run in parallel, since they are not connected to one another. Going back to the custom layer I would like to implement, you can see that Layer1, Layer2 and Layer3 could be run in parallel. Once their outputs are computed, they can be fed to Layer4. The point is: how do I run them in parallel? I couldn't find any ""ParallelCombinator"" or things like that among the available Keras layers. If I were to follow the examples provided in the documentation, I would write something along these lines: This, however, would imply that Layer1, Layer2 and Layer3 are run sequentially, not in parallel. One possible solution that I came up with involves structuring MyLayer as a tf.keras.Model built with Keras's functional API rather than as a subclass of tf.keras.Layer, like so: The reason why I think this would work is that I assume that when I feed some inputs to a tf.keras.Model, as in output = my_layer([input_1, input_2, input_3]), the layers that can be run in parallel are effectively run in parallel (or are they?). This solution, however, feels like a hack to me, as MyLayer is supposed to be a layer, not a model. In fact, a tf.keras.Model instance exposes methods like fit(...) that aren't meant to be called on a layer. Does anybody know what's the best approach to implement MyLayer?",1,Documentation Replication on Other Examples
432,64552543,Tensorflow 2.2.0 :- WARNING:tensorflow:Gradients do not exist for variables when minimizing the loss,"After implementing Custom Loss class as per the tensorflow api documentation and when invoking model.fit , facing this warning alongwith below error:- This is reference link on github and they have asked to raise here in stack overflow.https://github.com/tensorflow/tensorflow/issues/42542# TypeError: An op outside of the function building code is being passed a ""Graph"" tensor. It is possible to have Graph tensors leak out of the function building context by including a tf.init_scope in your function building code. For example, the following function will fail:",1,Documentation Replication on Other Examples
433,64611137,port TensorFlow 1 code to TensorFlow 2 (model learning process without sess.run),"I have this piece of tf1 code, which was taken from nice book ""Deep Learning"" by S. Nikolenko. It's a simple linear regression that learns k and b to 2 and 1 respectively. I'm striving to port it on TensorFlow 2 And for long time I can't wrap my head what should I use instead of sess.run() and feed_dict, which doing magic behind the scenes, official documentation go into to details with writing model class and so on, but I'm want to keep this as flat as possible. Also it's suggested to calculate derivatives with tf.GradientTape, but I'm struggling with applying it right to my example I need SGD optimizer minimize that given loss function and learn k and b values, how can I achieve it from this point?",1,Documentation Replicability
434,64642944,Steps of tf.summary.* operations in TensorBoard are always 0,"When I'm training my model with TensorFlow 2.3, I want to visualize some intermediate tensors calculated using the weight in the computation graph of my customized tf.keras.layers.Layer. So I use tf.summary.image() to record these tensors and visualize them as images like this: But in TensorBoard, no matter how many steps passed, there is only one image of step 0 shown. And I tried to set the parameter step of tf.summary.image() to the value obtained from tf.summary.experimental.get_step(): And update the step by calling tf.summary.experimental.set_step from a customized Callback using a tf.Variable like codes shown below: This Callback's instance is passed in the argument callbacks in model.fit() function. But the value tf.summary.experimental.get_step() returned is still 0. The TensorFlow document of ""tf.summary.experimental.set_step()"" says: Accroding to the document, I am already using a Variable to store the steps, but it's changes are still not reflected inside the function (or keras.Model). Note: My code produces expected results in TensorFlow 1.x with just a simple line of tf.summary.image() before I migrate it to TensorFlow 2. So I want to know if my approach is wrong in TensorFlow 2? In TF2, how can I get training steps inside the computation graph? Or there is other solution to summarize tensors (as scalar, image, etc.) inside a model in TensorFlow 2?",1,Documentation Replication on Other Examples
435,64687375,Get labels from dataset when using tensorflow image_dataset_from_directory,"I wrote a simple CNN using tensorflow (v2.4) + keras in python (v3.8.3). I am trying to optimize the network, and I want more info on what it is failing to predict. I am trying to add a confusion matrix, and I need to feed tensorflow.math.confusion_matrix() the test labels. My problem is that I cannot figure out how to access the labels from the dataset object created by tf.keras.preprocessing.image_dataset_from_directory() My images are organized in directories having the label as the name. The documentation says the function returns a tf.data.Dataset object. Here is the code: I have tried using (foo, foo1) = tf.keras.preprocessing.image_dataset_from_directory(dataDirectory, etc), but I get (trainData, trainLabels) = tf.keras.preprocessing.image_dataset_from_directory( ValueError: too many values to unpack (expected 2) And if I try to return as one variable and then split it as so: I get TypeError: 'BatchDataset' object is not subscriptable I can access the labels via testClasses = testData.class_names, but I get: I am open to any method to get those labels into the confusion matrix. Any ideas as to why what I am doing is not working would also be appreciated. UPDATE: I tried the method proposed by Alexandre Catalano, and I get the following error I printed the first element of the labels array, and it is zero",1,Documentation Replication on Other Examples
436,64759627,TensorFlow custom training step with different loss functions,"According to the TensorFlow documentation, a custom training step can be performed with the following But if I want to use a different loss function like categorical cross-entropy I would need to argmax the logits created in the gradient tape: The problem with this is that the tf.argmax function is not differentiable, so TensorFlow wouldn't be able to compute the gradients and you would get the error: My question: Without changing the loss function how could I make the second example work?",1,Documentation Replication on Other Examples
437,64769187,"Tensorflow - Interpreting the tf.estimator.ProfilerHook ""_Send"" op",I have a deep CNN/RNN that I train on Google AI platform. I distribute the training on 8 GPUs using the tf.distribute.MirroredStrategy. I recently upgraded my runtime version from 1.13 to 1.15 and my training is more than 2x slower than before. I read that tf.estimator.ProfilerHook can be used to identify performance bottlenecks. So I collected the profiling information and rendered it at chrome://tracing. I got this A training step spends an entire 1 second on these _Send ops. What is this? I can't find any documentation on the op or why it's in my graph. What does this mean?,1,Lack of Alternative Solutions/Documentation
438,64826405,Tensorflow: 'axis' argument in dot product,"Can someone show me the way I should use the axis argument in tf.tensordot? I read the documentation but it was complicated and I'm still confused. I saw another question that asks about axis in tf.one_hot and in the answers were some good insights about the matter, but that didn't help me with tf.tensordot. I thought you can give me some insights on this too. For example, I know I can dot product a vector and a tensor like this: But when I batch them and add one dimension to them to be of the shape (b, n) and (b, m, n) to obtain a (b, m, 1), now I don't know how to dot product every batch.",1,Documentation Ambiguity
439,65157852,How to mix tensorflow keras model and transformers,"I am trying to import a pretrained model from Huggingface's transformers library and extend it with a few layers for classification using tensorflow keras. When I directly use transformers model (Method 1), the model trains well and reaches a validation accuracy of 0.93 after 1 epoch. However, when trying to use the model as a layer within a tf.keras model (Method 2), the model can't get above 0.32 accuracy. As far as I can tell based on the documentation, the two approaches should be equivalent. My goal is to get Method 2 working so that I can add more layers to it instead of directly using the logits produced by Huggingface's classifier head but I'm stuck at this stage. Method 1: Method 2: Rest of the code for either method is identical, I am using Tensorflow 2.3.0 and have tried with transformers versions 3.5.0 and 4.0.0.",1,Documentation Replication on Other Examples
440,65277703,image normalization and TPU,"I'm trying to incorporate image normalization in my keras model to run on Google's cloud TPU. Therefore I inserted a line into my code: There was nor error thrown, but according the documentation of google tf.image.per_image_standardization is not a supported function. Does anybody know if it works anyhow, or does anybody have an idea how to check if it works?",1,Documentation Replicability
441,65436819,Keras: How to use `image_dataset_from_directory` to load test set?,"I am using tf.keras.preprocessing.image_dataset_from_directory to load dataset as follows, However, when I check the document looks like this argument labels seem to be a must-have one, but my test data has no labels, so how can I load test data? Is there a convenient and unified way to do this?",1,Documentation Replicability
442,65437493,convert string to float array in csv using tf.data,I have a csv like this : I want to convert column text_weight to tf.data . But I find nothing about it in tensorflow document website .,1,Lack of Alternative Solutions/Documentation
443,65464181,An alternative to tf.distribute.cluster_resolver.TPUClusterResolver( tpu_name) to be used in Sagemaker?,"According to the tensorflow documentation in tf.distribute.cluster_resolver.TPUClusterResolver, it says that this resolver works only on Google Cloud platform. But from this issue in github, I found out that a similar code also works in Azure. Is there a way I can bypass this resolver and initialize my tpu in sagemaker ? Even better, if I can find a way to insert the name or url of sagemaker gpu to the resolver and initiate it from there ?",1,Documentation Replicability
444,65481591,Keras Generator to tf.data.Dataset,"I am working with the Mask RCNN keras implementation but the data generator hard locks on my systems when using use_multiprocessing=True. The data generator runs fine in single thread. I am trying to convert the data generator to a tf.data.Dataset as recommended by tensorflow. I have no idea how to do this and have been unable to find any documentation on this. Mask RCNN data generator: I have tried to use the tf.data.Dataset.from_generator() however it requires the output_types= argument and the Mask RCNN outputs a number of lists, I can not figure out how to define output_types=. I am using python3.7, keras==2.2.5, tensorflow==2.2.0",1,Documentation Replication on Other Examples
445,65640885,CRNN tf.keras.backend.ctc_decode. What is log probability?,"Based on the documentation, the function tf.keras.backend.ctc_decode is supposed to return a Tuple. Its first field contains the best path (let's assume we use greedy search), whereas the second one contains its log probability. Is this probability actually the accuracy of the prediction? If not how am I supposed to calculate it? I've tried on some test images and this was my output: During the training part, the final CTC loss was around 0.1 and the prediction is always correct. However what I think it's the probability seems not to be what I expect. They looks like completely random numbers, even grater than 1 or 2! What am I doing wrong?",1,Documentation Replication on Other Examples
446,65712409,How to convert tf2 model so it will run on tflite interpreter,"Background: I am trying to convert the tf2 model for SSD MobileNet V2 FPNLite 320x320 (for example) from the official tf zoo. The model should run eventually on raspberry pi, so I would like it to run on the tflite interpreter (without full tf). The docs imply that ssd model conversion is supported. Whats happening: the process is detailed in this colab notebook. It is failing with the error: if I add the flag tf.lite.OpsSet.SELECT_TF_OPS, it works but wont run on the rpi, as it does not have the ops. Can this be done? Has anyone succeeded?",1,Documentation Replication on Other Examples
447,65779087,How to use tf.gradients within a model and still use a custom training loop?,"I would like to make a TensorFlow model where the outputs respect a mathematical condition, namely that output 0 is a scalar function and all subsequent outputs are its partial derivatives w.r.t. the input. This is because my observations are the scalar function and its partials, and not using the partials for training would be a waste of information. For now, using simply tf.gradients works if I don't build a custom training loop, i.e. when I don't utilize eager execution. The model is built like this, and training works as expected: However, them problem now comes when I want to do online training (i.e. with an incremental dataset). In this case, I wouldn't compile my model at the very end. Instead, I write a loop as such (before calling model.compile): This however gives the following exception at prediction_Y = model(batches_X[i_batch]): As most examples, tutorials and documentation solely deal with using gradients to do training, and not within the model, I can't find any good resources how to deal with this. I tried to find how to use gradient tape, but I can't figure out how to use it in the model design phase. Any pointers would be appreciated! Versions used:",1,Inadequate Examples
448,65794527,"Example of output_signature , output_types & output_shapes for complex object called by tf.data.Dataset.from_generator","I've a generator function that yields the following tuple: yield (transformed_input_array, set_y) transformed_input_array is a list of ndarrays with the following shape: (1024, 104), (1024, 142), (1024, 1), (1024, 1), (1024, 1), (1024, 1), (1024, 140) and the following types: tf.float64, tf.float64, tf.int8, tf.int16, tf.int8, tf.int8, tf.float64 set_y is a ndarray of shape 1024 and type of int64 I've wrapped my generator with tf.data.Dataset.from_generator function, here is the code: But when I run the training, I get the following error: If I try to run with output_signature param (commented out code), I get the following error: Can someone provide an example, of how I should treat complex type (list of ndarrays)? Couldn't find any example in TF documentation..",1,Inadequate Examples
449,65835387,ValueError: too many values to unpack (expected 2) when using tf.keras.preprocessing.image_dataset_from_directory,"I want to create a dataset-variable as well as a labels-variable using the function tf.keras.preprocessing.image_dataset_from_directory (https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory). The documentation states: My code is the following: I expect to get a tuple as return values, but instead I get the error message: When I save the output in one variable (just train_ds) and I inspect the variable, I get the following output: How can I access the two tuples inside seperatly?",1,Documentation Ambiguity
450,65863738,Changing order of Input Image in 3D convolutions,"According to the official documentation of tf.keras.layers.Conv3D . Now the whole idea around channels and batch shape makes sense, but will changing the general order of (conv_dim1, conv_dim2,conv_dim2) as (x,y,z) to say (z,x,y) affect the performance. Does Conv3D worry about order of x-y-z dimension ? I was training a U-net segmentation model and upon changing the order of axis I saw difference in performance. (x,y,z) order converges faster as compared to (y,x,z). I just wanted to make sure what's the correct way..",1,Documentation Ambiguity
451,65953591,Which format should have time series input for LSTM-Model in Tensorflow?,"I have a problem with the input for the fit-function of an LSTM-Model in TensorFlow. I have an input with the following shape: (5, 128, 78, 80) The fields are: (number of samples, timesteps, feature1, feature2) The output has the shape: (5, 128, 78, 2) This is my model: I get the following error: ValueError: Input 0 of layer sequential_38 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (5, 128, 78, 80) So I think, I have to change the shape of my data, but I don't know how. I tried already different values for input and input_shape-attribute. I read in https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM that the input has to be a tensor with shape [batch, timesteps, feature]. So I put the two features in a nested array, and gave [batch, timesteps, array of the features] to the fit-function. But it told me that the data could not be converted to a tensor. Also explicit converting with tf.convert_to_tensor did not work. I would be really glad, if someone could explain me, how I can pass input data with two features to an LSTM-model.",1,Documentation Ambiguity
452,66019998,"How to get a processed dataset, if the processing steps are not tensor operations?","I have an instance of tf.data.Dataset(), of images, basically, acquired this way: So, this dataset has (data, label) where the data is a tensor of shape (batch_size, image_height, image_width, channels) [I don't really need the labels it assigns]. So far so good. The problem is, I need to process this dataset, applying certain operations to the images, and, this dataset is too big to load everything in memory (that's why batch_size is there). According to the tensorflow documentation, tf.data.Dataset.map() is the function I need (or so I assume....). First of all, the shape returned by the print: (None, 200, 200, 3) instead of (32, 200, 200, 3), or, instead of (200, 200, 3) [which is what I'd expect from reading the documentation] [let's assume batch of 32, and images 200x200], and this is messing my code, because, I need to do assigments, like, take the ith image, and change a couple pixels: data[i][12:15,40:50] = np.array([1,2,3]) and things like that. Basically, that's the error message: TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'. In summary, my question: How can I get a processed_dataset, where the processing steps will not be whole tensor operations, but instead, will be changing individual values in the data (say, individual pixels), for certain images (say, the ith image, jth image, etc)? If you must know, I am running this in Ubuntu. Tensorflow version is:",1,Documentation Replication on Other Examples
453,66038861,Why are both branches in tf.cond being executed? And why does tf.while_loop finish the loop even though the condition still true?,"I am using keras for a while now, but usually I don't have to use customized layers or perform some more complex flow control, so I'm struggling trying to understand somethings. I am modeling a neural network with a customized layer on the top. This customized layer calls another function (search_sigma) and inside this function I execute tf.while_loop and inside of tf.while_loop I execute tf.cond. I cannot understand why the conditions are not working. Could someone help me understand what I am missing? I already tried to change both tf.cond and tf.while_loop conditions for true tensors, just to see what would happen. The behavior (exactly same errors) remained the same. I also tried to write this code without implementing a class (using just functions). Nothing changed. I tried to find solutions looking at tensorflow documentation, other stack overflow doubts and websites talking about tf.while_loop and tf.cond. I left some print()s in the body of the code to try to track what was happening. The piece of code that calls the above code is: 'inputs' is a (None, 10) size tensor 'self.sigma' is a (10,) size tensor 'self.clusters' is a (N, 10) size tensor",1,Inadequate Examples
454,66049816,Custom layer in sequential model tensorflow,"I'm trying to create a custom layer for my model, which can be used the classic Dense layer of Keras. Here my custom layer: It does not do anything 'custom' for now. But when I add it to my model I get this: Because probably I'm creating directly the object of class Custom Layer. But I do not find in the tf documentation how to add other properties to make it work as a normal layer, i.e. as something like layers.Dense(100, activation=tf.nn.relu) Is there a way to make it work like that ?",1,Lack of Alternative Solutions/Documentation
455,66062973,TensorFlow custom loss function error: No gradients provided for any variable,"I am creating a custom loss function using tf.raw_ops namespace to train my model using keras. Here is my loss function: Loss(pred,label)= { 0.0 if predlabel&lt;=0.1, 1.0 elsewhere And then I am using this in my I am getting an error I know this may be because some of the tf.operations that I am using are not differentiable, or doesn't have a gradient. However, I have checked this page https://docs.w3cub.com/tensorflow~2.3/raw_ops It shows which operations are differentiable and which are not. All of my operations are differentiable. I am not sure what am I missing. Any help is appreciated.",1,Documentation Ambiguity
456,66231467,How to set a minimum number of epoch in Optuna SuccessiveHalvingPruner()?,"I'm using Optuna 2.5 to optimize a couple of hyperparameters on a tf.keras CNN model. I want to use pruning so that the optimization skips the less promising corners of the hyperparameters space. I'm using something like this: Sometimes the model stops after 2 epochs, some other times it stops after 12 epochs, 48 and so forth. What I want is to ensure that the model always trains at least 30 epochs before being pruned. I guess that the parameter min_early_stopping_rate might have some control on this but I've tried to change it from 0 to 30 and then the models never get pruned. Can someone explain me a bit better than the Optuna documentation, what these parameters in the SuccessiveHalvingPruner() really do (specially min_early_stopping_rate)? Thanks",1,Requesting (Additional) Documentation/Examples
457,66385626,How does TensorFlow SavedModel handle additional dependencies,"I am trying to export my TF model using the tf.saved_model.save() function. However, I'm using additional external libraries during preprocessing. A working code using the example of nltk looks like this: As far as I've understood the docs, a savedModel includes the trained parameters and computations, but NOT my code. But when I load the same model in another script like so (i.e. without importing my external library at all) I get my output without any errors This is especially baffling to me as I am even using nltk to download a dataset of stopwords. How exactly do SavedModels deal with these external libraries during exportation?",1,Documentation Replicability
458,66546321,Proper way to input a scalar into a Tensorflow 2 model,"In my Tensorflow 2 model, I want my batch size to be parametric, such that I can build tensors which have appropriate batch size dynamically. I have the following code: The code throws an error in tf.range: I think the problem is with the fact that tf.keras.Input automatically tries to expand the input array at the first dimension, since it expects the partial shape of the input without the batch size and will attach the batch size according to the shape of the input array, which in my case a scalar. I can just feed the scalar value as a constant integer into tf.range but this time, I won't be able to change it after the model graph has been compiled. Interestingly, I failed to find a proper way to input only a scalar into a TF-2 model even though I checked the documentation, too. So, what would be the best way to handle such a case?",1,Documentation Replication on Other Examples
459,66711706,"Jax, jit and dynamic shapes: a regression from Tensorflow?","The documentation for JAX says, Now I am somewhat surprised because tensorflow has operations like tf.boolean_mask that does what JAX seems incapable of doing when compiled. EDIT The gradient passes through tf.boolean_mask (obviously not on mask values, which are discrete); case in point here using TF1-style graphs where values are unknown, so TF cannot rely on them:",1,Documentation Replication on Other Examples
460,66874943,Why iterations over the same tf.data.Dataset give different data each iteration?,"I'm trying to understand how tf.data.Dataset works. It says on the documentation that take returns a dataset with a certain amount of elements from that dataset. You can then iterate over a single sample (in this case a batch): Outputs: However, iterating over it again, gives different labels: (continuation of last code) Outputs: Shouldn't the labels be the same, given that the dataset is the same?",1,Documentation Replicability
461,66879748,What is the difference between tf.keras.model and tf.keras.sequential?,"In some tf. keras tutorials, I've seen them instantiated their model class like this: model = tf.keras.Sequential() While in some places, they use something like this: model = tf.keras.Model(inputs=input, outputs=output) But seeing here in the docs, they do seem the same, but I am not sure nor is it explicitly mentioned. What are the differences between the two?",1,Documentation Ambiguity
462,67066760,Configuring labels in TensorFlow BinaryCrossentropy loss function,"I want to compute cross-entropy loss using tf.keras.losses.BinaryCrossentropy. The documentation has the following example, and specifies that true labels and predicted labels should have the shape [batch_size]: From the example, it is inferred that each sample's label should be formatted as [probability of belonging to Class 0, probability of belonging to Class 1]. Is it correct? If it is, why y_true[1] probabilities do not add up to 1?",1,Documentation Ambiguity
463,67197448,How to extract multiple rows from tensor at the same time?,"TL;DR: TensorFlow tensor is of shape (50, 50, 6), want these indices (:, :, (0, 2, 3)). How to extract them? Here is an example array I am working with: I want to extract the the first, third, and fourth row; in other words I need all these entries (:, :, (1, 3)), which works for numpy arrays: Working with a tensor t = tf.convert_to_tensor(a) and then calling the index like throws an error: For numpy I have found the following relevant questions, but they naturally focus on numpy arrays: How to slice a 2D array non-consecutively in Python Slicing a numpy array along a dynamically specified axis Looking at the TF documentation I found gather_nd and boolean_mask, which I feel are helpful, but I must freely admit that I have not understood the docs at this part. On SO I found this question How to select elements of a tensor along a specific axis in TensorFlow, which focuses on single elements; I am looking for complete dimensions (if that's the right wording here). How can I do the numpy thing in TensorFlow?",1,Documentation Replication on Other Examples
464,67211152,Tensorlow - please decipher what the tf.where document says,Please decipher what the tf.where documentation says about what it does when both x and y are provided. I suppose it tries to say it will produce a result by: Is this correct?,1,Documentation Replicability
465,67523944,"Tensorflow2 - Use ""tf.data.experimental.make_csv_dataset"" with ""tf.keras.preprocessing.timeseries_dataset_from_array""","I am trying to get TensorFlow to read +100 CSV files that don't fit in memory (+1GB size each). The files contain time series data (EEG signals), with the labels in the first column. From the TensorFlow documentation it seems like I should be able to use the tf.data API to load my data off-disk. For the sake of simplicity and reproducibility, let's consider the following ""sample_data.csv"" dataset: I've tried using tf.data.experimental.make_csv_dataset to load the CSV files into tf.data.Dataset objects, and then tf.keras.preprocessing.timeseries_dataset_from_array to process the data into sliding windows with overlap. For the dataset above, I would do: Which we can check works correctly by looking at the output from list(input_data.as_numpy_iterator()). We can then feed input_data to the next function: Which unfortunately throws this error: I also tried using my_dataset = input_data.window(3, shift=2) (see the tf.data.Dataset.window documentation) and it didn't throw an error, but it seems to be returning an empty dataset? See ""_VariantDataset shapes: (None,)"" in the output: If I load the ""sample_data.csv"" in memory using pandas and then feed the timeseries_dataset_from_array function a numpy array instead, it works correctly. Any ideas on how to solve this? What's the best method to input overlapping windows from off-memory time-series data into TensorFlow? Thank you!",1,Documentation Replication on Other Examples
466,67563475,How to convert a tensorflow model and load as tfds,I need help converting my dataset from how I usually make it using tf.keras.preprocessing.image_dataset_from_directory To be used to replace this in an example How can I do so? I am unable to find related documentation so if you can link that it would be amazing. Thanks This is how the dataset is used in the example,1,Documentation Ambiguity
467,67698111,Show version of tensorflow 2.4.1 in python 3.7,"Using python 3.7, how do you print the version of tensorflow if you have tensorflow 2.4.1. None of the documented styles: 'tf. __ version__', tf.version, nor tf.version.VERSION seem to work.",1,Documentation Ambiguity
468,67723809,Memory leak when using tf.data Datasets with shuffle,"I have a memory leak somehow when I create my tf.data.dataset pipeline, but I don't know where. My code works fine with ImageDataGenerator but is really slow. Reading a lot of documentation I thought it might be albumentations. However I now switched my transform to be entirely in tensorflow: And I made the shuffle buffer extremely small: Could autotune cause this? On Colab I usually hit the RAM restart at 500 batches I would like to use tf.data.datasets because it's really much faster if possible. Thank you for anyone who can point me to the flaw in my code, I have always used generators and only recently made the switch.",1,Documentation Replication on Other Examples
469,67740346,TimeDistributed layer to apply several convolutional layers error,"I have an issue with the tf.keras.layers.TimeDistributed layer (https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed). I am aware that TimeDistributed can be used to apply a single layer (dense, convolutional...) to a set of inputs, obtaining a set of outputs. Not only that, but until recently I was able to use it to apply an entire ""submodel"" to all the inputs. That is, a series of layers, not just one. An example of this is explained here by Patrice Ferlet (https://medium.com/smileinnovation/training-neural-network-with-image-sequence-an-example-with-video-as-input-c3407f7a0b0f). Using that source as example I can define a sequential ""submodel"" like this: And then include this submodel inside a superior model which calls TimeDistributed with the whole initial submodel (convnet). Now this works well, and I can get the model structure calling But if instead of this I define the convnet model using as backbone a predefined arquitecture from keras, like VGG16, there seems to be an error. (I also needed to change as well keras.Sequential by tf.keras.models.Sequential) When I run this after defining my VGG16-based architecture I get the following error: So now it appears like python is complaining that I am using an input for TimeDistributed that is not a single layer. This doesn't make any sense, since the initial example works well and also involves using several layers with TimeDistributed. Apart from that, the VGG16 model also worked fine some weeks ago. I am running all of this in Google CoLab. Could someone help me figure out what is going on here? Is this caused by the new tensorflow 2.5.0 version? Everywhere I look I see people using TimeDistributed to apply a single layer, but applying a whole sequential model worked just fine until now (despite no apparent mention in the documentation). Thank you!",1,Documentation Replication on Other Examples
470,67747314,Finding precision and recall for the tutorial federated learning model on MNIST,"I'm using this tutorial to try to learn how federated models work through TensorFlow's tutorial here: https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynb Currently, the model is defined like this which uses accuracy as its metric. I want to either use precision and recall as metrics, or find them after the model is trained, but I can't figure out how to do so. I tried adding precision to metrics like this metrics = [tf.keras.metrics.SparseTopKCategoricalAccuracy(), tf.keras.metrics.Precision()] and run this code but it gives me an error. Error output: Previously, I asked a similar question for a regular centralized model here, but I don't think I can use that same method since you can't get the results of the predictions back in the same way from what I've found. I've also tried looking at other documentation such as this, but it also uses accuracy as the metric, so that wasn't helpful. How can I get the precision and recall of this federated model?",1,Lack of Alternative Solutions/Documentation
471,67947583,"Defining a callable ""loss"" function","I am trying to optimize a loss function (defined using evidence lower bound) with tf.train.AdamOptimizer.minimize() on Tensorflow version 1.15.2 with eager execution enabled. I tried the following: and got the following : RuntimeError: ""loss"" passed to Optimizer.compute_gradients should be a function when eager execution is enabled. This works fine if I disable eager execution but since I need to save a tensorflow variable as a numpy array so I need eager execution enabled. The documentation mentions that when eager execution is enabled, the loss must be a callable. So the loss function should be defined in a way that it takes no inputs but gives out loss. I am not exactly sure how do I achieve such a thing. I tried train_op = optim.minimize(lambda: loss) but got ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [] and loss &lt;function &lt;lambda&gt; at 0x7f3c67a93b00&gt;",1,Documentation Replication on Other Examples
472,68217076,How to parse a tfds.features.Sequence object ? It is not compatible with tf.io.FixedLenSequenceFeature,"Recently I was trying to train a model on the Wider-Face Dataset. I found it is prebuilt into tfds (https://www.tensorflow.org/datasets/catalog/wider_face). However I am having difficulty parsing it. It's feature map is of the following form - So I passed the following nested dictionary to tf.io.parse_single_example But it gives me a value error of ValueError: Unsupported dict. Later I also learnt that Sequence does not support features which are of type tf.io.FixedLenSequenceFeature. Please let me know how can I parse this type of TFRecords. I didn't find much documentation of how to use the object detection datasets that are build into Tensorflow, so providing some links with examples will also be helpful. Thanks",1,Inadequate Examples
473,68319579,tfa.optimizers.MultiOptimizer - TypeError: 'Not JSON Serializable:',"I'm trying to use tfa.optimizers.MultiOptimizer(). I did everything according to the docs (https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/MultiOptimizer) yet I'm getting the following error: TypeError: ('Not JSON Serializable:', &lt;tf.Tensor 'gradient_tape/model_80/dense_3/Tensordot/MatMul/MatMul:0' shape=(1, 1) dtype=float32&gt;) Below is a minimal, working example that reproduces the error, just copy and paste it. The error occurs when the first epoch is finished and the callback trys to save the model.",1,Documentation Replicability
474,68354367,Getting an error when using tf.keras.metrics.Mean in functional Keras API,"I'm trying to add a Mean metric to a Keras functional model (Tensorflow 2.5), and am getting the following error: Here is the code: If I remove the following line (from which the exception is thrown): the code works as expected. I Tried disabling eager execution, but I get the following error instead: The above usage was pretty much copied from the tf.keras.metrics.Mean documentation (see Usage with compile() API)",1,Lack of Alternative Solutions/Documentation
475,68422887,Slow tensorflow code; can I batch evaluate and obtain multiple loss scores?,"I recently switched from using the 'keras' package in Python to using 'tensorflow.keras', since this seems to be preferred now. The latest version of Keras was also giving me issues that seemed like I'd have to modify the internal Keras code to fix, whereas tf.keras works fine. However, upon making this switch, some of my code was slowed down by a factor of 30-40. I've identified the following calls to ""model.evaluate"" as a bottleneck, though I'm not sure why it's so much slower than before. The code is structured something like this: I'm thinking the major bottleneck is that I'm making a bunch of SMALL calls to tensorflow, rather than one LARGE call. Using a GPU actually makes it even slower, presumably due to all the loading/unloading. I'd like to just make a call like but model.evaluate() seems to always output a single scalar, when I need the whole list of 10000 loss scores. I have been unable to find a solution in the documentation, is there a builtin way to do sort of a ""batch evaluate"" but get individual loss scores out for each sample?",1,Documentation Replication on Other Examples
476,68431633,tf.image.stateless_random_crop VS. tf.image.random_crop. Shouldn't these be the same thing?,"In tf 2.5, there are two functions for cropping an image: tf.image.stateless_random_crop, and tf.image.random_crop. The documentation states that stateless_random_crop is deterministic (always returns the same crop given one seed). However, random_crop has a seed parameter and is also deterministic, one would think. What is the actual difference between these two functions? I cannot find information about statelessness in Tensorflow anywhere. The differences between tf.image.stateless_random_crop, and tf.image.random_crop are one line where stateless_random_uniform is used instead of a random_uniform: stateless_random_crop: https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L415-L465 random_crop: https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L360-L412 I always thought that random_crop would always return the same crop given a seed, but it looks like maybe that wasn't always true? Any enlightenment about statelessness in Tensorflow is greatly appreciated!",1,Documentation Ambiguity
477,68550779,Slicing a 2D tensor similar to numpy np.ix_,"I have learned how to slice a tensor on one dimension here. I have learned how to slice a 2D tensor giving a 1D tensor of specific values here. Both use tf.gather() but I'm pretty sure I need tf.gather_nd() though I'm obviously using it wrong. In numpy, I have a 5x5 2D array, and I can slice a 2x2 array by using np.ix_() with row and column indices (I always need the same indices for rows and columns, resulting in a squared matrix): Reading over the tf.gather_nd() docs I assumed this is the way to do it in TF, but I'm using it wrong: I would have to do something like: Which leads me down another rabbit hole I'm not keen on. My indices vector is a lot longer of course. My indices, BTW, are 1D integer tensors themselves. So bottom-line I want to slice a with the same indices for rows and columns as I do with np._ix(), and my indices are something like:",1,Documentation Replication on Other Examples
478,68878231,tf.gradients() vs tf.gradientTape.gradient() in graph mode,"I had a question regarding the behavior of tf.gradients() as opposed tf.gradientTape.gradient() in graph mode. Given a differentiable function y = f(x), where x and y are each single tensorflow tensors, is there any difference between the behavior of tf.gradient(y, x) vs tape.gradient(y, x) where tape is an instance of tf.gradientTape (assuming the use of graph mode) ? Not sure why tensorflow has two different gradient methods which can be used with graph mode - maybe there are some subtle differences in the implementations? Ive looked at the documentation for gradientTape and tf.gradients but its not clear whether there is any difference between the behavior of these methods for a single (x, y) pair, or whether its just that tf.gradients() can be used in this case for a speedup when using graph mode. Thank you so much for your help!",1,Documentation Replication on Other Examples
479,68984841,How can I understand the kernel of tf.keras.layers.Dense for rank >2?,"How can I understand the kernel of tf.keras.layers.Dense for rank &gt;2? The official API doc states that: My understanding is that for a rank larger than 2 (for example rank 3) only one kernel is created and thus the same kernel is applied on all slices of the second dimension, like above. That would consequently mean that the outputs for different indices of the second dimension are not independent of each other (especially during training). Is my understanding correct? And if yes, is there a simple way to use a stack of kernels instead or do I have to implement the tensor multiplication?",1,Documentation Replicability
480,69025002,Calculate logarithm in tensorflow not in-place,"I have the following function (see below). It takes an (N,8,3) tensor as argument. I need to choose along axis 0 and 2, so I receive a selection with size (N,1,3) from probabilities along axis 1. I solved that with tf.random.categorical. Sadly this function needs the vector along axis 1 to be calculated with tf.log(vector) beforehand. (I don't see the reason, but that's what the docs say). Now, this calculates log in-place, so it destroys my argument from outside. How is it possible to calculate the logarithm not-in-place (or choose from probability without log(prop) ) ? Thank you",1,Documentation Replication on Other Examples
481,69195950,Problem with inputs when building a model with TFBertModel and AutoTokenizer from HuggingFace's transformers,"I'm trying to build the model illustrated in this picture: I obtained a pre-trained BERT and respective tokenizer from HuggingFace's transformers in the following way: The model will be fed a sequence of italian tweets and will need to determine if they are ironic or not. I'm having problems building the initial part of the model, which takes the inputs and feeds them to the tokenizer in order to get a representation I can feed to BERT. I can do it outside of the model-building context: but I'm having troubles building a model that does this. Here is the code for building such a model (the problem is in the first 4 lines ): And when I call the function for creating this model I get this error: One thing I thought was that maybe I had to use the tokenizer.batch_encode_plus function which works with lists of strings: but I get this error: and beside the fact I haven't found a way to convert that tensor to a list with a quick google search, it seems weird that I have to go in and out of tensorflow in this way. I've also looked up on the huggingface's documentation but there is only a single usage example, with a single phrase, and what they do is analogous at my ""out of model-building context"" example. EDIT: I also tried with Lambdas in this way: but I get the following error: EDIT 2: I also tried the approach suggested by @mdaoust of wrapping everything in a tf.py_function and got this error. Then I defined Tout as the type of the value returned by the tokenizer: transformers.tokenization_utils_base.BatchEncoding and got the following error: Finally I unpacked the value in the BatchEncoding in the following way: And get an error in the line below:",1,Inadequate Examples
482,69458522,What does tf.squeeze does to the audio and how can I load an mp3?,"I'm using TensorFlow and I would like to be able to load audio and generate a spectrogram from it. I have little knowledge of how audio internally works. Currently, this is the code I'm using: I have been reading the documentation and in order to create a tensor I need to either use the tf.squeeze method or audio.to_tensor(). I have no clue what the tf.squeeze method does, but when I use it I get the error: If I instead use the method audio.to_tensor(), I'm unable to display the created spectrogram on the plt and instead I get the following error:",1,Documentation Replication on Other Examples
483,69509388,TF BERT input packer on more than two inputs,"Some of the TensorFlow examples using BERT models show a use of the BERT preprocessor to ""pack"" inputs. E.g. in this example, text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok], tf.constant(20)) The documentation implies that this works equally well with more than two input sentences, such that (I would expect) one can do something like: text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok, tok], tf.constant(20)) However, so doing causes the error at the bottom[1] of this post. I get that there isn't a matching signature; if I read this correctly (and I may not!), there's a signature for a single input and one for two. But what's the recommended way to pack more than two sentences into input suitable for a classification task, as suggested in the above colab? 1.",1,Inadequate Examples
484,69587392,How to apply map() when working with a batched Dataset?,"I am creating a timeseries Dataset using tf.keras.utils.timeseries_dataset_from_array. According to the docs, it returns a tf.data.Dataset instance. I also pass the batch size argument when calling the timeseries_dataset_from_array function, so my dataset is a BatchDataset. I am using map on this batched Dataset (ds), passing my_fun. Data in the code below is a pandas dataframe containing continuous timesteps. What does the my_fun function expect as input parameters - aka what does it apply on each iteration? Whole batches of shape (samples, sequence_length, features) or a single element of shape (None, sequence_length, features)? I am confused because when I print the single argument that I define in my my_fun function, it yields a shape of (None, None, features), but I cannot inspect it further... My code is inspired from the TF tutorial (see split_window function) https://www.tensorflow.org/tutorials/structured_data/time_series",1,Documentation Replication on Other Examples
485,69672777,Compute Hessian of lossfunction in Tensorflow,"I would like to compute the hessian of a loss function of a neural network in Tensorflow with respect to all the parameters (or trainable variables). By modifying the example code from the Tensorflow documentation (https://www.tensorflow.org/api_docs/python/tf/GradientTape) I managed to compute the hessian w.r.t the weight matrix for the first layer (if I'm not mistaken): If I try to compute it w.r.t model.trainable_variables instead the tape.jacobian complains that 'list object has no attribute shape'. I instead tried to flatten the model.trainable_variables and compute it w.r.t the flattened vector: The problem now is that g is empty (NoneType) for some reason. I noticed that source is tf.Tensor-type but model.trainable_variables[0] was of type tf.ResourceVariable so I tried changing this by declaring source as This didn't change anything though, so I'm guessing that this is not the issue. I also thought that the problem might be that the source-variable is not watched, but it seems that it is set to trainable and even if i do tape.watch(source), g is still empty. Does anybody know how I can solve this?",1,Documentation Replication on Other Examples
486,69777802,How to create the same structure of tf.data.experimental.make_csv_dataset from pandas,"tf.data.experimental.make_csv_dataset creates a TF Dataset ready for Kears supervised training. How to create the same from Pandas? Tried below but the label is dictionary instead of int32. By the way, the data structure ready for Keras supervised training is (features, labels) but which document defines it?",1,Lack of Alternative Solutions/Documentation
487,69792031,Explanation of tf.keras.layers.CategoryEncoding output_mode='multi_hot' behavior,"Please help understand the definition of multi hot encoding of tf.keras.layers.CategoryEncoding and the behavior of output_mode='multi_hot'. According to What exactly is multi-hot encoding and how is it different from one-hot?: The document says num_tokens is the total number of tokens the layer should support. According to the definitions of multi hot encoding above, I expected tf.keras.layers.CategoryEncoding(num_tokens=5, output_mode=""multi_hot"") encodes 5 tokens into an array of size 3. However, the document says ""multi_hot"" encodes each sample into a single array of num_tokens size, containing a 1 for each vocabulary term present in the sample, and behaves as such. Which has no difference from One Hot Encoding. For multi value inputs, multi_hot only handles the first value. To handle multiple inputs, need to be 2D array. Apparently the definition of multi hot encoding of tf.keras.layers.CategoryEncoding is not the same with the one in What exactly is multi-hot encoding and how is it different from one-hot?.",1,Lack of Alternative Solutions/Documentation
488,69843239,How does tf.keras.util.array_to_image() work with regards to memory?,"I have image data that I want to use in a TensorFlow model, but I have to retrieve the image as an (Numpy) array of pixel values. From what I've read, TensorFlow has to read an image in as some image format and from some location. I know that tf.keras.util.array_to_image() can convert an array to a PIL instance of an image, and I know that there are several other libraries that have similar functionality, such as PIL.Image.fromarray(). My problem is that I don't want to duplicate the image data by copying it to a new format. The API documentation for the tf.keras.util.array_to_image() says that it returns a ""PIL image instance"". Does that mean that it is copying all array values to a new data structure and returning that, or is it creating an image data structure that references the original array pixel values? As a follow-up question, if the keras method does duplicate the data (by having both the original array and the image instance have independent values), is there a way to have TensorFlow accept an array representation of an image without needing to duplicate it as a separate image file?",1,Documentation Replication on Other Examples
489,70196272,Overcoming incompatibilities between tensorflow 1.x and 2.x when trying to view layer activity with backend,I would like to run newer tensorflow routines like: for which I get error in 1.x: ImportError: cannot import name image_dataset_from_directory while preserving older functionality of 1.x like running the routine to see activations in various layers like: for which I get the following error in tf 2.x: ValueError: Input tensors to a Functional must come from tf.keras.Input. Received: 0 (missing previous layer metadata). The code: The documentation I looked at suggests the problem may have something to do with eager computation mode eg https://github.com/tensorflow/tensorflow/issues/34201 But I cannot figure out how to resolve this. Thank you for suggestions!,1,Lack of Alternative Solutions/Documentation
490,70328363,Extra dimension to MaxPool1D layer from Conv1D layer,"I'm very new to Tensorflow (this is my first project using it), and I don't really understand how input shapes work. I am trying to train a CNN-LSTM on a set of financial time series data. For my use case, I have a tf.keras.data.DataLoader object which is meant to serve batches of training data to the model. One training instance corresponds to the price history over the last 30 days, and hence should have shape (30,). running the following code: I get that x.shape is (4, 30), where the Dataset object I have defined serves training instances in batches of 4. Here is my code: with accompanying error message: I don't understand what's going on--why is there an extra dimension coming out of the Conv1D layer? I mean, should the output of 1-D convolution not simply be (BATCH_SIZE, WIDTH, 32) (padding was set to 'same')? I apologize if this is addressed in the documentation, but I have been looking everywhere for an answer and I can't seem to fix this problem. I would really appreciate some help here. Thanks!",1,Lack of Alternative Solutions/Documentation
491,70363340,Question about tensorflow.tile with a tensor of 5 dimensions,"I'm trying to understand the following thing from an implementation of a paper I'm currently reading: In tensorflow, if I have a tensor x of shape (4,64,5,5) ending up with something of shape (4,64,5,5,5) Reading the documentation for tf.tile, I still don't understand what is it exactly doing in this case. Am I replicating the new dimension for 5 times? And if yes, what is exactly placed in the new dimension by tensorflow? What am I exactly replicating?",1,Documentation Ambiguity
492,70747499,Using tf.map_fn when the function has multiple outputs,"I can easily use tf.map_fn when the function has one output: output: But, when the function has two outputs: I get an error. Not sure what is going on. I read the information about tf.map_fn in here https://www.tensorflow.org/api_docs/python/tf/map_fn, but not sure how to fix this: map_fn also supports functions with multi-arity inputs and outputs: If elems is a tuple (or nested structure) of tensors, then those tensors must all have the same outer-dimension size (num_elems); and fn is used to transform each tuple (or structure) of corresponding slices from elems. E.g., if elems is a tuple (t1, t2, t3), then fn is used to transform each tuple of slices (t1[i], t2[i], t3[i]) (where 0 &lt;= i &lt; num_elems). If fn returns a tuple (or nested structure) of tensors, then the result is formed by stacking corresponding elements from those structures. Output:",1,Documentation Replication on Other Examples
493,70880589,what does cardinality mean in relation to an image dataset?,"After successfully creating a tensorflow image Dataset with: dataset = tf.keras.utils.image_dataset_from_directory(...) which returns Found 21397 files belonging to 5 classes. Using 17118 files for training. There is the cardinality method: dataset.cardinality() which returns a tensor containing the single value tf.Tensor(535, shape=(), dtype=int64) I've read the docs here but I don't understand what 535 represents or why its different to the number of files? I ask, because I would like to understand how cardinality plays into this equation: steps_per_epoch = dataset.cardinality().numpy() // batch_size",1,Lack of Alternative Solutions/Documentation
494,71019644,Equivalent tensorflow expression to numpy mask,"I have a numpy array named PixelData of unknown shape, and I am using the following condition to filter values in the array greater than some value x using a mask: When I convert this numpy array to a tensor, I cannot perform the same masking operation. I have tried using tf.where as follows: In the official documentation, they always seem to define the mask dimensions in advance to equal the dimensions of the tensor being masked, but then they talk about the dimensions being broadcasted automatically, so I am a bit confused. Are these two functions equivalent? Are there any situations where they may produce different outputs?",1,Documentation Ambiguity
495,71090570,How do I create a tf.Tensor from a pandas DataFrame containing arrays?,"I have a pandas DataFrame like below. Output: I would to create a Tensor from the values of the col column, in order to use them in a specific use case. I have tried converting the values like Which looks like: When I try to convert this to a Tensor, by I get an exception: I can see from the documentation that the tf.constant method should work on a very similar array The values variable I create have .shape like (2,) while the image below have (2, 3), which might be the problem. I can't seem to get the dtype and/or shape to match exactly, and I'm unsure how to get it to work. Any ideas?",1,Documentation Replicability
496,71129505,"Is it possible to split a tensorflow dataset into train, validation AND test datasets when using image_dataset_from_directory?","I am using tf.keras.utils.image_dataset_from_directory to load a dataset of 4575 images. While this function allows to split the data into two subsets (with the validation_split parameter), I want to split it into training, testing, and validation subsets. I have tried using dataset.skip() and dataset.take() to further split one of the resulting subsets, but these functions return a SkipDataset and a TakeDataset respectively (by the way, contrary to the documentation, where it is claimed that these functions return a Dataset). This leads to problems when fitting the model - the metrics calculated on validation sets (val_loss, val_accuracy) disappear from model history. So, my question is: is there a way to split a Dataset into three subsets for training, validation and testing, so that all three subsets are also Dataset objects? Code used to load the data Model compilation and fitting When using a normal Dataset, val_accuracy and val_loss are present in the history of the model: But when using a SkipDataset, they are not:",1,Documentation Replication on Other Examples
497,71130645,Correct axes to use dot product to evaluate the final output of a listwise learning to rank model,"I'm not being able to find the correct configuration to pass to a tf.keras.layers.Dot to make a pairwise dot product when the entries each have lists of values, like from a listwise learning to rank model. For instance, suppose: Calling tf.keras.layers.Dot(axes=??)([repeated_query_vector, document_vectors]) I want the output to be like: All examples I found in the documentation have one dimension less than my use case. What would be the correct value of axes for this call?",1,Inadequate Examples
498,71149271,"How to remove single feature from tensorflow dataset, how to use apply on single feture?","I created dataset from csv file with dataset = tf.data.experimental.make_csv_dataset() function but My dataset has categorical and numeric features. Question 1: The question is how can I modify only single feature, for example weight +2, to: I try to do something like: but the error is: ""TypeError: 'FilterDataset' object is not subscriptable"" Example from the documentation https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply doesn't show it. Question 2: How can I remove single feature ? Is there any equivalent to pandas drop column?",1,Documentation Replication on Other Examples
499,71294464,@tf_gradient peculiar implementation in StyleGan,"I've been reading the source code for the StyleGAN implementation, and I cannot understand the peculiar use of the @tf_gradient decorator. Let us take the concrete example of their implementation of Leaky_Relu. The way I would do it is as follows : Which follows the tf documentation for the use of tf.custom_gradient. But in the styleGan paper, they implement it as follows (I removed the ""variable_scope"" in my implementation as I'm not sure what it does): There are two @tf.custom_gradient decorators used, and I don't understand why since there clearly aren't any second order derivatives being computed (as they are identically 0 anyway for LRelu). Is this a trick to somehow speed up computations ? If so, how does it work ? EDIT : To clarify why I think this is somehow a ""trick"" to make computations of gradients faster, the authors make the following comment in the code : And for completeness, here is the repo from which I took the code from",1,Documentation Replication on Other Examples
500,71335830,What is the difference between tf.keras.layers.Input() and tf.keras.layers.Flatten(),"I have seen multiple uses of both tf.keras.layers.Flatten() (ex. here) and tf.keras.layers.Input() (ex. here). After reading the documentation, it is not clear to me",1,Documentation Ambiguity
501,71588962,Solving a set of linear systems in tensorflow,"I'm having a problem understanding the working mechanism of tensorflow's function: tf.linalg.solve. I want to solve a set of linear systems (AX = Y), where the linear coefficients (A) were shared but there are multiple batches of Y, which are different. Using numpy, I can simply do it via: which gives me a quite consistent solution. But when I switch to tensorflow, it gives me the results: According to the document, I assume the solution should be solved according to the corresponding vec. But it does not seem to give me the expected results in tensorflow. Since I'm a new user, I could have messed up something. It would be appreciated if any information could be offered.",1,Documentation Ambiguity
502,71596616,Predicting Data using an Untrained Keras Model,"Essentially, I want to propagate data through a Keras model, without first training the Keras model. I tried using both predict() and feeding in raw tensors into the model. The data is a 2D Numpy float64 array with shape (3, 3), filled entirely with zeros. The model itself is outlined below: The model needed two different activation functions in it's output layer, hence why I used Functional API. I first attempted to use hyperparameter.predict(data[0]), but kept getting the following error: I fiddled around with array dimensions a bit, but the model continued to give the same error. I then tried feeding raw tensors into the model, with the following code: This code continued to give the following error: I've looked online for a while, and I've searched through the tf.data documentation, but I'm still not sure how to fix this. Again, I've tried multiple variations of this code, and I continue to get mostly the same errors.",1,Documentation Ambiguity
503,71619495,Image normalization by tf.image.convert_image_dtype function,"According to documentation tf.image.convert_image_dtype ""Images that are represented using floating point values are expected to have values in the range [0,1)."" But in the keras tutorial(https://keras.io/examples/vision/cutmix/) i have seen the following preprocessing function: My question is: why did they divide by 255, when tf.image.convert_image_dtype already did that job?",1,Documentation Ambiguity
504,71791115,Nan Loss when training Deep neural Recommender model using tensorflow,"I am trying to follow tensorflow documentation and applying same technique to one of toy dataset. During training I am getting all loss as Nan. I have tried to debug the same using Debugger V2 and I could see that tf.keras.layers.GlobalAveragePooling1D is giving Nan due to division by 0, which is causing all values to be Nan during backpropagation. But what is not clear from the debugger V2 GUI why the sum is becoming 0. I did try to reduce the number of features and the size of the dataset, but each of this activity is giving me new error (probably I shall start a separate question thread for each issues at a later point ). Below is the code for reference. I am providing the dataset as well here. I had tried below code on Google Colab. Preparing Data Reading data to tensorflow dataset Preparing train and test dataset User model class Item model Query model . Creating Deep model Creating deep model for the Item model Combining both query and candidate model training the model Below id the outout that I got",1,Documentation Replication on Other Examples
505,71933464,How to make true_fn of tf.cond skip a for loop in tensorflow v1.0/python?,"I want to use tf.cond to mimic the python if-else logic in the _preprocessing_fn of transform.py. Specifically, if the condition of tf.cond is true, I want to skip the current iteration of the for loop. This seems problematic because true_fn and false_fn parameters of tf.cond are expected to return Tensors according to the documentation. However, in my case, I want true_fn (aka skip_feature_fn)to simply ""continue"" to the next for loop iteration. Also, I want false_fn to take in two inputs (feature and sp) and simply feed them to some other API (e.g. tft.vocabulary). I don't expect either of true_fn or false_fn to return anything. Could someone help me accomplish my goal? Here is the code snippet I'm working with: Thank you.",1,Documentation Replication on Other Examples
506,72165812,Connecting BatchDataset with Keras VGG16 preprocess_input,"I am using tf.keras.preprocessing.image_dataset_from_directory to get a BatchDataset, where the dataset has 10 classes. I am trying to integrate this BatchDataset with a Keras VGG16 (docs) network. From the docs: However, I am struggling to get this preprocess_input working with a BatchDataset. Can you please help me figure out how to connect these two dots? Please see the below code: This will throw TypeError: 'BatchDataset' object is not subscriptable: From TypeError: 'DatasetV1Adapter' object is not subscriptable (from BatchDataset not subscriptable when trying to format Python dictionary as table) the suggestion was to use: However, this also fails: This is all using Python==3.10.3 with tensorflow==2.8.0. How can I get this working? Thank you in advance.",1,Documentation Replication on Other Examples
507,72184958,Pydantic: Type hinting tensorflow tensor,"any idea of how to type-hint tf tensors using pydantic??. Tried default tf.Tensor and tf.flaot32 Looking at documentation in pydantic, i believe something like this arbitrary class need to be defined... with following in main..",1,Documentation Replicability
508,72329108,Is there a simple way to know which Tensorflow ops have a registered GPU kernel?,"I have been trying to optimize some Tensorflow code that was pretty memory inefficient (use of large dense tensors containing very sparse information), and would thus limit batch size and scalability, by trying to make use of SparseTensors. After some struggle I finally come up with a decent solution with satisfactory speedup on CPU and very low memory usage, and when the time comes to use a GPU I realize that the previous memory inefficient is orders of magnitude faster... Using tensorboard profiling I've discovered that two of the operations I have used in my """"optimized"""" version only run on CPU (namely UniqueV2 and sparse_dense_matmul), but I could not see any hint of that in the documentation. The only related piece of documentation states: In turn there is nothing in the tf.cast documentation hinting that the op has no GPU kernel. Thus, is there a simple way to know whether a TF ops has a registered GPU kernel, without having to use a GPU to find it out? The custom ops guide suggest that this could be seen by looking at the ops C files, but this seems a rather cumbersome way to do it... I'm using TF v2.8 Thanks!",1,Inadequate Examples
509,72707453,How to save a tensorflow dataset to multiple shards without using enumerate,"I have a tensorflow dataset with some elements in it, and I want to save it with tf.data.Dataset.save such that each element gets its own shard. Thus if the dataset contains 2,000 elements, it would be saved to 2,000 shards. The documentation here specifies how to create 1 shard only, but not how to make a shard for each element. Below, I am able to do it with enumerate, but is there another way to do it without also saving the index from enumerate?",1,Documentation Replicability
510,72720129,Understanding tf.keras.metrics.Precision and Recall for multiclass classification,"I am building a model for a multiclass classification problem. So I want to evaluate the model performance using the Recall and Precision. I have 4 classes in the dataset and it is provided in one hot representation. I was reading the Precision and Recall tf.keras documentation, and have some questions: Any clarification of this function will be appreciated. Thanks in advance",1,Documentation Replicability
511,72850120,"Keras - Specifying from_logits=False when using tf.keras.layers.Dense(1,activation='sigmoid')(x)","I am working on a binary classification problem, using transfer learning and image inputs and have a question regarding the I have been working through using the correct activation layers (e.g. Softmax or Sigmoid - sigmoid for binary softmax for multiclass) and noticed when I specify 'sigmoid' as part of the Dense() output layer, I no longer need to specify from_logits=True during model.compile(). This means when I am obtaining predictions, I don't use the tf.nn.sigmoid() function and instead simply check if the value is greater than 0.5, then 1, else 0. Is this correct? Here is my code: And then when I obtain predictions, I have the following: My intuition is that as I am specifying the sigmoid activation function during the Dense layer build, I no longer work with 'logits' and therefore do not need to apply the sigmoid function later on. In the documentation, I've seen both examples used but it's quite sparse on information when working with model.predict(), would appreciate any guidance.",1,Documentation Replicability
512,72928149,Difference between Experimental Preprocessing layers and normal preprocessing layers in Tensorflow,"I am trying to figure out which I should use for Data Augmentation. In the documentation, there is: tf.keras.layers.RandomFlip and RandomRotation Then we have in tf.keras.layers.experimental.preprocessing the same things, randomFlip and RandomRotation. Which should I use? I've seen guides that use both. This is my current code: and this is a part of my model: I am a bit confused here..",1,Documentation Replication on Other Examples
513,73049510,How to dynamically set pool size for AveragePooling2D layer/ How to pass external value to an sequential layer,"Trying to understand listwise documentation while trying to replicate by mixing deep model to listwise I am stuck at point where I am not able to set the pool size inside the sequential layer in an dynamic manner. For example consider below code After we create movie model lets try to test it before we can use it on proper movie data below is the test code This now works perfect and I will get an output as below Now if you notice in the code above I have hardcoded the pool_size as 1,4 ( tf.keras.layers.AveragePooling2D(pool_size=(1,4),strides=1, padding='valid',),) because the test sample I had used above only have maximum 4 words, so the vectorization will produce vector of size 4, now problem is how to I ensure the right pool size when I pass the whole dataset (movies) to the model. How can I pass such external value (pool_size) to an sequential layer from outside? The above code was run on google colab using tensorflow version 2.9.1",1,Documentation Replication on Other Examples
514,73165980,Tensorflow: how to feed a variable-time-step input to a RNN,"I have a simple X_train and Y_train data: Arrays are numpy arrays. I am now trying to use the tf.data.Dataset class to load these as tensors. Before I have done a similar thing successfully using the following code: As this input is fed into a RNN, I have used the expand_dims method in the first RNN layer (the expand_dimension is passed as a function to overcome an apparent bug in tensorflow: see https://github.com/keras-team/keras/issues/5298#issuecomment-281914537): This worked although because I had arrays of equal length. In the example I posted instead the 1st array has 13 numbers and the 2nd one 18. In this case the method above doesn't work, and the recommended method seems to be using tf.data.Dataset.from_generator. Reading this How to use the Tensorflow Dataset Pipeline for Variable Length Inputs?, the accepted solution shows something like the following would work (where I am not caring here about y_train for simplicity): However, the syntax in tensorflow has changed since this answer, and now it requires to use the output_signature argument (see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator). I've tried different ways but I'm finding hard to understand from tensorflow documentation what the output_signature should exactly be in my case. Any help would be much appreciated.",1,Documentation Ambiguity
515,73179836,tensorflow.py_function fails to temporarily switch to eager execution while in graph mode,"I'm not sure if this is a Tensorflow bug or my misunderstanding about what this function is supposed to do, but I can't get tf.py_function to return an EagerTensor while in graph mode. Consequently, calling .numpy() on the output of this function fails. The issue can be reproduced using the exact example given in the official documentation (https://www.tensorflow.org/api_docs/python/tf/py_function): This generates the following error: I am running Python 3.8 and Tensorflow v2.9.1. Any help would be greatly appreciated!",1,Documentation Replicability
516,73213159,How to apply tf.data transformations to a DataFrame,"I want to apply tf.data transformations to a panda dataframe. According to the tensorflow docs HERE I can apply tf.data to a dataframe directly but the dtype of the dataframe should be uniform. When I apply tf.data to my dataframe like below it generates this error When I print df['reports'].dtype it is dtype('O') which seems to be not uniformed, if this is the case then how can I convert this dataframe to uniform dtype.",1,Documentation Replicability
517,73279782,Tensorboard profiling a predict call using Cloud TPU Node,"I've been trying to profile a predict call of a custom NN model using a Cloud TPU v2-8 Node. It is important to say that my prediction call takes about 2 minutes to finish and I do it using data divided in TFRecord batches. I followed the official documentation ""Profile your model with Cloud TPU Tools"" and I tryied to capture a profile: I could generate some data in both cases (Tensorboard UI and profiler call), but when I try to open it in Tensorboard pointing the logdir path, I received a ""No dashboard are active for the current data set"" message. Is there any way to profile a Tensorflow/Keras prediction call with a model running in a Cloud TPU Node? Curious fact - There seems to be an inconsistency in the Tensorflow docs and Cloud TPU docs: in Tensorflow Optimization Docs we can see that tf.profiler.experimental.start/stop calls are not supported by TPU hardware, but in Google Cloud docs this is the recommended method to capture a profile in TPU. Config:",1,Documentation Replication on Other Examples
518,73645574,Why keras AUC returns zero when multi-label is set?,"I'm trying to understand how tf.keras.metrics.AUC(multi_label=True) works. From the docs, I'm led to understand that when working with multi-label vectors, each class is computed individually, then averaged. However, I can't seem to get the following trivial case to compute correctly. That is, if the prediction is the same as the expected vector, why is the output not 1.0?",1,Documentation Ambiguity
519,73657854,Is SageMaker Distributed Data-Parallel (SMDDP) supported for keras models?,"Is SageMaker Distributed Data-Parallel (SMDDP) supported for keras models? In documentation it says ""SageMaker distributed data parallel is adaptable to TensorFlow training scripts composed of tf core modules except tf.keras modules. SageMaker distributed data parallel does not support TensorFlow with Keras implementation."" https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-modify-sdp.html But inside the training script and how to modify it, I can see the tf.keras and tf.keras.model is used. https://sagemaker.readthedocs.io/en/stable/api/training/sdp_versions/v1.0.0/smd_data_parallel_tensorflow.html",1,Documentation Replication on Other Examples
520,73794766,what is the meaning of axis=-1 in tf.keras.layers.Normalization?,"I'm trying to learn deep learning using keras and tensorflow and I came across a code explaining linear regression at https://www.tensorflow.org/tutorials/keras/regression wherein they have created a normalization layer using normalizer = tf.keras.layers.Normalization(axis=-1). Someone please explain the meaning of axis =-1 . I tried looking at the API documentation but I couldnt understand the explanation from there?I know that axis=0 represent rows and axis=1 columns, right? Thanks in advance",1,Documentation Completeness
521,74005009,How to create output_signature for tensorflow.dataset.from_generator,"I have a generator yielding data and labels yield data, labels where the data is an numpy.ndarray with variable rows and 500 columns of type dtype=float32 and the labels are integers of numpy.int64. I'm trying to pass this data into TensorFlow from_generator function to create a TensorFlow dataset: tf.data.Dataset.from_generator The docs say that the from_generator function needs a parameter output_signature as an input. But I'm having trouble understanding how to build this output_signature. How can I make the output_signature for the generator I described? Thank you! Edit: I used tf.type_spec_from_value to get this: But is it correct to use None when the number of rows is varying for the first data type?",1,Documentation Replicability
522,74029376,Tensorflow custom reduction function with axis support,"I would like to get the value with the maximum absolute value in a tensor, with respect to an axis. Note that I don't want the maximum absolute value, I want the value that has the maximum absolute value (so I need to keep the sign). Ideally, I would like something similar to reduce_max or reduce_min: but I did not find anything useful in tensorflow documentation. With a flat tensor, I know that I could use tf.foldl or tf.foldr: However, I don't know how to handle an axis parameter in the case of multidimensional tensors.",1,Lack of Alternative Solutions/Documentation
523,74060508,How to Save a Tensorflow Dataset,As the title says I'm trying to save a TensorSliceDataset object to file. Viewing tensorflow's website it seems that the tf.data.Dataset class has a save function but it is not implemented for TensorSliceDataset objects. Pickling also did not work for me. Example code returns error: AttributeError: 'TensorSliceDataset' object has no attribute 'save',1,Documentation Replicability
524,74182037,"How to ""update"" from module tf.keras.preprocessing.image to tf.keras.utils.image_dataset_from_directory for features extraction","This code part is common to both ""problematic"" codes below: The following code I successfully use to extract features of a set of images using module tf.keras.preprocessing.image. Thereafter I train a simple nearest-neighbor model using brute-force algorithm and I'm able to find three other images that are really similar to the query image as you can see below: But as pointed in the documentation this preprocessing module is deprecated. So, I would like to ""update"" the code as suggested in the documentation: ""Prefer loading data with tf.keras.utils.image_dataset_from_directory, and then transforming the output tf.data.Dataset with preprocessing layers"". For that I'm trying the following: And now I begin with the features extraction After that I do the same and train the nearest neighbor model with this features, but the results are catastrophic as you can see below: What I'm doing so wrong that I have such different results? == EDIT 1 == Answering @DWKOT using the same image we have following results: And the code that give us the distance to the 5 nearest neighbors: With the following results: With the second code we have following result for query / similar image: / And following results for the first five ""similar"" images: What is also strange as I would expect similar images having values next to zero and different ones far from zero...",1,Documentation Replication on Other Examples
525,74359221,Tensorflow v2.10 mutate output of signature function to be a map of label to results,"I'm trying to save my model so that when called from tf-serving the output is: where label1 and label2 are my labels and x.xxxxx are the probability of that label. This is what I'm trying: Side Note: Apparently in TensorFlow v2.0 if signatures is omitted it should scan the object for the first @tf.function (according to this: https://www.tensorflow.org/api_docs/python/tf/saved_model/save) but in reality that doesn't seem to work. Instead, the model saves successfully with no errors and the @tf.function is not called, but default output is returned instead. The error I get from the above is: I wrapped the result in tf.constant above because of this error, thinking it might be a quick fix, but I think it's me just being naive and not understanding Tensors properly. I tried a bunch of other things before learning that [all outputs must be return values].1 How can I change the output to be as I want it to be?",1,Documentation Replication on Other Examples
526,74434308,Setting only global level seed gives same output in consecutive iterations of loop in Tensorflow,"I am testing out the tf.random.set_seed according to the rules given at - https://www.tensorflow.org/api_docs/python/tf/random/set_seed In particular I am testing the second rule - where we set only global level seed and no operation level seed. According to the documentation (the link is mentioned above), the second rule is: To explain the second rule, the documentation uses the following snippet: and states that Now, I tested this rule on a 1D tensor of shape (3,) to check if the output of shuffling the tensor does not give the same sequence within consecutive iterations of the loop as follows: I got the following output: From the output you can see that the sequence on line number 7 and 8 match. Also the sequence on line number 13 and 14 match. According to the documentation, tensorflow should not output the same sequence in a consecutive iteration. Then why am I getting this kind of output? Have I misunderstood the concept? To test this further, I also tested to following snippet which I used to generate 14 1-D tensors and check if any tensor is repeated within consecutive runs as follows: And I got the following output: You can see that no two consecutive tensors are repeated. Why didn't I see this behaviour for my first snippet?",1,Documentation Ambiguity
527,75136950,How to visualize tf.compat.v1 static graph in tensorboard?,"For a given graph, how can we visualize the graph using tensorboard for tf.compat.v1 ? Sharing this here after searching everywhere. Most of the documentations explains tf.keras and not for tf.compat.v1 static graphs",1,Documentation Replication on Other Examples
528,75371111,"when trying to load external tfrecord with TFDS, given tf.train.Example, how to get tfds.features?","What I need help with / What I was wondering Hi, I am trying to load external tfrecord files with TFDS. I have read the official doc here, and find I need to define the feature structure using tfds.features. However, since the tfrecords files are alreay generated, I do not have control the generation pipeline. I do, however, know the tf.train.Example structre used in TFRecordWriter during generation, shown as follows. The doc only describes how to translate tfds.features into the human readable structure of the tf.train.Example. But nowhere does it mention how to translate a tf.train.Example into tfds.features, which is needed to automatically add the proper metadata fileswith tfds.folder_dataset.write_metadata. I wonder how to translate the above tf.train.Example into tfds.features? Thanks a lot! BTW, while I understand that it is possible to directly read the data as it is in TFRecord with tf.data.TFRecordDataset and then use map(decode_fn) for decoding as suggested here, it seems to me this approach lacks necessary metadata like num_shards or shard_lengths. In this case, I am not sure if it is still ok to use common operations like cache/repeat/shuffle/map/batch on that tf.data.TFRecordDataset. So I think it is better to stick to the tfds approach. What I've tried so far I have searched the official doc for quite some time but cannot find the answer. There is a Scalar class in tfds.features, which I assume could be used to decode Int64List. But How can I decode the BytesList? Environment information",1,Documentation Completeness
529,75401761,Change Verbosity of Keras train_on_batch()?,"I am training a GAN using Keras's train_on_batch() command. This is very similar to Keras's fit(). However, in the documentation for fit(), there is a parameter for verbose, which changes how often a progress bar is printed to the console. My model has many batches, and so it is printing tons of progress bars to the command line. Unfortunately, train_on_batch() does not have a verbose parameter. Is there a workaround for this? Is there a Keras global variable/environment variable that I can set? I don't want to disable my program from printing to the console, I just want to change the verbosity of specifically train_on_batch(). For clarify, I am using Keras directly from the Keras package, I am not using tf.keras.",1,Documentation Replicability
530,75572543,What to look out for when passing a generator into model.fit in tensorflow?,"I want to replace the x and y training data parameters in tf.keras.Model.fit with a generator. However, some subtlety seems to escape me, as the model accuracy doesn't improve with the generator when training. As far as I understand the documentation, the generator is supposed to yield tuples (x_vals,y_vals), such that x_vals is a concatenation of batch_size-many training samples along a new 0th dimension, and 'v_vals' is the concatenation of their corresponding labels. As long as the generator fulfills this, as I understand it, we can just replace the x parameter in tf.keras.Model.fit with the generator and omit the y parameter, though to define an epoch, we also need to specify 'steps_per_epoch' in fit. There however seems to be something here I misunderstood or forgot, because starting with a model and input data that trains (i.e. its accuracy improves) and replacing the training data array with a generator as discussed, results in a model that doesn't train (i.e. its accuracy instead goes up a little, then however goes back down till its equal to chance). The corresponding code: The model also trains if one first let's the generator generate a long list of samples and then passes those into fit as xand y:",1,Documentation Replication on Other Examples
531,75639137,TF1 to TF2 migration,"Hello I am new to tensorflow and I am working on a code that I would like to migrate from tensorflow 1 to 2. I have this line of code: As mentioned in https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder, I should use keras.Input. But even when specifying the shape, I can't have the same tensor as with compat.v1: To check the shape I use tf.shape(x1) or tf.shape(x2), but the shapes are not the same. Could anyone explain to me how to have, in TF2, the same shape as in TF1 ? Thanks and regards",1,Documentation Replicability
532,75851842,tensorflow map function to mulitple tensors,"I am using the following function in a custom layer in TensorFlow to rearrange query, key values: and it throws this warning: WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23. Instructions for updating: Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089 Is there a more TensorFlowic way of doing this? I tried using map_fn as follows and it throws the the same warning and an error: From documentation, it seems tf.map_fn but it seems to work on a stack of tensors. Will it be better to stack the tensors?",1,Documentation Replication on Other Examples
533,75996642,Is there a good equivalent of pandas' `apply` for TensorFlow datasets?,"BACKGROUND The use of tf.data.Dataset is promoted by TensorFlow as the best practice for implementing input pipelines due to their efficient implementation of common operations such as batching, shuffling, as well as their seamless integration with the Keras API. I may just be lousy at looking up the documentation on the matter, but it seems to me that the major drawback of TensorFlow datasets is that they are quite unwieldy, if not impossible to work with, when trying to implement feature engineering tasks whereby a new column is created via the application of some generic Python function. This is in contrast to pandas' very nifty apply() function which can produce new columns from preexisting ones both efficiently (i.e., via vectorization) and in a pythonic manner. To the best of my understanding, the closest thing to pandas' apply() is TensorFlow dataset's map(). However, one can't simply use it with arbitrary Python functions since they'd first need to be converted to tensors. This becomes very difficult as one has to have arcane knowledge of the miscellaneous tensor analogues of arbitrary Python functions (e.g., tf.strings.length() instead of Python's len()). Even when one finds such functions, the idiosyncracies of tensor operations makes them very un-pythonic and prone to obscure dimensionality or type errors. I've read about TensorFlow's py_function as some sort of wrapper that magically converts Pythonic code into a tensor representation, but judging from the documentation, it became clear to me that this is far from the case. QUESTION Is TensorFlow's tf.data just not mature yet to be able to handle feature engineering in the same way that pandas apply() does? If not, what am I missing in my understanding? MINIMUM WORKING EXAMPLE In the code below, I compare a pandas DataFrame df with the equivalent TensorFlow dataset ds. My goal is to engineer two extra features, namely As you can see for yourself, the first operation works intuitively for both pandas and TensorFlow, but the second only works easily for pandas. Getting it to work with TensorFlow is either extremely complex, or just plain impossible. The output is of the above is as follows.",1,Documentation Ambiguity
534,76040030,Problem using Huggingface imagenet-1k dataset in Keras / Tensorflow,"I'm having a problem using the imagenet-1k dataset from Huggingface with a Keras model. I'm just experimenting with simple models, but am stuck trying to get the dataset to work with the model fit function. Here is how I load the dataset: Here is the fit invocation: I get the following error: When I inspect one of the elements of the datasets it looks like a tf.Tensor, so I don't understand why it can't be passed directly. None of the examples or docs I can find make it clear how to do this. Huggingface examples for images produce the same format that I'm getting, but apparently there is a step I'm missing before it can be used with model.fit()",1,Documentation Replication on Other Examples
535,76244268,Tensorflow: Build new model from input and middle layers of another model,"I'm trying to build new_model from another model layers for class activation mapping purposes. And with this code i get the following error: Already tried functional model API, providing Input() layer inside vgg_sequential() with the same error that my Input layer is disconected from the rest of my model. Beside this when using tf.keras.applications.efficientnet_v2 that provides input layers for rescaling and resizing images i don't have any problem. Any help, information, tips or links to docs that getas me to a solution will be very much appreciated. Thanks in advance.",1,Inadequate Examples
536,76324368,Understanding tf.keras.layers.Dense(),"I am trying to understand why there is a difference between calculating a dense layer operation directly and using the keras implementation. Following the documentation (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) tf.keras.layers.Dense() should implement the operation output = activation(dot(input, kernel) + bias) but result and result1 below are not the same. output Using test.get_weights() I can see that the kernel and bias (b) are getting set to the correct values. I am using TF version 2.12.0.",1,Documentation Replication on Other Examples
537,76380927,Tensorflow decode image,"I am a beginner in tensorflow and I am training a small cnn, I am using the tf.io.decode_image function but I can't figure out if this function does preprocess. The tensorflow documentation about it doesn't say anything. When I open images with this function the values are between 0 and 1. The images are single channel grayscale. This is the code. I would like to have more explanations",1,Requesting (Additional) Documentation/Examples
538,76391276,Custom gradient for broadcasting operation,"I have an operation for which I want to define a custom gradient with tf.custom_gradient. The operation takes two broadcastable arguments and produces a result with the broadcasted shape. The problem is how to handle the broadcasting rules ""backwards"" in the custom gradient. Let's take the example for a multiplication operation from the documentation of tf.custom_gradient: I can use this gradient alright for the non-broadcasting case: However, when the inputs are broadcasted, the result is not correct: In fact, trying to use it in graph mode fails: Is there a way to handle this ""unbroadcasting"" of the input gradients automatically?",1,Documentation Ambiguity
539,76396532,"Ragged tensors in dataset, tensorflow, how do I train the model","I have in my model, for fitting, and my dataset contains ragged tensors, basically context and x are ragged tensor, of variable length, everything I try gives me some sort of error, for example first I tried [ [ context, x], ...] where all the arrays were np.ndarray, but it said something along the lines that np.ndarray is an unrecognised datatype and cannot be converted to a tf.Tensor.Then when I tried putting this in a tf.data.Dataset, it says ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type tensorflow.python.framework.ops.EagerTensor). I am totally lost on how to train my model I tried several different data types from list, to tf.data.Dataset, but none of them were working, and now I am totally in the dark. Consulting the documentation has not helped me figure out how does the fit function actually treat data",1,Documentation Replicability
540,76444107,Are 'validation_steps' used if the validation_dataset is 'DirectoryIterator'?,"I was trying to use the Keras API to train a given model by using its fit function. In its documentation we can see the following: By reading the above text I understand that if the validation_data is not a tf.data dataset the validation_steps is ignored. However I am not sure if in my case this principle is applied. I am using an ImageDataGenerator to then use its flow_from_directory function that returns a DirectoryIterator. In its documentation we can see the following: So, by reading this I am convinced that the whole validation dataset is going to be used despite the fact that validation_steps might be set to a given number. Is there a way I can check whether it is using the whole dataset or only the given validation_steps? Does it simply ignore the validation_steps because the validation_data is not a tf.data and therefore there is no need to check it? Thanks! Code example of usage: PS: I know that we can let validation_steps be None to use it fully for validating the model in each epoch. My question is especifically if the validation_steps parameter is ignored or not if provided while using data from a ImageDataGenerator.",1,Documentation Replication on Other Examples
541,76447508,How to retrain a model that was saved using the tf.saved_model.save() function in Tensorflow,"I am building a Neural Machine Translator for English to Konkani (a local language) language using the Transformer architecture proposed by (Vaswani et, al. 2017). I am following the tutorial code from https://www.tensorflow.org/text/tutorials/transformer. I have trained the model and used the tf.saved_model.save() method to save the model files locally. I now want to retrain that saved model on a new dataset that I have gathered recently, but I've realised that after loading the model using the tf.saved_model.load() method, I am not able to train it again as the loaded model now lacks the necessary method model.fit() . Here is a part of the model training code: Here's the error I get when I try to retrain the model using the following code: The error: After reading the documentation I've realised that when saving the model in the above method, the model.fit() and other methods are not saved hence they are not callable. I need help in finding a way to retrain my saved model, It is not feasible for me to train a new model on a combined dataset as It will take up lot of time and I have very limited resources. I have been looking up on the web for days but couldn't find a solution. Any help in this regards will be appreciated!",1,Inadequate Examples
