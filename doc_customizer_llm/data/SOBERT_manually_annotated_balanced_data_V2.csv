QuestionId,Title,Body,DocRelated
33762831,Error when building seq2seq model with tensorflow,"<p>I'm trying to understand the seq2seq models defined in seq2seq.py in tensorflow. I use bits of code I copy from the translate.py example that comes with tensorflow. I keep getting the same error and really do not understand where it comes from.</p>

<p>A minimal code example to reproduce the error:</p>

<pre><code>import tensorflow as tf
from tensorflow.models.rnn import rnn_cell
from tensorflow.models.rnn import seq2seq

encoder_inputs = []
decoder_inputs = []
for i in xrange(350):  
    encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],
                                              name=""encoder{0}"".format(i)))

for i in xrange(45):
    decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],
                                         name=""decoder{0}"".format(i)))

model = seq2seq.basic_rnn_seq2seq(encoder_inputs,
                                  decoder_inputs,rnn_cell.BasicLSTMCell(512))
</code></pre>

<p>The error I get when evaluating the last line (I evaluated it interactively in the python interpreter):</p>

<pre><code>    &gt;&gt;&gt;  Traceback (most recent call last):
      File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
      File ""/tmp/py1053173el"", line 12, in &lt;module&gt;
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/seq2seq.py"", line 82, in basic_rnn_seq2seq
        _, enc_states = rnn.rnn(cell, encoder_inputs, dtype=dtype)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/rnn.py"", line 85, in rnn
        output_state = cell(input_, state)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/rnn_cell.py"", line 161, in __call__
        concat = linear.linear([inputs, h], 4 * self._num_units, True)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/models/rnn/linear.py"", line 32, in linear
        raise ValueError(""Linear is expecting 2D arguments: %s"" % str(shapes))
    ValueError: Linear is expecting 2D arguments: [[None], [None, 512]]
</code></pre>

<p>I suspect the error comes from my side :)
On a sidenote. The documentation and the tutorials are really great but the example code for the sequence to sequence model (the english to french translation example) is quite dense. You also have to jump a lot between files to understand what's going on. Me at least got lost several times in the code.</p>

<p>A minimal example (perhaps on some toy data) of constructing and training a basic seq2seq model would really be helpful here. Somebody know if this already exist somewhere?</p>

<p><strong>EDIT</strong>
I have fixed the code above according @Ishamael suggestions (meaning, no errors returns) (see below), but there are still some things not clear in this fixed version. My input is a sequence of vectors of length 2 of real valued values. And my output is a sequence of binary vectors of length 22. Should my tf.placeholder code not look like the following? <strong>(EDIT yes)</strong></p>

<pre><code>tf.placeholder(tf.float32, shape=[None,2],name=""encoder{0}"".format(i))
tf.placeholder(tf.float32, shape=[None,22],name=""encoder{0}"".format(i))
</code></pre>

<p>I also had to change tf.int32 to tf.float32 above. Since my output is binary. Should this not be tf.int32 for the tf.placeholder of my decoder? But tensorflow complains again if I do this. I'm not sure what the reasoning is behind this.</p>

<p>The size of my hidden layer is 512 here.</p>

<p>the complete fixed code</p>

<pre><code>import tensorflow as tf
from tensorflow.models.rnn import rnn_cell
from tensorflow.models.rnn import seq2seq

encoder_inputs = []
decoder_inputs = []
for i in xrange(350):  
    encoder_inputs.append(tf.placeholder(tf.float32, shape=[None,512],
                                          name=""encoder{0}"".format(i)))

for i in xrange(45):
    decoder_inputs.append(tf.placeholder(tf.float32, shape=[None,512],
                                         name=""decoder{0}"".format(i)))

model = seq2seq.basic_rnn_seq2seq(encoder_inputs,
                                  decoder_inputs,rnn_cell.BasicLSTMCell(512))
</code></pre>
",1
33916981,Using Sparse Tensors to feed a placeholder for a softmax layer in TensorFlow,"<p>Has anyone tried using Sparse Tensors for Text Analysis with TensorFlow with success? Everything is ready and I manage to feed <code>feed_dict</code> in <code>tf.Session</code> for a Softmax layer with numpy arrays, but I am unable to feed the dictionary with SparseTensorValues. </p>

<p>I have not found either documentation about using sparse matrices to train a model ( softmax for example ) with Tensor Flow, which is strange, as classes <code>SparseTensor</code> and <code>SparseTensorValues</code> or <code>TensorFlow.sparse_to_dense</code> methods are ready for it, but there is no documentation about how to feed the <code>feed_dict</code> dictionary of values in the <code>session.run(fetches,feed_dict=None)</code> method. </p>

<p>Thanks a lot,</p>
",1
34163953,"Tensorflow, binary classification","<p>Is it possible to make a binary classification? Especially for pedestrian detection, whether it is or not a pedestrian. I couldn't find anything in the API or any good tutorials for this. 
I tried to adapt the code from the deep MNIST tutorial, that was used for multi-class classification; I made the images with pedestrians in them labeled with 1, and the negatives with 0, and used 3 channels(for colours, shouldn't be a problem right?), but the accuracy just jumps all over the place.</p>

<p>The code</p>

<pre><code>    import dataset as input_data
    import tensorflow as tf


    def weight_variable(shape):
        initial = tf.truncated_normal(shape, stddev=0.1)
        return tf.Variable(initial)


    def bias_variable(shape):
        initial = tf.constant(0.1, shape=shape)
        return tf.Variable(initial)


    def conv2d(x, W):
        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')


    def max_pool_2x2(x):
        return tf.nn.max_pool(x, ksize=[1, 3, 3, 1],
                              strides=[1, 2, 2, 1], padding='SAME')


    data = input_data.read_data_sets()

    sess = tf.InteractiveSession()

    x = tf.placeholder(""float"", shape=[None, input_data.HEIGHT * input_data.WIDTH * 3])
    y_ = tf.placeholder(""float"", shape=[None, 2])

    W_conv1 = weight_variable([5, 5, 3, 64])
    b_conv1 = bias_variable([64])

    x_image = tf.reshape(x, [-1, input_data.WIDTH, input_data.HEIGHT, 3])

    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
    h_pool1 = max_pool_2x2(h_conv1)
    h_norm1 = tf.nn.lrn(h_pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)

    W_conv2 = weight_variable([5, 5, 64, 64])
    b_conv2 = bias_variable([64])

    h_conv2 = tf.nn.relu(conv2d(h_norm1, W_conv2) + b_conv2)
    h_norm2 = tf.nn.lrn(h_conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)
    h_pool2 = max_pool_2x2(h_norm2)

    W_fc1 = weight_variable([input_data.HEIGHT / 4 * input_data.WIDTH / 4 * 64, 1024])
    b_fc1 = bias_variable([1024])

    h_pool2_flat = tf.reshape(h_pool2, [-1, input_data.HEIGHT / 4 * input_data.WIDTH / 4 * 64])
    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)

    keep_prob = tf.placeholder(""float"")
    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

    W_fc2 = weight_variable([1024, 2])
    b_fc2 = bias_variable([2])

    y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)

    cross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))
    train_step = tf.train.AdamOptimizer(1e-6).minimize(cross_entropy)
    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, ""float""))
    sess.run(tf.initialize_all_variables())
    for i in range(20000):
        batch = data.train.next_batch(50)
        if i % 100 == 0:
            train_accuracy = accuracy.eval(feed_dict={
                x: batch[0], y_: batch[1], keep_prob: 1.0})
            print ""step %d, training accuracy %g"" % (i, train_accuracy)
        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})

    print ""test accuracy %g"" % accuracy.eval(feed_dict={
        x: data.test.images, y_: data.test.labels, keep_prob: 1.0})
</code></pre>

<p>The output</p>

<pre><code>    step 0, training accuracy 0.14
    step 100, training accuracy 0.54
    step 200, training accuracy 0.28
    step 300, training accuracy 0.46
    step 400, training accuracy 0.32
    step 500, training accuracy 0.52
    step 600, training accuracy 0.56
    step 700, training accuracy 0.76
    step 800, training accuracy 0.66
</code></pre>

<p>Help would be apriciated, thanks.</p>
",0
34619177,What does tf.nn.conv2d do in tensorflow?,"<p>I was looking at the docs of tensorflow about <code>tf.nn.conv2d</code> <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/conv2d"" rel=""noreferrer"">here</a>. But I can't understand what it does or what it is trying to achieve. It says on the docs,</p>
<blockquote>
<p>#1 : Flattens the filter to a 2-D matrix with shape</p>
<p><code>[filter_height * filter_width * in_channels, output_channels]</code>.</p>
</blockquote>
<p>Now what does that do? Is that element-wise multiplication or just plain matrix multiplication? I also could not understand the other two points mentioned in the docs. I have written them below :</p>
<blockquote>
<p># 2: Extracts image patches from the the input tensor to form a virtual tensor of shape</p>
<p><code>[batch, out_height, out_width, filter_height * filter_width * in_channels]</code>.</p>
<p># 3: For each patch, right-multiplies the filter matrix and the image patch vector.</p>
</blockquote>
<p>It would be really helpful if anyone could give an example, a piece of code (extremely helpful) maybe and explain what is going on there and why the operation is like this.</p>
<p>I've tried coding a small portion and printing out the shape of the operation. Still, I can't understand.</p>
<p>I tried something like this:</p>
<pre class=""lang-python prettyprint-override""><code>op = tf.shape(tf.nn.conv2d(tf.random_normal([1,10,10,10]), 
              tf.random_normal([2,10,10,10]), 
              strides=[1, 2, 2, 1], padding='SAME'))

with tf.Session() as sess:
    result = sess.run(op)
    print(result)
</code></pre>
<p>I understand bits and pieces of convolutional neural networks. I studied them <a href=""http://cs231n.github.io/convolutional-networks/"" rel=""noreferrer"">here</a>. But the implementation on tensorflow is not what I expected. So it raised the question.</p>
<p><strong>EDIT</strong>:
So, I implemented a much simpler code. But I can't figure out what's going on. I mean how the results are like this. It would be extremely helpful if anyone could tell me what process yields this output.</p>
<pre class=""lang-python prettyprint-override""><code>input = tf.Variable(tf.random_normal([1,2,2,1]))
filter = tf.Variable(tf.random_normal([1,1,1,1]))

op = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME')
init = tf.initialize_all_variables()
with tf.Session() as sess:
    sess.run(init)

    print(&quot;input&quot;)
    print(input.eval())
    print(&quot;filter&quot;)
    print(filter.eval())
    print(&quot;result&quot;)
    result = sess.run(op)
    print(result)
</code></pre>
<p>output</p>
<pre><code>input
[[[[ 1.60314465]
   [-0.55022103]]

  [[ 0.00595062]
   [-0.69889867]]]]
filter
[[[[-0.59594476]]]]
result
[[[[-0.95538563]
   [ 0.32790133]]

  [[-0.00354624]
   [ 0.41650501]]]]
</code></pre>
",1
34642595,Tensorflow Strides Argument,"<p>I am trying to understand the <strong>strides</strong> argument in tf.nn.avg_pool, tf.nn.max_pool, tf.nn.conv2d. </p>

<p>The <a href=""https://www.tensorflow.org/versions/master/api_docs/python/nn.html#max_pool"" rel=""noreferrer"">documentation</a> repeatedly says </p>

<blockquote>
  <p>strides: A list of ints that has length >= 4. The stride of the sliding window for each dimension of the input tensor.</p>
</blockquote>

<p>My questions are:</p>

<ol>
<li>What do each of the 4+ integers represent?</li>
<li>Why must they have strides[0] = strides[3] = 1 for convnets?</li>
<li>In <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3%20-%20Neural%20Networks/convolutional_network.ipynb"" rel=""noreferrer"">this example</a> we see <code>tf.reshape(_X,shape=[-1, 28, 28, 1])</code>. Why -1?</li>
</ol>

<p>Sadly the examples in the docs for reshape using -1 don't translate too well to this scenario.</p>
",1
34699260,How to serialize both the graph and values in a protobuf file?,"<p>The Android example that comes with Tensorflow downloads a protobuf file for InceptionV3 which contains both the graph and the values from the model. In the docs, I could only find how to serialize the graph (<code>tf.Graph.as_graph_def</code>) or save the variable values with a <code>tf.train.Saver</code>. How can you save everything to a single file, as done for that example?</p>
",0
34706950,How many processes does TensorFlow open?,"<p>I am using torque to run some CNN-based learning using <code>tensorflow</code> library. (1 CPU per task)</p>

<p>When I run <code>top</code> on my server, I noticed that: <code>load average: 677.29, 668.59, 470.</code></p>

<p>I create a session like this: <code>sess = tf.Session()</code></p>

<p>So my question is there some place in documentation where I can read when and how many processes TensorFlow uses.</p>
",1
34756903,tf.Print() causing an error with gradients,"<p>I was trying to use <a href=""https://www.tensorflow.org/versions/master/api_docs/python/control_flow_ops.html#Print"" rel=""nofollow"">tf.Print</a> debug statements to better understand the format of the reported gradients and variables from compute_gradients() but ran into an unexpected problem. The training routine and the debug routine (gvdebug) are as follows:</p>

<pre class=""lang-py prettyprint-override""><code>def gvdebug(g, v):
    #g = tf.Print(g,[g],'G: ')
    #v = tf.Print(v,[v],'V: ')
    g2 = tf.zeros_like(g, dtype=tf.float32)
    v2 = tf.zeros_like(v, dtype=tf.float32)
    g2 = g
    v2 = v
    return g2,v2

# Define training operation
def training(loss, global_step, learning_rate=0.1):
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
    grads_and_vars = optimizer.compute_gradients(loss)
    gv2 = [gvdebug(gv[0], gv[1]) for gv in grads_and_vars]
    train_op = optimizer.apply_gradients(gv2, global_step=global_step)
    return train_op
</code></pre>

<p>This code works fine (but doesn't print), but if I uncomment the two tf.Print lines in gvdebug() I get an error message from apply_gradients: 'TypeError: Variable must be a tf.Variable'. I thought tf.Print just passed through tensors -- what am I doing wrong?</p>
",0
34902782,Interpolated sampling of points in an image with TensorFlow,"<p>Given is a grayscale image <strong>I</strong> as 2D Tensor (Dimension W,H) and a Tensor of coordinates <strong>C</strong> (Dim. None,2). I want to interpret the rows of <strong>C</strong> as coordinates in <strong>I</strong>, sample <strong>I</strong> at those coordinates using some kind of interpolation (bilinear would probably be fine for my use case), and store the resulting values in a new Tensor <strong>P</strong> (of dimension None, i.e. 1-dimensional with as many entries as <strong>C</strong> has rows).</p>

<p>Is this possible (efficiently) with TensorFlow? All I can find are functions for resizing (equidistant resampling if you like) of images. But I can't find anything out-of-the-box to sample at a list of coordinates.</p>

<p>I.e. I would have expected to find something like a tf.interpolate() function:</p>

<pre><code>I = tf.placeholder(""float"", shape=[128, 128])
C = tf.placeholder(""float"", shape=[None, 2])
P = tf.interpolate(I, C, axis=[0, 1], method=""linear"")
</code></pre>

<p>Ideally I would be looking for a solution that would allow me to interpolate in an N dimensional tensor <strong>I</strong> along M dimensions using a <strong>C</strong> with shape (None, M) and produce an N-M+1 dimensional output, as indicated by the ""axis"" parameter in the code above.</p>

<p>(The ""image"" in my application isn't a picture btw., it's sampled data from a physical model (when used as placeholder) or an alternative learned model (when used as variable). Right now this physical model has 2 degrees of freedom, thus interpolating in an ""image"" is sufficient for now, but I might look into higher dimensional models in the future.)</p>

<p>In case something like that is not possible with existing TensorFlow features: Where should I start when I'd like to implement something like this tf.interpolate() operator? (documentation and/or simple example code)</p>
",0
34931121,Can cond support TF ops with side effects?,"<p>The (source code) documentation for <code>tf.cond</code> is unclear on whether the functions to be performed when the predicate is evaluated can have side effects or not. I've done some tests but I'm getting conflicting results. For example the code below does not work:</p>

<pre><code>import tensorflow as tf
from tensorflow.python.ops import control_flow_ops

pred = tf.placeholder(tf.bool, [])
count = tf.Variable(0)
adder = count.assign_add(1)
subtractor = count.assign_sub(2)

my_op = control_flow_ops.cond(pred, lambda: adder, lambda: subtractor)

sess = tf.InteractiveSession()
tf.initialize_all_variables().run()

my_op.eval(feed_dict={pred: True})
count.eval() # returns -1

my_op.eval(feed_dict={pred: False})
count.eval() # returns -2
</code></pre>

<p>I.e. no matter what value the predicate evaluates to, both functions are getting run, and so the net result is a subtraction of 1. On the other hand, this code snippet does work, where the only difference is that I add new ops to the graph every time <code>my_op</code> is called:</p>

<pre><code>pred = tf.placeholder(tf.bool, [])
count = tf.Variable(0)

my_op = control_flow_ops.cond(pred, lambda:count.assign_add(1), lambda:count.assign_sub(2))

sess = tf.InteractiveSession()
tf.initialize_all_variables().run()

my_op.eval(feed_dict={pred: False})
count.eval() # returns -2

my_op.eval(feed_dict={pred: True})
count.eval() # returns -1
</code></pre>

<p>Not sure why creating new ops every time works while the other case doesn't, but I'd obviously rather not be adding nodes as the graph will eventually become too big.</p>
",1
35689547,How to process single training file in parallel,"<p>I have a file <code>train.csv</code> that contains paths to images and their labels. ie:</p>

<pre><code>img1.jpg 3
img2.jpg 1
...
</code></pre>

<p>After going through the <a href=""https://www.tensorflow.org/versions/r0.7/how_tos/reading_data/index.html"" rel=""nofollow"">reading data tutorial</a> I came up with some code to go through each image, resize it and apply distortions:</p>

<pre><code>def apply_distortions(resized_image):
    # do a bunch of tf.image distortion...
    return float_image

def processing(filename):
    file_contents = tf.read_file(filename)
    image = tf.image.decode_jpeg(file_contents, channels=3)
    resized_image = tf.image.resize_images(image, 299, 299)
    distorted_image = apply_distortions(resized_image)
    return distorted_image

def parse_csv(filename_queue):
    line_reader = tf.TextLineReader()
    key, line = line_reader.read(filename_queue)
    filename, label = tf.decode_csv(line,     # line_batch or line (depending if you want to batch)
                               record_defaults=[tf.constant([],dtype=tf.string),
                                                tf.constant([],dtype=tf.int32)],
                               field_delim=' ')
    processed_image = processing(filename)
    return processed_image, label
</code></pre>

<p>The problem now is that I'm confused how to do these operations across the file in parallel. The documentation suggests either using <code>tf.train.batch_join</code> or <code>tf.train.batch</code> with num_threads=N.</p>

<p>I first tried following the example code using <code>tf.train.batch_join</code> but this seems to be intended for processing multiple files in parallel. In my case however I just have 1 file. </p>

<pre><code>filename_queue = tf.train.string_input_producer([""train.txt""], num_epochs=1, shuffle=True)    
example_list = [parse_csv(filename_queue) for _ in range(8)]
example_batch, label_batch = tf.train.batch_join(example_list, batch_size)
</code></pre>

<p>I also tried setting <code>tf.train.batch([example, label], batch_size, num_threads=8)</code> but its not clear to me if this is doing the right thing (although I can see more cpu cores in use)</p>

<pre><code>filename_queue = tf.train.string_input_producer([""train.txt""], num_epochs=1, shuffle=True)
example, label = parse_csv(filename_queue)
example_batch, label_batch = tf.train.batch([example, label], batch_size, num_threads=8)
</code></pre>

<p>Here is my code for executing the graph:</p>

<pre><code>sess.run(tf.initialize_all_variables())
coord = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess,coord)
try:
    while not coord.should_stop():
        X, Y = sess.run([example_batch, label_batch])
        # Now run a training step
except tf.errors.OutOfRangeError:
    print('Done training -- epoch limit reached')
finally:
    # When done, ask the threads to stop.
    coord.request_stop()
coord.join(threads)
sess.close()
</code></pre>

<p>Whats the best way to process this file in parallel?</p>
",1
36072672,How to import word2vec into TensorFlow Seq2Seq model?,"<p>I am playing with Tensorflow sequence to sequence translation model. I was wondering if I could import my own word2vec into this model? Rather than using its original 'dense representation' mentioned in the tutorial. </p>

<p>From my point of view, it looks TensorFlow is using One-Hot representation for seq2seq model. Firstly,for function <code>tf.nn.seq2seq.embedding_attention_seq2seq</code> the encoder's input is a tokenized symbol, e.g. 'a' would be '4' and 'dog' would be '15715' etc. and  requires num_encoder_symbols. So I think it makes me provide the position of the word and the total number of words, then the function could represent the word in One-Hot representation. I am still learning the source code, but it hard to understand.</p>

<p>Could anyone give me an idea on above problem?</p>
",0
36152571,Tensorflow reshape tensor gives None dimension,"<p>I have used the model described <a href=""https://www.tensorflow.org/versions/r0.7/tutorials/recurrent/index.html"" rel=""nofollow"">here</a> on the 0.6.0 branch. The code can be found <a href=""https://github.com/tensorflow/tensorflow/blob/0.6.0/tensorflow/models/rnn/ptb/ptb_word_lm.py"" rel=""nofollow"">here</a>. I have done some minor changes to the linked code.</p>

<p>In my code I create two models, one for training and one for validation, very similar as it is done in the Tensorflow Tutorial.</p>

<pre><code>with tf.variable_scope(""model"", reuse=None, initializer=initializer):
    m = PTBModel_User(is_training=True, config=config, name='Training model')
with tf.variable_scope(""model"", reuse=True, initializer=initializer):
    mtest = PTBModel_User(is_training=False, config=config_valid, name='Validation model')
</code></pre>

<p>The first model, the one for training, seems to be created just fine, but the second, used for validation, does not. The output gets a None dimension! The row I'm refering to is on row 134 in the <a href=""https://github.com/tensorflow/tensorflow/blob/0.6.0/tensorflow/models/rnn/ptb/ptb_word_lm.py"" rel=""nofollow"">linked</a> code:</p>

<p><code>output = tf.reshape(tf.concat(1, outputs), [-1, size])</code></p>

<p>I've added these lines right after the reshape of the output:</p>

<pre><code>output_shape = output.get_shape()
print(""Model num_steps:"", num_steps)
print(""Model batch_size:"", batch_size)
print(""Output dims"", output_shape[0], output_shape[1])
</code></pre>

<p>and that gives me this:</p>

<pre><code>Model num_steps: 400
Model batch_size: 1
Output dims Dimension(None) Dimension(650)
</code></pre>

<p>This problem only happens with the 'validation model', not with the 'training model'. For the 'training model' I get expected output:</p>

<pre><code>Model num_steps: 400
Model batch_size: 2
Output dims Dimension(800) Dimension(650)
</code></pre>

<p>(Note that with the 'validation model' I use a <code>batch_size=1</code> instead of <code>batch_size=2</code> that I use for the training model)</p>

<p>From what I understand, using <code>-1</code> as input to the <code>reshape</code> function, will figure the output shape out automagically! But then why do I get None? Nothing in my config fed to the model has a None value.</p>

<p>Thank you for all the help and tips!</p>
",0
36223157,Set weight and bias tensors of tensorflow conv2d operation,"<p>I have been given a trained neural network in torch and I need to rebuild it exactly in tensorflow. I believe I have correctly defined the network's architecture in tensorflow but I am having trouble transferring the weight and bias tensors. Using a third party package, I converted all the weight and bias tensors from the torch network to numpy arrays then wrote them to disk. I can load them back into my python program but I cannot figure out a way to assign them to the corresponding layers in my tensorflow network. </p>

<p>For instance, I have a convolution layer defined in tensorflow as</p>

<pre><code>kernel_1 = tf.Variable(tf.truncated_normal([11,11,3,64], stddev=0.1))
conv_kernel_1 = tf.nn.conv2d(input, kernel_1, [1,4,4,1], padding='SAME')
biases_1 = tf.Variable(tf.zeros[64])
bias_layer_1 = tf.nn_add(conv_kernel_1, biases_1)
</code></pre>

<p>According to the tensorflow documentation, the tf.nn.conv2d operation uses the shape defined in the kernel_1 variable to construct the weight tensor. However, I cannot figure out how to access that weight tensor to set it to the weight array I have loaded from file. </p>

<p><strong>Is it possible to explicitly set the weight tensor? And if so, how?</strong> </p>

<p>(The same question applies to bias tensor.)</p>
",1
36460144,TensorFlow: Labeling summary images with filenames,"<p>I'm training a CNN to learn an unusual task where the labels for each image depend on the other images in the minibatch. Further, the data undergo a number of transformations and through a bunch of different threads and queues before the net can actually axes them. </p>

<p>I'd like to be able to validate that the label and image mapping is correct. However, it doesn't seem like it's possible to get TensorFlow to surface the filename for each image summary as part of <a href=""https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#image_summary"" rel=""nofollow""><code>tf.image_summary(...)</code></a>. Does anyone know if it's possible to do this? Because the labeling regime is so unusual, you cannot gather immediately from the image if the label itself is correct. I need to be able to get the filename to recover the label properly. I can provide the filenames along with the images themselves and their labels in the queue without any problem.</p>

<p>Edit: To clarify, I'm using TensorBoard to display the image summaries.</p>
",0
36631868,Tensorflow: Noise contrastive estimation language model,"<p>I want to change the loss function in the <a href=""https://tensorflow.googlesource.com/tensorflow/+/master/tensorflow/models/rnn/ptb/ptb_word_lm.py"" rel=""nofollow"">ptb_word_lm.py</a> example to <code>tf.nn.nce_loss</code>. Looking at the <code>tf.nn.nce_loss</code> implementation:</p>

<pre><code>def nce_loss(weights, biases, inputs, labels, num_sampled, num_classes,
         num_true=1,
         sampled_values=None,
         remove_accidental_hits=False,
         partition_strategy=""mod"",
         name=""nce_loss""):
</code></pre>

<p>I think </p>

<ul>
<li>the 3rd parameter (inputs) is the logits of language model,  </li>
<li>4th parameter (labels) is the next word (self._targets) of language
model, </li>
<li>num_classes is the vocab_size</li>
</ul>

<p>But I do not know what are the first two parameters, weights and biases. How could I adapt <code>tf.nn.nce_loss</code> to language model? Thanks.</p>

########UPDATES

<p>@Aaron:</p>

<p>Thanks, I have tried the following:</p>

<pre><code>loss = tf.reduce_mean(
        tf.nn.nce_loss(softmax_w, softmax_b, logits, tf.reshape(self._targets, [-1,1]),
                                     64, vocab_size))
</code></pre>

<p>According to the document at <a href=""https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#nce_loss"" rel=""nofollow"">here</a>:</p>

<ul>
<li><p>weights: A Tensor of shape [num_classes, dim], or a list of Tensor
objects 
           whose concatenation along dimension 0 has shape [num_classes, dim]. The (possibly-partitioned) class embeddings.</p></li>
<li><p>biases: A Tensor of shape [num_classes]. The class biases.</p></li>
<li><p>inputs: A Tensor of shape [batch_size, dim]. The forward activations
of the input network.</p></li>
<li><p>labels: A Tensor of type int64 and shape [batch_size, num_true]. The
target classes.</p></li>
<li><p>num_sampled: An int. The number of classes to randomly sample per
batch.</p></li>
<li><p>num_classes: An int. The number of possible classes.</p></li>
</ul>

<p>So, </p>

<ul>
<li>weights is the softmax_w tensor, which has shape (hidden_size,
vocab_size)</li>
<li>biases is softmax_b, which has shape (vocab_size)</li>
<li>inputs is logits, which has shape (batch_size*num_steps, vocab_size)</li>
<li>labels is self._targets, which has shape (batch_size, num_steps),
thus, we need to reshape it, tf.reshape(self._targets, [-1,1])</li>
</ul>

<p>My PTBModel model looks like</p>

<pre><code>class PTBModel(object):
    def __init__(self, is_training, config):
        self.batch_size = batch_size = config.batch_size
        self.num_steps = num_steps = config.num_steps
        size = config.hidden_size
        vocab_size = config.vocab_size
        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps])
        self._targets = tf.placeholder(tf.int32, [batch_size, num_steps])

        lstm_cell = rnn_cell.BasicLSTMCell(size, forget_bias=0.0)
        if is_training and config.keep_prob &lt; 1:
            lstm_cell = rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=config.keep_prob)
        cell = rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers)
        self._initial_state = cell.zero_state(batch_size, tf.float32)
        with tf.device(""/cpu:0""):
            embedding = tf.get_variable(""embedding"", [vocab_size, size])
            inputs = tf.nn.embedding_lookup(embedding, self._input_data)
        if is_training and config.keep_prob &lt; 1:
            inputs = tf.nn.dropout(inputs, config.keep_prob)

        outputs = []
        states = []
        state = self._initial_state
        with tf.variable_scope(""RNN""):
            for time_step in range(num_steps):
                if time_step &gt; 0: tf.get_variable_scope().reuse_variables()
                (cell_output, state) = cell(inputs[:, time_step, :], state)
                outputs.append(cell_output)
                states.append(state)
        output = tf.reshape(tf.concat(1, outputs), [-1, size])
        softmax_w = tf.get_variable(""softmax_w"", [size, vocab_size])
        softmax_b = tf.get_variable(""softmax_b"", [vocab_size])
        logits = tf.matmul(output, softmax_w) + softmax_b

        '''
        #minimize the average negative log probability using sequence_loss_by_example
        loss = seq2seq.sequence_loss_by_example([logits],
                                                [tf.reshape(self._targets, [-1])],
                                                [tf.ones([batch_size * num_steps])],
                                                vocab_size)

        loss = tf.reduce_mean(
            tf.nn.nce_loss(nce_weights, nce_biases, embed, train_labels,
                                         num_sampled, vocabulary_size))
        weights: A Tensor of shape [num_classes, dim], or a list of Tensor objects 
            whose concatenation along dimension 0 has shape [num_classes, dim]. The (possibly-partitioned) class embeddings.
        biases: A Tensor of shape [num_classes]. The class biases.
        inputs: A Tensor of shape [batch_size, dim]. The forward activations of the input network.
        labels: A Tensor of type int64 and shape [batch_size, num_true]. The target classes.
        num_sampled: An int. The number of classes to randomly sample per batch.
        num_classes: An int. The number of possible classes.

        '''
        loss = tf.reduce_mean(
            tf.nn.nce_loss(softmax_w, softmax_b, logits, tf.reshape(self._targets, [-1,1]),
                                         64, vocab_size))


        self._cost = cost = tf.reduce_sum(loss) / batch_size
        self._final_state = states[-1]
        if not is_training:
            return
        self._lr = tf.Variable(0.0, trainable=False)
        tvars = tf.trainable_variables()
        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),
                                          config.max_grad_norm)
        optimizer = tf.train.GradientDescentOptimizer(self.lr)
        self._train_op = optimizer.apply_gradients(zip(grads, tvars))
</code></pre>

<p>However, I got an error</p>

<pre><code>Epoch: 1 Learning rate: 1.000
W tensorflow/core/common_runtime/executor.cc:1102] 0x528c980 Compute status: Invalid argument: Index 9971 at offset 0 in Tindices is out of range
     [[Node: model/nce_loss/embedding_lookup = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, validate_indices=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](model/softmax_w/read, model/nce_loss/concat)]]
W tensorflow/core/common_runtime/executor.cc:1102] 0x528c980 Compute status: Invalid argument: Index 9971 at offset 0 in Tindices is out of range
     [[Node: model/nce_loss/embedding_lookup = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, validate_indices=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](model/softmax_w/read, model/nce_loss/concat)]]
     [[Node: _send_model/RNN/concat_19_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1438650956868917036, tensor_name=""model/RNN/concat_19:0"", _device=""/job:localhost/replica:0/task:0/cpu:0""](model/RNN/concat_19)]]
Traceback (most recent call last):
  File ""/home/user/works/workspace/python/ptb_word_lm/ptb_word_lm.py"", line 235, in &lt;module&gt;
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""/home/user/works/workspace/python/ptb_word_lm/ptb_word_lm.py"", line 225, in main
    verbose=True)
  File ""/home/user/works/workspace/python/ptb_word_lm/ptb_word_lm.py"", line 189, in run_epoch
    m.initial_state: state})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 315, in run
    return self._run(None, fetches, feed_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 511, in _run
    feed_dict_string)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 564, in _do_run
    target_list)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 586, in _do_call
    e.code)
tensorflow.python.framework.errors.InvalidArgumentError: Index 9971 at offset 0 in Tindices is out of range
     [[Node: model/nce_loss/embedding_lookup = Gather[Tindices=DT_INT64, Tparams=DT_FLOAT, validate_indices=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](model/softmax_w/read, model/nce_loss/concat)]]
Caused by op u'model/nce_loss/embedding_lookup', defined at:
  File ""/home/user/works/workspace/python/ptb_word_lm/ptb_word_lm.py"", line 235, in &lt;module&gt;
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/default/_app.py"", line 30, in run
    sys.exit(main(sys.argv))
  File ""/home/user/works/workspace/python/ptb_word_lm/ptb_word_lm.py"", line 214, in main
    m = PTBModel(is_training=True, config=config)
  File ""/home/user/works/workspace/python/ptb_word_lm/ptb_word_lm.py"", line 122, in __init__
    64, vocab_size))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn.py"", line 798, in nce_loss
    name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn.py"", line 660, in _compute_sampled_logits
    weights, all_ids, partition_strategy=partition_strategy)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/embedding_ops.py"", line 86, in embedding_lookup
    validate_indices=validate_indices)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 447, in gather
    validate_indices=validate_indices, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py"", line 655, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2040, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1087, in __init__
    self._traceback = _extract_stack()
</code></pre>

<p>Did I miss anything here? Thanks again.</p>
",1
37071788,Tensorflow: How to modify the value in tensor,"<p>Since I need to write some preprocesses for the data before using Tensorflow to train models, some modifications on the <code>tensor</code> is needed. However, I have no idea about how to modify the values in <code>tensor</code> like the way using <code>numpy</code>. </p>

<p>The best way of doing so is that it is able to modify <code>tensor</code> directly. Yet, it seems not possible in the current version of Tensorflow. An alternative way is changing <code>tensor</code> to <code>ndarray</code> for the process, and then use <code>tf.convert_to_tensor</code> to change back. </p>

<p>The key is how to change <code>tensor</code> to <code>ndarray</code>.<br>
1) <code>tf.contrib.util.make_ndarray(tensor)</code>:
<a href=""https://www.tensorflow.org/versions/r0.8/api_docs/python/contrib.util.html#make_ndarray"" rel=""noreferrer"">https://www.tensorflow.org/versions/r0.8/api_docs/python/contrib.util.html#make_ndarray</a><br>
It seems the easiest way as per the document, yet I cannot find this function in the current version of the Tensorflow. Second, the input of it is <code>TensorProto</code> rather than <code>tensor</code>.<br>
2) Use <code>a.eval()</code> to copy <code>a</code> to another <code>ndarray</code><br>
Yet, it works only at using <code>tf.InteractiveSession()</code> in notebook. </p>

<p>A simple case with codes shows below. The purpose of this code is making that the <code>tfc</code> has the same output as <code>npc</code> after the process.   </p>

<p><strong>HINT</strong><br>
You should treat that <code>tfc</code> and <code>npc</code> are independent to each other. This meets the situation that at first the retrieved training data is in <code>tensor</code> format with <a href=""https://www.tensorflow.org/versions/r0.8/api_docs/python/io_ops.html#placeholder"" rel=""noreferrer""><code>tf.placeholder()</code></a>. </p>

<hr>

<p><strong>Source code</strong></p>

<pre><code>import numpy as np
import tensorflow as tf
tf.InteractiveSession()

tfc = tf.constant([[1.,2.],[3.,4.]])
npc = np.array([[1.,2.],[3.,4.]])
row = np.array([[.1,.2]])
print('tfc:\n', tfc.eval())
print('npc:\n', npc)
for i in range(2):
    for j in range(2):
        npc[i,j] += row[0,j]

print('modified tfc:\n', tfc.eval())
print('modified npc:\n', npc)
</code></pre>

<hr>

<p><strong>Output:</strong></p>

<p>tfc:<br>
 [[ 1.  2.]<br>
 [ 3.  4.]]<br>
npc:<br>
 [[ 1.  2.]<br>
 [ 3.  4.]]<br>
modified tfc:<br>
 [[ 1.  2.]<br>
 [ 3.  4.]]<br>
modified npc:<br>
 [[ 1.1  2.2]<br>
 [ 3.1  4.2]]  </p>
",0
37146272,How do I get TensorFlow's 'import_graph_def' to return Tensors,"<p>If I attempt to import a saved <a href=""https://www.tensorflow.org"" rel=""noreferrer"">TensorFlow</a> graph definition with</p>

<pre><code>import tensorflow as tf
from tensorflow.python.platform import gfile

with gfile.FastGFile(FLAGS.model_save_dir.format(log_id) + '/graph.pb', 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
x, y, y_ = tf.import_graph_def(graph_def, 
                               return_elements=['data/inputs',
                                                'output/network_activation',
                                                'data/correct_outputs'],
                               name='')
</code></pre>

<p>the returned values are not <code>Tensor</code>s as expected, but something else: instead, for example, of getting <code>x</code> as </p>

<pre><code>Tensor(""data/inputs:0"", shape=(?, 784), dtype=float32)
</code></pre>

<p>I get</p>

<pre><code>name: ""data/inputs_1""
op: ""Placeholder""
attr {
  key: ""dtype""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""shape""
  value {
    shape {
    }
  }
}
</code></pre>

<p>That is, instead of getting the expected tensor <code>x</code> I get, <code>x.op</code>. This confuses me because the <a href=""https://www.tensorflow.org/versions/r0.8/api_docs/python/framework.html#import_graph_def"" rel=""noreferrer"">documentation</a> seems to say I should get a <code>Tensor</code> (though there are a bunch of <em>or</em>s there that make it hard to understand).</p>

<p>How do I get <code>tf.import_graph_def</code> to return specific <code>Tensor</code>s that I can then use (e.g. in feeding the loaded model, or running analyses)?</p>
",1
37376861,what does the tf.nn.lrn() method do?,"<p>Here is the code-snipped from the cifar10-tutorial. It's from the cifar10.py.</p>

<pre><code># conv1
with tf.variable_scope('conv1') as scope:
kernel = _variable_with_weight_decay('weights', shape=[5, 5, 3, 64],
                                     stddev=1e-4, wd=0.0)
conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')
biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))
bias = tf.nn.bias_add(conv, biases)
conv1 = tf.nn.relu(bias, name=scope.name)
_activation_summary(conv1)

# pool1
pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],
                     padding='SAME', name='pool1')
# norm1
norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,
                name='norm1')
</code></pre>

<p>What does the tf.nn.lrn-Method do? I can't find a definition in the API Documentation on <a href=""https://www.tensorflow.org/versions/r0.8/api_docs/python/index.html"" rel=""noreferrer"">https://www.tensorflow.org/versions/r0.8/api_docs/python/index.html</a></p>
",1
37441140,How to use tf.while_loop() in tensorflow,"<p>This is a generic question. I found that in the tensorflow, after we build the graph, fetch data into the graph, the output from graph is a tensor. but in many cases, we need to do some computation based on this output (which is a <code>tensor</code>), which is not allowed in tensorflow. </p>

<p>for example, I'm trying to implement a RNN, which loops times based on data self property. That is, I need use a <code>tensor</code> to judge whether I should stop (I am not using dynamic_rnn since in my design, the rnn is highly customized). I find <code>tf.while_loop(cond,body.....)</code> might be a candidate for my implementation. But the official tutorial is too simple. I don't know how to add more functionalities into the 'body'. Can anyone give me few more complex example? </p>

<p>Also, in such case that if the future computation is based on the tensor output (ex: the RNN stop based on the output criterion), which is very common case. Is there an elegant way or better way instead of dynamic graph?</p>
",0
37518834,Load two copies of a model in TensorFlow,"<p>I have trained an LSTM model in TensowFlow and have saved it using the tf.train.Saver class as per the <a href=""https://www.tensorflow.org/versions/r0.8/how_tos/variables/index.html#saving-and-restoring"" rel=""nofollow noreferrer"">instructions on saving and loading variables</a>. I now want to load two copies of this model and create a larger RNN which combines them, for further training. Unfortunately, as far as I can see there is no way to load variables from one scope into another. Is there any way to do this?</p>

<p>As a simple example, suppose I had a simple model:</p>

<pre class=""lang-py prettyprint-override""><code>with tf.variable_scope(""model"", reuse=None, initializer=initializer):
      W = tf.get_variable(""W"", [input_size, output_size])
      b = tf.get_variable(""b"", [input_size])

      inputs = tf.placeholder(tf.float32, [batch_size, input_size]      
      outputs = tf.matmul(inputs, W) + b
</code></pre>

<p>And I trained this model on some data and saved the variables using:</p>

<pre class=""lang-py prettyprint-override""><code>saver = tf.train.Saver()
saved.save(session, ""model"")
</code></pre>

<p>Then later I wanted to create a combined model:</p>

<pre class=""lang-py prettyprint-override""><code>with tf.variable_scope(""combinedl"", reuse=None, initializer=initializer):
    with tf.variable_scope(""model0"" , reuse=None, initializer=initializer):
         W0 = tf.get_variable(""W"", [input_size, output_size])
         b0 = tf.get_variable(""b"", [input_size])

         inputs0 = tf.placeholder(tf.float32, [batch_size, input_size]      
         outputs0 = tf.matmul(inputs, W0) + b0

    with tf.variable_scope(""model1"" , reuse=None, initializer=initializer):
         W1 = tf.get_variable(""W"", [input_size, output_size])
         b1 = tf.get_variable(""b"", [input_size])

         inputs1 = tf.placeholder(tf.float32, [batch_size, input_size]      
         outputs1 = tf.matmul(inputs, W1) + b1

    output = outputs1 + outputs2
</code></pre>

<p>Would it be possible to load the trained values for W and b in the original model into the variables W0, b0 and W1, b1? Would it be possible to train these new variables independently?</p>
",0
37578876,How to feed Cifar10 trained model with my own image and get label as output?,"<p>I am trying to use the trained model based on the <a href=""https://www.tensorflow.org/versions/r0.8/tutorials/deep_cnn/index.html"" rel=""nofollow"">Cifar10 tutorial</a> and would like to feed
it with an external image 32x32 (jpg or png).<br>
My goal is to be able to get <strong>the label as an output</strong>.
In other words, I want to feed the Network with a single jpeg image of size 32 x 32, 3 channels with no label as an input and have the inference process <em>give me</em> the <code>tf.argmax(logits, 1)</code>.<br>
Basically I would like to be able to use the trained cifar10 model on an external image and see what class it will spit out.</p>

<p>I have been trying to do that based on the Cifar10 Tutorial and unfortunately always have issues. especially with the Session concept and the batch concept.</p>

<p>Any help doing that with Cifar10 would be greatly appreciated. </p>

<p>Here is the implemented code so far with compilation issues :</p>

<pre class=""lang-py prettyprint-override""><code>#!/usr/bin/env python

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from datetime import datetime
import math
import time

import tensorflow.python.platform
from tensorflow.python.platform import gfile
import numpy as np
import tensorflow as tf

import cifar10
import cifar10_input
import os
import faultnet_flags
from PIL import Image

FLAGS = tf.app.flags.FLAGS

def evaluate():

  filename_queue = tf.train.string_input_producer(['/home/tensor/.../inputImage.jpg'])

  reader = tf.WholeFileReader()
  key, value = reader.read(filename_queue)

  input_img = tf.image.decode_jpeg(value)

  init_op = tf.initialize_all_variables()

# Problem in here with Graph / session
  with tf.Session() as sess:
    sess.run(init_op)

    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)

    for i in range(1): 
      image = input_img.eval()

    print(image.shape)
    Image.fromarray(np.asarray(image)).show()

# Problem in here is that I have only one image as input and have no label and would like to have
# it compatible with the Cifar10 network
    reshaped_image = tf.cast(image, tf.float32)
    height = FLAGS.resized_image_size
    width = FLAGS.resized_image_size
    resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image, width, height)
    float_image = tf.image.per_image_whitening(resized_image)  # reshaped_image
    num_preprocess_threads = 1
    images = tf.train.batch(
      [float_image],
      batch_size=128,
      num_threads=num_preprocess_threads,
      capacity=128)
    coord.request_stop()
    coord.join(threads)

    logits = faultnet.inference(images)

    # Calculate predictions.
    #top_k_predict_op = tf.argmax(logits, 1)

    # print('Current image is: ')
    # print(top_k_predict_op[0])

    # this does not work since there is a problem with the session
    # and the Graph conflicting
    my_classification = sess.run(tf.argmax(logits, 1))

    print ('Predicted ', my_classification[0], "" for your input image."")


def main(argv=None):
  evaluate()

if __name__ == '__main__':
  tf.app.run() '''
</code></pre>
",0
37746583,"tensorflow tutorial of convolution, scale of logit","<p>I am trying to edit my own model by adding some code to cifar10.py and here is the question.</p>

<p>In cifar10.py, the [tutorial][1] says:</p>

<blockquote>
  <p><strong>EXERCISE:</strong> The output of inference are un-normalized logits. Try editing the network architecture to return normalized predictions using tf.nn.softmax().</p>
</blockquote>

<p>So I directly input the output from ""local4"" to <code>tf.nn.softmax()</code>. This gives me the <em>scaled</em> logits which means the sum of all logits is 1.</p>

<p>But in the loss function, the cifar10.py code uses:</p>

<pre><code>tf.nn.sparse_softmax_cross_entropy_with_logits()
</code></pre>

<p>and description of this function says </p>

<blockquote>
  <p><strong>WARNING:</strong> This op expects unscaled logits, since it performs a softmax on logits internally for efficiency. Do not call this op with the output of softmax, as it will produce incorrect results.</p>
</blockquote>

<p>Also, according to the description, logits as input to above funtion must have the shape [batch_size, num_classes] and it means logits should be unscaled softmax, like sample code calculate unnormalized softmaxlogit as follow.</p>

<pre><code>  # softmax, i.e. softmax(WX + b)
  with tf.variable_scope('softmax_linear') as scope:
    weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES],
                                          stddev=1/192.0, wd=0.0)
    biases = _variable_on_cpu('biases', [NUM_CLASSES],
                              tf.constant_initializer(0.0))
    softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)
    _activation_summary(softmax_linear)
</code></pre>

<p>Does this mean I don't have to use <code>tf.nn.softmax</code> in the code?</p>
",0
38033079,Tensorflow understanding tf.train.shuffle_batch,"<p>I have a single file of training data, about 100K rows, and I'm running a straightforward <code>tf.train.GradientDescentOptimizer</code> on each training step.  The setup is essentially taken directly from Tensorflow's MNIST example.  Code reproduced below:</p>

<pre><code>x = tf.placeholder(tf.float32, [None, 21])
W = tf.Variable(tf.zeros([21, 2]))
b = tf.Variable(tf.zeros([2]))
y = tf.nn.softmax(tf.matmul(x, W) + b)

y_ = tf.placeholder(tf.float32, [None, 2])
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
</code></pre>

<p>Given that I'm reading training data from a file, I'm using <code>tf.train.string_input_producer</code> and <code>tf.decode_csv</code> to read rows from the csv, and then <code>tf.train.shuffle_batch</code> to create batches that I then train on.</p>

<p>I'm confused as to what my parameters should be for <code>tf.train.shuffle_batch</code>.  I read Tensorflow's documentation, and yet I'm still not sure what the ""optimal"" batch_size, capacity, and min_after_dequeue values are.  Can anyone help shed some light on how I go about choosing proper values for these parameters, or link me to a resource where I can learn more?  Thanks-- </p>

<p>Here's the API link: <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/io_ops.html#shuffle_batch"" rel=""nofollow"">https://www.tensorflow.org/versions/r0.9/api_docs/python/io_ops.html#shuffle_batch</a></p>
",1
38061080,How to transform vector into unit vector in Tensorflow,"<p>This is a pretty simple question that I just can't seem to figure out.  I am working with an an output tensor of shape [100, 250].  I want to be able to access the 250 Dimensional array at any spot along the hundred and modify them separately.  The tensorflow mathematical tools that I've found either do element-wise modification or scalar modification on the entire tensor.  However, I am trying to do scalar modification on subsets of the tensor.</p>

<p>EDIT:</p>

<p>Here is the numpy code that I would like to recreate with tensorflow methods:</p>

<pre><code>update = sess.run(y, feed_dict={x: batch_xs})
for i in range(len(update)):
        update[i] = update[i]/np.sqrt(np.sum(np.square(update[i])))
        update[i] = update[i] * magnitude
</code></pre>

<p>This for loop follows this formula in 250-D instead of 3-D
<a href=""https://i.stack.imgur.com/Xru79.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Xru79.png"" alt=""Unit vector formula, which is the first line of the for-loop""></a>
. I then multiply each unit vector by magnitude to re-shape it to my desired length.</p>

<p>So update here is the numpy [100, 250] dimensional output. I want to transform each 250 dimensional vector into its unit vector.  That way I can change its length to a magnitude of my choosing.  Using this numpy code, if I run my train_step and pass update into one of my placeholders</p>

<pre><code>sess.run(train_step, feed_dict={x: batch_xs, prediction: output}) 
</code></pre>

<p>it returns the error:</p>

<pre><code>No gradients provided for any variable
</code></pre>

<p>This is because I've done the math in numpy and ported it back into tensorflow.  <a href=""https://stackoverflow.com/questions/35325480/tensorflow-performing-this-loss-computation"">Here</a> is a related stackoverflow question that did not get answered.</p>

<p>the <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#l2_normalize"" rel=""nofollow noreferrer"">tf.nn.l2_normalize</a> is very close to what I am looking for, but it divides by the square root of the <em>maximum</em> sum of squares.  Whereas I am trying to divide each vector by its own sum of squares.</p>

<p>Thanks!</p>
",0
38083325,How to save a SaverDef as protobuff in Tensorflow,"<p>Saving a <code>GraphDef</code>  can that's returned from calling <code>as_graph_def()</code> to <code>graph.pb</code> is done with <code>tf.train.write_graph</code>. However, I can't seem to find any documentation on how to save off a <code>SaverDef</code> to <code>saver.pb</code> Any help would be greatly appreciated!</p>
",1
38114534,Basic 1d convolution in tensorflow,"<p>OK, I'd like to do a 1-dimensional convolution of time series data in Tensorflow. This is apparently supported using <code>tf.nn.conv2d</code>, according to <a href=""https://github.com/tensorflow/tensorflow/issues/2165"" rel=""noreferrer"">these</a> <a href=""https://github.com/tensorflow/tensorflow/issues/1136"" rel=""noreferrer"">tickets</a>, and <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#convolution"" rel=""noreferrer"">the manual</a>. the only requirement is to set <code>strides=[1,1,1,1]</code>. Sounds simple!</p>

<p>However, I cannot work out how to do this in even a very minimal test case. What am I doing wrong?</p>

<p>Let's set this up.</p>

<pre><code>import tensorflow as tf
import numpy as np
print(tf.__version__)
&gt;&gt;&gt; 0.9.0
</code></pre>

<p>OK, now generate a basic convolution test on two small arrays. I will make it easy by using a batch size of 1, and since time series are 1-dimensional, I will have an ""image height"" of 1. And since it's a univariate time series, clearly the number of ""channels"" is also 1, so this will be simple, right?</p>

<pre><code>g = tf.Graph()
with g.as_default():
    # data shape is ""[batch, in_height, in_width, in_channels]"",
    x = tf.Variable(np.array([0.0, 0.0, 0.0, 0.0, 1.0]).reshape(1,1,-1,1), name=""x"")
    # filter shape is ""[filter_height, filter_width, in_channels, out_channels]""
    phi = tf.Variable(np.array([0.0, 0.5, 1.0]).reshape(1,-1,1,1), name=""phi"")
    conv = tf.nn.conv2d(
        phi,
        x,
        strides=[1, 1, 1, 1],
        padding=""SAME"",
        name=""conv"")
</code></pre>

<p>BOOM. Error.</p>

<pre><code>ValueError: Dimensions 1 and 5 are not compatible
</code></pre>

<p>OK, For a start, I don't understand how this should happen with <em>any</em> dimension, since I've specified that I'm padding the arguments in the convolution OP. </p>

<p>but fine, maybe there are limits to that. I must have got the documentation confused and set up this convolution on the wrong axes of the tensor. I'll try all possible permutations:</p>

<pre><code>for i in range(4):
    for j in range(4):
        shape1 = [1,1,1,1]
        shape1[i] = -1
        shape2 = [1,1,1,1]
        shape2[j] = -1
        x_array = np.array([0.0, 0.0, 0.0, 0.0, 1.0]).reshape(*shape1)
        phi_array = np.array([0.0, 0.5, 1.0]).reshape(*shape2)
        try:
            g = tf.Graph()
            with g.as_default():
                x = tf.Variable(x_array, name=""x"")
                phi = tf.Variable(phi_array, name=""phi"")
                conv = tf.nn.conv2d(
                    x,
                    phi,
                    strides=[1, 1, 1, 1],
                    padding=""SAME"",
                    name=""conv"")
                init_op = tf.initialize_all_variables()
            sess = tf.Session(graph=g)
            sess.run(init_op)
            print(""SUCCEEDED!"", x_array.shape, phi_array.shape, conv.eval(session=sess))
            sess.close()
        except Exception as e:
            print(""FAILED!"", x_array.shape, phi_array.shape, type(e), e.args or e._message)
</code></pre>

<p>Result:</p>

<pre><code>FAILED! (5, 1, 1, 1) (3, 1, 1, 1) &lt;class 'ValueError'&gt; ('Filter must not be larger than the input: Filter: (3, 1) Input: (1, 1)',)
FAILED! (5, 1, 1, 1) (1, 3, 1, 1) &lt;class 'ValueError'&gt; ('Filter must not be larger than the input: Filter: (1, 3) Input: (1, 1)',)
FAILED! (5, 1, 1, 1) (1, 1, 3, 1) &lt;class 'ValueError'&gt; ('Dimensions 1 and 3 are not compatible',)
FAILED! (5, 1, 1, 1) (1, 1, 1, 3) &lt;class 'tensorflow.python.framework.errors.InvalidArgumentError'&gt; No OpKernel was registered to support Op 'Conv2D' with these attrs
     [[Node: conv = Conv2D[T=DT_DOUBLE, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x/read, phi/read)]]
FAILED! (1, 5, 1, 1) (3, 1, 1, 1) &lt;class 'tensorflow.python.framework.errors.InvalidArgumentError'&gt; No OpKernel was registered to support Op 'Conv2D' with these attrs
     [[Node: conv = Conv2D[T=DT_DOUBLE, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x/read, phi/read)]]
FAILED! (1, 5, 1, 1) (1, 3, 1, 1) &lt;class 'ValueError'&gt; ('Filter must not be larger than the input: Filter: (1, 3) Input: (5, 1)',)
FAILED! (1, 5, 1, 1) (1, 1, 3, 1) &lt;class 'ValueError'&gt; ('Dimensions 1 and 3 are not compatible',)
FAILED! (1, 5, 1, 1) (1, 1, 1, 3) &lt;class 'tensorflow.python.framework.errors.InvalidArgumentError'&gt; No OpKernel was registered to support Op 'Conv2D' with these attrs
     [[Node: conv = Conv2D[T=DT_DOUBLE, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x/read, phi/read)]]
FAILED! (1, 1, 5, 1) (3, 1, 1, 1) &lt;class 'ValueError'&gt; ('Filter must not be larger than the input: Filter: (3, 1) Input: (1, 5)',)
FAILED! (1, 1, 5, 1) (1, 3, 1, 1) &lt;class 'tensorflow.python.framework.errors.InvalidArgumentError'&gt; No OpKernel was registered to support Op 'Conv2D' with these attrs
     [[Node: conv = Conv2D[T=DT_DOUBLE, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x/read, phi/read)]]
FAILED! (1, 1, 5, 1) (1, 1, 3, 1) &lt;class 'ValueError'&gt; ('Dimensions 1 and 3 are not compatible',)
FAILED! (1, 1, 5, 1) (1, 1, 1, 3) &lt;class 'tensorflow.python.framework.errors.InvalidArgumentError'&gt; No OpKernel was registered to support Op 'Conv2D' with these attrs
     [[Node: conv = Conv2D[T=DT_DOUBLE, data_format=""NHWC"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](x/read, phi/read)]]
FAILED! (1, 1, 1, 5) (3, 1, 1, 1) &lt;class 'ValueError'&gt; ('Dimensions 5 and 1 are not compatible',)
FAILED! (1, 1, 1, 5) (1, 3, 1, 1) &lt;class 'ValueError'&gt; ('Dimensions 5 and 1 are not compatible',)
FAILED! (1, 1, 1, 5) (1, 1, 3, 1) &lt;class 'ValueError'&gt; ('Dimensions 5 and 3 are not compatible',)
FAILED! (1, 1, 1, 5) (1, 1, 1, 3) &lt;class 'ValueError'&gt; ('Dimensions 5 and 1 are not compatible',)
</code></pre>

<p>Hmm. OK, it looks like there are two problems now. Firstly, the <code>ValueError</code> is about applying the filter along the wrong axis, I guess, although there are two forms.</p>

<p>But then the axes along which I can apply the filter are confusing too - notice that it actually constructs the graph with input shape (5, 1, 1, 1)  and filter shape (1, 1, 1, 3). AFAICT from the documentation, this should be a filter that looks at on example from the batch, one ""pixel"" and one ""channel"" and outputs 3 ""channels"". Why does that one work, then, when others do not?</p>

<p>Anyway, sometimes it does not fail while constructing the graph.
Sometime it constructs the graph; then we get the <code>tensorflow.python.framework.errors.InvalidArgumentError</code>. From some <a href=""https://github.com/tensorflow/tensorflow/issues/524"" rel=""noreferrer"">confusing github tickets</a> I gather this is probably due to <del>the fact that I'm running on CPU instead of GPU, or vice versa</del> the fact that the convolution Op is only defined for 32 bit floats, not 64 bit floats. If anyone could throw some light on <em>which</em> axes I should be aligning <em>what</em> on, in order to convolve a time series with a kernel, I'd be very grateful.</p>
",1
38136932,data format in tensorflow for word2vec,"<p>I am using word2vec module of TF. I would like to read a data file which consists of multiple lines. Each line is made of multiple words. Each word in one line could be either source or target word.</p>

<p>I followed the <a href=""https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html#optimizing-the-implementation"" rel=""nofollow"">tutorial</a>, but I didn't find the so-called example to read customized data in </p>

<blockquote>
  <p>For the case of Skip-Gram modeling, we've actually already done this for you as an example in tensorflow/models/embedding/word2vec.py.</p>
</blockquote>

<p>Should I change the code in word2vec_kernel.cc or use tf.TextLineReader. If so, how? Thanks!</p>
",0
38265061,"Tensorflow, missing checkpoint files. Does saver only allow for keeping 5 check points?","<p>I am working with tensorflow and have been training some models and saving them after each epoch using the  <code>tf.saver()</code> method. I am able to save and load models just fine and I am doing this in the usual way.</p>

<pre><code>with tf.Graph().as_default(), tf.Session() as session:
    initialiser = tf.random_normal_initializer(config.mean, config.std)

    with tf.variable_scope(""model"",reuse=None, initializer=initialiser):
        m = a2p(session, config, training=True)

    saver = tf.train.Saver()   
    ckpt = tf.train.get_checkpoint_state(model_dir)
    if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path)
        saver.restore(session, ckpt.model_checkpoint_path)
    ...
    for i in range(epochs):
       runepoch()
       save_path = saver.save(session, '%s.ckpt'%i)
</code></pre>

<p>My code is set up to save a model for each epoch which should be labelled accordingly. However, I have noticed that after fifteen epochs of training I only have check point files for the last five epochs (10, 11, 12, 13,14). The documentation doesn't say anything about this so I am at a loss as to why it is happening.</p>

<p>Does the saver only allow for keeping five checkpoints or have I done something wrong?</p>

<p>Is there a way to make sure that all of the checkpoints are kept?</p>
",1
38356622,Distributing graphs across several machines in Distributed Tensorflow,"<p>I am currently working on a project using Distributed Tensorflow. My goal is to run several independent graphs across several different machines.</p>

<p>As an example, I want to do something like this (assume that the server is open on each machine)</p>

<pre><code>import tensorflow as tf
a = tf.constant(3)
b = tf.constant(2)
x = tf.mul(a,b)             # To be run on ""grpc://www.example0.com:2222""
y = tf.mul(a,b)             # To be run on ""grpc://www.example1.com:2222""
z = tf.mul(a,b)             # To be run on ""grpc://www.example2.com:2222""

with tf.Session() as sess:
    sess.run([x,y,z])       # Ops x,y,z are run on different machines in parallel
</code></pre>

<p>My current attempt at this is shown in the following code. However, this code runs the sessions in serial, but I want them to be executed in a parallel distributed manner.</p>

<pre><code>import tensorflow as tf
a = tf.constant(3)
b = tf.constant(2)
x = tf.mul(a,b)             # To be run on ""grpc://www.example0.com:2222""
y = tf.mul(a,b)             # To be run on ""grpc://www.example1.com:2222""
z = tf.mul(a,b)             # To be run on ""grpc://www.example2.com:2222""

with tf.Session(""grpc://www.example0.com:2222"") as sess:
    sess.run(x)
with tf.Session(""grpc://www.example1.com:2222"") as sess:
    sess.run(y)
with tf.Session(""grpc://www.example2.com:2222"") as sess:
    sess.run(z)
</code></pre>

<p>While reading the documentation about Distributed Tensorflow, I found that <code>tf.device</code> allows me to set which CPU or GPU to run Tensorflow Ops on. Is there something similar that allows me to set the <code>session target</code> to specify which machine will run which op? Or is there another way of distributing Tensorflow Ops?</p>
",1
38461214,how to increment matrix element in tensorflow using tf.scatter_add?,"<p>tf.scatter_add works nicely for 1d (shape <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/state_ops.html#scatter_add"" rel=""nofollow"">1</a>) tensors:</p>

<pre><code>&gt; S = tf.Variable(tf.constant([1,2,3,4]))
&gt; sess.run(tf.initialize_all_variables())
&gt; sess.run(tf.scatter_add(S, [0], [10]))

array([11,  2,  3,  4], dtype=int32)

&gt; sess.run(tf.scatter_add(S, [0, 1], [10, 100]))

array([ 21, 102,   3,   4], dtype=int32)
</code></pre>

<p>But how can I increment, say [0,0] element of </p>

<pre><code>M = tf.Variable(tf.constant([[1,2], [3,4]]))
</code></pre>

<p>to make it [[2, 2], [3, 4]]
using tf.scatter_add?</p>

<p>the <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/state_ops.html#scatter_add"" rel=""nofollow"">official documentation</a> is kind'a cryptic. And I tried different arg values, say</p>

<pre><code>&gt; sess.run(tf.scatter_add(M, [[0, 0]], [1]))
*** ValueError: Shapes (1,) and (1, 2, 2) are not compatible
</code></pre>

<p>and haven't succeeded.</p>

<p>Btw, in my case, M is quite large and is resized dynamically.
So adding zero-but-one equal to 1 element matrix to M is not the case.</p>
",1
38543850,How to Display Custom Images in Tensorboard (e.g. Matplotlib Plots)?,"<p>The <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md#image-dashboard"" rel=""noreferrer"">Image Dashboard</a> section of the Tensorboard ReadMe says:</p>

<blockquote>
  <p>Since the image dashboard supports arbitrary pngs, you can use this to embed custom visualizations (e.g. matplotlib scatterplots) into TensorBoard.</p>
</blockquote>

<p>I see how a pyplot image could be written to file, read back in as a tensor, and then used with tf.image_summary() to write it to TensorBoard, but this statement from the readme suggests there is a more direct way. Is there? If so, is there any further documentation and/or examples of how to do this efficiently?  </p>
",1
38641887,How to save a trained tensorflow model for later use for application?,"<p>I am a bit of a beginner with tensorflow so please excuse if this is a stupid question and the answer is obvious. </p>

<p>I have created a Tensorflow graph where starting with placeholders for X and y I have optimized some tensors which represent my model. Part of the graph is something where a vector of predictions can be calculated, e.g. for linear regression something like </p>

<pre class=""lang-py prettyprint-override""><code>y_model = tf.add(tf.mul(X,w),d)
y_vals = sess.run(y_model,feed_dict={....})
</code></pre>

<p>After training has been completed I have acceptable values for w and d and now I want to save my model for later. Then, in a different python session I want to restore the model so that I can again run</p>

<pre class=""lang-py prettyprint-override""><code>## Starting brand new python session
import tensorflow as tf
## somehow restor the graph and the values here: how????
## so that I can run this:
y_vals = sess.run(y_model,feed_dict={....})
</code></pre>

<p>for some different data and get back the y-values. </p>

<p>I want this to work in a way where the graph for calculating the y-values from the placeholders is also stored and restored - as long as the placeholders get fed the correct data, this should work transparently without the user (the one who applies the model) needing to know what the graph looks like). </p>

<p>As far as I understand tf.train.Saver().save(..) only saves the variables but I also want to save the graph. I think that tf.train.export_meta_graph could be relevant here but I do not understand how to use it correctly, the documentation is a bit cryptic to me and the examples do not even use export_meta_graph anywhere. </p>
",1
38725224,Tensorflow DNNClassifier return wrong prediction,"<p>I try to make a sentence classifier using tensorflow as in the example of the official site <a href=""https://www.tensorflow.org/versions/r0.9/tutorials/tflearn/index.html"" rel=""nofollow"">tf.contrib.learn Quickstart</a> but using my own data, first I convert all my data (which are strings of varying lengths) to ids through the use of dictionaries and so transform each sentence in an array of integers.</p>

<p>Each record for training has its own assigned label.</p>

<p>The problem is that predictions are not exact, only some, but others even when the input is equal to a record of the training base the result is wrong.<br>
My code looks something like this:</p>

<pre><code>def launchModelData(values, labels, sample, actionClasses):

    #Tensor for trainig data
    v = tf.Variable(values)
    l = tf.Variable(labels)

    #Data Sample
    s = tf.Variable(sample)

    # Build 3 layer DNN with 10, 20, 10 units respectively.
    classifier = tf.contrib.learn.DNNClassifier(hidden_units=[10, 20, 10], n_classes=actionClasses)

    # Add an op to initialize the variables.
    init_op = tf.initialize_all_variables()

    # Later, when launching the model
    with tf.Session() as sess:
        # Run the init operation.
        sess.run(init_op)

        # Fit model.
        classifier.fit(x=v.eval(), y=l.eval(), steps=200)

        # Classify one new sample.
        new_sample = np.array(s.eval(), dtype=int)
        y = classifier.predict(new_sample)
        print ('Predictions: {}'.format(str(y)))

    return y
</code></pre>

<p>Values and classes axample:  </p>

<pre><code>[0 1] 0  
[0 2] 0  
[0 4] 0  
[7 8] 1  
[7 9] 1  
[ 7 13] 1  
[14 15] 2  
[14 16] 2  
[14 18] 2  
[20 21] 3  
[26 27] 5  
[29 27] 5  
[31 32] 5  
... 
</code></pre>

<p>I'm new to tensorflow so I try to make it less complex possible, any help will be welcome.</p>

<p><strong>EDIT</strong><br>
My actual training data is <a href=""https://drive.google.com/open?id=0B3uEZ76zDg_wNXJnT3g5c0lWWjQ"" rel=""nofollow"">this.</a></p>

<p>I try it with 8 classes and the predictions were fine, so maybe i need a bigger corpus, i will try and show my outputs in a new edit.  </p>

<p><strong>EDIT2</strong>  </p>

<p>Now i use a composition of five layer [n,2n,4n,8n,16n] where n = Classes  and steps = 20000, this reduce the loss and  increase the accuracy really well but again it just work with a few targets (10 aprox) with a bigger amount the predictions become wrong.</p>
",0
38732502,TensorFlow Master and Worker Service,"<p>I am trying to understand the exact roles of the master and worker service in TensorFlow.</p>

<p>So far I understand that each TensorFlow task that I start is associated with a <code>tf.train.Server</code> instance. This instance exports a ""master service"" and ""worker service"" by implementing the <a href=""https://www.tensorflow.org/versions/r0.10/api_docs/cc/ClassSession.html#class-tensorflow-session"" rel=""noreferrer""><code>tensorflow::Session</code></a> interface"" (master) and <a href=""https://github.com/tensorflow/tensorflow/blob/27eeb441bad8bcaa1bcba42a4b4ee49fb50ea0d3/tensorflow/core/protobuf/worker_service.proto"" rel=""noreferrer""><code>worker_service.proto</code></a> (worker).</p>

<p><strong>1st Question: Am I right that this means, that ONE task is only associated with ONE worker?</strong></p>

<hr>

<p><em>Furthermore, I understood...</em></p>

<p><em>...about the Master:</em>
It is the scope of the master service...</p>

<p>(1) ...to offer functions to the client so that the client can run a session for example.</p>

<p>(2) ...to delegate work to the available workers in order to compute a session run.</p>

<p><strong>2nd Question:
In case we execute a graph distributed using more than one task, does only one master service get used?</strong></p>

<p><strong>3rd Question:
Should tf.Session.run get only called once?</strong></p>

<p>This is at least how I interpret this figure from <a href=""http://download.tensorflow.org/paper/whitepaper2015.pdf"" rel=""noreferrer"">the whitepaper</a>:</p>

<p><a href=""https://i.stack.imgur.com/mf8Dx.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mf8Dx.png"" alt=""enter image description here""></a></p>

<hr>

<p><em>... about the Worker:</em>
It is the scope of the worker service...</p>

<p>(1) to execute the nodes (which got delegated to him by the master service) on the devices the worker manages.</p>

<p><strong>4th Question:
How does one worker make use of multiple devices? Does a worker decide automatically how to distribute single operations?</strong></p>

<hr>

<p>Please also correct me in case I came up with wrong statements!
Thank you in advance!!</p>
",0
38890211,What is concrete meaning of replica in DeviceSpec,"<p>In class <a href=""https://www.tensorflow.org/versions/r0.10/api_docs/python/framework.html#DeviceSpec"" rel=""nofollow"">tf.DeviceSpec</a>, there is a field <strong>Replica</strong>. Also in tensorflow logs, we can see some messages such as ""<strong>/job:localhost/replica:0/task:0/cpu:0</strong>"". Anybody can tell me what the concrete meaning of replica is and how I should use it?</p>
",0
38902433,TensorFlow strings: what they are and how to work with them,"<p>When I read file with <code>tf.read_file</code> I get something with type <code>tf.string</code>. Documentation says only that it is ""Variable length byte arrays. Each element of a Tensor is a byte array."" (<a href=""https://www.tensorflow.org/versions/r0.10/resources/dims_types.html"" rel=""noreferrer"">https://www.tensorflow.org/versions/r0.10/resources/dims_types.html</a>). I have no idea how to interpret this.</p>

<p>I can do nothing with this type. In usual python you can get elements by index like <code>my_string[:4]</code>, but when I run following code I get an error.</p>

<pre><code>import tensorflow as tf
import numpy as np

x = tf.constant(""This is string"")
y = x[:4]


init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)
result = sess.run(y)
print result
</code></pre>

<p>It says </p>

<pre>  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py"", line 621, in assert_has_rank
    raise ValueError(""Shape %s must have rank %d"" % (self, rank))
ValueError: Shape () must have rank 1
</pre>

<p>Also I cannot convert my string to <code>tf.float32</code> tensor. It is <code>.flo</code> file and it has magic header ""PIEH"". This numpy code successfuly convert such header into number (see example here <a href=""https://stackoverflow.com/a/28016469/4744283"">https://stackoverflow.com/a/28016469/4744283</a>) but I can't do that with tensorflow. I tried <code>tf.string_to_number(string, out_type=tf.float32)</code> but it says </p>

<pre>tensorflow.python.framework.errors.InvalidArgumentError: StringToNumberOp could not correctly convert string: PIEH
</pre>

<p>So, what string is? What it's shape is? How can I at least get part of the string? I suppose that if I can get part of it I can just skip ""PIEH"" part.</p>

<p><strong>UPD</strong>: I forgot to say that <code>tf.slice(string, [0], [4])</code> also doesn't work with same error.</p>
",0
38912659,tensorflow distribute seq2seq stuck forever,"<p>I'm trying to start the distributed seq2seq model in Tensorflow. This is the original single-process seq2seq model.
I set a cluster(1ps, 3workers) follow the tensorflow distributed tutorial <a href=""https://www.tensorflow.org/versions/r0.10/how_tos/distributed/index.html#putting-it-all-together-example-trainer-program"" rel=""nofollow"">here</a>.</p>

<p>But all workers are stuck forever, and output the same <strong>pooling log info:</strong></p>

<pre><code>start running session
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7623 get requests, put_count=3649 evicted_count=1000 eviction_rate=0.274048 and unsatisfied allocation rate=0.665617
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110
</code></pre>

<p>This is the cluster setting of <strong>translate.py</strong>:</p>

<pre><code>  ps_hosts = [""9.91.9.129:2222""]
  worker_hosts = [""9.91.9.130:2223"", ""9.91.9.130:2224"", ""9.91.9.130:2225""]
  #worker_hosts = [""9.91.9.130:2223""]

  cluster = tf.train.ClusterSpec({""ps"":ps_hosts, ""worker"":worker_hosts})
  server = tf.train.Server(cluster,
                            job_name=FLAGS.job_name,
                            task_index=FLAGS.task_index)
  if FLAGS.job_name == ""ps"":
        server.join()
  elif FLAGS.job_name == ""worker"":
      # Worker server 
      is_chief = (FLAGS.task_index == 0)      
      gpu_num = FLAGS.task_index
      with tf.Graph().as_default():
        with tf.device(tf.train.replica_device_setter(cluster=cluster,
            worker_device=""/job:worker/task:%d/gpu:%d"" % (FLAGS.task_index, gpu_num))):
</code></pre>

<p>And I used the <strong>tf.train.SyncReplicasOptimizer</strong> to implement the SyncTraining.</p>

<p>This is part of my <strong>seq2seq_model.py</strong>:</p>

<pre><code># Gradients and SGD update operation for training the model.
params = tf.trainable_variables()
if not forward_only:
  self.gradient_norms = []
  self.updates = []
  opt = tf.train.GradientDescentOptimizer(self.learning_rate)
  opt = tf.train.SyncReplicasOptimizer(
    opt,
    replicas_to_aggregate=num_workers,
    replica_id=task_index,
    total_num_replicas=num_workers)      

  for b in xrange(len(buckets)):
    gradients = tf.gradients(self.losses[b], params)
    clipped_gradients, norm = tf.clip_by_global_norm(gradients,
                                                     max_gradient_norm)
    self.gradient_norms.append(norm)
    self.updates.append(opt.apply_gradients(
          zip(clipped_gradients, params), global_step=self.global_step))


self.init_tokens_op = opt.get_init_tokens_op
self.chief_queue_runners = [opt.get_chief_queue_runner]
self.saver = tf.train.Saver(tf.all_variables())
</code></pre>

<p>This is my complete python code [here]</p>
",0
38995254,Tensorflow cifar10_input.py,"<p>I am trying to implement cifar10 model in tensorflow.The code for the tutorial is <a href=""https://github.com/tensorflow/tensorflow/tree/r0.10/tensorflow/models/image/cifar10"" rel=""nofollow"">here</a>.</p>

<p>The file <a href=""https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/models/image/cifar10/cifar10_input.py"" rel=""nofollow"">cifar10_input.py</a> deals with handling of cifar10 input in binary form.I am not able to comprehend-</p>

<p>1)The function of tf.FixedLengthRecordReader.As much I could understand,It takes binary files and expresses them as a fixed length records.It is highly unclear.An example might help.</p>

<p>2)The variable ""value""(Argument of tf.decode_raw).What does it contain and how does tf.decode_raw converts the value in ""value"" to uint8?Again an example might help.</p>

<p>Long story short,I need an working example to get a feel what these three lines of code are actually doing-</p>

<pre><code>reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)
result.key, value = reader.read(filename_queue)
record_bytes = tf.decode_raw(value, tf.uint8)
</code></pre>
",1
39014876,get numeric padding values from tensorflow,"<p>I have a Tensorflow GraphDef :
<a href=""https://github.com/gauravgupta22/tensorflow-caffe/blob/master/lenet.pbtxt"" rel=""nofollow"">https://github.com/gauravgupta22/tensorflow-caffe/blob/master/lenet.pbtxt</a></p>

<p>I am interested in calculating the real numeric padding values that are used internally by tensorflow (instead of 'SAME' or 'VALID')</p>

<p>Is there any built-in method or some function or any way to get these?</p>

<p>I found a link which explains how the values are calculated : <a href=""https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html"" rel=""nofollow"">https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html</a></p>

<p>For the 'VALID' padding, the padding values are always zero.</p>

<p>For the 'SAME' padding</p>

<pre><code>out_height = ceil(float(in_height) / float(strides[1]))
out_width  = ceil(float(in_width) / float(strides[2]))

pad_along_height = ((out_height - 1) * strides[1] +
                filter_height - in_height)
pad_along_width = ((out_width - 1) * strides[2] +
                   filter_width - in_width)
pad_top = pad_along_height / 2
pad_left = pad_along_width / 2
</code></pre>

<p>To use this calculations, it requires me to calculate input shapes of tensors for the operations involved.</p>

<pre><code># code to import a graph and print input/output tensor shapes of its operations

import tensorflow as tf
from google.protobuf import text_format
from tensorflow.core.framework import graph_pb2

graph_def = graph_pb2.GraphDef()
with open('lenet.pbtxt', ""rb"") as f:
    text_format.Merge(f.read(), graph_def)

tf.import_graph_def(graph_def,name='')

graph = tf.get_default_graph()
for i in graph.get_operations():
    print i.name
    print 'INPUTS ',
    for j in i.inputs:
        print j.get_shape(),
    print '\nOUTPUTS ',
    for j in i.outputs:
        print j.get_shape(),
    print
</code></pre>

<p>But I get some shapes like:</p>

<pre><code>INPUTS  (?, ?) &lt;unknown&gt; 
OUTPUTS  (?, ?)
</code></pre>

<p>Clearly these cannot be used.<br>
<b>So how can i get complete tensor shapes or indirectly how do i calculate the numeric padding values?</b></p>

<p>Please help</p>

<p><strong>EDIT</strong><br>
I made a tensorflow model and exported it to graphdef using <code>tf.get_default_graph().as_graph_def()</code><br>
Now if i import this graphdef again, i get tensors with <code>&lt;unknown&gt;</code> shapes. I lose the info of inferred tensor shapes!!</p>

<p>On the other hand, if i export my model using <code>tf.get_default_graph().as_graph_def(add_shapes=True)</code>and import this again, I am able to tensor shapes!</p>

<p>But i want my code to work in general with any graphdef<br>
<b>Is <code>tf.get_default_graph().as_graph_def()</code> not the perfect way to save a graph or there is some other way i could get inferred tensor shapes out of it?</b></p>

<p><strong>EDIT</strong><br>
I submitted an issue<br>
<a href=""https://github.com/tensorflow/tensorflow/issues/3903"" rel=""nofollow"">https://github.com/tensorflow/tensorflow/issues/3903</a></p>
",0
39027488,Distributed tensorflow replicated training example: grpc_tensorflow_server - No such file or directory,"<p>I am trying to make a <code>distributed tensorflow</code> implementation by following the instructions in this blog: <a href=""http://leotam.github.io/general/2016/03/13/DistributedTF.html"" rel=""nofollow noreferrer"">Distributed TensorFlow by Leo K. Tam</a>. My aim is to perform <code>replicated training</code> as mentioned in this <a href=""https://stackoverflow.com/a/37733117/5082406"">post</a></p>

<p>I have completed the steps till <code>installing tensorflow</code> and successfully running the following command and getting results:</p>

<pre><code>sudo bazel-bin/tensorflow/cc/tutorials_example_trainer --use_gpu
</code></pre>

<p>Now the next thing, which I want to implement is to launch the <code>gRPC server</code> on one of the nodes by the following command :</p>

<pre><code>bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server --cluster_spec='worker|192.168.555.254:2500;192.168.555.255:2501' --job_name=worker --task_id=0 &amp;
</code></pre>

<p>Though, when I run it, I get the following error: <code>rpc/grpc_tensorflow_server:No such file directory</code></p>

<pre><code>-bash: bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server: No such file or directory
</code></pre>

<p>The contents of my <code>rpc</code> folder are:</p>

<pre><code> libgrpc_channel.pic.a              libgrpc_remote_master.pic.lo       libgrpc_session.pic.lo             libgrpc_worker_service_impl.pic.a  _objs/                             
 libgrpc_master_service_impl.pic.a  libgrpc_remote_worker.pic.a        libgrpc_tensor_coding.pic.a        libgrpc_worker_service.pic.a       
 libgrpc_master_service.pic.lo      libgrpc_server_lib.pic.lo          libgrpc_worker_cache.pic.a         librpc_rendezvous_mgr.pic.a
</code></pre>

<p>I am clearly missing out on a step in between, which is not mentioned in the blog. My objective is to be able to run the command mentioned above (to launch the <code>gRPC server</code>) so that I can start a worker process on one of the nodes. </p>
",0
39112622,How do I set TensorFlow RNN state when state_is_tuple=True?,"<p>I have written an <a href=""https://github.com/wpm/tfrnnlm"" rel=""noreferrer"">RNN language model using TensorFlow</a>. The model is implemented as an <code>RNN</code> class. The graph structure is built in the constructor, while <code>RNN.train</code> and <code>RNN.test</code> methods run it.</p>

<p>I want to be able to reset the RNN state when I move to a new document in the training set, or when I want to run a validation set during training. I do this by managing the state inside the training loop, passing it into the graph via a feed dictionary.</p>

<p>In the constructor I define the the RNN like so</p>

<pre><code>    cell = tf.nn.rnn_cell.LSTMCell(hidden_units)
    rnn_layers = tf.nn.rnn_cell.MultiRNNCell([cell] * layers)
    self.reset_state = rnn_layers.zero_state(batch_size, dtype=tf.float32)
    self.state = tf.placeholder(tf.float32, self.reset_state.get_shape(), ""state"")
    self.outputs, self.next_state = tf.nn.dynamic_rnn(rnn_layers, self.embedded_input, time_major=True,
                                                  initial_state=self.state)
</code></pre>

<p>The training loop looks like this</p>

<pre><code> for document in document:
     state = session.run(self.reset_state)
     for x, y in document:
          _, state = session.run([self.train_step, self.next_state], 
                                 feed_dict={self.x:x, self.y:y, self.state:state})
</code></pre>

<p><code>x</code> and <code>y</code> are batches of training data in a document. The idea is that I pass the latest state along after each batch, except when I start a new document, when I zero out the state by running <code>self.reset_state</code>.</p>

<p>This all works.  Now I want to change my RNN to use the recommended <code>state_is_tuple=True</code>. However, I don't know how to pass the more complicated LSTM state object via a feed dictionary. Also I don't know what arguments to pass to the <code>self.state = tf.placeholder(...)</code> line in my constructor.</p>

<p>What is the correct strategy here? There still isn't much example code or documentation for <code>dynamic_rnn</code> available.</p>

<hr>

<p>TensorFlow issues <a href=""https://github.com/tensorflow/tensorflow/issues/2695"" rel=""noreferrer"">2695</a> and <a href=""https://github.com/tensorflow/tensorflow/issues/2838"" rel=""noreferrer"">2838</a> appear relevant.</p>

<p>A <a href=""http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/"" rel=""noreferrer"">blog post</a> on WILDML addresses these issues but doesn't directly spell out the answer.</p>

<p>See also <a href=""https://stackoverflow.com/questions/38241410/tensorflow-remember-lstm-state-for-next-batch-stateful-lstm"">TensorFlow: Remember LSTM state for next batch (stateful LSTM)</a>.</p>
",1
39125489,Tensorflow Documentation,"<p>I am increasingly irritated and frustrated by the <code>Tensorflow</code> documentation. I searched on google for documentation regarding </p>

<pre><code>tf.reshape
</code></pre>

<p>I'm getting directed to a generic page like <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/array_ops.html"" rel=""nofollow noreferrer"">here</a>. I want to see the details of <code>tf.reshape</code> and not the entirety of the documentation.</p>

<p>Am I doing something wrong here?</p>
",1
39135902,Can I run a .bp/.meta graph generated by single node tf.sess on a distributed environ with multi-ps/worker nodes?,"<p>Will future versions of tensorflow provide a way to run the tensorflow graph generated by single node tf.sess on a distributed environments with multiple ps nodes and worker nodes through python interfaces?
Or is it supported right now?</p>

<p>I am trying to build my tf.graph on my notebook (single node) and save then graph into a binary file, 
and then loading the binary graph into a distributed environment (with multiply ps and worker nodes) to train and verify it. It seems it is not supported now.</p>

<p>I tried it on tensorflow-0.10 and failed.
By using</p>

<pre><code>tf.train.write_graph(sess.graph_def, path, pb_name)
</code></pre>

<p>interface: The graph saved is not trainable as loading the <code>.pb</code> file through <code>import_graph_def</code> will only <code>g.create_ops</code> according to the <code>.bp</code> file but not add then into <code>ops.collections</code>. So the graph loaded is not trainable.</p>

<p>By using <code>tf.saver.save</code> to save a <code>.meta</code> file: The loaded graph cannot fit into the distributed environment as devices assignment is messy.</p>

<p>I tried the</p>

<pre><code>tf.train.import_meta_graph('test_model.meta', clear_devices=True)
</code></pre>

<p>interface to let the load clean the original device assignment and let the <code>with tf.device(device_setter)</code> reassign the device for each variable, but there is a problem as operations belonging to <code>Saver</code> and <code>Restore</code> still can not be assigned correctly. When creating operations for <code>Saver</code> and <code>Restore</code> ops through <code>g.create_op</code> inside <code>import_graph_def</code> called by <code>import_meta_graph</code>, the device_setter will not assign ps node to these ops as their name is not <code>Variable</code>.
Is there any way to do so?</p>
",0
39211332,Custom initializer for get_variable,"<p>How can one specify a custom initializer as the third argument for <code>tf.get_variable()</code>? Specifically, I have a variable <code>y</code> which I want to initialize using another (already initialized) variable <code>x</code>. </p>

<p>This is easy to do using <code>tf.Variable()</code>, just say, <code>y = tf.Variable(x.initialized_value())</code>. But I couldn't find an analog in the documentation for <code>tf.get_variable()</code>.</p>
",1
39260059,Using Tensorflow to perform chain rule,"<p>I just read through Tensorflow Docs, and found that <code>tf.gradients(tf.gradients(ys, xs, grad_ys=None, name='gradients', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None)</code> could be used for computing the gradients with respects to <code>dy/dx</code>. Currently, I want to compute the gradients of <code>dz(f(x))/dx</code> (which should be decomposed as <code>dz/df * df/dx</code> using the chain rule), and I wonder if there is a way in Tensorflow to do this chain rule. I also wonder what it means in terms of <code>grad_ys is a list of Tensor, holding the gradients received by the ys. The list must be the same length as ys.</code> in Tensorflow Doc.</p>
",0
39322441,tf.train.shuffle_batch not working for me,"<p>I'm trying to process my input data using the TensorFlow clean way (tf.train.shuffle_batch), most of this code I gathered from the tutorials with slight modifications like the decode_jpeg function.</p>

<pre><code>size = 32,32
classes = 43
train_size = 12760
batch_size = 100
max_steps = 10000

def read_and_decode(filename_queue):
    reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)
    features = tf.parse_single_example(
        serialized_example,
        # Defaults are not specified since both keys are required.
        features={
            'image/encoded': tf.FixedLenFeature([], tf.string),
            'image/class/label': tf.FixedLenFeature([], tf.int64),
            'image/height': tf.FixedLenFeature([], tf.int64),
            'image/width': tf.FixedLenFeature([], tf.int64),
        })
    label = tf.cast(features['image/class/label'], tf.int32)
    reshaped_image = tf.image.decode_jpeg(features['image/encoded'])
    reshaped_image = tf.image.resize_images(reshaped_image, size[0], size[1], method = 0)
    reshaped_image = tf.image.per_image_whitening(reshaped_image)
    return reshaped_image, label

def inputs(train, batch_size, num_epochs):
    subset = ""train""
    tf_record_pattern = os.path.join(FLAGS.train_dir + '/GTSRB', '%s-*' % subset)
    data_files = tf.gfile.Glob(tf_record_pattern)
    filename_queue = tf.train.string_input_producer(
        data_files, num_epochs=num_epochs)

    # Even when reading in multiple threads, share the filename
    # queue.
    image, label = read_and_decode(filename_queue)

    # Shuffle the examples and collect them into batch_size batches.
    # (Internally uses a RandomShuffleQueue.)
    # We run this in two threads to avoid being a bottleneck.
    images, sparse_labels = tf.train.shuffle_batch(
        [image, label], batch_size=batch_size, num_threads=2,
        capacity=1000 + 3 * batch_size,
        # Ensures a minimum amount of shuffling of examples.
        min_after_dequeue=1000)
    return images, sparse_labels
</code></pre>

<p>When I try to run</p>

<pre><code>batch_x, batch_y = inputs(True, 100,100)
</code></pre>

<p>I get the following error:</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-6-543290a0c903&gt; in &lt;module&gt;()
----&gt; 1 batch_x, batch_y = inputs(True, 100,100)

&lt;ipython-input-5-a8c07c7fc263&gt; in inputs(train, batch_size, num_epochs)
     73         capacity=1000 + 3 * batch_size,
     74         # Ensures a minimum amount of shuffling of examples.
---&gt; 75         min_after_dequeue=1000)
     76     #return image, label
     77     return images, sparse_labels

/Users/Kevin/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/input.pyc in shuffle_batch(tensors, batch_size, capacity, min_after_dequeue, num_threads, seed, enqueue_many, shapes, allow_smaller_final_batch, shared_name, name)
    800     queue = data_flow_ops.RandomShuffleQueue(
    801         capacity=capacity, min_after_dequeue=min_after_dequeue, seed=seed,
--&gt; 802         dtypes=types, shapes=shapes, shared_name=shared_name)
    803     _enqueue(queue, tensor_list, num_threads, enqueue_many)
    804     full = (math_ops.cast(math_ops.maximum(0, queue.size() - min_after_dequeue),

/Users/Kevin/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.pyc in __init__(self, capacity, min_after_dequeue, dtypes, shapes, names, seed, shared_name, name)
    580     """"""
    581     dtypes = _as_type_list(dtypes)
--&gt; 582     shapes = _as_shape_list(shapes, dtypes)
    583     names = _as_name_list(names, dtypes)
    584     # If shared_name is provided and an op seed was not provided, we must ensure

/Users/Kevin/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/data_flow_ops.pyc in _as_shape_list(shapes, dtypes, unknown_dim_allowed, unknown_rank_allowed)
     70   if not unknown_dim_allowed:
     71     if any([not shape.is_fully_defined() for shape in shapes]):
---&gt; 72       raise ValueError(""All shapes must be fully defined: %s"" % shapes)
     73   if not unknown_rank_allowed:
     74     if any([shape.dims is None for shape in shapes]):

ValueError: All shapes must be fully defined: [TensorShape([Dimension(32), Dimension(32), Dimension(None)]), TensorShape([])]
</code></pre>

<p>I'm not sure what is causing this error, I imagine it has something to do with the way I'm processing my image because it shows that they have no dimensions when they should have 3 channels (RGB).</p>
",0
39415539,How does xavier_initializer() know the activation?,"<p>How does tf.contrib.layers.xavier_initializer() know the activation functions?</p>

<p>The standard deviation to initialize well depends on the non-linearity used. Right? How then does tf.contrib.layers.xavier_initializer() know whats going on?</p>

<p>Take the following case:</p>

<pre><code>W = tf.get_variable(""W"", shape=[784, 256],
           initializer=tf.contrib.layers.xavier_initializer())
</code></pre>

<p>This W does something to X and then the result is passed to a tanh or relu or have you. Now, the initializer is in the W. How does tensorflow figure the activation out? Or do I have to intervene, knowing the activation I am going to use?</p>

<p>Looked at the arguments in <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard9/tf.contrib.layers.xavier_initializer.md"" rel=""nofollow"" title=""tf.contrib.layers.xavier_initializer"">tf.contrib.layers.xavier_initializer</a> and there I can chose unifrom or normal distribution. But that doesn't solve it, right?</p>
",0
39493229,How to use tf.nn.max_pool_with_argmax correctly,"<p>currently I play a little bit around with tensorflow to create a better understanding of machine learning an tensorflow itself. Therefore I want to visualize the methods (as much as possible) of tensorflow. To visualize max_pool I loaded an image and perform the method. After that I displayed both: input and output image.</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import cv2
import numpy as np

import matplotlib.pyplot as plt

image = cv2.imread('lena.png')
image_tensor = tf.expand_dims(tf.Variable(image, dtype=tf.float32), 0)

#output, argmax = tf.nn.max_pool_with_argmax(image_tensor, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')
output = tf.nn.max_pool(image_tensor, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')

init = tf.initialize_all_variables()
session = tf.Session()
session.run(init)

output = session.run(output)

session.close()

image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
plt.figure()
plt.imshow(image)
plt.show()

output = cv2.cvtColor(output[0], cv2.COLOR_RGB2BGR)
plt.figure()
plt.imshow(255-output)
plt.show() 
</code></pre>

<p>Everything works fine and I get this output (as expected)</p>

<p><a href=""https://i.stack.imgur.com/FFoKmm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FFoKmm.png"" alt=""image (input)""></a>
<a href=""https://i.stack.imgur.com/YZhogm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YZhogm.png"" alt=""enter image description here""></a></p>

<p>Now I wanted to test the method <code>tf.nn.max_pool_with_argmax</code> to get the argmax of the pooling operations. But if I uncomment the line </p>

<p><code>output, argmax = tf.nn.max_pool_with_argmax(image_tensor, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')
</code></p>

<p>Python crashes with </p>

<blockquote>
  <p>tensorflow.python.framework.errors.InvalidArgumentError: No OpKernel was registered to support Op 'MaxPoolWithArgmax' with these attrs
       [[Node: pool1 = MaxPoolWithArgmaxT=DT_FLOAT, Targmax=DT_INT64, ksize=[1, 2, 2, 1], padding=""SAME"", strides=[1, 2, 2, 1]]]</p>
</blockquote>

<p>I don't have an idea which argument is wrong because every argument should be correct (<a href=""https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#max_pool_with_argmax"" rel=""nofollow noreferrer"">tensorflow docs</a>) ...</p>

<p>Does anyone know what went wrong?</p>
",0
39507405,"TensorFlow, Python, Sharing Variables, initialize at top","<p>I'm having an issue in TensorFlow with Sharing Variables with the Python API.</p>

<p>I've read the official documentation (<a href=""https://www.tensorflow.org/versions/r0.10/how_tos/variable_scope/index.html"" rel=""nofollow"">https://www.tensorflow.org/versions/r0.10/how_tos/variable_scope/index.html</a>), but I still can't figure out what is going on.</p>

<p>I've written below a minimal working example to illustrate the issue.</p>

<p>In a nutshell, I'd like the code below do to the following:</p>

<p>1) Initialize one variable ""fc1/w"" immediately after I create the session,</p>

<p>2) Create a npy array ""x_npy"" to feed into a placeholder ""x"",</p>

<p>3) Run an operation ""y"", which should realize that the variable ""fc1/w"" is already created, and then use that variable values (rather than initialize new ones) to compute its output.</p>

<p>4) Please note that I added the flag "", reuse=True"" in the variable scope in the function ""linear"", but that doesn't seem to help, since I keep getting the error:</p>

<pre><code>ValueError: Variable fc1/w does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?
</code></pre>

<p>This is quite confusing since if I were to remove the flag "", reuse=True"", then TensorFlow would tell me that the variable does exist:</p>

<pre><code>ValueError: Variable fc1/w already exists, disallowed. Did you mean to set reuse=True in VarScope?
</code></pre>

<p>5) Please note that I'm working with a larger code base, and I'd really like to be able to use the Sharing Variables capability, rather than come up with a hack without using Sharing Variables that might solve the particular example code I wrote below, but might not generalize well.</p>

<p>6) Finally, please note also that I'd really like to keep separated the creation of the graph from it's evaluation. In particular, I would not like to use ""tf.InteractiveSession()"" or create ""y"" in the session scope, i.e., below: ""with tf.Session() as sess:"".</p>

<p>This is my first post on Stack Overflow, and I'm quite new to TensorFlow, so please accept my apologies if the question is not completely clear. In any case, I'd be happy to provide more details or clarify any aspect further.</p>

<p>Thank you in advance.</p>

<pre><code>import tensorflow as tf
import numpy as np


def linear(x_, output_size, non_linearity, name):
    with tf.variable_scope(name, reuse=True):
        input_size = x_.get_shape().as_list()[1]
        # If doesn't exist, initialize ""name/w"" randomly:
        w = tf.get_variable(""w"", [input_size, output_size], tf.float32,
                            tf.random_normal_initializer())
        z = tf.matmul(x_, w)
        return non_linearity(z)


def init_w(name, w_initializer):
    with tf.variable_scope(name):
        w = tf.get_variable(""w"", initializer=w_initializer)
        return tf.initialize_variables([w])


batch_size = 1
fc1_input_size = 7
fc1_output_size = 5

# Initialize with zeros
fc1_w_initializer = tf.zeros([fc1_input_size, fc1_output_size])

#
x = tf.placeholder(tf.float32, [None, fc1_input_size])

#
y = linear(x, fc1_output_size, tf.nn.softmax, ""fc1"")

with tf.Session() as sess:

    # Initialize ""fc1/w"" with zeros.
    sess.run(init_w(""fc1"", fc1_w_initializer))

    # Create npy array to feed into placeholder x
    x_npy = np.arange(batch_size * fc1_input_size, dtype=np.float32).reshape((batch_size, fc1_input_size))

    # Run y, and print result.
    print(sess.run(y, dict_feed={x: x_npy}))
</code></pre>
",0
39547572,tensorflow wide model: how to use one-hot feature?,"<p>I have read about the model in <a href=""https://www.tensorflow.org/versions/r0.9/tutorials/wide_and_deep/index.html"" rel=""nofollow"">https://www.tensorflow.org/versions/r0.9/tutorials/wide_and_deep/index.html</a>
the feature in article has two type: Categorical and Continuous</p>

<p>In my case, I have a column which describe the userid ,range from 0 to 10000000</p>

<p>I treat this column as Categorical and use hash-bucket , but only get a pool auc value about 0.50010</p>

<p>1)is it need to use one-hot to process this id column?</p>

<p>2)if it's needed, how to achieve this?  I find a ""tf.contrib.layers.one_hot_encoding"" ,but it's not support column names so cannot be used in wide-n-deep demo.</p>
",0
39682918,Accuracy issue with multiple output in tensorflow,"<p>I have a system which may has 65 different property in a one moment. I want to predict them using a dnn. My inputs are the properties of the system (79 binary inputs) and the output is 65 discrete statuses, which can be 0,1,2. 
So, for each input I may have a vector of output like <code>[0,0,1,2,...,2,1,0,1,1]</code>. In order to use the dnn algorithm, I wanted to have 65 softmax outputs, which each has three output. So, the output of dnn is vector <code>y</code> of size <code>[,65*3]</code>. </p>

<p>I implemented this problem with a fully connected network in <code>tensorflow</code>.
However, I have problem in obtaining the accuracy of each solution. For each of the samples and for a given <code>y_</code>, the accuracy can be obtained with:</p>

<p><code>correct_predct = tf.reduce_sum(tf.cast([tf.equal( tf.argmax(y_[:,i*3:(i+1)*3],0) , tf.argmax(y[:,i*3:(i+1)*3],0)) for i in range(65)],tf.float32))</code></p>

<p><code>accuracy = tf.reduce_mean(tf.scalar_mul(1/65.0,correct_predct))</code></p>

<p>However, it does not work because the way that <code>y_</code> and <code>y</code> are defined. </p>

<p>Here is my code:</p>

<pre><code>import tensorflow as tf
import numpy as np
import scipy.io as sio
from tensorflow.python.training import queue_runner
tf.logging.set_verbosity(tf.logging.FATAL)

sess = tf.InteractiveSession()

maxiter = 50000
display = 100
decay_rate = 0.9
starter_learning_rate = 0.001
power = 0.75
l2lambda = .01
init_momentum = 0.9
decay_step = 5000

nnodes1 = 350
nnodes2 = 100
batch_size = 50
var = 2.0/(67+195)

print decay_rate,starter_learning_rate,power,l2lambda,init_momentum,decay_step,nnodes1,nnodes2,batch_size

result_mat = sio.loadmat('binarySysFuncProf.mat')
feature_mat = sio.loadmat('binaryDurSamples.mat')

result = result_mat['binarySysFuncProf']
feature = feature_mat['binaryDurSamples']

train_size=750000
test_size=250000
train_feature = feature[0:train_size,:]
train_output = result[0:train_size,:]

test_feature = feature[train_size + 1 : train_size + test_size , :]
test_output = result[train_size + 1 : train_size + test_size , :]

# import the data
#from tensorflow.examples.tutorials.mnist import input_data
# placeholders, which are the training data
x = tf.placeholder(tf.float64, shape=[None,79])
y_ = tf.placeholder(tf.float64, shape=[None,195])
learning_rate = tf.placeholder(tf.float64, shape=[])

# define the variables
W1 = tf.Variable(np.random.normal(0,var,(79,nnodes1)))
b1 = tf.Variable(np.random.normal(0,var,nnodes1))

W2 = tf.Variable(np.random.normal(0,var,(nnodes1,nnodes2)))
b2 = tf.Variable(np.random.normal(0,var,nnodes2))

W3 = tf.Variable(np.random.normal(0,var,(nnodes2,1)))
b3 = tf.Variable(np.random.normal(0,var,1))

# Passing global_step to minimize() will increment it at each step.
global_step = tf.Variable(0, trainable=False)
momentum = tf.Variable(init_momentum, trainable=False)

# prediction function (just one layer)                                                                                                          
layer1 = tf.nn.sigmoid(tf.matmul(x,W1) + b1)
layer2 = tf.nn.sigmoid(tf.matmul(layer1,W2) + b2)
y = tf.nn.softmax(tf.matmul(layer2,W3) + b3)

cost_function =tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y)))
#cost_function = tf.sum([tf.reduce_mean((-tf.reduce_sum(y_[:,i*3:(i+1)*3])*tf.log(y[:,i*3:(i+1)*3]))) for i in range(65)])
correct_predct = tf.reduce_sum(tf.cast([tf.equal( tf.argmax(y_[:,i*3:(i+1)*3],0) , tf.argmax(y[:,i*3:(i+1)*3],0)) for i in range(65)],tf.float32))
accuracy = tf.reduce_mean(tf.scalar_mul(1/65.0,correct_predct))

l2regularization = tf.reduce_sum(tf.square(W1)) + tf.reduce_sum(tf.square(b1)) + tf.reduce_sum(tf.square(W2)) + tf.reduce_sum(tf.square(b2)) + tf.reduce_sum(tf.square(W3)) + tf.reduce_sum(tf.square(b3))

loss = (cost_function) + l2lambda*l2regularization

# define the learning_rate and its decaying procedure.
learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,decay_step, decay_rate, staircase=True)

# define the training paramters and model, gradient model and feeding the function
#train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)
train_step = tf.train.MomentumOptimizer(learning_rate,0.9).minimize(loss, global_step=global_step)

# initilize the variables                                                                                                                       
sess.run(tf.initialize_all_variables())

# Train the Model for 1000 times. by defining the batch number we determine that it is sgd
for i in range(maxiter):
  batch = np.random.randint(0,train_size,size=batch_size)
  train_step.run(feed_dict={x:train_feature[batch,:], y_:train_output[batch,:]})  
  if np.mod(i,display) == 0:
    train_loss = cost_function.eval(feed_dict={x: train_feature[0:train_size,:], y_: train_output[0:train_size,:]})
    test_loss = cost_function.eval(feed_dict={x: test_feature, y_: test_output})
    train_acc = 0#accuracy.eval(feed_dict={x: train_feature[0:train_size,:], y_: train_output[0:train_size,:]})
    test_acc = 0#accuracy.eval(feed_dict={x: test_feature, y_: test_output})
    print ""Iter"" , i, ""lr"" , learning_rate.eval() , ""| Train loss"" , train_loss , ""| Test loss"", test_loss  , ""| Train Accu"", train_acc , ""| Test Accu"", test_acc  ,""||W||"",l2regularization.eval() , ""lmbd*||W||"", l2lambda*l2regularization.eval()
</code></pre>

<p>Since I have <code>y_</code> and <code>y</code> as a matrix with column size 195, I am getting the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 555, in eval
    return _eval_using_default_session(self, feed_dict, self.graph, session)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3498, in _eval_using_default_session
    return session.run(tensors, feed_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 372, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 625, in _run
    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))
ValueError: Cannot feed value of shape (750000, 195) for Tensor u'Placeholder_7:0', which has shape '(?, 3)'
</code></pre>

<p>I appreciate any comment or help to obtain the accuracy. </p>

<p>Afshin</p>
",0
39704282,Tensorflow examples as rows or columns?,"<p>Given the Tensorflow API <code>nn.softmax</code>, <code>nn.sparse_softmax_cross_entropy_with_logits</code>, <code>tf.nn.rnn</code> etc. it seems that the convention is to have the samples placed as rows in a batch.</p>

<p>A forward pass in a neural network is then <code>tf.matmul(input_batch, W) + b</code>, where <code>input_batch</code> is a matrix of shape <code>[n_samples, input_size]</code>. The <code>n_samples</code> is often named <code>batch_size</code> in the API. The addition of the row vector <code>b</code> is broadcasted on all samples(rows) in the batch.</p>

<p>Is this a general adopted convention when working with neural networks? I have read a lot of research articles that have the samples as columns and use left multiplication of the weight matrix for a forward pass. Why was this convention chosed? </p>

<p>EDIT</p>

<p>This article uses multiplication of the weight matrix on the left side of the input, as well as vector concatenation, so the samples must be  columns:
<a href=""http://colah.github.io/posts/2015-08-Understanding-LSTMs/"" rel=""nofollow noreferrer"">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>. It is referred to in the Tensorflow documentation here: <a href=""https://www.tensorflow.org/versions/r0.11/tutorials/recurrent/index.html"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r0.11/tutorials/recurrent/index.html</a>. </p>

<hr>

<p>The implementation of BasicLSTMCell is based on this article <a href=""https://arxiv.org/pdf/1409.2329v5.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1409.2329v5.pdf</a> according to the source here <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py</a>. You also see multiplication from the left.</p>

<hr>

<p>Finally in this article <a href=""https://arxiv.org/pdf/1506.00019.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1506.00019.pdf</a>, the activations are also column vectors.</p>
",0
39841490,Tensorflow what operations are differentiable and what are not?,"<p>In Tensorflow, it is hard to figure out if a function is differentiable or not. For instance, <code>tf.argmax</code> is not differentiable. I am wondering is there any documentation to specify which operations is differentiable?</p>
",1
39886715,Fractional Max Pooling in Tensorflow,"<p>When using the function <code>tf.nn.fractional_max_pool</code> in Tensorflow, in addition to the output pooled tensor it returns, it also returns a <code>row_pooling_sequence</code> and a <code>col_pooling_sequence</code>, which I presume is used in backpropagation to find the gradient of. This is in contrast to the normal $2 \times 2$ max pooling, which just returns the pooled tensor.</p>

<p>My question is: do we have to handle the row_pooling and col_pooling values ourselves? How would we include them into a network to get backpropagation working properly? I modified a simple convolutional neural network to use fractional max pooling instead of 2 x 2 max pooling without making use of these values and the results were much poorer, leading me to believe we must explicitly handle these. </p>

<p>Here's the relevant portion of my code that makes use of the FMP:</p>

<pre><code>def add_layer_ops_FMP(conv_func, x_input, W, keep_prob_layer, training_phase):

    h_conv = conv_func(x_input, W, stride_l = 1)
    h_BN = batch_norm(h_conv, training_phase, epsilon)
    h_elu = tf.nn.elu(h_BN) # Rectified unit layer - change accordingly

    def dropout_no_training(h_elu=h_elu):
        return dropout_op(h_elu, keep_prob = 1.0)

    def dropout_in_training(h_elu=h_elu, keep_prob_layer=keep_prob_layer):
        return dropout_op(h_elu, keep_prob = keep_prob_layer)

    h_drop = tf.cond(training_phase, dropout_in_training, dropout_no_training)
    h_pool, row_pooling_sequence, col_pooling_sequence = tf.nn.fractional_max_pool(h_drop) # FMP layer. See Ben Graham's paper 

    return h_pool
</code></pre>

<p><a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard3/tf.nn.fractional_max_pool.md"" rel=""nofollow"">Link to function on github.</a></p>
",0
40029904,Understanding why tensorflow RNN is not learning toy data,"<p>I am trying to train a Recurrent Neural Network using Tensorflow (r0.10, python 3.5) on a toy classification problem, but I am getting confusing results.</p>

<p>I want to feed in a sequence of zeros and ones into an RNN, and have the target class for a given element of the sequence to be the number represented by the current and previous values of the sequence, treated as a binary number. For example:</p>

<pre><code>input sequence: [0,     0,     1,     0,     1,     1]
binary digits : [-, [0,0], [0,1], [1,0], [0,1], [1,1]]
target class  : [-,     0,     1,     2,     1,     3]
</code></pre>

<p>It seems like this is something an RNN should be able to learn quite easily, but instead my model is only able to distinguish classes [0,2] from [1,3]. In other words, it is able to distinguish the classes whose current digit is 0 from those whose current digit is 1. This is leading me to believe that the RNN model is not correctly learning to look at the previous value(s) of the sequence.</p>

<p>There are several tutorials and examples ([<a href=""https://www.tensorflow.org/versions/r0.9/tutorials/recurrent/index.html"" rel=""nofollow"">1</a>], [<a href=""https://github.com/sherjilozair/char-rnn-tensorflow"" rel=""nofollow"">2</a>], [<a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/recurrent_network.ipynb"" rel=""nofollow"">3</a>]) that demonstrate how to build and use Recurrent Neural Networks (RNNs) in tensorflow, but after studying them I still do not see my problem (it does not help that all the examples use text as their source data).</p>

<p>I am inputting my data to <code>tf.nn.rnn()</code> as a list of length <code>T</code>, whose elements are <code>[batch_size x input_size]</code> sequences. Since my sequence is one dimensional, <code>input_size</code> is equal to one, so essentially I believe I am inputting a list of sequences of length <code>batch_size</code> (the <a href=""https://www.tensorflow.org/versions/r0.9/tutorials/recurrent/index.html"" rel=""nofollow"">documentation</a> is unclear to me about which dimension is being treated as the time dimension). <strong>Is that understanding correct?</strong> If that is the case, then I don't understand why the RNN model is not learning correctly.</p>

<p>It's hard to get a small set of code that can run through my full RNN, this is the best I could do (it is mostly adapted from <a href=""https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/rnn/ptb"" rel=""nofollow"">the PTB model here</a> and <a href=""https://github.com/sherjilozair/char-rnn-tensorflow"" rel=""nofollow"">the char-rnn model here</a>):</p>

<pre><code>import tensorflow as tf
import numpy as np

input_size = 1
batch_size = 50
T = 2
lstm_size = 5
lstm_layers = 2
num_classes = 4
learning_rate = 0.1

lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size, state_is_tuple=True)
lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * lstm_layers, state_is_tuple=True)

x = tf.placeholder(tf.float32, [T, batch_size, input_size])
y = tf.placeholder(tf.int32, [T * batch_size * input_size])

init_state = lstm.zero_state(batch_size, tf.float32)

inputs = [tf.squeeze(input_, [0]) for input_ in tf.split(0,T,x)]
outputs, final_state = tf.nn.rnn(lstm, inputs, initial_state=init_state)

w = tf.Variable(tf.truncated_normal([lstm_size, num_classes]), name='softmax_w')
b = tf.Variable(tf.truncated_normal([num_classes]), name='softmax_b')

output = tf.concat(0, outputs)

logits = tf.matmul(output, w) + b

probs = tf.nn.softmax(logits)

cost = tf.reduce_mean(tf.nn.seq2seq.sequence_loss_by_example(
    [logits], [y], [tf.ones_like(y, dtype=tf.float32)]
))

optimizer = tf.train.GradientDescentOptimizer(learning_rate)
tvars = tf.trainable_variables()
grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),
                                  10.0)
train_op = optimizer.apply_gradients(zip(grads, tvars))

init = tf.initialize_all_variables()

with tf.Session() as sess:
    sess.run(init)
    curr_state = sess.run(init_state)
    for i in range(3000):
        # Create toy data where the true class is the value represented
        # by the current and previous value treated as binary, i.e.
        train_x = np.random.randint(0,2,(T * batch_size * input_size))
        train_y = train_x + np.concatenate(([0], (train_x[:-1] * 2)))

        # Reshape into T x batch_size x input_size
        train_x = np.reshape(train_x, (T, batch_size, input_size))

        feed_dict = {
            x: train_x, y: train_y
        }
        for j, (c, h) in enumerate(init_state):
            feed_dict[c] = curr_state[j].c
            feed_dict[h] = curr_state[j].h

        fetch_dict = {
            'cost': cost, 'final_state': final_state, 'train_op': train_op
        }

        # Evaluate the graph
        fetches = sess.run(fetch_dict, feed_dict=feed_dict)

        curr_state = fetches['final_state']

        if i % 300 == 0:
            print('step {}, train cost: {}'.format(i, fetches['cost']))

    # Test
    test_x = np.array([[0],[0],[1],[0],[1],[1]]*(T*batch_size*input_size))
    test_x = test_x[:(T*batch_size*input_size),:]
    probs_out = sess.run(probs, feed_dict={
            x: np.reshape(test_x, [T, batch_size, input_size]),
            init_state: curr_state
        })
    # Get the softmax outputs for the points in the sequence
    # that have [0, 0], [0, 1], [1, 0], [1, 1] as their
    # last two values.
    for i in [1, 2, 3, 5]:
        print('{}: [{:.4f} {:.4f} {:.4f} {:.4f}]'.format(
                [1, 2, 3, 5].index(i), *list(probs_out[i,:]))
             )
</code></pre>

<p>The final output here is</p>

<pre><code>0: [0.4899 0.0007 0.5080 0.0014]
1: [0.0003 0.5155 0.0009 0.4833]
2: [0.5078 0.0011 0.4889 0.0021]
3: [0.0003 0.5052 0.0009 0.4936]
</code></pre>

<p>which indicates that it is only learning to distinguish [0,2] from [1,3]. <strong>Why isn't this model learning to use the previous value in the sequence?</strong></p>
",1
40081697,Getting low test accuracy using Tensorflow batch_norm function,"<p>I am using the official Batch Normalization (BN) function (<a href=""https://github.com/tensorflow/tensorflow/blob/b826b79718e3e93148c3545e7aa3f90891744cc0/tensorflow/contrib/layers/python/layers/layers.py#L100"" rel=""noreferrer"">tf.contrib.layers.batch_norm()</a>) of Tensorflow on the MNIST data. I use the following code for adding BN:</p>

<pre><code>local4_bn = tf.contrib.layers.batch_norm(local4, is_training=True)
</code></pre>

<p>During testing, I change ""is_training=False"" in the above line of code and observe only 20% accuracy. However, it gives ~99% accuracy if I use the  above code also for testing (i.e., keeping is_training=True) with a batch size of 100 images. This observation indicates that the <em>exponential moving average and variance</em> computed by <a href=""https://github.com/tensorflow/tensorflow/blob/b826b79718e3e93148c3545e7aa3f90891744cc0/tensorflow/contrib/layers/python/layers/layers.py#L100"" rel=""noreferrer"">batch_norm()</a> are probably incorrect or I am missing something in my code.</p>

<p>Can anyone please answer about the solution of the above problem.</p>
",0
40143169,Using tensorflow batch normalization stalls loss decrease,"<p>I'm using tensorflow version r0.11
I'm trying to use the batch normalization (tf.contrib.layers.batch_norm()) in a conv net. As a headstart, I followed the discussions made in the following github <a href=""https://github.com/tensorflow/tensorflow/issues/1122"" rel=""nofollow"">issue</a>. It seems that 'is_training', 'reuse' and 'updates_collections' flags are still confusing (in usage), partly because of the lack of good use cases. However, my problem is that the loss is not decreasing if I add batch norm layer.</p>

<p>I framed the code following the structure as in CIFAR. And I am running it in a multi-gpu fashion (for training). I have one script for training (similar to cifar10_multigpu.py) and one for testing (similar to cifar10_eval.py).</p>

<pre><code>for i in xrange(all_flags.num_gpus): # Number of GPUs is 2
    with tf.device('/gpu:%d' % i):
        with tf.name_scope('%s_%d' % (all_flags.TOWER_NAME, i)) as scope:
            # Calculate the loss for one tower of the model. This  function
            # constructs the entire model but shares the variables across all
            # towers.
            loss = _tower_loss(inputs[i], labels[i], scope) 

            # Reuse variables for the next tower. This line makes it possible
            tf.get_variable_scope().reuse_variables()

            # More stuff happening like compute_gradients (inside gpus loop),
            # averaging gradients (outside gpus loop), applying them (outside
            # gpus loop)
</code></pre>

<p>The inference/model building happens within (a nested function in) the function _tower_loss. (below is an example of the function, in reality I use more layers and neurons).</p>

<pre><code>def inference(inputs): #(This gets called from _tower_loss())
    # conv1
    with tf.variable_scope('conv1') as scope:
        kernel = # define kernel
        conv = tf.nn.conv2d(inputs, kernel, strides=[1, 1, 1, 1], padding='SAME')
        biases = _variable_on_gpu('biases', [64], tf.constant_initializer(0.0))
        preactivation = tf.nn.bias_add(conv, biases)
        # ReLU.
        conv1 = tf.nn.relu(preactivation, name=scope.name)

    # pool1
    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1],
                           strides=[1, 2, 2, 1], padding='SAME', name='pool1')

    # Similarly more conv+pool and then fcs and finally logits
    return logits
</code></pre>

<p>I want to perform batch nomalization. So I passed in an additional placeholder input argument inside my '_tower_loss' as well as 'inference' functions.</p>

<pre><code>def inference(inputs, is_training):
    # BN1
    with tf.variable_scope('norm0') as scope:
        # Note that I'm using the dafault for 'updates_collections'
        # which is None
        norm0 = tf.contrib.layers.batch_norm(inputs, is_training=is_training,
                scope=scope, reuse=None)

    # conv1
    with tf.variable_scope('conv1') as scope:
        kernel = # define kernel
        conv = tf.nn.conv2d(norm0, kernel, strides=[1, 1, 1, 1], padding='SAME')
        # Rest is same
</code></pre>

<p>I also added normalization layers in the couple of fc layers</p>

<p>In the train code the instructions go like this
...</p>

<pre><code>variable_averages = tf.train.ExponentialMovingAverage(0.9999, global_step)
variables_averages_op = variable_averages.apply(tf.trainable_variables())

train_op = tf.group(apply_gradient_op, variables_averages_op)
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # Line A
</code></pre>

<p>...</p>

<pre><code>sess.run([train_op, loss, update_ops],feed_dict={is_training: True}) # Line B
</code></pre>

<p>...</p>

<p>When Batch normalization is not there, Line A is not there and in Line B, 'update_ops' is run in the session.</p>

<p>What I'm seeing is that when batch normalization is not used loss starts at aorund 6.5 and consistently decreases to near 0, but when I'm using batch normalization loss does not decrease after 2 or 3 hundred (minibatch) iterations and gets stuck at around 5.5 or so. Speedwise, I'd say the performance is same. I'm not sure what is the issue. I tried with different learning rate (I'm using Adam optimizer) with no effect. I'm not sure whether 'variables_averages_op' along with 'update_ops' is messing things up. Any help will be appreciated.</p>
",0
40187726,Tensorflow PDE Tutorial Questions,"<p>I would like to know how to let Tensorflow only update the specific matrix elements? The following code is from Tensorflow tutorials (<a href=""https://www.tensorflow.org/versions/r0.11/tutorials/pdes/index.html#partial-differential-equations"" rel=""nofollow"">https://www.tensorflow.org/versions/r0.11/tutorials/pdes/index.html#partial-differential-equations</a>).   </p>

<pre><code>  #Import libraries for simulation
  import tensorflow as tf
  import numpy as np

  #Imports for visualization
  import PIL.Image

  def DisplayArray(a, fmt='jpeg', rng=[0,1]):
    """"""Display an array as a picture.""""""
    a = (a - rng[0])/float(rng[1] - rng[0])*255
    a = np.uint8(np.clip(a, 0, 255))
    with open(""fig/image.jpg"",""w"") as f:
        PIL.Image.fromarray(a).save(f, ""jpeg"")

  #sess = tf.Session()
  sess = tf.InteractiveSession()

  # Computational Convenience Functions

  def make_kernel(a):
    """"""Transform a 2D array into a convolution kernel""""""
    a = np.asarray(a)
    a = a.reshape(list(a.shape) + [1,1])
    return tf.constant(a, dtype=1)

  def simple_conv(x, k):
    """"""A simplified 2D convolution operation""""""
    x = tf.expand_dims(tf.expand_dims(x, 0), -1)
    y = tf.nn.depthwise_conv2d(x, k, [1, 1, 1, 1], padding='SAME')
    return y[0, :, :, 0]

  def laplace(x):
    """"""Compute the 2D laplacian of an array""""""
    laplace_k = make_kernel([[0.5, 1.0, 0.5],
                             [1.0, -6., 1.0],
                             [0.5, 1.0, 0.5]])
    return simple_conv(x, laplace_k)

  # Define the PDE

  N = 500

  # Initial Conditions -- some rain drops hit a pond

  # Set everything to zero
  u_init = np.zeros([N, N], dtype=np.float32)
  ut_init = np.zeros([N, N], dtype=np.float32)

  # Some rain drops hit a pond at random points
  for n in range(40):
    a,b = np.random.randint(0, N, 2)
    u_init[a,b] = np.random.uniform()

  DisplayArray(u_init, rng=[-0.1, 0.1])

  # Parameters:
  # eps -- time resolution
  # damping -- wave damping
  eps = tf.placeholder(tf.float32, shape=())
  damping = tf.placeholder(tf.float32, shape=())

  # Create variables for simulation state
  U  = tf.Variable(u_init)
  Ut = tf.Variable(ut_init)

  # Discretized PDE update rules
  U_ = U + eps * Ut
  Ut_ = Ut + eps * (laplace(U) - damping * Ut)

  # Operation to update the state
  step = tf.group(
    U.assign(U_),
    Ut.assign(Ut_))

  # Initialize state to initial conditions
  tf.initialize_all_variables().run()

  # Run 1000 steps of PDE
  for i in range(1000):
    # Step simulation
    step.run({eps: 0.03, damping: 0.04})
    DisplayArray(U.eval(), rng=[-0.1, 0.1])
</code></pre>

<p>In <code>step = tf.group(U.assign(U_),Ut.assign(Ut_))</code>, I would like to know if it is possible to only update the values within U_[1:-1, 1:-1] and Ut_[1:-1, 1:-1], and keep the rest values as constants. </p>

<p>Thank you very much!</p>
",0
40195549,tf.rank function in Tensorflow,"<p>I ma trying to understand tf.rank function in tensorflow. From the documentation <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#rank"" rel=""nofollow"">here</a>, I understood that rank should return the number of distinct elements in the tensor. </p>

<p>Here x and weights are 2 distinct 2*2 tensors with 4 distinct elemnts in each of them. However, rank() function outputs are:</p>

<blockquote>
  <p>Tensor(""Rank:0"", shape=(), dtype=int32) Tensor(""Rank_1:0"", shape=(),
  dtype=int32)</p>
</blockquote>

<p>Also, for the tensor x, I used tf.constant() with dtype = float to convert ndarray into float32 tensor but the rank() still outputs as int32.</p>

<pre><code>g = tf.Graph()
with g.as_default():
    weights = tf.Variable(tf.truncated_normal([2,2]))
    x = np.asarray([[1 , 2], [3 , 4]])
    x = tf.constant(x, dtype = tf.float32)
    y = tf.matmul(weights, x)
    print (tf.rank(x), tf.rank(weights))


with tf.Session(graph = g) as s:
    tf.initialize_all_variables().run()
    print (s.run(weights), s.run(x))
    print (s.run(y))
</code></pre>

<p>How should I interpret the output.</p>
",1
40224495,"Use ""With"" with Tensorflow Sessions","<p>The tutorial says in order to release resources after computation is complete, I can use <code>with tf.Session() as sess</code> as the scope, and the session will be closed after the scope.</p>

<p>However, if I want to make predictions and do some further computations with the model trained in this session, I am not able to do it because the session is closed after the training. What shall I do? Shall I use <code>sess = tf.Session()</code> at the beginning and <code>sess.close()</code> after I finish my job with this model? If so, what is the point of using <code>with</code> statement which does not allow me doing anything after the training?</p>
",0
40272567,How to extract the cell state and hidden state from an RNN model in tensorflow?,"<p>I am new to TensorFlow and have difficulties understanding the RNN module. I am trying to extract hidden/cell states from an LSTM. 
For my code, I am using the implementation from <a href=""https://github.com/aymericdamien/TensorFlow-Examples"" rel=""noreferrer"">https://github.com/aymericdamien/TensorFlow-Examples</a>. </p>

<pre><code># tf Graph input
x = tf.placeholder(""float"", [None, n_steps, n_input])
y = tf.placeholder(""float"", [None, n_classes])

# Define weights
weights = {'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))}
biases = {'out': tf.Variable(tf.random_normal([n_classes]))}

def RNN(x, weights, biases):
    # Prepare data shape to match `rnn` function requirements
    # Current data input shape: (batch_size, n_steps, n_input)
    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)

    # Permuting batch_size and n_steps
    x = tf.transpose(x, [1, 0, 2])
    # Reshaping to (n_steps*batch_size, n_input)
    x = tf.reshape(x, [-1, n_input])
    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)
    x = tf.split(0, n_steps, x)

    # Define a lstm cell with tensorflow
    #with tf.variable_scope('RNN'):
    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)

    # Get lstm cell output
        outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)

    # Linear activation, using rnn inner loop last output
    return tf.matmul(outputs[-1], weights['out']) + biases['out'], states

pred, states = RNN(x, weights, biases)

# Define loss and optimizer
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

# Evaluate model
correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
# Initializing the variables
init = tf.initialize_all_variables()
</code></pre>

<p>Now I want to extract the cell/hidden state for each time step in a prediction. The state is stored in a LSTMStateTuple of the form (c,h), which I can find out by evaluating <code>print states</code>. However, trying to call <code>print states.c.eval()</code> (which according to the documentation should give me values in the tensor <code>states.c</code>), yields an error stating that my variables are not initialized even though I am calling it right after I am predicting something. The code for this is here: </p>

<pre><code># Launch the graph
with tf.Session() as sess:
    sess.run(init)
    step = 1
    # Keep training until reach max iterations
    for v in tf.get_collection(tf.GraphKeys.VARIABLES, scope='RNN'):
        print v.name
    while step * batch_size &lt; training_iters:
        batch_x, batch_y = mnist.train.next_batch(batch_size)
        # Reshape data to get 28 seq of 28 elements
        batch_x = batch_x.reshape((batch_size, n_steps, n_input))
        # Run optimization op (backprop)
        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})

        print states.c.eval()
        # Calculate batch accuracy
        acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})

        step += 1
    print ""Optimization Finished!""
</code></pre>

<p>and the error message is </p>

<pre><code>InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype float
     [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
</code></pre>

<p>The states are also not visible in <code>tf.all_variables()</code>, only the trained matrix/bias tensors (as described here: <a href=""https://stackoverflow.com/questions/39168025/tensorflow-show-or-save-forget-gate-values-in-lstm"">Tensorflow: show or save forget gate values in LSTM</a>). I don't want to build the whole LSTM from scratch though since I have the states in the <code>states</code> variable, I just need to call it. </p>
",0
40305692,How to learn multi-class multi-output CNN with TensorFlow,"<p>I want to train a convolutional neural network with TensorFlow to do multi-output multi-class classification.</p>

<p>For example: If we take the MNIST sample set and always combine two random images two a single one and then want to classify the resulting image. The result of the classification should be the two digits shown in the image. </p>

<p>So the output of the network could have the shape [-1, 2, 10] where the first dimension is the batch, the second represents the output (is it the first or the second digit) and the third is the ""usual"" classification of the shown digit. </p>

<p>I tried googling for this for a while now, but wasn't able find something useful. Also, I don't know if multi-output multi-class classification is the correct naming for this task. If not, what is the correct naming? Do you have any links/tutorials/documentations/papers explaining what I'd need to do to build the loss function/training operations?</p>

<p>What I tried was to split up the output of the network into the single outputs with tf.split and then use softmax_cross_entropy_with_logits on every single output. The result I averaged over all outputs but it doesn't seem to work. Is this even a reasonable way?</p>
",0
40330775,Compute efficiently a pairwise ranking loss function in Tensorflow,"<p>I'm currently implementing <a href=""http://www.aclweb.org/anthology/P15-1061"" rel=""nofollow"">http://www.aclweb.org/anthology/P15-1061</a> in tensorflow.</p>

<p>I have implemented the pairwise ranking loss function (section 2.5 of the paper) as follow :</p>

<pre><code>s_theta_y = tf.gather(tf.reshape(s_theta, [-1]), y_true_index)
s_theta_c_temp = tf.reshape(tf.gather(tf.reshape(s_theta, [-1]), y_neg_index), [-1, classes_size])
s_theta_c = tf.reduce_max(s_theta_c_temp, reduction_indices=[1])
</code></pre>

<p>I had to use tf.gather rather than tf.gather_nd because the latter is not yet implemented with gradient descent. I also had to transform all indices to be correct with the flatten matrix.</p>

<p>If tf.gather_nd was implemented with gradient descent, my code would have been as follow :</p>

<pre><code>s_theta_y = tf.gather_nd(s_theta, y_t_index)
s_theta_c_temp = tf.gather_nd(s_theta, y_neg_index)
s_theta_c = tf.reduce_max(s_theta_c_temp, reduction_indices=[1])
</code></pre>

<p>s_theta is the computed score for each class label, as in the paper.
y_true_index contains the index of the true class, in order to compute s_theta_y. y_neg_index is the index of all the negative classes, its dimensions is either #class-1 or #class is the relation is classified as other.</p>

<p>However, several sentences are classified as Other and so, s_theta_y
doesn't exist and we shouldn't take it into account for the calculation. To handle such case, I have a constant factor of 0 which cancel the term, and to have the same dimension vector for the negative class, I just copy a random value of the index, because at the end, we are only interested in the max value among all negative classes (and not the index).</p>

<p>Is there a more efficient way to compute those terms in the loss function ? I have the impression that using tf.gather with so much reshape is very slow</p>
",0
40404519,What is the intended use for tf.contrib.framework functions?,"<p>See the topic, but I am especially interested what is the functional difference between tf.contrib.framework.variable() and tf.get_variable()? The documentation for tf.contrib.framework is not very informative.</p>
",1
40451974,"Tensorflow, restore variables in a specific device","<p>Maybe my question is a bit naive, but I really didn't find anything in the tensorflow documentation.</p>

<p>I have a trained tensorflow model where the variables of it was placed in the GPU. Now I would like to restore this model and test it using the CPU.</p>

<p>If I do this via 'tf.train.Saver.restore` as in the example:
<code>
 saver = tf.train.import_meta_graph(""/tmp/graph.meta"")
 saver.restore(session, ""/tmp/model.ckp"")
</code></p>

<p>I have the following excpetion:</p>

<p><code>
InvalidArgumentError: Cannot assign a device to node 'b_fc8/b_fc8/Adam_1': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0
</code></p>

<p>How can I make restore these variables in the <code>CPU</code>?</p>

<p>Thanks</p>
",1
40491470,TensorFlow: shape rank 2 in tf.matrix_inverse(),"<p>I have a problem when I try the tf.matrix_inverse() method. </p>

<p>I have a Tensor of dimension [17,400,400]. And we can read in the API documentation:</p>

<blockquote>
  <p>The input is a tensor of shape [..., M, M] whose inner-most 2 dimensions form square matrices. </p>
</blockquote>

<p>In my case M=400. And when I try to calculate the 17 inverse matrix: </p>

<pre><code>Out[56]: L
Out[57]: &lt;tf.Tensor 'while_4/Exit_1:0' shape=(17, 400, 400) dtype=float32&gt;
Out[59]: InvL = tf.matrix_inverse(L)
        ValueError: Shape (17, 400, 400) must have rank 2
</code></pre>

<p>I understand that L have a shape rank 2, but the method fails. I don't know if I'm doing something wrong. Any help?</p>
",0
40569758,How to use a CSV as input data for a Tensorflow neural network?,"<p>I'm currently attempting to write a neural network by slightly changing the <a href=""https://www.tensorflow.org/versions/r0.11/tutorials/mnist/beginners/index.html#mnist-for-ml-beginners"" rel=""nofollow noreferrer"">MNIST for ML Beginners code</a>. I have a CSV that's organized like this:</p>

<p><code>Image_Name     |Nevus?  |Dysplastic Nevus?|  Melanoma?
asdfgjkgdsl.png |1       |0                |0
</code></p>

<p>An image name, and it's one-hot result. Each image is 1022 x 767, and I'd like to use the color of each pixel as an input as well. As such, I changed the MNIST code to have 2,351,622 inputs (1022 pixels wide * 767 pixels high * 3 colors per pixel) and 3 outputs.</p>

<pre><code># from tensorflow.examples.tutorials.mnist import input_data
# mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)

def main():
    x = tf.placeholder(tf.float32, [None, 2351622])
    W = tf.Variable(tf.zeroes([2351622, 3]))
    b = tf.Variable(tf.zeroes([3]))

    y = tf.nn.softmax(tf.matmul(x, W) + b)

    y_ = tf.placeholder(tf.float32, [None, 3])
    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))
    train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)

    init = tf.initialize_all_variables()
    sess = tf.Session()
    sess.run(init)

    for i in range(1000):
    example, label = sess.run([features, col5])
        # batch_xs, batch_ys = mnist.train.next_batch(100)
        # sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})

    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
</code></pre>

<p>The commented lines are the ones that I have to replace to get my data loaded into the neural network. The easiest way to get the 2.3M inputs for each image (that I've found) is to:</p>

<pre><code>from PIL import Image
import numpy as np

list(np.array(Image.open('asdfgjkgdsl.png')).ravel().flatten())
</code></pre>

<p>How can I load this dataset into tensorflow to be used for training a neural network?</p>
",0
40698709,Tensorflow: Interpretation of Weight in Weighted Cross Entropy,"<p>The Tensorflow function <code>tf.nn.weighted_cross_entropy_with_logits()</code> takes the argument <code>pos_weight</code>. The <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#weighted_cross_entropy_with_logits"" rel=""noreferrer"">documentation</a> defines <code>pos_weight</code> as <em>""A coefficient to use on the positive examples</em>."" I assume this means that increasing <code>pos_weight</code> increases the loss from false positives and decreases the loss from false negatives. Or do I have that backwards?</p>
",1
40731433,Understanding tf.extract_image_patches for extracting patches from an image,"<p>I found the following method <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/array_ops.html#extract_image_patches"" rel=""noreferrer"">tf.extract_image_patches</a> in tensorflow API, but I am not clear about its functionality. </p>

<p>Say the <code>batch_size = 1</code>, and an image is of size <code>225x225x3</code>, and we want to extract patches of size <code>32x32</code>. </p>

<p>How exactly does this function behave? Specifically, the documentation mentions the dimension of the output tensor to be <code>[batch, out_rows, out_cols, ksize_rows * ksize_cols * depth]</code> , but what <code>out_rows</code> and <code>out_cols</code> are is not mentioned.</p>

<p>Ideally, given an input image tensor of size <code>1x225x225x3</code> (where 1 is the batch size), I want to be able to get <code>Kx32x32x3</code> as output, where <code>K</code> is the total number of patches and <code>32x32x3</code> is the dimension of each patch. Is there something in tensorflow that already achieves this?</p>
",1
40824149,Complete Tensorflow usage for training from Iris CSV data,"<p>I'm looking to use Tensorflow to train a neural network model for classification, and I want to read data from a CSV file, such as the Iris data set.</p>

<p>The <a href=""https://www.tensorflow.org/versions/r0.10/tutorials/tflearn/index.html#tf-contrib-learn-quickstart"" rel=""nofollow noreferrer"">Tensorflow documentation</a> shows an example of loading the Iris data and building a prediction model, but the example uses the high-level <code>tf.contrib.learn</code> API. I want to use the low-level Tensorflow API and run gradient descent myself. How would I do that?</p>
",1
40846881,"How do I finde the Code of ""tf.nn.dynamic_rnn"" in the tensorflow repository?","<p>I'm trying to understand the structure and the coding of tensorflow. While going through this tutorial ""<a href=""https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/"" rel=""nofollow noreferrer"">https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/</a>"" I have searched for the code that is used in the functions.
For example the line</p>

<pre><code>cell = GRUCell(num_neurons)  # Or LSTMCell(num_neurons)
</code></pre>

<p>uses the function GRUCell, which I can find in the file ""rnn_cell.py"" in the tensorflow repository. Furthermore the GRUCell is wrapped by a function called ""tf.nn.dynamic_rnn"" as follows:</p>

<pre><code>output, state = tf.nn.dynamic_rnn(cell, data, dtype=tf.float32)
</code></pre>

<p>Unfortunately I am not able to find the code for this function. Where do I find it? Everything I find is this documentation:</p>

<p><a href=""https://github.com/tensorflow/tensorflow/blob/eb56a8af24695bf8258addf28b0c53fbabff72e6/tensorflow/g3doc/api_docs/python/functions_and_classes/shard8/tf.nn.dynamic_rnn.md"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/eb56a8af24695bf8258addf28b0c53fbabff72e6/tensorflow/g3doc/api_docs/python/functions_and_classes/shard8/tf.nn.dynamic_rnn.md</a></p>

<p>Thanks for helping!</p>
",0
40907769,How to get current TensorFlow name scope,"<p>I create relative name scopes with <code>tf.name_scope</code>.</p>

<p>How can I get the current absolute name scope?</p>

<p>From the code, it looks like <code>tf.get_default_graph()._name_stack</code> would give me that but that looks like a non-official way. Is there any official way? (I think not, thus I made an <a href=""https://github.com/tensorflow/tensorflow/issues/6010"" rel=""noreferrer"">upstream feature request</a>.)</p>

<p>(I implemented a bunch of functions like <code>get_current_name_scope()</code> or <code>reuse_name_scope()</code> <a href=""https://github.com/rwth-i6/returnn/blob/master/TFUtil.py"" rel=""noreferrer"">here</a>. Note that you need to be careful when mixing <code>tf.name_scope</code> and <code>tf.variable_scope</code>.)</p>
",0
40910834,"How to duplicate input tensors conditional on a tensor attribute (""oversampling"") in a Tensorflow queue?","<p>I am porting the Tensorflow Cifar-10 tutorial files for my own purpose, and have run into an interesting problem that I can not easily conceptualize due to the graph and session architecture of Tensorflow.</p>

<p>The issue is that my input dataset is highly imbalanced, and as such I need to ""oversample"" (and augment) certain images in the input pipeline conditional on their labels.  In a normal Python environmental I could set up a simple control flow statement of the form <code>if label then duplicate</code>, but I am not able to write the same syntax in Tensorflow due to the control flow operation existing outside of a running session and <code>label</code> in this case does not return a value.</p>

<p>My question is, what is the easiest method for oversampling a tensor inside of a Tensorflow queue?</p>

<p>I know that I could simply duplicate the data of interest prior to the input operation, but this obviously removes any storage savings incurred by oversampling during runtime.</p>

<p>What I want to do is evaluate a Tensor's label (in the Cifar-10 case, by checking the 1D image.label attribute) and then duplicate that Tensor by a fixed factor (say, 4x if the label is ""dog"") and send all the Tensors down to the batching operation.  My original approach was to attempt the duplication step after the Reader operation and before the batching operation, but this too is outside of a running session.  I was thinking of utilizing TF's <code>while</code> control flow statement but I'm not sure this will function has the ability to do anything other than <em>modify</em> the input Tensor.  What do you think?</p>

<hr>

<p>Update #1</p>

<p>Basically I attempted to create a py_func() that took in the flattened image bytes and the label bytes, and vertically stack the same image bytes N times depending on the value of the label and then return that as a (N x image_bytes) tensor (py_func() auto converted the input tensor to numpy and back). i attempted to create an input_queue from the variable-height tensor whose shape reports as (?,image_bytes) and then instantiate a reader to rip off image_byte size records. Well it seems like you can't build queues off of unknown data sizes so this approach is not working for me which makes sense in hindsight but I still can't conceptualize a method for identifying a record in a queue, and repeating that record a specific number of times.</p>

<hr>

<p>Update #2</p>

<p>Well after 48 hours I finally figured out a workaround, thanks to <a href=""https://stackoverflow.com/questions/38484075/online-oversampling-in-tensorflow-input-pipeline"">this SO thread</a> that I was able to dig up.  The solution outlined in that thread only assumes 2 classes of data though, so the <code>tf.cond()</code> function suffices to oversample one class if the <code>pred</code> is True, and to oversample the other if <code>pred</code> is False.  In order to have an n-way conditional, I attempted to institute a <code>tf.case()</code> function that resulted in <code>ValueError: Cannot infer Tensor's rank</code>.  Turns out that the <code>tf.case()</code> function does not retain <code>shape</code> properties and the graph construction fails as any batching op at the end of the input pipeline must take a shape argument, or take tensors of defined shape, as per this note in the <a href=""https://www.tensorflow.org/versions/r0.11/api_docs/python/io_ops.html#shuffle_batch"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p>N.B.: You must ensure that either (i) the shapes argument is passed, or (ii) all of the tensors in tensors must have fully-defined shapes. ValueError will be raised if neither of these conditions holds.</p>
</blockquote>

<p>Further digging shows that this is a <a href=""https://github.com/tensorflow/tensorflow/issues/3334"" rel=""nofollow noreferrer"">known issue</a> with <code>tf.case()</code> that has yet to be resolved as of December 2016.  Just one of the many control-flow head-scratchers in Tensorflow.  Anyway, my stripped-down solution to the n-way oversampling issue is thus:</p>

<pre><code># Initiate a queue of ""raw"" input data with embedded Queue Runner.
queue = tf.train.string_input_producer(rawdata_filename) 

# Instantiate Reader Op to read examples from files in the filename queue.
reader = tf.FixedLengthRecordReader(record_bytes)

# Pull off one instance, decode and cast image and label to 3D, 1D Tensors.
result.key, value = reader.read(queue)
image_raw, label_raw = decode(value)
image = tf.cast(image_raw, dtype) #3D tensor
label = tf.cast(label_raw, dtype) #1D tensor

# Assume your oversampling factors per class are fixed
# and you have 4 classes.
OVERSAMPLE_FACTOR = [1,2,4,10]

# Now we need to reshape input image tensors to 4D, where the 
# first dimension is the image number in a batch of oversampled tensors.
# images = tf.expand_dims(image, 0) # so, (*,height,width,channels) in 4D

# Set up your predicates, which are 1D boolean tensors.
# Note you will have to squash the boolean tensors to 0-dimension.
# This seems illogical to me, but it is what it is.
pred0 = tf.reshape(tf.equal(label, tf.convert_to_tensor([0])), []) #0D tf.bool
pred1 = tf.reshape(tf.equal(label, tf.convert_to_tensor([1])), []) #0D tf.bool
pred2 = tf.reshape(tf.equal(label, tf.convert_to_tensor([2])), []) #0D tf.bool
pred3 = tf.reshape(tf.equal(label, tf.convert_to_tensor([3])), []) #0D tf.bool

# Build your callables (functions) that vertically stack an input image and
# label tensors X times depending on the accompanying oversample factor.
def f0(): return tf.concat(0, [images]*OVERSAMPLE_FACTOR[0]), tf.concat(0, [label]*OVERSAMPLE_FACTOR[0])
def f1(): return tf.concat(0, [images]*OVERSAMPLE_FACTOR[1]), tf.concat(0, [label]*OVERSAMPLE_FACTOR[1])
def f2(): return tf.concat(0, [images]*OVERSAMPLE_FACTOR[2]), tf.concat(0, [label]*OVERSAMPLE_FACTOR[2])
def f3(): return tf.concat(0, [images]*OVERSAMPLE_FACTOR[3]), tf.concat(0, [label]*OVERSAMPLE_FACTOR[3])

# Here we have N conditionals, one for each class.  These are exclusive
# but due to tf.case() not behaving every conditional gets evaluated.
[images, label] = tf.cond(pred0, f0, lambda: [images,label])
[images, label] = tf.cond(pred1, f1, lambda: [images,label])
[images, label] = tf.cond(pred2, f2, lambda: [images,label])
[images, label] = tf.cond(pred3, f3, lambda: [images,label])

# Pass the 4D batch of oversampled tensors to a batching op at the end
# of the input data queue.  The batching op must be set up to accept
# batches of tensors (4D) as opposed to individual tensors (in our case, 3D).
images, label_batch = tf.train.batch([images, label],
                                     batch_size=batch_size,
                                     num_threads=num_threads,
                                     capacity=capacity,
                                     enqueue_many = True) #accept batches
</code></pre>
",0
41048819,How to restore a model by filename in Tensorflow r12?,"<p>I have run the distributed mnist example:
<a href=""https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/tools/dist_test/python/mnist_replica.py"" rel=""noreferrer"">https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/tools/dist_test/python/mnist_replica.py</a></p>

<p>Though I have set the </p>

<p><code>saver = tf.train.Saver(max_to_keep=0)</code></p>

<p>In previous release, like r11, I was able to run over each check point model and evaluate the precision of the model. This gave me a plot of the progress of the precision versus global steps (or iterations). </p>

<p>Prior to r12, tensorflow checkpoint models were saved in two files, <code>model.ckpt-1234</code> and <code>model-ckpt-1234.meta</code>. One could restore a model by passing the <code>model.ckpt-1234</code> filename like so <code>saver.restore(sess,'model.ckpt-1234')</code>. </p>

<p>However, I've noticed that in r12, there are now three output files <code>model.ckpt-1234.data-00000-of-000001</code>, <code>model.ckpt-1234.index</code>, and <code>model.ckpt-1234.meta</code>. </p>

<p>I see that the the restore documentation says that a path such as <code>/train/path/model.ckpt</code> should be given to restore instead of a filename. Is there any way to load one checkpoint file at a time to evaluate it? I have tried passing the <code>model.ckpt-1234.data-00000-of-000001</code>, <code>model.ckpt-1234.index</code>, and <code>model.ckpt-1234.meta</code> files, but get errors like below:</p>

<p><code>W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open logdir/2016-12-08-13-54/model.ckpt-0.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?</code></p>

<p><code>NotFoundError (see above for traceback): Tensor name ""hid_b"" not found in checkpoint files logdir/2016-12-08-13-54/model.ckpt-0.index
     [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_recv_save/Const_0, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]</code></p>

<p><code>W tensorflow/core/util/tensor_slice_reader.cc:95] Could not open logdir/2016-12-08-13-54/model.ckpt-0.meta: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?</code></p>

<p>I'm running on OSX Sierra with tensorflow r12 installed via pip.</p>

<p>Any guidance would be helpful.</p>

<p>Thank you.</p>
",0
41125183,Tensorflow: split_v with variable num_splits,"<p>I am wondering if the same holds for <code>tf.split_v()</code> as <code>tf.split()</code>.</p>

<p>According to the documentation <code>split_v</code> also accepts a Tensor as second argument.</p>

<p>However, when I try this code</p>

<pre><code>batch_size_ph = tf.placeholder(dtype = tf.int32, name='BatchSize')
seq_length_ph = tf.placeholder(dtype = tf.int32, name='SeqLength')
input_data = tf.placeholder(dtype=tf.float32, shape=[50, 25, 10])


inputs = tf.split_v(input_data,seq_length_ph, 1) #tf.ones(seq_length_ph, tf.int32)
#inputs = [tf.squeeze(input_, [1]) for input_ in inputs]

with tf.Session() as sess:
    [out] = sess.run([inputs],feed_dict = {batch_size_ph: 50,
                                           seq_length_ph: 25,
                                           input_data: np.random.rand(50,25,10)})

print out
print len(out)
print out[0].shape
</code></pre>

<p>The error is</p>

<blockquote>
  <p>NameError: global name 'value_shape' is not defined</p>
</blockquote>

<p>Is this possible or not?</p>
",1
41302986,tensorflow cnn error: InvalidArgumentError (see above for traceback): logits and labels must be same size,"<p>I wrote this code by modified from tensorflow official tutorial.
I had a network as following:</p>

<pre><code>def weight_variable(shape):
    initial = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(initial)

def bias_variable(shape):
    initial = tf.constant(0.1, shape=shape)
    return tf.Variable(initial)

def conv2d(x, w):
    return tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME')

def max_pool_2x2(x):
    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')

import tensorflow as tf
import numpy as np

train_feature = np.array(model.dataset[0])
train_label = np.array(model.labelset[0])
print(train_feature.shape)
print(train_label.shape)

x_placeholder = tf.placeholder(tf.float32, shape=[None, train_feature.shape[1]])
y_placeholder = tf.placeholder(tf.float32, shape=[None, 8])
# network structure
w_conv1 = weight_variable([5, 5, 1, 32])
b_conv1 = bias_variable([32])
x_image = tf.reshape(x_placeholder, [-1, 160, 120, 1])
h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)
h_pool1 = max_pool_2x2(h_conv1)

w_conv2 = weight_variable([5, 5, 32, 64])
b_conv2 = bias_variable([64])

h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)
h_pool2 = max_pool_2x2(h_conv2)

w_fc1 = weight_variable([320, 1024])
b_fc1 = bias_variable([1024])

h_pool2_flat = tf.reshape(h_pool2, [-1, 320])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)

keep_prob = tf.placeholder(tf.float32)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

w_fc2 = weight_variable([1024, 8])
b_fc2 = bias_variable([8])

y_conv = tf.matmul(h_fc1_drop, w_fc2) + b_fc2

cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_placeholder))
train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
sess = tf.InteractiveSession()
sess.run(tf.global_variables_initializer())
batch = (train_feature, np.eye(8)[train_label])
train_step.run(feed_dict={x_placeholder: batch[0], y_placeholder: batch[1], keep_prob: 0.5})
</code></pre>

<p>Error is following:</p>

<pre><code>cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_placeholder))
File ""/Users/lintseju/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py"", line 1449, in softmax_cross_entropy_with_logits
precise_logits, labels, name=name)
File ""/Users/lintseju/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 2265, in _softmax_cross_entropy_with_logits
features=features, labels=labels, name=name)
File ""/Users/lintseju/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
op_def=op_def)
File ""/Users/lintseju/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
original_op=self._default_original_op, op_def=op_def)
File ""/Users/lintseju/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[2400,8] labels_size=[10,8]
 [[Node: SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Reshape_2, Reshape_3)]]
</code></pre>

<p>train_feature is (10, 19200) numpy array, and train label is (10,) numpy array. Anyone know why logits_size=[2400,8] ?</p>
",0
41353079,Tensorflow image.central_crop (mis)behavior,"<p>In the Tensorflow documentation for <a href=""https://www.tensorflow.org/api_docs/python/image/cropping#central_crop"" rel=""nofollow noreferrer"">tf.image.central_crop</a> function:</p>

<pre><code>Remove the outer parts of an image but retain the central region of
the image along each dimension. If we specify central_fraction = 0.5,
this function returns the region marked with ""X"" in the below diagram.

 --------
|        |
|  XXXX  |
|  XXXX  |
|        |   where ""X"" is the central 50% of the image.
 --------
</code></pre>

<p>Consider the following code:</p>

<pre><code>In [2]: import tensorflow as tf
In [3]: image_raw = tf.placeholder(tf.string)
In [4]: image = tf.image.decode_jpeg(image_raw, channels=3)
In [5]: crop = tf.image.central_crop(image, central_fraction=0.5)
In [6]: init_op = tf.global_variables_initializer()
In [7]: sess = tf.Session()
In [8]: sess.run(init_op)
In [9]: image_np, crop_np = sess.run([image, crop],
   ...:     feed_dict={image_raw: open(""/tmp/test.jpg"", 'rb').read()})
In [10]: image_np.shape
Out[10]: (456, 450, 3)
</code></pre>

<p>Original image size is 456x450</p>

<pre><code>In [11]: crop_np.shape
Out[11]: (228, 226, 3)
</code></pre>

<p>Crop size is 228x226</p>

<p>Which gives area ratio of:</p>

<pre><code>In [12]: 228*226 / (456*450.)
Out[12]: 0.2511111111111111
</code></pre>

<p>Not 0.5 as I expected. Can someone help to clarify this?</p>
",1
41435673,syntaxnet ./configure error,"<p>I was trying to using syntaxnet and I have finished most of processes. Upgrade bazel version to 0.43 in case of errors (Ubuntu 16.04 Ver, Anaconda python 2.7).
However, I am having a troubles with ./configure part. I am reading the official instruction via tensorflow github.</p>

<pre><code>git clone --recursive https://github.com/tensorflow/models.git
cd models/syntaxnet/tensorflow
**./configure**
cd ..
bazel test syntaxnet/... util/utf8/...
# On Mac, run the following:
bazel test --linkopt=-headerpad_max_install_names \
  syntaxnet/... util/utf8/...
</code></pre>

<p>Following logs will help you to understand what’s going on my machine.  Thanks for the advice </p>

<pre><code>Please specify the location of python. [Default is /home/ryan/anaconda2/bin/python]: 
Do you wish to build TensorFlow with Google Cloud Platform support? [y/N] n
No Google Cloud Platform support will be enabled for TensorFlow
Do you wish to build TensorFlow with Hadoop File System support? [y/N] n
No Hadoop File System support will be enabled for TensorFlow
Found possible Python library paths:
  /home/ryan
  /home/ryan/pynaoqi-python2.7
  /home/ryan/anaconda2/lib/python2.7/site-packages
Please input the desired Python library path to use.  Default is [/home/ryan]
/home/ryan/anaconda2/lib/python2.7/site-packages
Do you wish to build TensorFlow with GPU support? [y/N] y
GPU support will be enabled for TensorFlow
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 8.0
Please specify the location where CUDA 8.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 5.0
Please specify the location where cuDNN 5.0 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Invalid path to cuDNN  toolkit. Neither of the following two files can be found:
/usr/local/cuda-8.0/lib64/libcudnn.so.5.0
/usr/local/cuda-8.0/libcudnn.so.5.0
.5.0
Please specify the Cudnn version you want to use. [Leave empty to use system default]: 
Please specify the location where cuDNN  library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
libcudnn.so resolves to libcudnn.5
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size.
[Default is: ""3.5,5.2""]: 
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading options for 'clean' from /home/ryan/git_ryan/models/syntaxnet/tensorflow/tools/bazel.rc:
  Inherited 'build' options: --force_python=py2 --host_force_python=py2 --python2_path=/home/ryan/anaconda2/bin/python --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --define PYTHON_BIN_PATH=/home/ryan/anaconda2/bin/python --spawn_strategy=standalone --genrule_strategy=standalone
**INFO: Reading options for 'clean' from /etc/bazel.bazelrc:
  Inherited 'build' options: --action_env=PATH --action_env=LD_LIBRARY_PATH --action_env=TMPDIR --test_env=PATH --test_env=LD_LIBRARY_PATH
Unrecognized option: --action_env=PATH
ERROR: /home/ryan/git_ryan/models/syntaxnet/tensorflow/tensorflow/tensorflow.bzl:568:26: Traceback (most recent call last):
    File ""/home/ryan/git_ryan/models/syntaxnet/tensorflow/tensorflow/tensorflow.bzl"", line 562
        rule(attrs = {""srcs"": attr.label_list...""), &lt;3 more arguments&gt;)}, &lt;2 more arguments&gt;)
    File ""/home/ryan/git_ryan/models/syntaxnet/tensorflow/tensorflow/tensorflow.bzl"", line 568, in rule
        attr.label_list(cfg = ""data"", allow_files = True)
expected ConfigurationTransition or NoneType for 'cfg' while calling label_list but got string instead: data.
ERROR: com.google.devtools.build.lib.packages.BuildFileContainsErrorsException: error loading package '': Extension file 'tensorflow/tensorflow.bzl' has errors.
Configuration finished**
</code></pre>
",0
41437483,How does tf.nn.conv2d calculate its values?,"<p>I've looked at the <a href=""https://www.tensorflow.org/versions/master/api_docs/python/nn/convolution"" rel=""nofollow noreferrer"">documentation</a> for <code>tf.nn.conv2d</code> but it didn't really help much. So I tried to multiply the first 4 values of my input array that form a square <code>(0, 1, 2, 2.5)</code> with the first column of the <code>filter_weights</code> array <code>(0.19041163, -0.36705261, 0.69018674, 1.7655524)</code>. But regardless of how I multiply these values I'm not getting <code>1.13938534</code>, I don't know what I'm doing wrong. </p>

<p>Below I have the code that I used.</p>

<p>Given an array:</p>

<pre><code>x = np.array([
[0, 1, 0.5, 10],
[2, 2.5, 1, -8],
[4, 0, 5, 6],
[15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))

X = tf.constant(x)
</code></pre>

<p>And weights:</p>

<pre><code>filter_weights = tf.truncated_normal([2,2,1,3])
</code></pre>

<p>Which prints:</p>

<pre><code>[[[[ 0.19041163  0.59322315  0.27544078]]

  [[-0.36705261 -1.18738699  1.45393717]]]


 [[[ 0.69018674  1.08702338 -1.15911126]]

  [[ 1.76255524 -1.42660797 -0.18624328]]]]
</code></pre>

<p>How does:</p>

<pre><code>strides = [1, 2, 2, 1]
padding = ""SAME""
tf.nn.conv2d(input, F_W, strides, padding)
</code></pre>

<p>Give:</p>

<pre><code> [[[[  1.13938534  -5.06340981  -4.23629761]
   [-18.01530266  -4.67841816  16.20156288]]

  [[ -9.49576092  -4.87288237   6.77371216]
   [ -0.57718992 -12.70932484   0.89242649]]]]
</code></pre>
",1
41467115,"Using word2vec pretrained vectors, how to generate ids of a sentence as input to tf.nn.embedding_lookup function in tensorflow?","<p>To extract the embedding representations of input data, the tensorflow documentation says we can use the following:</p>

<pre><code>embed = tf.nn.embedding_lookup(embeddings, input_data)
</code></pre>

<p>Accdg to the <a href=""https://www.tensorflow.org/api_docs/python/nn/embeddings#embedding_lookup"" rel=""nofollow noreferrer"">TF documentation</a>, the 2nd parameter of the function tf.nn.embedding_lookup is a tensor of ids:</p>

<blockquote>
  <p>ids: A Tensor with type int32 or int64 containing the ids to be looked up in params.</p>
</blockquote>

<p>My question is: Given a sentence, say, </p>

<blockquote>
  <p>""Welcome to the world""</p>
</blockquote>

<p>how can I represent and transform it into <code>ids</code>? In the code below, how can I transform my sentence into <code>input_data</code>. </p>

<pre><code>from gensim import models
embedding_path = ""../embeddings/GoogleNews-vectors-negative300.bin""
w = models.Word2Vec.load_word2vec_format(embedding_path, binary=True)
X = w.syn0
W = tf.Variable(tf.constant(0.0, shape=X.shape),trainable=False, name=""W"")
embedding_placeholder = tf.placeholder(tf.float32, X.shape)
embedding_init = W.assign(embedding_placeholder)
embed = tf.nn.embedding_lookup(embedding_init, input_data)
sess = tf.Session()
sess.run(embed, feed_dict={embedding_placeholder: X})
</code></pre>
",1
41539658,Tensorflow error when I try to use tf.contrib.layers.convolution2d,"<p>When I invoke tf.contrib.layers.convolution2d the tensorflow execution terminates with an error about one of the parameters used</p>

<pre><code>got an unexpected keyword argument 'weight_init'
</code></pre>

<p>The parameter passed are the follows:</p>

<pre><code>layer_one = tf.contrib.layers.convolution2d(
    float_image_batch,
    num_output_channels=32,     
    kernel_size=(5,5),          
    activation_fn=tf.nn.relu,
    weight_init=tf.random_normal,
    stride=(2, 2),
    trainable=True)
</code></pre>

<p>That is exactly as described in the book that I'm reading. I suspect a possible syntax problem with <code>weight_init=tf.random_normal</code> written directly inside the call, but I don't know how to fix. I'm using Tensorflow 0.12.0 </p>
",0
41602374,tf.zeros doesn't return a 1D tensor?,"<p>I'm trying to duplicate a tensor across a new axis, like this:</p>

<pre><code>original_tensor = tf.constant([1,2,3,4,5])
made_copies_tensor = tf.tile(original_tensor, 5)
final_result = tf.reshape([5,5])
</code></pre>

<p>However I'm getting this error:</p>

<pre><code>File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py"", line 650, in with_rank
raise ValueError(""Shape %s must have rank %d"" % (self, rank))
ValueError: Shape () must have rank 1
</code></pre>

<p>In the documentation it says the way I wrote tf.constant is supposed to have it return a 1D tensor but when I checked its shape with get_shape(), it has (5,) as its shape. I tried reshaping it but nothing changed.</p>

<p>Why am I getting this error? Thanks.</p>
",1
41666964,model variables in Tensorflow's batch_norm,"<p>The documentation online says moving_average and moving_variance are both model_variables, and tf.model_variables() returns tensors of the type local_variables. Does that mean model_variables are not saved when I save my state?</p>

<p>I'm trying to apply batch normalization to a couple of 3D convolution and fully connected layers. I trained my network with batch_norm and saved a checkpoint file, but when I went to restore my saved state, it said moving_mean could not be located. The exact error was, when TF went to assign the restored value to moving_mean, the shape of the lhs tensor, [], could not be reconciled with the that of the rhs, [20].</p>

<p>The graph restores fine when I don't add batch_norm around my layers. 
I'm planning to add a global variable at the end of training that saves my moving_mean and moving_variance values. Is this the way TF intended for me to use batch_norm?</p>

<p>Thanks!</p>
",1
41673889,TensorFlow: does tf.train.batch automatically load the next batch when the batch has finished training?,"<p>For instance, after I have created my operations, fed the batch data through the operation and run the operation, does tf.train.batch automatically feed in another batch of data to the session?</p>

<p>I ask this because tf.train.batch has an attribute of <code>allow_smaller_final_batch</code> which makes it possible for the final batch to be loaded as a size lesser than the indicated batch size. Does this mean even without a loop, the next batch could be automatically fed? From the tutorial codes I am rather confused. When I load a single batch, I get literally a single batch size of shape [batch_size, height, width, num_channels], but the <a href=""https://www.tensorflow.org/api_docs/python/io_ops/input_pipeline#batch"" rel=""noreferrer"">documentation</a> says it <code>Creates batches of tensors in tensors.</code> Also, when I read the tutorial code in the <a href=""https://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb"" rel=""noreferrer"">tf-slim walkthrough tutorial</a>, where there is a function called load_batch, there are only 3 tensors returned: <code>images, images_raw, labels</code>. Where are 'batches' of data as explained in the documentation?</p>

<p>Thank you for your help.</p>
",0
41705377,What is the right initializer one should give to the tf.contrib.layers.convolution2d function in TensorFlow?,"<p>I was reading the documentation for <a href=""https://www.tensorflow.org/api_docs/python/contrib.layers/higher_level_ops_for_building_neural_network_layers_#convolution2d"" rel=""nofollow noreferrer"">making 2d convolutional layers</a> in tensorflow from the contrib section and was wondering what was the right or best way to initialize the weights when using the <a href=""https://www.tensorflow.org/api_docs/python/contrib.layers/higher_level_ops_for_building_neural_network_layers_#convolution2d"" rel=""nofollow noreferrer"">tf.contrib.layers.convolution2d</a> function. Unfortunately they don't really say explicitly nor provide an example, so it was unclear to me what is the intended way to use this. The function has a <code>weights_initializer</code> parameter which can be set. I have tried setting it to both:</p>

<ol>
<li>tf.contrib.layers.xavier_initializer</li>
<li>tf.contrib.layers.xavier_initializer_conv2d</li>
</ol>

<p>neither seem to return an error and the first one seems to train fine (as far as I can tell). However, it would be awesome to check if this is the right way of using this contrib layer (or maybe since it seems to be a contrib function, how does one check the ""official"" source code maybe to see their docs or test cases or maybe address the my question in their gitissues, if appropriate).</p>
",1
41708106,Linking Tensorboard Embedding Metadata to checkpoint,"<p>I'm using the tflearn wrapper over tensorflow to build a model, and would like to add metadata (labels) to the resultant embedding visualization. Is there a way to link a metadata.tsv file to a saved checkpoint after the fact of running it?  </p>

<p>I've created a projector_config.pbtxt file in the logdir of the checkpoint summaries, with the metadata.tsv being in the same folder. The config looks like this:</p>

<pre><code>embeddings {
  tensor_name: ""Embedding/W""
  metadata_path: ""C:/tmp/tflearn_logs/shallow_lstm/""
}
</code></pre>

<p>and was created using the code from the docs - <a href=""https://www.tensorflow.org/how_tos/embedding_viz/"" rel=""noreferrer"">https://www.tensorflow.org/how_tos/embedding_viz/</a> </p>

<p>I've commented out the tf.Session part in the hopes of creating the metadata link without the need of doing so directly within a Session object, but I'm not sure if that's possible.</p>

<pre><code>from tensorflow.contrib.tensorboard.plugins import projector
#with tf.Session() as sess:
config = projector.ProjectorConfig()
# One can add multiple embeddings.
embedding = config.embeddings.add()
embedding.tensor_name = 'Embedding/W'
# Link this tensor to its metadata file (e.g. labels).
embedding.metadata_path = 'C:/tmp/tflearn_logs/shallow_lstm/'
# Saves a config file that TensorBoard will read during startup.
projector.visualize_embeddings(tf.summary.FileWriter('/tmp/tflearn_logs/shallow_lstm/'), config)
</code></pre>

<p>Below is a snap of the current embedding visualization. Note the empty metadata. Is there a way to directly attach the desired metafile to this embedding?</p>

<p><a href=""https://i.stack.imgur.com/ANcPY.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ANcPY.png"" alt=""Embedding Visualization""></a></p>
",0
41714318,Input parameters of tf.contrib.learn.read_batch_features,"<p>I am working through these tensorflow <a href=""https://github.com/dennybritz/chatbot-retrieval/tree/master/models%20&#39;codes&#39;"" rel=""nofollow noreferrer"">codes</a> which implement a LSTM in tensorflow. While going through the codes, I came across this function (in <a href=""https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_inputs.py"" rel=""nofollow noreferrer"">input_fn code</a> - line 38) <code>tf.contrib.learn.read_batch_features</code>. I looked up the documentation of <code>tf.contrib.learn.read_batch_features</code> <a href=""https://www.tensorflow.org/api_docs/python/contrib.learn/input_processing#read_batch_features"" rel=""nofollow noreferrer"">here</a>. This is what I got - </p>

<pre>
file_pattern: List of files or pattern of file paths containing Example records. 
batch_size: An int or scalar Tensor specifying the batch size to use.
features: A dict mapping feature keys to FixedLenFeature or VarLenFeature values.
randomize_input: Whether the input should be randomized.
num_epochs: Integer specifying the number of times to read through the dataset. If None, cycles through the dataset forever. NOTE - If specified, creates a variable that must be initialized, so call tf.local_variables_initializer() as shown in the tests.
queue_capacity: Capacity for input queue.
reader_num_threads: The number of threads to read examples.
name: Name of resulting op.
</pre>

<p>There are few input parameters that I am not able to understand and was hoping someone could help me with it.</p>

<ol>
<li><p>The <code>randomize_input</code> parameter. Does it mean it will shuffle the entire dataset?</p></li>
<li><p>For <code>num_epochs</code>, if I specify <code>None</code> does it mean that my <code>input_fn</code> will keep feeding to the <code>model_fn</code>. In that case the training wouldn't stop. This doesn't make sense to me. I guess I'm going wrong somewhere here.</p></li>
<li><p><code>queue_capacity</code> I am not sure what this means</p></li>
</ol>

<p>Would appreciate any help around these questions. Thanks in advance!</p>
",1
41726804,Tensorflow update only selected variables,"<p><strong>Overview:</strong> I want to update only selected variables in a network. The network has parts <code>A</code>-><code>B</code> (in forward direction) and each of them has separate losses <code>La</code> and <code>Lb</code>. I want to train the weights <code>a</code> of <code>A</code> to optimize <code>Lb</code>. While doing this, the weights <code>b</code> of <code>B</code> should be fixed. How can I do this?</p>

<p><strong>Approach 1:</strong> Select only <code>a</code> as variables to minimize using <code>var_list</code> in <code>optimizer.minimize(loss, var_list=[a])</code>.
<a href=""https://github.com/tensorflow/tensorflow/issues/834"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/834</a> . This crashes with an error <code>ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables (...) and loss (...)</code>. This actually works fine in other scenarios, but apparently it does not like that weights <code>b</code> are not in the var_list.</p>

<p><strong>Edit 1:</strong> The line that causes the error: <code>a_optim = tf.train.AdamOptimizer(args.lr, beta1=args.beta1).minimize(self.a_loss, var_list=self.a_vars, global_step=self.global_step)</code></p>

<p><strong>Approach 2:</strong> Same as Approach 1, but also include <code>b</code> in the var_list. The problem is now that the network updates a and b, whereas it should just send the gradients through <code>B</code> and only update <code>A</code>.</p>

<p><strong>Edit 2:</strong> The line that works, but is not what I want: <code>a_optim = tf.train.AdamOptimizer(args.lr, beta1=args.beta1).minimize(self.a_loss, var_list=self.a_vars+self.b_vars, global_step=self.global_step)</code></p>

<p><strong>Approach 3:</strong> Use <code>tf.stop_gradient(tensor)</code> <a href=""https://stackoverflow.com/questions/34477889/holding-variables-constant-during-optimizer/34478044#34478044"">Holding variables constant during optimizer</a> . From the documentation I infer that this only stops the gradients from flowing further to the left in the graph. I want the ignore variables on the right.</p>

<p><strong>Approach 4:</strong> Set <code>tf.Variable(..., trainable=True)</code>, but that looks very inflexible if I want to alternate training between A and B.</p>
",0
41756054,Tensorflow VariableScope: original_name_scope vs name,"<p>In TensorFlow, the <code>VariableScope</code> class has both a <code>original_name_scope</code> and <code>name</code> attribute. What are their differences and when should I use one over the other? I can't seem to find much documentation on them.</p>

<p><strong>Use case:</strong>
I'm using the <code>tf.get_collection(key, scope)</code> method. Its second argument expects a string, but my variable <code>my_scope</code> has type <code>VariableScope</code>. I'm trying both</p>

<pre><code>tf.get_collection(key, my_scope.name)
</code></pre>

<p>and</p>

<pre><code>tf.get_collection(key, my_scope.original_scope_name)
</code></pre>

<p>. Both seem to work, but I'm not sure which is ""right"" and won't give me problems later down the road.</p>
",1
41789133,What are c_state and m_state in Tensorflow LSTM?,"<p>Tensorflow r0.12's documentation for tf.nn.rnn_cell.LSTMCell describes this as the init:</p>

<pre><code>tf.nn.rnn_cell.LSTMCell.__call__(inputs, state, scope=None)
</code></pre>

<p>where <code>state</code> is as follows:</p>

<blockquote>
  <p>state: if state_is_tuple is False, this must be a state Tensor, 2-D, batch x state_size. If state_is_tuple is True, this must be a tuple of state Tensors, both 2-D, with column sizes c_state and m_state.</p>
</blockquote>

<p>What aare <code>c_state</code> and <code>m_state</code> and how do they fit into LSTMs? I cannot find reference to them anywhere in the documentation.</p>

<p><a href=""https://web.archive.org/web/20170223030652/https://www.tensorflow.org/versions/r0.11/api_docs/python/rnn_cell/rnn_cells_for_use_with_tensorflow_s_core_rnn_methods"" rel=""noreferrer"">Here is a link to that page in the documentation.</a></p>
",1
41797136,"How to Concatenate ""Jagged"" Tensors","<p>I am trying to write an implementation of <a href=""http://www.aclweb.org/anthology/P14-1062"" rel=""nofollow noreferrer"">this</a> paper in TensorFlow and I have come across a bit of a snag. In my pooling layer, I have to concatenate everything together. This is the code I use:</p>

<pre><code>    pooled_outputs = []
    for i, filter_size in enumerate(filter_sizes):
        with tf.name_scope(""conv-maxpool-%s"" % filter_size):
            # Conv layer
            filter_shape = [filter_size, embedding_size, 1, num_filters]
            # W is the filter matrix
            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W"")
            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=""b"")
            conv = tf.nn.conv2d(
                self.embedded_chars_expanded,
                W,
                strides=[1, 1, 1, 1],
                padding=""VALID"",
                name=""conv""
            )

            # Apply nonlinearity
            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")

            # Max-pooling layer over the outputs
            pooled = tf.nn.max_pool(
                h,
                ksize=[1, sequence_lengths[i] - filter_size + 1, 1, 1],
                strides=[1, 1, 1, 1],
                padding=""VALID"",
                name=""pool""
            )
            pooled_outputs.append(pooled)

    # Combine all of the pooled features
    num_filters_total = num_filters * len(filter_sizes)

    print(pooled_outputs)
    pooled_outputs = [tf.reshape(out, [""?"", 94, 1, self.max_length]) for out in pooled_outputs] # The problem line

    self.h_pool = tf.concat(3, pooled_outputs)
</code></pre>

<p>When I run this code, it prints out this for <code>pooled_outputs</code>:</p>

<pre><code>[&lt;tf.Tensor 'conv-maxpool-3/pool:0' shape=(?, 94, 1, 128) dtype=float32&gt;, &lt;tf.Tensor 'conv-maxpool-4/pool:0' shape=(?, 51, 1, 128) dtype=float32&gt;, &lt;tf.Tensor 'conv-maxpool-5/pool:0' shape=(?, 237, 1, 128) dtype=float32&gt;]
</code></pre>

<p>I originally tried this code without the <code>pooled_outputs = [tf.reshape(out, [""?"", 94, 1, self.max_length]) for out in pooled_outputs]</code> line in there and I got this error:</p>

<pre><code>ValueError: Dimension 1 in both shapes must be equal, but are 51 and 237
</code></pre>

<p>When I added in the reshape line, I got this error:</p>

<pre><code>TypeError: Expected binary or unicode string, got 94
</code></pre>

<p>The second error I know is because I passed a ""?"" for the new size, and the first error I think is because the tensors aren't the same size. <strong>How could I properly pad these Tensors so I can concatenate them with no problems?</strong></p>
",0
41830723,Batch normalization with 3D convolutions in TensorFlow,"

<p>I'm implementing a model relying on 3D convolutions (for a task that is similar to action recognition) and I want to use batch normalization (see <a href=""https://arxiv.org/abs/1502.03167"" rel=""noreferrer"">[Ioffe &amp; Szegedy 2015]</a>). I could not find any tutorial focusing on 3D convs, hence I'm making a short one here which I'd like to review with you.</p>

<p>The code below refers to TensorFlow r0.12 and it explicitly instances variables - I mean I'm not using tf.contrib.learn except for the tf.contrib.layers.batch_norm() function. I'm doing this both to better understand how things work under the hood and to have more implementation freedom (e.g., variable summaries).</p>

<p>I will get to the 3D convolution case smoothly by first writing the example for a fully-connected layer, then for a 2D convolution and finally for the 3D case. While going through the code, it would be great if you could check if everything is done correctly - the code runs, but I'm not 100% sure about the way I apply batch normalization. I end this post with a more detailed question.</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

# This flag is used to allow/prevent batch normalization params updates
# depending on whether the model is being trained or used for prediction.
training = tf.placeholder_with_default(True, shape=())
</code></pre>

<h2>Fully-connected (FC) case</h2>

<pre class=""lang-py prettyprint-override""><code># Input.
INPUT_SIZE = 512
u = tf.placeholder(tf.float32, shape=(None, INPUT_SIZE))

# FC params: weights only, no bias as per [Ioffe &amp; Szegedy 2015].
FC_OUTPUT_LAYER_SIZE = 1024
w = tf.Variable(tf.truncated_normal(
    [INPUT_SIZE, FC_OUTPUT_LAYER_SIZE], dtype=tf.float32, stddev=1e-1))

# Layer output with no activation function (yet).
fc = tf.matmul(u, w)

# Batch normalization.
fc_bn = tf.contrib.layers.batch_norm(
    fc,
    center=True,
    scale=True,
    is_training=training,
    scope='fc-batch_norm')

# Activation function.
fc_bn_relu = tf.nn.relu(fc_bn)
print(fc_bn_relu)  # Tensor(""Relu:0"", shape=(?, 1024), dtype=float32)
</code></pre>

<h2>2D convolutional (CNN) layer case</h2>

<pre class=""lang-py prettyprint-override""><code># Input: 640x480 RGB images (whitened input, hence tf.float32).
INPUT_HEIGHT = 480
INPUT_WIDTH = 640
INPUT_CHANNELS = 3
u = tf.placeholder(tf.float32, shape=(None, INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS))

# CNN params: wights only, no bias as per [Ioffe &amp; Szegedy 2015].
CNN_FILTER_HEIGHT = 3  # Space dimension.
CNN_FILTER_WIDTH = 3  # Space dimension.
CNN_FILTERS = 128
w = tf.Variable(tf.truncated_normal(
    [CNN_FILTER_HEIGHT, CNN_FILTER_WIDTH, INPUT_CHANNELS, CNN_FILTERS],
    dtype=tf.float32, stddev=1e-1))

# Layer output with no activation function (yet).
CNN_LAYER_STRIDE_VERTICAL = 1
CNN_LAYER_STRIDE_HORIZONTAL = 1
CNN_LAYER_PADDING = 'SAME'
cnn = tf.nn.conv2d(
    input=u, filter=w,
    strides=[1, CNN_LAYER_STRIDE_VERTICAL, CNN_LAYER_STRIDE_HORIZONTAL, 1],
    padding=CNN_LAYER_PADDING)

# Batch normalization.
cnn_bn = tf.contrib.layers.batch_norm(
    cnn,
    data_format='NHWC',  # Matching the ""cnn"" tensor which has shape (?, 480, 640, 128).
    center=True,
    scale=True,
    is_training=training,
    scope='cnn-batch_norm')

# Activation function.
cnn_bn_relu = tf.nn.relu(cnn_bn)
print(cnn_bn_relu)  # Tensor(""Relu_1:0"", shape=(?, 480, 640, 128), dtype=float32)
</code></pre>

<h2>3D convolutional (CNN3D) layer case</h2>

<pre class=""lang-py prettyprint-override""><code># Input: sequence of 9 160x120 RGB images (whitened input, hence tf.float32).
INPUT_SEQ_LENGTH = 9
INPUT_HEIGHT = 120
INPUT_WIDTH = 160
INPUT_CHANNELS = 3
u = tf.placeholder(tf.float32, shape=(None, INPUT_SEQ_LENGTH, INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS))

# CNN params: wights only, no bias as per [Ioffe &amp; Szegedy 2015].
CNN3D_FILTER_LENGHT = 3  # Time dimension.
CNN3D_FILTER_HEIGHT = 3  # Space dimension.
CNN3D_FILTER_WIDTH = 3  # Space dimension.
CNN3D_FILTERS = 96
w = tf.Variable(tf.truncated_normal(
    [CNN3D_FILTER_LENGHT, CNN3D_FILTER_HEIGHT, CNN3D_FILTER_WIDTH, INPUT_CHANNELS, CNN3D_FILTERS],
    dtype=tf.float32, stddev=1e-1))

# Layer output with no activation function (yet).
CNN3D_LAYER_STRIDE_TEMPORAL = 1
CNN3D_LAYER_STRIDE_VERTICAL = 1
CNN3D_LAYER_STRIDE_HORIZONTAL = 1
CNN3D_LAYER_PADDING = 'SAME'
cnn3d = tf.nn.conv3d(
    input=u, filter=w,
    strides=[1, CNN3D_LAYER_STRIDE_TEMPORAL, CNN3D_LAYER_STRIDE_VERTICAL, CNN3D_LAYER_STRIDE_HORIZONTAL, 1],
    padding=CNN3D_LAYER_PADDING)

# Batch normalization.
cnn3d_bn = tf.contrib.layers.batch_norm(
    cnn3d,
    data_format='NHWC',  # Matching the ""cnn"" tensor which has shape (?, 9, 120, 160, 96).
    center=True,
    scale=True,
    is_training=training,
    scope='cnn3d-batch_norm')

# Activation function.
cnn3d_bn_relu = tf.nn.relu(cnn3d_bn)
print(cnn3d_bn_relu)  # Tensor(""Relu_2:0"", shape=(?, 9, 120, 160, 96), dtype=float32)
</code></pre>

<p>What I would like to make sure is whether the code above exactly implements batch normalization as described in <a href=""https://arxiv.org/abs/1502.03167"" rel=""noreferrer"">[Ioffe &amp; Szegedy 2015]</a> at the end of Sec. 3.2:</p>

<blockquote>
  <p>For convolutional layers, we additionally want the normalization to obey the convolutional property – so that different elements of the same feature map, at different locations, are normalized in the same way. To achieve this, we jointly normalize all the activations in a minibatch, over all locations. [...] Alg. 2 is modified similarly, so that during inference the BN transform applies the same linear transformation to each activation in a given feature map.</p>
</blockquote>

<p><strong>UPDATE</strong>
I guess the code above is also correct for the 3D conv case. In fact, when I define my model if I print all the trainable variables, I also see the expected numbers of beta and gamma variables. For instance:</p>

<pre class=""lang-py prettyprint-override""><code>Tensor(""conv3a/conv3d_weights/read:0"", shape=(3, 3, 3, 128, 256), dtype=float32)
Tensor(""BatchNorm_2/beta/read:0"", shape=(256,), dtype=float32)
Tensor(""BatchNorm_2/gamma/read:0"", shape=(256,), dtype=float32)
</code></pre>

<p>This looks ok to me since due to BN, one pair of beta and gamma are learned for each feature map (256 in total).</p>

<hr>

<p>[Ioffe &amp; Szegedy 2015]: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</p>
",0
41839648,Tensorflow GPU installation Ubuntu 16.04 Error (libcuda.so not found),"<p>I have installed the gpu version of tensorflow but when I try to import it I get the following error message: </p>

<pre><code>I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:116] Couldn't open CUDA library libcuda.so.1. LD_LIBRARY_PATH: usr/local/cuda-8.0/lib64
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:165] hostname: sep-GS60-2QE
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:189] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program
I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:193] kernel reported version is: Permission denied: could not open driver version path for reading: /proc/driver/nvidia/version
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1066] LD_LIBRARY_PATH: usr/local/cuda-8.0/lib64
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1067] failed to find libcuda.so on this system: Failed precondition: could not dlopen DSO: libcuda.so.1; dlerror: libnvidia-fatbinaryloader.so.367.57: cannot open shared object file: No such file or directory
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.8.0 locally
</code></pre>

<p>I have followed the installation guide from the tensorflow website, and have made sure that I have installed Cuda and CudNN correctly. </p>

<p>I have tried creating sym-links from a <code>libcuda.so</code> that I found in my system to the cuda directory as suggested on other posts on the nvidia's website as well but they did not help.</p>

<p>I have also set the parameters in ~/.bashrc to <code>export LD_LIBRARY_PATH=""usr/local/cuda-8.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}""</code>and <code>export CUDA_HOME=""/usr/local/cuda""</code> but still no luck :( .</p>

<p>It would be most helpful if anyone could help me get this sorted as it is a crucial part of my project to run the code on my gpu.</p>
",0
41865218,Interleaving slim.dropout and slim.fully_connected in slim.stack?,"<p>In tf.slim, I'd like to create a stack of fully-connected layers with dropout.</p>

<p>To the example from documentation:
<code>slim.stack(x, slim.fully_connected, [32, 64, 128], scope='fc')</code>, I'd like to add dropout. </p>

<p>Is it possible to use slim.stack or do I have to go back to the verbose approach?</p>

<pre><code>(pseudo-code) for every layer:
   slim.dropout(slim.fully_connected(...)
</code></pre>
",1
41941940,TensorFlow: Understanding the `collections` argument in tf.summary.scalar,"<p>I am working with TensorBoard, specifically <code>tf.summary.scalar</code>. In the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/summary.md#tfsummaryscalarname-tensor-collectionsnone-scalar"" rel=""nofollow noreferrer"">documentation</a> it has an arugment <code>collections=None</code>, which is described as:</p>

<blockquote>
  <p><code>collections</code>: Optional list of graph collections keys. The new summary op is added to these collections. Defaults to <code>[GraphKeys.SUMMARIES]</code>.</p>
</blockquote>

<p>I don't understand this description, and what <code>collections</code> is used for. Can someone please explain this to me, and perhaps point me towards a good example use-case?</p>
",1
42012906,Create a custom Tensorflow histogram summary,"<p>There are a couple of SO answers on creating a custom scalar summary in TF (<a href=""https://stackoverflow.com/questions/37902705/how-to-manually-create-a-tf-summary"">here</a> and <a href=""https://stackoverflow.com/questions/37530228/how-do-i-add-an-arbitrary-value-to-a-tensorflow-summary"">here</a>), but I can't find anything on creating a custom <em>histogram</em> summary. The documentation seems to be very lacking for custom summaries. I have a numpy array of that I'd like to make a summary of - any ideas on how?</p>

<p>(tf.Summary.Value has a histo field that I tried using, but it requires a tensorflow::HistogramProto; there's no documentation on that class either, so I'm at a loss on how to make it. I've tried creating a minimal failing example below).</p>

<pre><code>import tensorflow as tf
import numpy as np
sess = tf.Session()
means_placeholder = tf.placeholder(tf.float32)
tf.summary.histogram('means', means_placeholder)
summaries = tf.summary.merge_all()
writer = tf.summary.FileWriter('./summaries')
means = np.random.random(10)    
writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag='means', histo=means)]))
</code></pre>
",1
42022950,Which seeds have to be set where to realize 100% reproducibility of training results in tensorflow?,"<p>In a general tensorflow setup like</p>

<pre><code>model = construct_model()
with tf.Session() as sess:
    train_model(sess)
</code></pre>

<p>Where <code>construct_model()</code> contains the model definition including random initialization of weights (<code>tf.truncated_normal</code>) and <code>train_model(sess)</code> executes the training of the model -</p>

<p>Which seeds do I have to set where to ensure 100% reproducibility between repeated runs of the code snippet above? <a href=""https://www.tensorflow.org/api_docs/python/tf/random/set_random_seed"" rel=""nofollow noreferrer"">The documentation</a> for <code>tf.random.set_random_seed</code> may be concise, but left me a bit confused. I tried:</p>

<pre><code>tf.set_random_seed(1234)
model = construct_model()
    with tf.Session() as sess:
        train_model(sess)
</code></pre>

<p>But got different results each time. </p>
",1
42049256,TensorFlow: Does queuing examples in parallel speed up a batch creation if tf.train.batch is already dequeuing examples in parallel?,"<p>In the TensorFlow-slim documentation, there is a ParallelReader object that can read TFRecords data in parallel through having multiple readers to take in example strings into queue. However, if I am not mistaken tf.train.batch dequeues examples from a queue and is able to do so in parallel with the argument <code>num_threads</code>. If that is the case, is it necessary that both the batch creation and reading of data must have the same speed otherwise one will cause a slower creation of a batch?</p>

<p>I am actually not very sure whether the dequeue operation happens in tf.train.batch or when a queue_runner is manually created to dequeue the examples since I believe tf.train.batch can effectively replace the queue_runner operation. Is this correct?</p>
",0
42058690,Separate gradients in tf.gradients,"<p>Based on this <a href=""https://www.tensorflow.org/api_docs/python/train/gradient_computation#gradients"" rel=""nofollow noreferrer"">reference</a>, <code>tf.gradients</code> returns the partial derivatives of <strong>sum</strong> of <code>ys</code> w.r.t. <code>x</code> in <code>xs</code>.</p>

<p>Is it possible to get separate values for each <code>x</code> in <code>xs</code>?</p>

<p><strong>Example</strong>. Please look at the following code:</p>

<pre><code>import tensorflow as tf
import numpy as np

x = tf.placeholder(tf.float32, shape=[None, 2])
w = tf.Variable(dtype=tf.float32, initial_value=[[4],[5]])
z = tf.matmul(x, w)
g = tf.gradients(z, w)

s = tf.InteractiveSession()
s.run(tf.global_variables_initializer())

x_ = np.matrix('1,0;0,1;0,0;2,1')
s.run(g, feed_dict={x: x_})
</code></pre>

<p>It simply calculates <code>dz/dw</code> which is <code>x</code>. But <code>g</code> returns the sum of <code>x</code>:</p>

<pre><code>[array([[ 3.],
    [ 2.]], dtype=float32)]
</code></pre>

<p>What I need is a separate value for each row in <code>x</code>.</p>
",0
42141054,BatchNormalization in TensorFlow,"<p>What is the difference between <code>tf.nn.batch_normalization</code> and <code>tf.contrib.layers.batch_norm</code>? I have checked the documentation but did not understand if one is better than the other.</p>
",1
42184863,How do you make TensorFlow + Keras fast with a TFRecord dataset?,"<p><strong>What is an example of how to use a TensorFlow TFRecord with a Keras Model and tf.session.run() while keeping the dataset in tensors w/ queue runners?</strong></p>

<p>Below is a snippet that works but it needs the following improvements:</p>

<ul>
<li>Use the <a href=""https://keras.io/models/model/"" rel=""nofollow noreferrer"">Model API</a></li>
<li>specify an Input()</li>
<li>Load a dataset from a TFRecord</li>
<li>Run through a dataset in parallel (such as with a queuerunner)</li>
</ul>

<p>Here is the snippet, there are several TODO lines indicating what is needed:</p>



<pre class=""lang-python prettyprint-override""><code>from keras.models import Model
import tensorflow as tf
from keras import backend as K
from keras.layers import Dense, Input
from keras.objectives import categorical_crossentropy
from tensorflow.examples.tutorials.mnist import input_data

sess = tf.Session()
K.set_session(sess)

# Can this be done more efficiently than placeholders w/ TFRecords?
img = tf.placeholder(tf.float32, shape=(None, 784))
labels = tf.placeholder(tf.float32, shape=(None, 10))

# TODO: Use Input() 
x = Dense(128, activation='relu')(img)
x = Dense(128, activation='relu')(x)
preds = Dense(10, activation='softmax')(x)
# TODO: Construct model = Model(input=inputs, output=preds)

loss = tf.reduce_mean(categorical_crossentropy(labels, preds))

# TODO: handle TFRecord data, is it the same?
mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)

train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

sess.run(tf.global_variables_initializer())

# TODO remove default, add queuerunner
with sess.as_default():
    for i in range(1000):
        batch = mnist_data.train.next_batch(50)
        train_step.run(feed_dict={img: batch[0],
                                  labels: batch[1]})
    print(loss.eval(feed_dict={img:    mnist_data.test.images, 
                               labels: mnist_data.test.labels}))
</code></pre>

<p><strong>Why is this question relevant?</strong></p>

<ul>
<li>For high performance training without going back to python

<ul>
<li>no <a href=""https://stackoverflow.com/questions/36026892/how-can-i-convert-tfrecords-into-numpy-arrays"">TFRecord to numpy</a> to tensor conversions</li>
</ul></li>
<li><a href=""https://github.com/fchollet/keras/issues/5358"" rel=""nofollow noreferrer"">Keras will soon be part of tensorflow</a></li>
<li>Demonstrate how Keras Model() classes can accept tensors for input data correctly.</li>
</ul>

<p><strong>Here is some starter information for a semantic segmentation problem example:</strong></p>

<ul>
<li>example unet Keras model <a href=""https://github.com/ahundt/tf-image-segmentation/blob/cd7df53e6f59ef9e3dff1ed0119e12922ae98a3a/tf_image_segmentation/models/unet.py"" rel=""nofollow noreferrer"">unet.py</a>, happens to be for semantic segmentation.</li>
<li><a href=""https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html"" rel=""nofollow noreferrer"">Keras + Tensorflow Blog Post</a></li>
<li>An <a href=""https://github.com/ahundt/tf-image-segmentation/blob/cd7df53e6f59ef9e3dff1ed0119e12922ae98a3a/tf_image_segmentation/recipes/pascal_voc/FCNs/densenet_fcn.py#L78"" rel=""nofollow noreferrer"">attempt at running the unet model a tf session with TFRecords and a Keras model</a> (not working)</li>
<li>Code to create the TFRecords: <a href=""https://github.com/ahundt/tf-image-segmentation/blob/cd7df53e6f59ef9e3dff1ed0119e12922ae98a3a/tf_image_segmentation/utils/tf_records.py"" rel=""nofollow noreferrer"">tf_records.py</a></li>
<li>An attempt at running the unet model a tf session with TFRecords and a Keras model is in <a href=""https://github.com/ahundt/tf-image-segmentation/blob/cd7df53e6f59ef9e3dff1ed0119e12922ae98a3a/tf_image_segmentation/recipes/pascal_voc/FCNs/densenet_fcn.py#L78"" rel=""nofollow noreferrer"">densenet_fcn.py</a> (not working)</li>
</ul>
",0
42201565,Using Tensorflow.slim to apply convolution2d_transpose,"<p>I am trying to apply 2 convolutional layers with the tf.slim.conv2d function, they basically reduce the size of my input image by half each time. Then I want to apply the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py#L906"" rel=""nofollow noreferrer"">convolution2d_transpose</a> to get my original image shape back. The problem is I don't exactly know how to use the transpose convolution function, and the documentation is not much help. </p>

<p>I am using a custom wrapper, but here is what I have so far:</p>

<pre><code>Input Batch [8, 161, 141] ----&gt; Conv2d [outputs = 32, 
kernel_size = [41,11], stride= [2,2]] 
which cuts the original image in half, and another such layer which cuts it again.
</code></pre>

<p>How can I apply the convolution_transpose function to reverse the effect of these two layers now ? </p>
",1
42256938,What does tf.gfile do in TensorFlow?,"<p>I've seen people using several functions from <code>tf.gfile</code> such as <code>tf.gfile.GFile</code> or <code>tf.gfile.Exists</code>. I have the idea that <code>tf.gfile</code> deals with files. However, I haven't been able to find the official documentation to see what else it offers. </p>

<p>It'd be great if you could help me with it.</p>
",1
42288907,"Tensorflw MultiRNNCell's parameter in doc ,I think it is wrong","<p>I found in the <a href=""https://www.tensorflow.org/tutorials/recurrent"" rel=""nofollow noreferrer"">tensorflow doc</a>:</p>

<p><code>
stacked_lstm = tf.contrib.rnn.MultiRNNCell([lstm] * number_of_layers,
                ...
</code></p>

<p>I need to use MultiRNNCell</p>

<p>but, I write those lines</p>

<p><code>
a = [tf.nn.rnn_cell.BasicLSTMCell(10)]*3
print id(a[0]), id(a[1])
</code></p>

<p>Its output is <code>[4648063696 4648063696]</code>.</p>

<p>Can <code>MultiRNNCell</code> use the same object <code>BasicLSTMCell</code> as a list for parameter?</p>
",0
42333101,Predicting Next Word of LSTM Model from Tensorflow Example,"<p>My buddy and I are trying to utilize the trained model from the LSTM tensorflow example <a href=""https://www.tensorflow.org/tutorials/recurrent"" rel=""nofollow noreferrer"">here</a>. We've been able to train our model, save the model, and then import the model. We've just used tensorflow's Supervisor. It was in the tutorial, but you can read more about it <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard6/tf.train.Supervisor.md"" rel=""nofollow noreferrer"">here</a>. </p>

<p>It's weird because there's not a whole lot of clear documentation for this. I understand that tensorflow is an API that's going through a lot of changes and adaptations right now, but it's hard to find clear answers. For example, we want to use <code>tf.train.Saver()</code>, but we aren't sure if there is anything comparable to <code>tf.train.Supervisor()</code>'s <code>managed_session</code>. </p>

<p>More to the point, however, we <em>just want to use our model</em>. We want to be able to map a string using <code>tensorflow.models.rnn.ptb.reader</code>. We're not sure how to do this. We pass in a string, and we want to do a simple prediction in terms of like predicting the next word in a string. So, something similar to this:</p>

<pre><code>import tensorflow as tf
sess = tf.Session()
new_saver = tf.train.import_meta_graph('ptbmodel.meta')
new_saver.restore(sess, tf.train.latest_checkpoint('./')) # latest checkpoint
all_vars = tf.global_variables()
# just want to make sure all our variables are there!
for v in all_vars:
    v_ = sess.run(v)
    print(""This is {} with value: {}"".format(v.name, v_))


sent = raw_input(""Enter a string where you want to predict the next word: "")
split_sent = sent.split()
# THEN map these words into our LSTM model and pull off the most likely word that 
# is coming next
</code></pre>

<p>But again, my buddy and I are pretty new to this, so we're not sure about where to go. I know this is probably too broad of a question for stack, but we've been pouring over the documentation and haven't been able to make much progress. <strong>ANY</strong> help would be appreciated so much! </p>

<p>We've already found these other Stack links. Check them out <a href=""https://stackoverflow.com/questions/36286594/predicting-the-next-word-using-the-lstm-ptb-model-tensorflow-example"">here</a> and <a href=""https://stackoverflow.com/questions/33773661/predicting-next-word-using-the-language-model-tensorflow-example"">here</a>.</p>

<p>We are not sure how to associate the <code>logits</code> probability list with any meaningful words.</p>
",1
42334855,state output from tf.nn.dynamic_rnn operation,"<p>For this code snippet:</p>

<pre><code>rnn_cell = tf.contrib.rnn.BasicRNNCell(config.hidden_size, activation=tf.tanh)
initial_state = rnn_cell.zero_state(config.batch_size, tf.float32)
rnn_out, state = tf.nn.dynamic_rnn(rnn_cell, embed_out, initial_state=initial_state)
</code></pre>

<p>I was expected the last time index from rnn_out to be equal to state. Or, perhaps the tanh of the state. But this isn't what I am seeing - they don't match. In the context of this RNN recurrence relation, what value does state contain?</p>

<p>h(t) = tanh[b + W<em>h(t-1) + U</em>x(t)]</p>

<p>The answer here, implies the last time index of rnn_out and state should be equal (but they are not):</p>

<p><a href=""https://stackoverflow.com/questions/40384791/for-the-tf-nn-rnn-cell-basicrnn-whats-the-difference-between-the-state-and-outp"">for the tf.nn.rnn_cell.BasicRNN,what&#39;s the difference between the state and output</a></p>

<p>The TF documentation isn't clear to me on this point.</p>
",1
42356027,What is the upside of using `tf.nn.rnn` instead of `tf.nn.dynamic_rnn` in TensorFlow?,"<p>What is the upside of using <a href=""https://web.archive.org/web/20170220232722/https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/recurrent_neural_networks#rnn"" rel=""nofollow noreferrer""><code>tf.nn.rnn</code></a> instead of <a href=""https://web.archive.org/web/20170220232722/https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/recurrent_neural_networks#dynamic_rnn"" rel=""nofollow noreferrer""><code>tf.nn.dynamic_rnn</code></a>?</p>

<p>The <a href=""https://web.archive.org/web/20170220232722/https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/recurrent_neural_networks#dynamic_rnn"" rel=""nofollow noreferrer"">documentation</a>  says :</p>

<blockquote>
  <p>[<code>dynamic_rnn</code>] is functionally identical to the function <code>rnn</code> above, but performs fully dynamic unrolling of inputs.</p>
</blockquote>

<p>Is there any case where one might prefer to use <code>tf.nn.rnn</code> instead of <code>tf.nn.dynamic_rnn</code>?</p>
",1
42361668,Tensorflow bidirectional_dynamic_rnn() FailedPreconditionError: Attempting to use uninitialized value BiRNN/FW/LSTMCell/B,"<p>I'm getting the above error when attempting to call <code>tf.nn.bidirectional_dynamic_rnn()</code>. I've called <code>tf.global_variables_initializer()</code>. At first I thought it's because I didn't pass in <code>sequence_length</code> to <code>tf.nn.bidirectional_dynamic_rnn()</code>. However, even after I did, it's still shows the same error.</p>

<p>Any idea?</p>

<p>Stacktrace:</p>

<pre><code> Traceback (most recent call last):
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py"", line 1580, in &lt;module&gt;
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py"", line 964, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/Users/Keven/Documents/stanford local/cs224n project/224n-project/bi_lstm_encoder.py"", line 49, in &lt;module&gt;
    test_bilstm()
  File ""/Users/Keven/Documents/stanford local/cs224n project/224n-project/bi_lstm_encoder.py"", line 43, in test_bilstm
    out = session.run(pred, feed_dict={input_placeholder: doc, sequence_placeholder: sequence_length})
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 964, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value BiRNN/FW/LSTMCell/B
     [[Node: BiRNN/FW/LSTMCell/B/read = Identity[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](BiRNN/FW/LSTMCell/B)]]

Caused by op u'BiRNN/FW/LSTMCell/B/read', defined at:
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py"", line 1580, in &lt;module&gt;
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/pydevd.py"", line 964, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/Users/Keven/Documents/stanford local/cs224n project/224n-project/bi_lstm_encoder.py"", line 49, in &lt;module&gt;
    test_bilstm()
  File ""/Users/Keven/Documents/stanford local/cs224n project/224n-project/bi_lstm_encoder.py"", line 42, in test_bilstm
    pred = BidirectionalLSTMEncoder().add_prediction_op(input_placeholder, sequence_placeholder, 6)
  File ""/Users/Keven/Documents/stanford local/cs224n project/224n-project/bi_lstm_encoder.py"", line 20, in add_prediction_op
    preds, _ = tf.nn.bidirectional_dynamic_rnn(cell_forward, cell_backward, inputs, sequence_length=sequence_length, dtype=tf.float32)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 652, in bidirectional_dynamic_rnn
    time_major=time_major, scope=fw_scope)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 845, in dynamic_rnn
    dtype=dtype)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 1012, in _dynamic_rnn_loop
    swap_memory=swap_memory)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2636, in while_loop
    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2469, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2419, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 995, in _time_step
    skip_conditionals=True)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 403, in _rnn_step
    new_output, new_state = call_cell()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn.py"", line 983, in &lt;lambda&gt;
    call_cell = lambda: cell(input_t, state)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell.py"", line 500, in __call__
    initializer=init_ops.zeros_initializer, dtype=dtype)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 1024, in get_variable
    custom_getter=custom_getter)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 850, in get_variable
    custom_getter=custom_getter)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 346, in get_variable
    validate_shape=validate_shape)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 331, in _true_getter
    caching_device=caching_device, validate_shape=validate_shape)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py"", line 677, in _get_single_variable
    expected_shape=shape)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 224, in __init__
    expected_shape=expected_shape)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/variables.py"", line 367, in _init_from_args
    self._snapshot = array_ops.identity(self._variable, name=""read"")
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1424, in identity
    result = _op_def_lib.apply_op(""Identity"", input=input, name=name)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
    self._traceback = _extract_stack()

FailedPreconditionError (see above for traceback): Attempting to use uninitialized value BiRNN/FW/LSTMCell/B
     [[Node: BiRNN/FW/LSTMCell/B/read = Identity[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](BiRNN/FW/LSTMCell/B)]]
</code></pre>

<p>Code:</p>

<pre><code>import tensorflow as tf
import numpy as np
from SubModel import SubModel


# input:
# shape=(?, max_timestep_doc2, 3 * word_vector_size)
#
# output:
# shape=(?, max_timestep_doc2, 2 * word_vector_size)

class BidirectionalLSTMEncoder(SubModel):

  def add_prediction_op(self, inputs, output_size=None):
    sequence_length = [5, 5]
    cell_forward = tf.nn.rnn_cell.LSTMCell(output_size, num_proj=output_size)
    cell_backward = tf.nn.rnn_cell.LSTMCell(output_size, num_proj=output_size)
    preds, _ = tf.nn.bidirectional_dynamic_rnn(cell_forward, cell_backward, inputs, sequence_length=sequence_length, dtype=tf.float32)
    return preds

  def __init__(self):
    pass


def test_bilstm():
  print('testing bidirectional lstm layer')

  with tf.variable_scope(""test_bilstm_layer""):
    input_placeholder = tf.placeholder(tf.float32, shape=(None, 5, 9))
    sequence_placeholder = tf.placeholder(tf.int32, shape=(None,))

  init = tf.global_variables_initializer()

  with tf.Session() as session:
    session.run(init)
    doc = np.ones(shape=(2, 5, 9), dtype=np.float32) * 0.5

    pred = BidirectionalLSTMEncoder().add_prediction_op(input_placeholder, 6)
    out = session.run(pred, feed_dict={input_placeholder: doc})
    print(""out = "" + str(out))

    # assert np.allclose(CD_correct, out, atol=1e-2), ""new state vector does not seem to be correct.""

if __name__ == ""__main__"":
  test_bilstm()
</code></pre>
",0
42399012,Tensorflow: Transforming manually build layers to tf.contrib.layers,"<p>I have these four layers defined:</p>

<pre><code>layer_1 = tf.add(
    tf.matmul(input, tf.Variable(tf.random_normal([n_input, n_hidden_1])),
    tf.Variable(tf.random_normal([n_hidden_1]))))
layer_2 = tf.nn.sigmoid(tf.add(
    tf.matmul(layer_1, tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),
    tf.Variable(tf.random_normal([n_hidden_2]))))
layer_3 = tf.nn.sigmoid(tf.add(
    tf.matmul(layer_2, tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])))),
    tf.Variable(tf.random_normal([n_hidden_1]))))
layer_4 = tf.add(
    tf.matmul(layer_3, tf.Variable(tf.random_normal([n_hidden_1, n_input]))),
    tf.Variable(tf.random_normal([n_input])))
</code></pre>

<p>I would like to transform this code into code based on <code>tf.contrib.layers</code>. So far I got</p>

<pre><code>layer_1 = tf.contrib.layers.fully_connected(
    inputs=input,
    num_outputs=n_hidden_1,
    activation_fn=None)
layer_2 = tf.contrib.layers.fully_connected(
    inputs=layer_1,
    num_outputs=n_hidden_2,
    activation_fn=tf.nn.sigmoid)
layer_3 = tf.contrib.layers.fully_connected(
    inputs=layer_2,
    num_outputs=n_hidden_1,
    activation_fn=tf.nn.sigmoid)
layer_4 = tf.contrib.layers.linear(
    inputs=layer_3,
    num_outputs=n_input)
</code></pre>

<p>by reading up on <a href=""https://www.tensorflow.org/versions/master/tutorials/layers/"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/master/tutorials/layers/</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected</a>. I read in <a href=""https://www.tensorflow.org/api_guides/python/contrib.layers#Higher_level_ops_for_building_neural_network_layers"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_guides/python/contrib.layers#Higher_level_ops_for_building_neural_network_layers</a> that <code>tf.contrib.layers.linear</code> is an alternative for the linear layer.</p>

<p>But my output is more different compared to what I got earlier, then that this could be by chance. What did I do wrong in the configuration of the layers?</p>
",0
42419837,Tensorflow tf.matmul example is incorrect?,"<p>I read the official document for <a href=""https://www.tensorflow.org/api_docs/python/tf/matmul"" rel=""nofollow noreferrer"">tf.matmul</a> 
and I understand the first example.
It is a simple [2,3] x [3,2] operation:</p>

<pre><code>a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])

b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])

c = tf.matmul(a, b) =&gt; [[58 64]
                    [139 154]]
</code></pre>

<p>However, the second example seems very strange :</p>

<pre><code>a = tf.constant(np.arange(1, 13, dtype=np.int32),
            shape=[2, 2, 3])

b = tf.constant(np.arange(13, 25, dtype=np.int32),
            shape=[2, 3, 2])

c = tf.matmul(a, b) =&gt; [[[ 94 100]
                     [229 244]],
                    [[508 532]
                     [697 730]]]
</code></pre>

<p>Why the matrix with shape [2,2,3] is allowed to multiply with [2,3,2] ?</p>
",1
42430222,Tensorflow's provided linear regression example is giving bad results,"<p>I'm following the <a href=""https://www.tensorflow.org/get_started/get_started"" rel=""nofollow noreferrer"">linear regression example</a> from the tensor flow get started tutorial (not the one using tf.contrib.learn) . While the provided example works, if I use any other data as an input to train on I always get the following printed out:</p>

<pre><code>W: [ nan] b: [ nan] loss: nan
</code></pre>

<p>I have tried it with various different data sets. Even simple data sets like this one </p>

<pre><code># training data
  x_train = [32,53,61,47,59]
  y_train = [31,68,62,71,87]
</code></pre>

<p>I can implement linear regression on the data without any problem if I implement it only in numpy and get correct weight and bias values.</p>

<p>I have tried adjusting the hyperparameters but still no luck. It always returns nan</p>

<p>I have also literally copied the code from the site and just replaced the data values so I know there is no hidden typo. This has been driving me crazy. Please help.</p>
",0
42440111,How to find Tensorflow Serving version?,"<p>To find Tensorflow version we can do that by:
python -c 'import tensorflow as tf; print(tf.<strong>version</strong>)' </p>

<p>Tensorflow Serving is a separate install, so how to find the version of Tensorflow Serving?</p>

<p>Is it same as Tensorflow? Do not see any reference/comments or documentation related to this.</p>
",1
42454973,Tensorflow: Slice Input image and grayscale,"<p>I am new to Tensorflow and am trying to slice an image to then grayscale it. My code so far looks like this. I am first taking the input, then extract image data into a tensor, then slice the middle part of the image. The weights etc. can be disregarded for this step, just wanted to give the reader context:</p>

<pre><code>##############################
#   NETWORK IMPLEMENTATION   #
##############################
tf.reset_default_graph()

########
#Forward

##Inputs
input = tf.placeholder(shape=[210, 160, 3], dtype=tf.float32)
print(input.get_shape())
#inputs = tf.reshape(input, [1, 210, 160, 3])
#print(inputs.get_shape())

##Grayscale
#inputs = tf.image.rgb_to_grayscale(input, name=None)
#print(inputs.get_shape())

#Slicing
inputs = tf.slice(input, [25, 0, 0], [160, 160, 3])
print(inputs.get_shape())

#Downsample to new dimensions
#inputs = tf.image.decode_jpeg(inputs)
inputs = tf.image.resize_images(inputs, [84, 84]) 
print(inputs.get_shape())
inputs = tf.reshape(inputs, [1, -1])
print(inputs.get_shape())

W = tf.Variable(tf.random_uniform([84*84, 6], 0.00, 0.01))
Qout = tf.matmul(inputs, W)
predict = tf.argmax(Qout, 1) #why exactly this operation?
</code></pre>

<p>Whenever I run this, however, I get an error stating:</p>

<pre><code>Traceback (most recent call last):
  File ""main.py"", line 43, in &lt;module&gt;
    inputs = tf.image.resize_images(inputs, [84, 84])
TypeError: resize_images() takes at least 3 arguments (2 given)
</code></pre>

<p>I have read the documentation on slicing and resizing, but I am still confused. As said, I'm new to tensorflow, would be grateful for any help :)</p>

<p>I tried commenting out to debug, but I'm not even sure if that caused the problems. I am always getting the above error.</p>
",1
42470319,Output of Tensorflow LSTM-Cell,"<p>I've got a question on Tensorflow LSTM-Implementation. There are currently several implementations in TF, but I use:</p>
<pre class=""lang-py prettyprint-override""><code>cell = tf.contrib.rnn.BasicLSTMCell(n_units)
</code></pre>
<ul>
<li>where n_units is the amount of 'parallel' LSTM-Cells.</li>
</ul>
<p>Then to get my output I call:</p>
<pre class=""lang-py prettyprint-override""><code> rnn_outputs, rnn_states = tf.nn.dynamic_rnn(cell, x,
                        initial_state=initial_state, time_major=False)
</code></pre>
<ul>
<li>where (as <code>time_major=False</code>) <code>x</code> is of shape <code>(batch_size, time_steps, input_length)</code></li>
<li>where <code>batch_size</code> is my batch_size</li>
<li>where <code>time_steps</code> is the amount of timesteps my RNN will go through</li>
<li>where <code>input_length</code> is the length of one of my input vectors (vector fed into the network on one specific timestep on one specific batch)</li>
</ul>
<p>I expect rnn_outputs to be of shape <code>(batch_size, time_steps, n_units, input_length)</code> as I have not specified another output size.
Documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer""><code>nn.dynamic_rnn</code></a> tells me that output is of shape <code>(batch_size, input_length, cell.output_size)</code>.
The documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell"" rel=""nofollow noreferrer""><code>tf.contrib.rnn.BasicLSTMCell</code></a> does have a property <code>output_size</code>, which is defaulted to n_units (the amount of LSTM-cells I use).</p>
<p>So does each LSTM-Cell only output a scalar for every given timestep? I would expect it to output a vector of the length of the input vector. This seems not to be the case from how I understand it right now, so I am confused. Can you tell me whether that's the case or how I could change it to output a vector of size of the input vector per single lstm-cell maybe?</p>
",1
42608245,How to use tf.contrib.seq2seq.simple_decoder_fn_inference API,"<p>I did not understand the parameters that needed to be passed into the API call to   <code>tf.contrib.seq2seq.simple_decoder_fn_inference</code> in Tensorflow 1.0
for building the inference block for a Seq2Seq Attention mechanism RNN.</p>

<p>Can someone explain, in detail, what each parameter of this function call means and is supposed to do?</p>

<p>The link to the documentation is here :
<a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/attention_decoder_fn_inference"" rel=""nofollow noreferrer"">tf.contrib.seq2seq.attention_decoder_fn_inference()</a></p>
",1
42695305,Accessing row in a 2-D tensor,"<p>I have the following code of an incredibly simple neural network (this code is actually an adaptation for an easy question):</p>

<pre><code>import numpy as np
import tensorflow as tf

with tf.device(""cpu:0""):
    sess = tf.InteractiveSession()
    nNodes = 3
    inputDim = 1

    rowIdxs = np.zeros([nNodes, nNodes])
    colIdxs = np.zeros([nNodes, nNodes])
    for rowIdx in range(nNodes):
        for colIdx in range(nNodes):
            rowIdxs[rowIdx, colIdx] = rowIdx
            colIdxs[rowIdx, colIdx] = colIdx

    rowIdxs = np.reshape(rowIdxs, [-1])
    colIdxs = np.reshape(colIdxs, [-1])

    # build a matrix with nNodes x nNodes elements
    # with each row i containing the distance from node i to all the other nodes
    distances = np.zeros([nNodes, nNodes])
    for i in range(nNodes):
        for j in range(nNodes):
            distances[i, j] = ((rowIdxs[i] - rowIdxs[j]) ** 2 + (colIdxs[i] - colIdxs[j]) ** 2)
    print('distances=', distances)

    # tensorflow constant from distances matrix
    distances_ = tf.constant(distances, dtype=tf.float32)

    # w corresponds to a weight matrix in a neural network
    w = tf.random_uniform((nNodes, inputDim), 0.0, 1.0)

    # x corresponds to the input to the network
    x = tf.random_uniform((1, inputDim), 0.0, 1.0)

    xx = tf.tile(x, [nNodes,1])
    print('w', w.shape)
    print('x', x.shape)
    print('xx', xx.shape)

    # differences between weights and input vector
    diff = tf.reduce_sum(tf.abs(tf.subtract(xx, w)), 1)
    print('diff.shape', diff.shape)


    # index of the best matching unit
    bmu = tf.arg_min(diff, 0)       

    # Now I need to access the distances from BMU to the other nodes
    slice = tf.slice(distances_, [bmu, 0], [bmu, -1])

    sess.run(tf.global_variables_initializer())
    sess.run(slice)
    print('slice=', slice.eval())
    print('diff', diff.eval())
    print('bmu=', bmu.eval())
</code></pre>

<p>Basically, given an input <code>x</code>, compare it to the weights <code>w</code> and choose the node <code>BMU</code> with the minimum differences.</p>

<p>I have several problems with that code:</p>

<p><strong>1. sometimes it works without errors sometimes it raises an exception.</strong> </p>

<p><strong>When it DOES NOT work</strong>, the output is this:</p>

<pre><code>distances= 
 [[ 0.  1.  4.]
 [ 1.  0.  1.]
 [ 4.  1.  0.]]

w (3, 1)
x (1, 1)
xx (3, 1)
diff.shape (3,)

InvalidArgumentError (see above for traceback): Expected size[0] in [0, 1], but got 2
 [[Node: Slice = Slice[Index=DT_INT64, T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Const, Slice/begin, Slice/size)]]
</code></pre>

<p>The full stack follows:</p>

<pre><code>Traceback (most recent call last):
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1022, in _do_call
    return fn(*args)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1004, in _run_fn
    status, run_metadata)
  File ""D:\python\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected size[0] in [0, 1], but got 2
     [[Node: Slice = Slice[Index=DT_INT64, T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Const, Slice/begin, Slice/size)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:/workspace/LiClipse Workspace/kerasPython/exercises/testIndexing.py"", line 44, in &lt;module&gt;
    sess.run(slice)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 767, in run
    run_metadata_ptr)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected size[0] in [0, 1], but got 2
     [[Node: Slice = Slice[Index=DT_INT64, T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Const, Slice/begin, Slice/size)]]

Caused by op 'Slice', defined at:
  File ""D:/workspace/LiClipse Workspace/kerasPython/exercises/testIndexing.py"", line 39, in &lt;module&gt;
    slice = tf.slice(distances_, [bmu, 0], [bmu, -1])
  File ""D:\python\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 561, in slice
    return gen_array_ops._slice(input_, begin, size, name=name)
  File ""D:\python\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 3053, in _slice
    name=name)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Expected size[0] in [0, 1], but got 2
     [[Node: Slice = Slice[Index=DT_INT64, T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](Const, Slice/begin, Slice/size)]]
</code></pre>

<p><strong>When it works</strong></p>

<pre><code>w (3, 1)
x (1, 1)
xx (3, 1)
diff.shape (3,)
slice= [[ 1.  0.  1.]]
diff [ 0.29777944  0.08669317  0.09722018]
bmu= 0
</code></pre>

<p><code>bmu</code> is wrong, it should be <code>1</code>, but the slice is correct.</p>

<p>Sometimes I get this:</p>

<pre><code>w (3, 1)
x (1, 1)
xx (3, 1)
diff.shape (3,)
slice= []
diff [ 0.33319855  0.12426794  0.49753141]
bmu= 1
</code></pre>

<p><code>bmu</code> is 1, but slice is empty.</p>

<p><strong>2. When I switch to the GPU, I have an exception telling me I cannot use <code>bmu</code> for indexing.</strong>
Starting with <code>with tf.device(""gpu:0""):</code>, I get this:</p>

<pre><code>InvalidArgumentError (see above for traceback): Cannot assign a device to node 'Slice/size': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
     [[Node: Slice/size = Pack[N=2, T=DT_INT64, axis=0, _device=""/device:GPU:0""](ArgMin, Slice/size/1)]]
</code></pre>

<p>The full stack trace follows:</p>

<pre><code>Traceback (most recent call last):
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1022, in _do_call
    return fn(*args)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1004, in _run_fn
    status, run_metadata)
  File ""D:\python\lib\contextlib.py"", line 66, in __exit__
    next(self.gen)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 469, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'Slice/size': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
     [[Node: Slice/size = Pack[N=2, T=DT_INT64, axis=0, _device=""/device:GPU:0""](ArgMin, Slice/size/1)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:/workspace/LiClipse Workspace/kerasPython/exercises/testIndexing.py"", line 45, in &lt;module&gt;
    sess.run(slice)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 767, in run
    run_metadata_ptr)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""D:\python\lib\site-packages\tensorflow\python\client\session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device to node 'Slice/size': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
     [[Node: Slice/size = Pack[N=2, T=DT_INT64, axis=0, _device=""/device:GPU:0""](ArgMin, Slice/size/1)]]

Caused by op 'Slice/size', defined at:
  File ""D:/workspace/LiClipse Workspace/kerasPython/exercises/testIndexing.py"", line 40, in &lt;module&gt;
    slice = tf.slice(distances_, [bmu, 0], [bmu, -1])
  File ""D:\python\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 561, in slice
    return gen_array_ops._slice(input_, begin, size, name=name)
  File ""D:\python\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 3053, in _slice
    name=name)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 491, in apply_op
    preferred_dtype=default_dtype)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\ops.py"", line 716, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""D:\python\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 923, in _autopacking_conversion_function
    return _autopacking_helper(v, inferred_dtype, name or ""packed"")
  File ""D:\python\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 886, in _autopacking_helper
    return gen_array_ops._pack(elems_as_tensors, name=scope)
  File ""D:\python\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 2041, in _pack
    result = _op_def_lib.apply_op(""Pack"", values=values, axis=axis, name=name)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 763, in apply_op
    op_def=op_def)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\ops.py"", line 2395, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""D:\python\lib\site-packages\tensorflow\python\framework\ops.py"", line 1264, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): Cannot assign a device to node 'Slice/size': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
     [[Node: Slice/size = Pack[N=2, T=DT_INT64, axis=0, _device=""/device:GPU:0""](ArgMin, Slice/size/1)]]
</code></pre>

<p>I cannot understand what's happening: I have an idea, but cannot find any reference in the documentation or anywhere else. May be I use the wrong keywords.</p>

<p>Is there anyone who can help me?</p>
",1
42703264,Select only one out of two input streams,"<p>I am having a graph which should choose between using either of two input streams at each session call. If a flag is set, stream_1 should be used, if it is not set, stream_2 should be used. <code>tf.cond</code> can accomplish this, but as the documentation says, <a href=""https://www.tensorflow.org/api_docs/python/tf/cond"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/cond</a>, the inputs are computed regardless of whether or not they are actually used. As stream_1 is computationally heavy I would rather not use such a solution. Is there another way of accomplishing the same thing, where stream_1 is not computed at all when the flag is not set?</p>
",0
42753297,Pass initial state in dynamic_rnn,"<p>I want to use the method <code>tf.nn.dynamic_rnn</code> with my own initial_state. The state of the cell which is given to <code>tf.nn.dynamic_rnn</code> is a tupel. I don't understand the documentation:</p>

<blockquote>
  <p>If cell.state_size is a tuple, this should be a tuple of tensors having shapes [batch_size, s] for s in cell.state_size.</p>
</blockquote>

<p>How can I pass my shape (batchSize, hiddenSize) so that the elements of hiddenSize are initial state for the hidden and the cell state?</p>
",1
42754259,Sampled softmax loss over variable sequence batches?,"<p><strong>Background info</strong>: I'm working on sequence-to-sequence models, and right now my model accepts variable-length input tensors (not lists) with input shapes corresponding to [batch size, sequence length]. However, in my implementation, sequence length is <em>unspecified</em> (set to None) to allow for variable length inputs. Specifically, input sequence batches are padded only to the length of the longest sequence in that batch. This has sped up my training time considerably, so I'd prefer to keep it this way, as opposed to going back to bucketed models and/or padded all sequences in the training data to the same length. I'm using TensorFlow 1.0.0.</p>

<p><strong>Problem</strong>: I'm currently using the following to compute the loss (which runs just fine).</p>

<pre><code>loss = tf.losses.sparse_softmax_cross_entropy(
    weights=target_labels,  # shape: [batch size, None]
    logits=outputs[:, :-1, :], # shape: [batch size, None, vocab size]
    weights=target_weights[:, :-1]) # shape: [batch size, None]
</code></pre>

<p>where vocab size is typically about 40,000. I'd like to use a sampled softmax, but I've ran into an issue that's due to the unspecified nature of the input shape. According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss"" rel=""nofollow noreferrer"">documentation for tf.nn.sampled_softmax_loss</a>, <strong>it requires the inputs to be fed separately for each timestep</strong>. However, I can't call, for example,  </p>

<pre><code>tf.unstack(target_labels, axis=1)
</code></pre>

<p>since the axis is unknown beforehand.Does anyone know how I might go about implementing this? One would assume that since both dynamic_rnn and <a href=""https://www.tensorflow.org/api_docs/python/tf/losses/sparse_softmax_cross_entropy"" rel=""nofollow noreferrer"">tf.losses.sparse_softmax_cross_entropy</a> seem to have no issue doing this, that a workaround could be implemented with the sampled softmax loss somehow. After digging around in the source code and even models repository, I've come up empty handed. Any help/suggestions would be greatly appreciated.</p>
",1
42787903,Feeding inputs to tf.contrib.learn estimators directly,"<p>I am having trouble using the DNNRegressor estimator from TensorFlow's tf.contrib.learn. In the <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/learn/DNNRegressor"" rel=""nofollow noreferrer"" title=""DNNRegressor"">documentation page</a> of the estimator two methods for providing inputs are presented.</p>

<p>The first method uses the <em>input_fn</em> function, which, as described, should be used for pre-processing and feeding the input to the estimators, and the second method feeds the input directly. Examples:</p>

<pre><code>def input_function:
    ...
    return feature_cols, label

estimator.fit(input_fn=input_function, steps=...)
</code></pre>

<p>In this case <em>feature_cols</em> is a <em>dict</em> with:</p>

<ul>
<li>Key: <em>string</em> specifying the column name,</li>
<li>Value: <em>tf.constant</em> specifying the column values,</li>
</ul>

<p><em>label</em> is a single <em>tf.constant</em> column containing the labels.</p>

<p>This worked.</p>

<pre><code>X_train = ...
y_train = ...
estimator.fit(x=X_train, y=y_train, steps=...)
</code></pre>

<p>In this case I don't know what to feed in as <em>X</em> and <em>y</em>. I have tried the following:</p>

<ul>
<li>Ordinary numpy arrays. This was a long shot and did not work with. The error message is: <code>KeyError: 'my_column0'</code></li>
<li>Pandas DataFrame with columns corresponding to the defined column names (defined on initialization of the estimator). I would again get the same KeyError, even though the key should be there now.</li>
<li>Pass X=<em>feature_cols</em> and y=<em>label</em> defined in the same way as in the case of the input_fn above. This yields: <code>ValueError: Inputs cannot be tensors. Please provide input_fn.</code></li>
</ul>

<p>I also tried other combinations with <em>dict</em> and numpy arrays, but nothing worked. I would like to be able to make this work using the second method, since this is also useful for passing the objects to the <em>evaluate</em> and <em>predict</em>. Does anyone know the correct format for this?
Also, is there a way to simply pass numpy arrays?</p>

<p>Thank you! </p>

<p>tl;dr
What should the inputs to tf.contrib.learn estimator be in order to feed them directly using <code>estimator.fit(x=X_train, y=y_train, steps=...)</code>?</p>
",0
42897816,"Why does ""tf.Variable([.3], tf.float32)"" work in tensorflow?","<p>The standard usage should be </p>

<p><code>tf.Variable([.3], dtype=tf.float32)</code>, isn't it?</p>

<p>I saw the <code>tf.Variable([.3], tf.float32)</code> in official documentation. The constructor function prototype of  <code>tf.Variable</code> is </p>

<p><code>__init__(self, initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None, expected_shape=None, import_scope=None)</code>. </p>

<p>If we pass the parameter <code>tf.float32</code> instead of <code>dtype=tf.float32</code> (key parameter), how does it know the <code>tf.float32</code> is employed for <code>dtype</code>. Does python interpreter check the parameter type?</p>
",0
42933599,Slice a tensor in half in tensorflow,"<p>I have a tensor of shape <code>(32, 32, 32, 1)</code> and I want to slice it into two tensors, along the first dimension, containing the first and second halves like so</p>

<pre><code>half1  with shape = (16, 32, 32, 1)
half2  with shape = (16, 32, 32, 1)
</code></pre>

<p>I am trying to use tf.slice but I don't know how to use the begin and end indices, and the documentation is anything but clear. </p>
",1
42940451,How can I get the file name of a tf.summary.FileWriter in TensorFlow?,"<p>How can I get the file name of a <a href=""https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter"" rel=""nofollow noreferrer""><code>tf.summary.FileWriter</code></a> (<a href=""https://web.archive.org/web/20170321224015/https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter"" rel=""nofollow noreferrer"">mirror</a>) in TensorFlow?</p>

<p>I am aware that I can use  <a href=""https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter#get_logdir"" rel=""nofollow noreferrer""><code>get_logdir()</code></a> but  I don't see any similar method to access the file name.</p>
",0
42956766,3D tensors with tensorflow tf.matmul,"<p>I want to do a multiplication with two 3-D tensors, as defined:  </p>

<pre><code>a = tf.random_uniform(shape = [5,3,3])   

b = tf.ones(shape = [5,3,1])   

c = tf.matmul(a,b) 
</code></pre>

<p>but I can't get the right answer as described in the tf.matmul function   </p>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/matmul"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/matmul</a>  </p>
",1
42981493,Weights and biases in tf.layers module in TensorFlow 1.0,"<p>How do you access the weights and biases when using tf.layers module in TensorFlow 1.0? The advantage of tf.layers module is that you don't have to separately create the variables when making a fully connected layer or convolution layer. </p>

<p>I couldn't not find anything in the documentation regarding accessing them or adding them in summaries after they are created.</p>
",1
42986653,Distributed TensorFlow - Not running some workers,"<p>I'm trying to get a very simple example of distributed TensorFlow working. However, I'm having a bug that appears non-deterministically between runs. On some runs, it works perfectly. Outputting something along the lines of:</p>

<pre><code>Worker 2 | step 0
Worker 0 | step 0
Worker 1 | step 0
Worker 3 | step 0
Worker 2 | step 1
Worker 0 | step 1
Worker 1 | step 1
Worker 3 | step 1
...
</code></pre>

<p>However, every once in a while, one or more of the workers fails to run, resulting in output like this:</p>

<pre><code>Worker 0 | step 0
Worker 3 | step 0
Worker 0 | step 1
Worker 3 | step 1
Worker 0 | step 2
Worker 3 | step 2
...
</code></pre>

<p>If I run the loop indefinitely, it seems that the missing workers always startup at some point, but only minutes later, which isn't practical.</p>

<p>I've found that two things make the issue go away (but make the program useless): 1. Not declaring any tf Variables inside the <code>with tf.device(tf.train.replica_device_setter())</code> scope. If I even declare one variable (e.g. <code>nasty_var</code> below), the issue starts cropping up. and 2. setting the <code>is_chief</code> param in <code>tf.train.MonitoredTrainingSession()</code> to <code>True</code> for all workers. This causes the bug to go away even if variables are declared, but it seems wrong to make all of the workers the chief. The way I'm currently setting it below - <code>is_chief=(task_index == 0)</code> - is taken directly from a TensorFlow tutorial.</p>

<p>Here's the simplest code I can get to replicate the issue. (You may have to run multiple times to see the bug, but it almost always shows up within 5 runs</p>

<pre><code>from multiprocessing import Process
import tensorflow as tf
from time import sleep
from numpy.random import random_sample

cluster = tf.train.ClusterSpec({'ps': ['localhost:2222'],
                                'worker': ['localhost:2223',
                                           'localhost:2224',
                                           'localhost:2225',
                                           'localhost:2226']})


def create_worker(task_index):
    server = tf.train.Server(cluster, job_name='worker', task_index=task_index)

    with tf.device(tf.train.replica_device_setter(worker_device=""/job:worker/task:%d"" % task_index, cluster=cluster)):
        nasty_var = tf.Variable(0)  # This line causes the problem. No issue when this is commented out.

    with tf.train.MonitoredTrainingSession(master=server.target, is_chief=(task_index == 0)):
        for step in xrange(10000):
            sleep(random_sample())  # Simulate some work being done.
            print 'Worker %d | step %d' % (task_index, step)


def create_ps(task_index):
    param_server = tf.train.Server(cluster, job_name='ps',
                                   task_index=task_index)
    param_server.join()

# Launch workers and ps in separate processes.
processes = []
for i in xrange(len(cluster.as_dict()['worker'])):
    print 'Forking worker process ', i
    p = Process(target=create_worker, args=[i])
    p.start()
    processes.append(p)

for i in xrange(len(cluster.as_dict()['ps'])):
    print 'Forking ps process ', i
    p = Process(target=create_ps, args=[i])
    p.start()
    processes.append(p)

for p in processes:
    p.join()
</code></pre>
",0
43007183,Encode OpenCV/NumPy image into Tensorflow string(Python),"<p>I have some tensorflow code that works with a string representation of an image as read from a file by the <code>tf.gfile.Gfile</code> function <code>read()</code>, which, according to the documentation, ""Returns the contents of a file as a string."". Looking at the code, it does a <code>sess.run()</code> on a <code>feed_dict={""image_feed:0"": encoded_image}</code> where <code>encoded_image</code> is the value passed directly from the <code>read()</code> function.</p>

<p>I want to be able to pass previously <code>imread</code> images in the form of <code>np.ndarray</code>s into this function, which would require me to convert them to this string format, without having to write the images to the disk, which would be slow.</p>

<p>I have a network that is already finalized, so I can't just <code>tf.image.encode_png()</code> the <code>np.ndarray</code> containing the image, because that gives me <code>RuntimeError: Graph is finalized and cannot be modified.</code>. Is there a way to convert a <code>ndarray</code> directly into this string format to feed it into the network, without creating a new tensor every time I need to feed an image to the net?</p>
",0
43114238,Tensorflow: how do I extract/export variable values at every iteration of training?,"<p>I have been playing around with some neural networks on Tensorflow and I wanted to make a visualization of the neural network's learning process. 
To do so, I intend to extract the following variables into text/JSON/csv: pre-activation result before 1st layer, activation, bias and weight values for testing and training, each layer and for all time steps. I am looking for a generalizable solution so that I don't have to modify my source code (or at least not more than one or two lines) when applying visualization to future networks. Ideally I could run some function from another python program to read any python/TF code and extract the variables described above. So far I have considered the following solutions:
1) use tf.summary and the filewriter to save as a serialized protocol buffer, then find a way to go from protocol buffer --> JSON format. This unfortunately would not fit the bill as it requires me to modify too much inner code. 
2) Perhaps using <a href=""https://www.tensorflow.org/api_docs/python/tf/train/export_meta_graph"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/train/export_meta_graph</a>
Although I am not sure how to implement given my TF foundations are not quite there yet
3) I have also found this solution:</p>

<pre><code>W_val, b_val= sess.run([W, b])
np.savetxt(""W1.csv"", W_val, delimiter="","")
np.savetxt(""b1.csv"", b_val, delimiter="","")
</code></pre>

<p>But the problem is that it only saves the final values of the weights and biases, whereas I am looking to save their values at all timesteps of training. </p>

<p>If anyone has any suggestions on how to tackle this problem or any guidance I would appreciate it. </p>

<p>Many thanks</p>
",0
43129196,Preloading data in Tensorflow with shared layers,"<p>I have Tensorflow code for multi-task learning (one input, several outputs, similar to this: <a href=""https://jg8610.github.io/Multi-Task/"" rel=""nofollow noreferrer"">https://jg8610.github.io/Multi-Task/</a>). For further explanation see below. The code works, but is slow as there's a lot of overhead from reading data in Python and feeding it to the GPU (with the tf.Session's <code>feed_dict</code>).</p>

<p>So my plan is now to preload the data according to <a href=""https://www.tensorflow.org/programmers_guide/reading_data#preloaded_data"" rel=""nofollow noreferrer"">https://www.tensorflow.org/programmers_guide/reading_data#preloaded_data</a> [storing it in a <code>tf.constant</code> and using TF's queuing system]. This raises some problems, of which the most central for now seems to be:</p>

<ul>
<li>If I preload the different task data into different tensors, I no longer have a task-generic <code>X_in</code>. That means that when declaring the shared layer, I now need to make a decision whether to connect it to <code>X_input_task_A</code> or <code>X_input_task_B</code>, and obviously that's not going to result in a <em>shared</em> layer.</li>
</ul>

<p><strong>My question</strong></p>

<p>Would you have any idea how to solve this problem, i.e. to define shared layers with task-specific tensors, and then training by alternating between tasks? How would you alternatively call the different optimizer operations?</p>

<p><strong>Further explanation on the Multi-task learning paradigm</strong></p>

<p>For background, what the mentioned blog post (as well as my code so far) does is to define a placeholder <code>X_in</code> plus a shared layer that consumes that input op. Then, for each task we want to learn, we have different projections and loss functions that use task-specific placeholders <code>y_task</code>, and training happens by alternately running <code>session.run(optimizer_task, feed_dict={X_in: X_batch_task, y_task: y_batch_task})</code>, where <code>optimizer_task</code> is some task-specific optimizer. This is basically what my code does now - it works but is slow because I need to feed the data:</p>

<pre><code># PLACEHOLDERS
X_in = tf.placeholder([batch_size, 100])
y_task_a = tf.placeholder([batch_size, 4])  # 4 output classes
y_task_b = tf.placeholder([batch_size, 2])  # 2 output classes

# SHARED LAYER
W = tf.get_variable(""W"", [100, 50])  
shared_layer = tf.sigmoid(tf.matmul(X_in, W))

# TASK-SPECIFIC OUTPUTS
W_task_a = tf.get_variable(""Wa"", [50, 4])
W_task_b = tf.get_variable(""Wb"", [50, 2])
pred_task_a = tf.sigmoid(tf.matmul(shared_layer, W_task_a))
pred_task_b = tf.sigmoid(tf.matmul(shared_layer, W_task_b))

# TASK-SPECIFIC LOSSES AND OPTIMIZERS
loss_task_a = tf.nn.softmax_cross_entropy_with_logits(logits=pred_task_a, labels=y_task_a)
loss_task_b = tf.nn.softmax_cross_entropy_with_logits(logits=pred_task_b, labels=y_task_b)
optimizer_a = ...(loss_task_a)
optimizer_b = ...(loss_task_b)

# TRAINING
with tf.Session() as sess:
  for i in range(ITERS):
    # ALTERNATE BETWEEN TASKS, GET BATCH FROM DATA PER TASK AND TRAIN
    X_a, y_a = data_task_a.get_batch()
    X_b, y_b = data_task_b.get_batch()
    sess.run(optimizer_a, feed_dict={X_in: X_a, y_task_a: y_a})
    sess.run(optimizer_b, feed_dict={X_in: X_b, y_task_b: y_b})
</code></pre>
",0
43148578,What is difference between tf.variable_scope and variable_scope.variable_scope in Tensorflow?,"<p>I know what <code>tf.variable_scope</code> means as it is clearly stated in the document. But in this <a href=""https://github.com/tensorflow/tensorflow/blob/b10f50ff15944badb7262a207f6628dfa52d6a9d/tensorflow/contrib/rnn/python/kernel_tests/lstm_ops_test.py#L62"" rel=""nofollow noreferrer"">example</a>, there is a <code>variable_scope.variable_scope</code>. And seems it has not been covered in the documentation. I wonder what is their difference?</p>
",1
43217669,Any danger of using reuse_variables() in v1.0+?,"<p>In the documentation about upgrading to TF 1.0 <a href=""https://www.tensorflow.org/install/migration"" rel=""nofollow noreferrer"">here</a>, there's the following ominous sounding statement:</p>

<p><em>Constructions like <code>tf.get_variable_scope().reuse_variables()</code> will likely not work. We recommend deleting those lines and replacing them with lines such as the following:</em></p>

<pre><code>with tf.variable_scope(tf.get_variable_scope(), reuse=True):
</code></pre>

<p>I would much prefer sticking with <code>.reuse_variables()</code> as the way my code is currently set up makes it rather difficult to use the suggested idiom. So far my code seems to work fine with v1.0, so I'm not sure if it's something I need to worry about or not. The documentation doesn't make it clear if there are plans for deprecation, or what exactly is the problem with using <code>.reuse_variables()</code>.</p>
",1
43262815,Using tensorboard histograms with tf.contrib.layers,"<p>Im trying to understand if there is a way to get a histogram of the weights in a <a href=""https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/layers/fully_connected"" rel=""nofollow noreferrer"">fully_connected</a> layer from <code>tf.contrib.layers</code>. In the documentation there is no obv way to get the weights.</p>

<p>So my question is, how can I visualise the weights from layers?</p>
",1
43284047,What is the default kernel initializer in tf.layers.conv2d and tf.layers.dense?,"<p>The official Tensorflow API doc claims that the parameter <code>kernel_initializer</code> defaults to <code>None</code> for <code>tf.layers.conv2d</code> and <code>tf.layers.dense</code>.</p>

<p>However, reading the layers tutorial (<a href=""https://www.tensorflow.org/tutorials/layers"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/layers</a>), I noted that this parameter is not set in the code. For example:</p>



<pre class=""lang-py prettyprint-override""><code># Convolutional Layer #1
conv1 = tf.layers.conv2d(
    inputs=input_layer,
    filters=32,
    kernel_size=[5, 5],
    padding=""same"",
    activation=tf.nn.relu)
</code></pre>

<p>The example code from the tutorial runs without any errors, so I think the default <code>kernel_initializer</code> is not <code>None</code>. So, which initializer is used?</p>

<p>In another code, I did not set the <code>kernel_initializer</code> of the conv2d and dense layers, and everything was fine. However, when I tried to set the <code>kernel_initializer</code> to <code>tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32)</code>, I got NaN errors. What is going on here? Can anyone help?</p>
",0
43300000,TensorFlow: Quantize model using python before save,"<p>There are tutorials online showing how to quantize a <code>.pb</code> TensorFlow model, see:</p>

<p><a href=""https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/"" rel=""nofollow noreferrer"">https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/</a></p>

<p>What I am wondering is if there is a way to quantize the graph using python before saving the <code>.pb</code> file with <code>tf.train.write_graph()</code></p>

<p>In other words is there some function like <code>quantize(graph_def)</code> that I can run to quantize the graph to 8bit weights and operations before I save it, saving me the hassle of having to do it via the command line after saving the file (like the tutorial linked above outlines).</p>
",0
43316230,TensorFlow LlinearRegressor example failed to create a directory in path C:\Users and raise a exception,"<p>I am learning machine-learning.At the same time,I use TensorFlow.<br></p>

<p><strong>Python</strong>:3.5.2<br></p>

<p><strong>System</strong>:Windows 10</p>

<p><strong>TensorFlow</strong>:1.0.1，installed by pip
<hr>
But I have some problems when I run the following code.The code is <a href=""https://www.tensorflow.org/get_started/get_started#basic_usage"" rel=""nofollow noreferrer"">TensorFlow Basic usage</a>.<br></p>

<pre><code>import tensorflow as tf
import numpy as np

features = [tf.contrib.layers.real_valued_column(""x"", dimension=1)]
estimator = tf.contrib.learn.LinearRegressor(feature_columns=features)
x = np.array([1., 2., 3., 4.])
y = np.array([0., -1., -2., -3.])
input_fn = tf.contrib.learn.io.numpy_input_fn({""x"":x}, y, batch_size=4, 
  num_epochs=1000)
estimator.fit(input_fn=input_fn, steps=1000)
print(estimator.evaluate(input_fn=input_fn))
</code></pre>

<p><hr>
This is the result:<br></p>

<pre><code>    WARNING:tensorflow:Using temporary folder as model directory: C:\Users\赵子龙\AppData\Local\Temp\tmp_dq8vxf6
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
    E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
    WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
    WARNING:tensorflow:From C:\Users\赵子龙\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
    Instructions for updating:
    Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.
    Traceback (most recent call last):
      File ""C:\Users\赵子龙\Desktop\Python\ML\ML\LinerRegressor.py"", line 10, in &lt;module&gt;
        estimator.fit(input_fn=input_fn, steps=1000)
      File ""C:\Users\赵子龙\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\util\deprecation.py"", line 280, in new_func
        return func(*args, **kwargs)
      File ""C:\Users\赵子龙\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 426, in fit
        loss = self._train_model(input_fn=input_fn, hooks=hooks)
      File ""C:\Users\赵子龙\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py"", line 981, in _train_model
        config=self.config.tf_config) as mon_sess:
      File ""C:\Users\赵子龙\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 315, in MonitoredTrainingSession
        return MonitoredSession(session_creator=session_creator, hooks=all_hooks)
      File ""C:\Users\赵子龙\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 601, in __init__
        session_creator, hooks, should_recover=True)
      File ""C:\Users\赵子龙\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 428, in __init__
        h.begin()
      File ""C:\Users\赵子龙\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\training\basic_session_run_hooks.py"", line 324, in begin
        self._summary_writer = SummaryWriterCache.get(self._checkpoint_dir)
      File ""C:\Users\赵子龙\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\summary\writer\writer_cache.py"", line 58, in get
        logdir, graph=ops.get_default_graph())
      File ""C:\Users\赵子龙\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\summary\writer\writer.py"", line 289, in __init__
        event_writer = EventFileWriter(logdir, max_queue, flush_secs)
      File ""C:\Users\赵子龙\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\summary\writer\event_file_writer.py"", line 63, in __init__
        gfile.MakeDirs(self._logdir)
      File ""C:\Users\赵子龙\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\lib\io\file_io.py"", line 301, in recursive_create_dir
        pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(dirname), status)
      File ""C:\Users\赵子龙\AppData\Local\Programs\Python\Python35\lib\contextlib.py"", line 66, in __exit__
        next(self.gen)
      File ""C:\Users\赵子龙\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 467, in raise_exception_on_not_ok_status
        pywrap_tensorflow.TF_GetCode(status))
    tensorflow.python.framework.errors_impl.InvalidArgumentError: Failed to create a directory: C:\Users/赵子龙
</code></pre>

<p><hr>
Now I can remove the first warning<br></p>

<pre><code>WARNING:tensorflow:Using temporary folder as model directory: C:\Users\赵子龙\AppData\Local\Temp\tmp_dq8vxf6
</code></pre>

<p>by modifying the code<br> <code>estimator = tf.contrib.learn.LinearRegressor(feature_columns=features)</code><br>to <br>
<code>estimator = tf.contrib.learn.LinearRegressor(feature_columns=features,model_dir='/tmp')</code>.<br>
<br>
<hr>
After the change,my code can run well and no exception and no the first warning.<br>
Here is the result:</p>

<pre><code>E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""BestSplits"" device_type: ""CPU""') for unknown op: BestSplits
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""CountExtremelyRandomStats"" device_type: ""CPU""') for unknown op: CountExtremelyRandomStats
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""FinishedNodes"" device_type: ""CPU""') for unknown op: FinishedNodes
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""GrowTree"" device_type: ""CPU""') for unknown op: GrowTree
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ReinterpretStringToFloat"" device_type: ""CPU""') for unknown op: ReinterpretStringToFloat
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""SampleInputs"" device_type: ""CPU""') for unknown op: SampleInputs
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""ScatterAddNdim"" device_type: ""CPU""') for unknown op: ScatterAddNdim
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNInsert"" device_type: ""CPU""') for unknown op: TopNInsert
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TopNRemove"" device_type: ""CPU""') for unknown op: TopNRemove
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""TreePredictions"" device_type: ""CPU""') for unknown op: TreePredictions
E c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\framework\op_kernel.cc:943] OpKernel ('op: ""UpdateFertileSlots"" device_type: ""CPU""') for unknown op: UpdateFertileSlots
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:From C:\Users\赵子龙\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you resize your input, as this behavior may change.
WARNING:tensorflow:From C:\Users\赵子龙\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.
WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
{'global_step': 1000, 'loss': 5.6782592e-09}
</code></pre>

<p>But I got another error:<em>the result <strong>{'global_step': 1000, 'loss': 5.6782592e-09}</strong> is different from the official answer <strong>{'global_step': 1000, 'loss': 1.9650059e-11}</strong>.</em></p>

<p><strong>Another Question:<br></strong><hr>
When I change <code>model_dir='/tmp'</code> to <code>model_dir='/Users/tmp'</code>,I will get the same
error as I didn't add the parameter.<br><br>
P.S:Users is a directory in C disk.
I want to know whether Users is a different directory so python failed to create a directory in Users in Win10.
<br><br><br>
<strong>Quession Summary</strong></p>

<hr>

<ol>
<li>Why the program will raise an exception if I don't add <code>model_dir</code> parameter?</li>
<li>Why I get a wrong result?</li>
<li>Why the program fail to create a directory in <code>C:\Users</code></li>
</ol>

<p>I hope someone can help me:)</p>
",0
43367697,Batching and shuffling padded tf.train.SequenceExample,"<p>I have some training example of a sequence-to-sequence scenario which are stored as <code>tf.train.SequenceExample</code> in one (or more) file(s) written <code>TFRecordWriter</code>. I would like to read, decode them and feed shuffled batches of them into my network. I have been struggling with the documentation and some tutorials found here and there but I could not make anything out of such stuff. I am working on a self-contained example, here below. </p>

<pre><code>import random

import tensorflow as tf

from six.moves import xrange


MIN_LEN = 6
MAX_LEN = 12
NUM_EXAMPLES = 20
BATCH_SIZE = 3
PATH = 'ciaone.tfrecords'
MIN_AFTER_DEQUEUE = 10
NUM_THREADS = 2
SAFETY_MARGIN = 1
CAPACITY = MIN_AFTER_DEQUEUE + (NUM_THREADS + SAFETY_MARGIN) * BATCH_SIZE


def generate_example():
    # fake examples which are just useful to have a quick visualization.
    # The input is a sequence of random numbers.
    # The output is a sequence made of those numbers from the
    # input sequence which are greater or equal then the average.
    length = random.randint(MIN_LEN, MAX_LEN)
    input_ = [random.randint(0, 10) for _ in xrange(length)]
    avg = sum([1.0 * item for item in input_]) / len(input_)
    output = [item for item in input_ if item &gt;= avg]
    return input_, output


def encode(input_, output):
    length = len(input_)
    example = tf.train.SequenceExample(
        context=tf.train.Features(
            feature={
                'length': tf.train.Feature(
                    int64_list=tf.train.Int64List(value=[length]))
            }),
        feature_lists=tf.train.FeatureLists(
            feature_list={
                'input': tf.train.FeatureList(
                    feature=[
                        tf.train.Feature(
                            int64_list=tf.train.Int64List(value=[item]))
                        for item in input_]),
                'output': tf.train.FeatureList(
                    feature=[
                        tf.train.Feature(
                            int64_list=tf.train.Int64List(value=[item]))
                        for item in output])
            }
        )
    )
    return example


def decode(example):
    context_features = {
        'length': tf.FixedLenFeature([], tf.int64)
    }
    sequence_features = {
        'input': tf.FixedLenSequenceFeature([], tf.int64),
        'output': tf.FixedLenSequenceFeature([], tf.int64)
    }
    ctx, seq = tf.parse_single_sequence_example(
        example, context_features, sequence_features)
    input_ = seq['input']
    output = seq['output']
    return input_, output

if __name__ == '__main__':
    # STEP 1. -- generate a dataset.
    with tf.python_io.TFRecordWriter(PATH) as writer:
        for _ in xrange(NUM_EXAMPLES):
           record = encode(*generate_example())
           writer.write(record.SerializeToString())

    with tf.Session() as sess:
        queue = tf.train.string_input_producer([PATH])
        reader = tf.TFRecordReader()
        _, value = reader.read(queue)
        input_, output = decode(value)

        # HERE I AM STUCK!

        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(coord=coord)
        sess.run(tf.local_variables_initializer())
        sess.run(tf.global_variables_initializer())
        try:
            while True:
                # do something...
        except tf.errors.OutOfRangeError, e:
            coord.request_stop(e)
        finally:
            coord.request_stop()
            coord.join(threads)
        coord.request_stop()
        coord.join(threads)
</code></pre>

<p>Can anyone suggest me how to proceed?
Thanks in advance!</p>

<p>P.S. as a side request: any pointer about resources to better understand the input pipeline APIs of TensorFlow is appreciated.</p>
",1
43420363,Generative sequences in Tensorflow,"<p>I'm using the following code as a way to generate a sequence of length <code>num_steps</code> given <code>starting_point</code> and <code>starting_state</code> using an instance of a <code>RNNCell</code>, for example <code>cell=GRUCell(number_of_neurons)</code>.</p>

<pre><code>outputs = [starting_point]
state = starting_state
for time_step in range(num_steps):
    if time_step &gt; 0: tf.get_variable_scope().reuse_variables()
    (cell_output, state) = cell(outputs[time_step], state)
    outputs.append(cell_output)
</code></pre>

<p>But this is slow and cumbersome for my use case, where <code>num_steps = 1000</code>. Even instantiating the graph takes forever. </p>

<p>Does this functionality exist somewhere in Tensroflow, and I just missed it?</p>

<p>Note that what I'm looking for is similar to, but distinct from, the behavior of <code>tf.contrib.rnn.static_rnn</code>. The <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/static_rnn"" rel=""nofollow noreferrer"">documentation</a> summarizes the behavior of this function as simply applying the RNN to each time step in a sequence:</p>

<pre><code>state = cell.zero_state(...)
  outputs = []
  for input_ in inputs:
    output, state = cell(input_, state)
    outputs.append(output)
  return (outputs, state)
</code></pre>

<p>But in my case, I want to feed the output from one step as the input to the next step.</p>
",0
43422949,CTC Loss InvalidArgumentError: sequence_length(b) <= time,"<p>I am running into this error while trying to use tf.nn.ctc_loss through keras (ctc_batch_cost):</p>

<blockquote>
  <p>InvalidArgumentError (see above for traceback): sequence_length(4) &lt;= 471</p>
</blockquote>

<p>According to the documentation for tf.nn.ctc_loss, Input requirements are:</p>

<blockquote>
  <p>sequence_length(b) &lt;= time for all b</p>
  
  <p>max(labels.indices(labels.indices[:, 1] == b, 2))   &lt;=
  sequence_length(b) for all b.</p>
</blockquote>

<p>I am having a hard time understanding what this means-- what is <code>b</code> and what is <code>sequence_length(b)</code>? </p>
",1
43443205,Tensorflow tf.nn.conv2d clarification,"<p>In reading through the Tensorflow tutorial and API documentation, I do not understand how they defined the shape of the convolution input and filter arguments. The method is: <code>tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)</code>, where the input is shape: <code>[batch, in_height, in_width, in_channels]</code> and the filter is shape: <code>[filter_height, filter_width, in_channels, out_channels]</code>. If anyone could shed light on how to properly define the ""in_channel"" and ""out_channel"" sizes, that would be very helpful. </p>
",1
43453712,What is output tensor of Max Pooling 2D Layer in TensorFlow?,"<p>I was trying to understand some basics about the tensorflow 
and I got stuck while reading documentation for max pooling 2D layer: <a href=""https://www.tensorflow.org/tutorials/layers#pooling_layer_1"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/layers#pooling_layer_1</a></p>

<p>This is how max_pooling2d is specified:</p>

<p><code>pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)</code></p>

<p>where <code>conv1</code> has a tensor with shape <code>[batch_size, image_width, image_height, channels]</code>, concretely in this case it's <code>[batch_size, 28, 28, 32]</code>.</p>

<p>So our input is a tensor with shape: <code>[batch_size, 28, 28, 32]</code>.</p>

<p>My understanding of a max pooling 2D layer is that it will apply a filter of size <code>pool_size</code> (2x2 in this case) and moving sliding window by <code>stride</code> (also 2x2). This means that both <code>width</code> and <code>height</code> of the image will be halfed, i.e. we will end up with 14x14 pixels per channel (32 channels in total), meaning our output is a tensor with shape: <code>[batch_size, 14, 14, 32]</code>.</p>

<p>However, according to the above link, the shape of the output tensor is <code>[batch_size, 14, 14, 1]</code>:</p>

<pre><code>Our output tensor produced by max_pooling2d() (pool1) has a shape of 
[batch_size, 14, 14, 1]: the 2x2 filter reduces width and height by 50%.
</code></pre>

<p>What am I missing here?</p>

<p>How was 32 converted to 1?</p>

<p>They apply the same logic later here: <a href=""https://www.tensorflow.org/tutorials/layers#convolutional_layer_2_and_pooling_layer_2"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/layers#convolutional_layer_2_and_pooling_layer_2</a></p>

<p>but this time it's correct, i.e. <code>[batch_size, 14, 14, 64]</code> becomes <code>[batch_size, 7, 7, 64]</code> (number of channels is the same).</p>
",0
43460838,tensorflow tfrecord storage for large datasets,"<p>I'm trying to understand the ""proper"" method of storage for large datasets for tensorflow ingestion. The documentation seems relatively clear that no matter what, tfrecord files are preferred. Large is a subjective measure, but the examples below are randomly generated regression datasets from sklearn.datasets.make_regression() of 10,000 rows and between 1 and 5,000 features, all float64.</p>

<p>I've experimented with two different methods of writing tfrecord files with dramatically different performance.</p>

<p>For numpy arrays, <code>X</code>, <code>y</code> (X.shape=(10000, n_features), y.shape=(10000,)</p>

<h2><code>tf.train.Example</code> with per-feature <code>tf.train.Features</code></h2>

<p>I construct a tf.train.Example in the way that tensorflow developers seem to prefer, at least judging by tensorflow example code at <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py</a>. </p>

<p>For each observation or row in <code>X</code>, I create a dictionary keyed with feature names (f_0, f_1, ...) whose values are <code>tf.train.Feature</code> objects with the feature's observation data as a single element of its float_list.</p>

<pre><code>def _feature_dict_from_row(row):
    """"""
    Take row of length n+1 from 2-D ndarray and convert it to a dictionary of
    float features:

      {
        'f_0': row[0],
        'f_1': row[1],
        ...
        'f_n': row[n]
      }
    """"""
    def _float64_feature(feature):
        return tf.train.Feature(float_list=tf.train.FloatList(value=[feature]))

    features = { ""f_{:d}"".format(i): _float64_feature(value) for i, value in enumerate(row) }
    return features

def write_regression_data_to_tfrecord(X, y, filename):
    with tf.python_io.TFRecordWriter('{:s}'.format(filename)) as tfwriter:
        for row_index in range(X.shape[0]):
            features = _feature_dict_from_row(X[row_index])
            features['label'] = y[row_index]

            example = tf.train.Example(features=tf.train.Features(feature=features))
            tfwriter.write(example.SerializeToString())
</code></pre>

<h2><code>tf.train.Example</code> with one large <code>tf.train.Feature</code> containing all features</h2>

<p>I construct a dictionary with one feature (really two counting the label) whose value is a <code>tf.train.Feature</code> with the entire feature row in as its float_list</p>

<pre><code>def write_regression_data_to_tfrecord(X, y, filename, store_by_rows=True):
    with tf.python_io.TFRecordWriter('{:s}'.format(filename)) as tfwriter:
        for row_index in range(X.shape[0]):
            features = { 'f_0': tf.train.Feature(float_list=tf.train.FloatList(value=X[row_index])) }
            features['label'] = y[row_index]

            example = tf.train.Example(features=tf.train.Features(feature=features))
            tfwriter.write(example.SerializeToString())
</code></pre>

<p>As the number of features in the dataset grows, the second option gets considerably faster than the first, as shown in the following graph. <em>Note the log scale</em></p>

<p><strong>10,000 rows:</strong></p>

<p><img src=""https://i.stack.imgur.com/p4w9M.png"" alt=""graph""></p>

<p>It makes intuitive sense to me that creating 5,000 <code>tf.train.Feature</code> objects is significantly slower than creating one object with a float_list of 5,000 elements, but it's not clear that this is the ""intended"" method for feeding large numbers of features into a tensorflow model.</p>

<p>Is there something inherently wrong with doing this the faster way?</p>
",1
43489549,Tensorflow: Input pipeline with sparse data for the SVM estimator,"<h2>Introduction:</h2>

<p>I am trying to train the tensorflow svm estimator <code>tensorflow.contrib.learn.python.learn.estimators.svm</code> with sparse data. Sample usage with sparse data at the github repo at <code>tensorflow/contrib/learn/python/learn/estimators/svm_test.py#L167</code> (I am not allowed to post more links, so here the relative path).</p>

<p>The svm estimator expects as parameter <code>example_id_column</code> and <code>feature_columns</code>, where the feature columns should be derived of class <code>FeatureColumn</code> such as <code>tf.contrib.layers.feature_column.sparse_column_with_hash_bucket</code>. See Github repo at <code>tensorflow/contrib/learn/python/learn/estimators/svm.py#L85</code> and the documentation at tensorflow.org at <code>python/contrib.layers#Feature_columns</code>.</p>

<h2>Question:</h2>

<ol>
<li>How do I have to set up my input pipeline to format sparse data in such a way that I can use one of the tf.contrib.layers feature_columns as input for the svm estimator.</li>
<li>How would a dense input function with many features look like?</li>
</ol>

<h2>Background</h2>

<p>The data that I use is the <code>a1a</code> dataset from the <a href=""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html"" rel=""noreferrer"">LIBSVM website</a>. The data set has 123 features (that would correspond to 123 feature_columns if the data would be dense). I wrote an user op to read the data like <code>tf.decode_csv()</code> but for the LIBSVM format. The op returns the labels as dense tensor and the features as sparse tensor. My input pipeline:</p>

<pre class=""lang-py prettyprint-override""><code>NUM_FEATURES = 123
batch_size = 200

# my op to parse the libsvm data
decode_libsvm_module = tf.load_op_library('./libsvm.so')

def input_pipeline(filename_queue, batch_size):
    with tf.name_scope('input'):
        reader = tf.TextLineReader(name=""TextLineReader_"")
        _, libsvm_row = reader.read(filename_queue, name=""libsvm_row_"")
        min_after_dequeue = 1000
        capacity = min_after_dequeue + 3 * batch_size
        batch = tf.train.shuffle_batch([libsvm_row], batch_size=batch_size,
                                       capacity=capacity,
                                       min_after_dequeue=min_after_dequeue,
                                       name=""text_line_batch_"")
        labels, sp_indices, sp_values, sp_shape = \
            decode_libsvm_module.decode_libsvm(records=batch,
                                               num_features=123,
                                               OUT_TYPE=tf.int64, 
                                               name=""Libsvm_decoded_"")
        # Return the features as sparse tensor and the labels as dense
        return tf.SparseTensor(sp_indices, sp_values, sp_shape), labels
</code></pre>

<p><a href=""https://gist.github.com/Rikorose/1a1240aea0a40d96ae8085583c68cd2b"" rel=""noreferrer"">Here</a> is an example batch with <code>batch_size = 5</code>.</p>

<pre class=""lang-py prettyprint-override""><code>def input_fn(dataset_name):
    maybe_download()

    filename_queue_train = tf.train.string_input_producer([dataset_name], 
                                                        name=""queue_t_"")
    features, labels = input_pipeline(filename_queue_train, batch_size)

    return {
        'example_id': tf.as_string(tf.range(1,123,1,dtype=tf.int64)),
        'features': features
    }, labels
</code></pre>

<p>This is what I tried so far:</p>

<pre class=""lang-py prettyprint-override""><code>with tf.Session().as_default() as sess:
    sess.run(tf.global_variables_initializer())

    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(sess=sess, coord=coord)

    feature_column = tf.contrib.layers.sparse_column_with_hash_bucket(
        'features', hash_bucket_size=1000, dtype=tf.int64)

    svm_classifier = svm.SVM(feature_columns=[feature_column],
                             example_id_column='example_id',
                             l1_regularization=0.0,
                             l2_regularization=1.0)
    svm_classifier.fit(input_fn=lambda: input_fn(TRAIN),
                       steps=30)

    accuracy = svm_classifier.evaluate(
        input_fn= lambda: input_fn(features, labels), 
        steps=1)['accuracy']                       
    print(accuracy)
    coord.request_stop()

    coord.join(threads)
    sess.close()
</code></pre>
",0
43504906,How do I read variable length 1D inputs in Tensorflow?,"<p>I'm trying to read variable length 1-D inputs into a Tensorflow CNN. </p>

<p>I have previously implemented reading fixed length inputs by first constructing a CSV file (where the first column is the label and the remaining columns are the input values - flattened spectrogram data all padded/truncated to the same length) using tf.TextLineReader(). </p>

<p>This time I have a directory full of files each one containing a line of data I want to use as input (flattened spectrogram data again but I do not want to force them to the same dimensions), and the line lengths are not fixed. I'm getting an error trying to use the previous approach of compiling a CSV first. I looked into the documentation of tf.TextLineReader() and it specifies that all CSV rows must be the same shape, so I am stuck! Any help would be much appreciated, thanks :) </p>
",0
43624625,Why do we have to specify output shape during deconvolution in tensorflow?,"<p>The TF documentation has an output_shape parameter in <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose"" rel=""nofollow noreferrer"">tf.conv2d_transpose</a>. Why is this needed? Don't the strides, filter size and padding parameters of the layer decide the output shape of that layer, similar to how it is decided during convolution? </p>
",1
43638488,Check failed: NDIMS == dims() (2 vs. 1) when I build a svm model,"<p>when I build a svm model with tf.learn, it get error like this:
<a href=""https://i.stack.imgur.com/612wH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/612wH.png"" alt=""enter image description here""></a>
I have run the LinearClassifier and it success!
I don't know how to fix the error, Could anyone help with it?</p>

<p>Here is the code to reproduce the error:</p>

<pre><code>import tensorflow as tf
import pandas as pd
from tensorflow.contrib.learn.python.learn.estimators import svm


detailed_occupation_recode = tf.contrib.layers.sparse_column_with_hash_bucket(
    column_name='detailed_occupation_recode', 
    hash_bucket_size = 1000
)
education = tf.contrib.layers.sparse_column_with_hash_bucket(
    column_name='education',
    hash_bucket_size=1000
)
# Continuous base columns
age = tf.contrib.layers.real_valued_column('age')
wage_per_hour = tf.contrib.layers.real_valued_column('wage_per_hour')



columns = ['age', 'detailed_occupation_recode', 'education', 'wage_per_hour','label']
FEATURE_COLUMNS = [
    # age, age_buckets, class_of_worker, detailed_industry_recode,
    age, detailed_occupation_recode, education, wage_per_hour

]


LABEL_COLUMN = 'label'

CONTINUOUS_COLUMNS = ['age', 'wage_per_hour']

CATEGORICAL_COLUMNS = ['detailed_occupation_recode','education']


df_train = pd.DataFrame([[12,'12','7th and 8th grade',40,'- 50000'],
                [40,'45','7th and 8th grade',40, '50000+'],
                [50,'50','10th grade',40,'50000+'],
                [60,'30','7th and 8th grade',40,'- 50000']],
                columns=['age', 'detailed_occupation_recode', 'education', 'wage_per_hour', 'label'])


df_test = pd.DataFrame([[12,'12','7th and 8th grade',40,'- 50000'],
                [40,'45','7th and 8th grade',40, '50000+'],
                [50,'50','10th grade',40,'50000+'],
                [60,'30','7th and 8th grade',40,'- 50000']],
                columns=['age', 'detailed_occupation_recode', 'education', 'wage_per_hour', 'label'])
df_train[LABEL_COLUMN] = (df_train[LABEL_COLUMN].apply(lambda x: '+' in x)).astype(int)
df_test[LABEL_COLUMN] = (df_test[LABEL_COLUMN].apply(lambda x: '+' in x)).astype(int)
dtypess = df_train.dtypes


def input_fn(df):
    continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}
    categorical_cols = {k: tf.SparseTensor(
        indices=[[i, 0] for i in range(df[k].size)],
        values=df[k].values,
        dense_shape=[df[k].size, 1]) for k in CATEGORICAL_COLUMNS}    
    feature_cols = dict(continuous_cols.items() + categorical_cols.items())
    feature_cols['example_id'] = tf.constant([str(i+1) for i in range(df['age'].size)])
    label = tf.constant(df[LABEL_COLUMN].values)
    return feature_cols, label

def train_input_fn():
    return input_fn(df_train)

def eval_input_fn():
    return input_fn(df_test)

model_dir = '../svm_model_dir'

model = svm.SVM(example_id_column='example_id', feature_columns=FEATURE_COLUMNS, model_dir=model_dir)
model.fit(input_fn=train_input_fn, steps=10)
results = model.evaluate(input_fn=eval_input_fn, steps=1)
for key in sorted(results):
    print(""%s: %s"" % (key, results[key]))
</code></pre>

<p>and the all output text result :</p>

<pre><code>WARNING:tensorflow:The default value of combiner will change from ""sum"" to 

""sqrtn"" after 2016/11/01.
WARNING:tensorflow:The default value of combiner will change from ""sum"" to ""sqrtn"" after 2016/11/01.
WARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you
 resize your input, as this behavior may change.
WARNING:tensorflow:Rank of input Tensor (1) should be the same as output_rank (2) for column. Will attempt to expand dims. It is highly recommended that you
 resize your input, as this behavior may change.
WARNING:tensorflow:From /Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:882: hinge_loss (from t
ensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.
Instructions for updating:
Use tf.losses.hinge_loss instead.
WARNING:tensorflow:From /Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (f
rom tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
Instructions for updating:
Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-dupl
icate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.
WARNING:tensorflow:From /Users/burness/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:882: hinge_loss (from t
ensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.
Instructions for updating:
Use tf.losses.hinge_loss instead.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machi
ne and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machi
ne and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine
and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine
 and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine
and could speed up CPU computations.
F tensorflow/core/framework/tensor_shape.cc:36] Check failed: NDIMS == dims() (2 vs. 1)Asking for tensor of 2 dimensions from a tensor of 1 dimensions
[1]    66225 abort      python simple-tf-svm.py
</code></pre>
",0
43684973,Feeding tf.contrib.learn inputs into DNNClassifier,"<p>I am new to tensor flow and stackoverflow, so apologies in advance for any silly errors.  I've had good success in feeding the lower level interfaces.  So I decided, to give the <code>tf.contrib.learn</code> higher level apis a try because it looked so easy.  I'm working in Google Cloud Datalab (Jupyter notebook) But I've hit a roadblock and am looking for help.</p>

<p><strong>Main Question</strong>: How do I instantiate a <code>DNNClassifier</code> so that I can feed it a feature that is itself a list of <code>tf.float32</code> numbers ?</p>

<p>Here are the details.  I am reading a <code>TFRecords</code> based input file with the following code:</p>

<pre><code>def read_and_decode(filename_queue):  
    # get a tensorflow reader and read in an example
    reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)

    # parse a single example
    features = tf.parse_single_example(serialized_example, features={ 
               'label': tf.FixedLenFeature([], tf.int64),
               'features': tf.FixedLenFeature([], tf.string)} )

    # convert to tensors and return
    bag_of_words = tf.decode_raw(features['features'], tf.float32)
    bag_of_words.set_shape([LEN_OF_LEXICON])
    label = tf.cast(features['label'], tf.int32) 

    return bag_of_words, label
</code></pre>

<p>My unit test of this looks this:</p>

<pre><code># unit test
filename = VALIDATION_FILE
my_filename_queue = tf.train.string_input_producer([filename], 
num_epochs=1)
x, y = read_and_decode(my_filename_queue)
print ('x[0] -&gt; ', x[0])
print ('x[1] -&gt; ', x[1])
print ('y -&gt; ', y, 'type -&gt; ', type(y))
print ('x -&gt; ', x, 'type -&gt; ', type(x))
</code></pre>

<p>And gives the following output:</p>

<pre><code>x[0] -&gt;  Tensor(""strided_slice_6:0"", shape=(), dtype=float32)
x[1] -&gt;  Tensor(""strided_slice_7:0"", shape=(), dtype=float32)
y -&gt;  Tensor(""Cast_6:0"", shape=(), dtype=int32) type -&gt;  &lt;class 
'tensorflow.python.framework.ops.Tensor'&gt;
x -&gt;  Tensor(""DecodeRaw_3:0"", shape=(2633,), dtype=float32) type -&gt;
&lt;class 'tensorflow.python.framework.ops.Tensor'&gt;
</code></pre>

<p>The read_and_decode function is called by input_pipeline which has the following def and unit test:</p>

<pre><code>def input_pipeline(filenames, batch_size, num_epochs=None):

    filename_queue = tf.train.string_input_producer(filenames, 
               num_epochs=num_epochs, shuffle=True)

    example, label = read_and_decode(filename_queue)

    min_after_dequeue = 10000
    capacity = min_after_dequeue + 3 * batch_size
    example_batch, label_batch = tf.train.shuffle_batch([example, 
           label], batch_size=batch_size, capacity=capacity, 
           min_after_dequeue=min_after_dequeue) 

    return example_batch, label_batch

# unit test
x, y = input_pipeline([VALIDATION_FILE], BATCH_SIZE, num_epochs=1)
print ('y -&gt; ', y, 'type -&gt; ', type(y))
print ('x -&gt; ', x, 'type -&gt; ', type(x))
</code></pre>

<p>And has the following output:</p>

<pre><code>y -&gt;  Tensor(""shuffle_batch_4:1"", shape=(100,), dtype=int32) type -&gt;
&lt;class 'tensorflow.python.framework.ops.Tensor'&gt;
x -&gt;  Tensor(""shuffle_batch_4:0"", shape=(100, 2633), dtype=float32) 
type -&gt; &lt;class 'tensorflow.python.framework.ops.Tensor'&gt;
</code></pre>

<p>The trainer that will take these feeds looks like this:</p>

<pre><code>def run_training():
    #feature_columns = ????????????
    feature_columns = tf.contrib.layers.real_valued_column("""", 
             dimension=LEN_OF_LEXICON, dtype=tf.float32)
    estimator = tf.contrib.learn.DNNClassifier(
                       feature_columns=feature_columns,
                       n_classes=5,
                       hidden_units=[1024, 512, 256], 
                       optimizer = 
                  tf.train.ProximalAdagradOptimizer(learning_rate=0.1, 
                             l1_regularization_strength=0.001) )

     estimator.fit(input_fn=lambda: input_pipeline([VALIDATION_FILE], 
            BATCH_SIZE, num_epochs=1))

# unit test
run_training()
</code></pre>

<p>The instantiation of <code>DNNClassifier</code> passes fine, but the call to <code>estimator.fit()</code> throws an exception (traceback below snippet below).  My <code>input_pipeline</code> is supplying the feed as shown in the tensor flow documentation, but somehow the form of the data inside the tensor does not appear to be correct.  Anyone have any thoughts on this?</p>

<pre><code>---------------- Traceback Snippet -----------------
&gt; `/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.pyc in _dnn_model_fn(features, labels, mode, params, config)
    126         feature_columns=feature_columns,
    127         weight_collections=[parent_scope],
--&gt; 128         scope=scope)
    129 
    130   hidden_layer_partitioner = (
/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/feature_column_ops.pyc in input_from_feature_columns(columns_to_tensors, feature_columns, weight_collections, trainable, scope)
    247                                      scope,
    248                                      output_rank=2,
--&gt; 249                                      default_name='input_from_feature_columns')
    250 
    251 
/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/feature_column_ops.pyc in _input_from_feature_columns(columns_to_tensors, feature_columns, weight_collections, trainable, scope, output_rank, default_name)
    145                                 default_name):
    146   """"""Implementation of `input_from(_sequence)_feature_columns`.""""""
--&gt; 147   check_feature_columns(feature_columns)
    148   with variable_scope.variable_scope(scope,
    149                                      default_name=default_name,
/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/feature_column_ops.pyc in check_feature_columns(feature_columns)
    806   seen_keys = set()
    807   for f in feature_columns:
--&gt; 808     key = f.key
    809     if key in seen_keys:
    810       raise ValueError('Duplicate feature column key found for column: {}. '
AttributeError: 'str' object has no attribute 'key'
`
</code></pre>
",0
43752099,Predictions for tf.contrib.metrics.streaming_auc distributed uniformly?,"<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/metrics/streaming_auc"" rel=""nofollow noreferrer"">TensorFlow documentation for tf.contrib.metrics.streaming_auc</a> mentions the following:</p>

<blockquote>
  <p>For best results, predictions should be distributed approximately
  uniformly in the range [0, 1] and not peaked around 0 or 1. The
  quality of the AUC approximation may be poor if this is not the case.</p>
</blockquote>

<p>I am a bit confused because I feel like a common paradigm is to have <code>predictions</code> be compared against a target 1-hot encoding:</p>

<pre><code>logits = tf.contrib.layers.fully_connected(
    last_fully_connected_layer,
    num_outputs=2,
    activation_fn=None)
loss = tf.losses.softmax_cross_entropy(target, logits)

# I believe this yields a 3-vector with all 0s but a 1 at 1 single position.
predictions = tf.argmax(logits, 1)
</code></pre>

<p>In those cases, the <code>predictions</code> tensor contains all 0s or 1s.</p>

<p>Should we avoid using <code>tf.contrib.metrics.streaming_auc</code> in those cases? I am not sure in what cases we would use <code>tf.contrib.metrics.streaming_auc</code> then.</p>
",1
43775463,Parse batch of SequenceExample,"<p>There is function to parse SequenceExample --> tf.parse_single_sequence_example().</p>

<p>But it parses only single SequenceExample, which is not effective.</p>

<p>Is there any possibility to parse a batch of SequenceExamples?</p>

<p>tf.parse_example can parse many Examples.
Documentation for tf.parse_example contain a little info about SequenceExample:</p>

<blockquote>
  <p>Each FixedLenSequenceFeature df maps to a Tensor of the specified type (or tf.float32 if not specified) and shape (serialized.size(), None) + df.shape. All examples in serialized will be padded with default_value along the second dimension.</p>
</blockquote>

<p>But it is not clear, how to do that. Have not found any examples in google.</p>

<p>Is it possible to parse many SequenceExamples using parse_example() or may be other function exists?</p>

<p>Edit:
Where can I ask question to tensorflow developers: does they plan to implement parse function for multiple SequenceExample -s?</p>

<p>Any help ll be appreciated. </p>
",1
43792961,Understanding the while loop in Tensorflow,"<p>I am using the <a href=""https://www.tensorflow.org/api_docs/python/"" rel=""noreferrer"">Python API for Tensorflow</a>. I am trying to implement the <a href=""https://www.sfu.ca/~ssurjano/rosen.html"" rel=""noreferrer"">Rosenbrock function</a> given below without the use of a Python loop:</p>

<p><a href=""https://i.stack.imgur.com/9AdOH.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/9AdOH.png"" alt=""Rosenbrock function""></a></p>

<p>My current implementation is as follows:</p>

<pre><code>def rosenbrock(data_tensor):
    columns = tf.unstack(data_tensor)

    summation = 0
    for i in range(1, len(columns) - 1):
        first_term = tf.square(tf.subtract(columns[i + 1], tf.square(columns[i])))
        second_term = tf.square(tf.subtract(columns[i], 1.0))
        summation += tf.add(tf.multiply(100.0, first_term), second_term)

    return summation
</code></pre>

<p>I have tried implementing the summation in a <a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""noreferrer""><code>tf.while_loop()</code></a>; however, I found the API somewhat unintuitive when it comes to using an index integer that is meant to remain separate from the data. The example given in the <a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""noreferrer"">documentation</a> uses the data as the index (or vice-versa):</p>

<pre><code>i = tf.constant(0)
c = lambda i: tf.less(i, 10)
b = lambda i: tf.add(i, 1)
r = tf.while_loop(c, b, [i])
</code></pre>
",1
43851999,resize 3D image with 5D tensor in tensorflow,"<p>I am using 3D convolution for my network. In a node of my network I need to resize my image from <code>[5,50,50,10,256]</code> to <code>[5,100,100,10,256]</code>. I just want to resize axis 1 and axis 2 of my image.</p>
<p>I tried to use <a href=""https://www.tensorflow.org/api_docs/python/tf/image/resize_images"" rel=""nofollow noreferrer"">tf.image.resize_images</a>, but it seems it is only working on 3D or4D tensors.</p>
<p>Any suggestion what can I do?</p>
",0
43885770,Clarification of tf.name_scope in TensorFlow documentation,"<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/name_scope"" rel=""nofollow noreferrer"">TensorFlow documentation</a> mentions the following for <code>tf.name_scope</code></p>

<pre><code>This context manager validates that the given values are from the same
graph, makes that graph the default graph, and pushes a name scope in 
that graph.
</code></pre>

<p>What is the meaning of <code>given values are from the same graph, makes that graph the default graph</code> ? </p>

<p><code>Same graph</code> refers to which graph ?</p>

<p>Also, what is the use of <code>values</code> parameter in <code>tf.name_scope</code> ? </p>
",1
43916019,Control dependencies and order of evaluation,"<p>Please consider the following code: </p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np

with tf.device(""gpu:0""):
    sess = tf.InteractiveSession()
    idx = tf.constant(0)
    # 10 iterations
    while_condition = lambda i: tf.less(i, tf.constant(10))        
    acc = tf.Variable(0, dtype=tf.float64)
    # the body of the while adds 1 to acc in each iteration
    def body_accumulator(i):
        mainOp = tf.assign_add(acc, 1.0)
        return tf.tuple([tf.add(i, 1)], control_inputs=[mainOp])
    whileOp = tf.while_loop(while_condition, body_accumulator, [idx])

    # My idea: return acc after evaluating whileOp, whose code modifies acc
    def f(dummy):
        with tf.control_dependencies([whileOp]):
            # with return tf.identity(acc) it works
            return acc
    def g():
        return acc

    sess.run(tf.global_variables_initializer())
    print('""g: return acc .eval()"" - this is the only time where I would expect 0')
    print(g().eval())
    print('f(dummy)')
    print(f(1).eval())
    print('whileOp.eval()')
    print(whileOp.eval())
    print('acc value:')
    print(acc.eval())
    print('""g: return acc .eval()""')
    print(g().eval())
</code></pre>

<p>The output is:</p>

<pre><code>""g: return acc .eval()"" - this is the only time where I would expect 0
0.0
f(dummy)
0.0
whileOp.eval()
10
acc value:
10.0
""g: return acc .eval()""
10.0
</code></pre>

<p>My question is:</p>

<p>why does <code>f(1).eval()</code> return 0 even if there is a control dependency on the <code>whileOp</code> that modifies the returned variable <code>acc</code>?</p>

<p>After reading the documentation, I was expecting <code>whileOp</code> to be evaluated before returning acc. How should I write the function <code>f(.)</code> in order to force the evaluation of <code>whileOp</code>?</p>

<p>In <code>f(.)</code>, if I return <code>tf.identity(acc)</code> instead of <code>acc</code>, it works as I expect.</p>
",1
43929501,Why is doing softmax and crossentropy separately produce different result than doing them together using softmax_cross_entropy_with_logits?,"<p>I was making a computer to predict a hand-written number from MNist data set using softmax function. and something weird happened. the cost was decreasing over time and eventually becomes something around 0.0038....(I used softmax_crossentropy_with_logits() for the cost function) However, the accuracy was pretty as low as 33%. So I thought ""well.. I don't know what happened there but if I do softmax and crossentropy separately maybe it will produce different result !"" and boom ! accuracy went up to 89 %. I have no idea why doing softmax and crossentropy separately makes such different result. I even looked up here :<a href=""https://stackoverflow.com/questions/34240703/difference-between-tensorflow-tf-nn-softmax-and-tf-nn-softmax-cross-entropy-with"">difference between tensorflow tf.nn.softmax and tf.nn.softmax_cross_entropy_with_logits</a></p>

<p>so this is the code that I used softmax_cross_entropy_with_logits() for the cost function (accuracy: 33%)</p>

<pre><code>import tensorflow as tf
import numpy as np
from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets(""MNIST_data"", one_hot=True)

X = tf.placeholder(shape=[None,784],dtype=tf.float32)
Y = tf.placeholder(shape=[None,10],dtype=tf.float32)

W1= tf.Variable(tf.random_normal([784,20]))
b1= tf.Variable(tf.random_normal([20]))
layer1 = tf.nn.softmax(tf.matmul(X,W1)+b1)

W2 = tf.Variable(tf.random_normal([20,10]))
b2 = tf.Variable(tf.random_normal([10]))

logits = tf.matmul(layer1,W2)+b2
hypothesis = tf.nn.softmax(logits) # just so I can figure our the accuracy 

cost_i= tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y)
cost = tf.reduce_mean(cost_i)
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)


batch_size  = 100
train_epoch = 25
display_step = 1
with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())

    for epoch in range(train_epoch):
        av_cost = 0
        total_batch = int(mnist.train.num_examples / batch_size)
        for batch in range(total_batch):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            sess.run(optimizer,feed_dict={X:batch_xs,Y:batch_ys})
        av_cost  += sess.run(cost,feed_dict={X:batch_xs,Y:batch_ys})/total_batch
        if epoch % display_step == 0:  # Softmax
            print (""Epoch:"", '%04d' % (epoch + 1), ""cost="", ""{:.9f}"".format(av_cost))
    print (""Optimization Finished!"")

    correct_prediction = tf.equal(tf.argmax(hypothesis,1),tf.argmax(Y,1))
    accuray = tf.reduce_mean(tf.cast(correct_prediction,'float32'))
    print(""Accuracy:"",sess.run(accuray,feed_dict={X:mnist.test.images,Y:mnist.test.labels}))
</code></pre>

<p>and this is the one that I did softmax and cross_entropy separately(accuracy: 89%)</p>

<pre><code>import tensorflow as tf  #89 % accuracy one 
import numpy as np
from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets(""MNIST_data"", one_hot=True)

X = tf.placeholder(shape=[None,784],dtype=tf.float32)
Y = tf.placeholder(shape=[None,10],dtype=tf.float32)

W1= tf.Variable(tf.random_normal([784,20]))
b1= tf.Variable(tf.random_normal([20]))
layer1 = tf.nn.softmax(tf.matmul(X,W1)+b1)

W2 = tf.Variable(tf.random_normal([20,10]))
b2 = tf.Variable(tf.random_normal([10]))


#logits = tf.matmul(layer1,W2)+b2
#cost_i= tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y)

logits = tf.matmul(layer1,W2)+b2

hypothesis = tf.nn.softmax(logits)
cost = tf.reduce_mean(tf.reduce_sum(-Y*tf.log(hypothesis)))


optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)

batch_size  = 100
train_epoch = 25
display_step = 1
with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())

    for epoch in range(train_epoch):
        av_cost = 0
        total_batch = int(mnist.train.num_examples / batch_size)
        for batch in range(total_batch):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            sess.run(optimizer,feed_dict={X:batch_xs,Y:batch_ys})
        av_cost  += sess.run(cost,feed_dict={X:batch_xs,Y:batch_ys})/total_batch
        if epoch % display_step == 0:  # Softmax
            print (""Epoch:"", '%04d' % (epoch + 1), ""cost="", ""{:.9f}"".format(av_cost))
    print (""Optimization Finished!"")

    correct_prediction = tf.equal(tf.argmax(hypothesis,1),tf.argmax(Y,1))
    accuray = tf.reduce_mean(tf.cast(correct_prediction,'float32'))
    print(""Accuracy:"",sess.run(accuray,feed_dict={X:mnist.test.images,Y:mnist.test.labels}))
</code></pre>
",0
43942274,Distributed custom model function for Estimator,"<p>I am trying to get my model that uses <code>tf.contrib.learn.Estimator</code> with a custom model function to perform distributed training.</p>

<p>The <a href=""https://www.tensorflow.org/deploy/distributed"" rel=""nofollow noreferrer"">Tensorflow documentation</a> specifies that jobs (ps and/or worker) should be manually placed in the model using the <code>tf.device</code> function. However, this documentation refers to the old way of declaring the model, without the use of the <code>Estimator</code> class and model functions.</p>

<p>I found some <a href=""http://terrytangyuan.github.io/2016/06/09/scikit-flow-v09/#distributed-estimator"" rel=""nofollow noreferrer"">information</a> stating that the new <code>Estimator</code> ""handles most of the complicated distributed logics of model training and evaluation"". Does this mean that the various worker/ps jobs still need to be manually placed in the model? And if yes, is there any change in the way they should be declared from the code in the Tensorflow documentation (i.e. using <code>tf.device</code>)?</p>
",1
44141986,Clarification on Tensorflow tensor shapes and matmul,"<p>I need some clarification on how Tensorflow treats the shape of its tensors. This is taken from the <a href=""https://www.tensorflow.org/get_started/mnist/pros"" rel=""nofollow noreferrer"">MNIST example</a>:</p>

<p>I define a placeholder that will at some later point be fed with some of my training data:</p>

<p><code>x = tf.placeholder(tf.float32, shape=[None, 784])</code></p>

<p>During runtime I feed it in batches of 100, so its shape during runtime is <code>(100, 784)</code>. I also define weights and biases: </p>

<pre><code>W = tf.Variable(tf.zeros([784,10]))
b = tf.Variable(tf.zeros([10]))
</code></pre>

<p><code>W</code>is of shape <code>(784, 10)</code> and <code>b</code>is of shape <code>(10)</code>. Now I compute </p>

<pre><code>y = tf.matmul(x,W) + b
</code></pre>

<p>And this is where I am stuck. The matrix product of <code>x</code> and <code>W</code> is of shape <code>(None, 10)</code> or <code>(100, 10)</code> during runtime. However I can without error add vector <code>b</code> to it. This confuses me. How can this work? And is there some better documentation for this?</p>
",1
44162432,Analysis of the output from tf.nn.dynamic_rnn tensorflow function,"<p>I am not able to understand the output from <code>tf.nn.dynamic_rnn</code> tensorflow function. The document just tells about the size of the output, but it doesn't tell what does each row/column means. From the documentation:</p>

<blockquote>
  <p><strong>outputs</strong>: The RNN output <code>Tensor</code>.</p>
  
  <p>If time_major == False (default), this will be a <code>Tensor</code> shaped:
      <code>[batch_size, max_time, cell.output_size]</code>.</p>
  
  <p>If time_major == True, this will be a <code>Tensor</code> shaped:
      <code>[max_time, batch_size, cell.output_size]</code>.</p>
  
  <p>Note, if <code>cell.output_size</code> is a (possibly nested) tuple of integers
  or <code>TensorShape</code> objects, then <code>outputs</code> will be a tuple having the<br>
  same structure as <code>cell.output_size</code>, containing Tensors having shapes
  corresponding to the shape data in <code>cell.output_size</code>.</p>
  
  <p><strong>state</strong>: The final state.  If <code>cell.state_size</code> is an int, this   will
  be shaped <code>[batch_size, cell.state_size]</code>.  If it is a<br>
  <code>TensorShape</code>, this will be shaped <code>[batch_size] + cell.state_size</code>.<br>
  If it is a (possibly nested) tuple of ints or <code>TensorShape</code>, this will
  be a tuple having the corresponding shapes.</p>
</blockquote>

<p>The <code>outputs</code> tensor is a 3-D matrix but what does each row/column represent?</p>
",1
44201892,How to save trained model in tensorflow?,"<p>I wrote a convolutional neural network in tensorflow to perform on the mnist dataset. Everything works just fine, but i want to save the model with the tf.train.Saver(). How am i gonna do it?
Here is my code:</p>

<pre><code>from __future__ import print_function

import tensorflow as tf

# Import MNIST data
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""/tmp/data/"", one_hot=True)

# Parameters
learning_rate = 0.001
training_iters = 200000
batch_size = 128
display_step = 10

# Network Parameters
n_input = 784 # MNIST data input (img shape: 28*28)
n_classes = 10 # MNIST total classes (0-9 digits)
dropout = 0.75 # Dropout, probability to keep units

# tf Graph input
x = tf.placeholder(tf.float32, [None, n_input])
y = tf.placeholder(tf.float32, [None, n_classes])
keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)


# Create some wrappers for simplicity
def conv2d(x, W, b, strides=1):
    # Conv2D wrapper, with bias and relu activation
    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')
    x = tf.nn.bias_add(x, b)
    return tf.nn.relu(x)


def maxpool2d(x, k=2):
    # MaxPool2D wrapper
    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],
                      padding='SAME')


# Create model
def conv_net(x, weights, biases, dropout):
    # Reshape input picture
    x = tf.reshape(x, shape=[-1, 28, 28, 1])

    # Convolution Layer
    conv1 = conv2d(x, weights['wc1'], biases['bc1'])
    # Max Pooling (down-sampling)
    conv1 = maxpool2d(conv1, k=2)

    # Convolution Layer
    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])
    # Max Pooling (down-sampling)
    conv2 = maxpool2d(conv2, k=2)

    # Fully connected layer
    # Reshape conv2 output to fit fully connected layer input
    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])
    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])
    fc1 = tf.nn.relu(fc1)
    # Apply Dropout
    fc1 = tf.nn.dropout(fc1, dropout)

    # Output, class prediction
    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])
    return out

# Store layers weight &amp; bias
weights = {
    # 5x5 conv, 1 input, 32 outputs
    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),
    # 5x5 conv, 32 inputs, 64 outputs
    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),
    # fully connected, 7*7*64 inputs, 1024 outputs
    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),
    # 1024 inputs, 10 outputs (class prediction)
    'out': tf.Variable(tf.random_normal([1024, n_classes]))
}

biases = {
    'bc1': tf.Variable(tf.random_normal([32])),
    'bc2': tf.Variable(tf.random_normal([64])),
    'bd1': tf.Variable(tf.random_normal([1024])),
    'out': tf.Variable(tf.random_normal([n_classes]))
}

# Construct model
pred = conv_net(x, weights, biases, keep_prob)

# Define loss and optimizer
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, 
labels=y))
optimizer = 
tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

# Evaluate model
correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

# Initializing the variables
init = tf.initialize_all_variables()

# Launch the graph
with tf.Session() as sess:
    sess.run(init)
    step = 1
    # Keep training until reach max iterations
    while step * batch_size &lt; training_iters:
        batch_x, batch_y = mnist.train.next_batch(batch_size)
        # Run optimization op (backprop)
        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,
                                       keep_prob: dropout})
        if step % display_step == 0:
            # Calculate batch loss and accuracy
            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,
                                                              y: batch_y,
                                                             keep_prob: 1.})
            print(""Iter "" + str(step*batch_size) + "", Minibatch Loss= "" + \
                  ""{:.6f}"".format(loss) + "", Training Accuracy= "" + \
                  ""{:.5f}"".format(acc))
        step += 1
    print(""Optimization Finished!"")

    # Calculate accuracy for 256 mnist test images
    print(""Testing Accuracy:"", \
        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],
                                      y: mnist.test.labels[:256],
                                      keep_prob: 1.}))
</code></pre>
",0
44206534,Why is tf.transpose so important in a RNN?,"<p>I've been reading the docs to learn TensorFlow and have been struggling on when to use the following functions and their purpose.</p>

<pre><code>tf.split()
tf.reshape()
tf.transpose()
</code></pre>

<p>My guess so far is that:</p>

<p>tf.split() is used because inputs must be a sequence.</p>

<p>tf.reshape() is used to make the shapes compatible (Incorrect shapes tends to be a common problem / mistake for me). I used numpy for this before. I'll probably stick to tf.reshape() now. I am not sure if there is a difference between the two. </p>

<p>tf.transpose() swaps the rows and columns from my understanding. If I don't use tf.transpose() my loss doesn't go down. If the parameter values are incorrect the loss doesn't go down. So the purpose of me using tf.transpose() is so that my loss goes down and my predictions become more accurate. </p>

<p>This bothers me tremendously because I'm using tf.transpose() because I have to and have no understanding why it's such an important factor. I'm assuming if it's not used correctly the inputs and labels can be in the wrong position. Making it impossible for the model to learn. If this is true how can I go about using tf.transpose() so that I am not so reliant on figuring out the parameter values via trial and error?  </p>
",1
44212125,TensorFlow: Take L2 norm over multiple dimensions,"<p>I have a TensorFlow placeholder with 4 dimensions representing a batch of images. Each image is 32 x 32 pixels, and each pixel has 3 color channels. The first dimensions represents the number of images. </p>

<p><code>X = tf.placeholder(tf.float32, [None, 32, 32, 3])</code></p>

<p>For each image, I would like to take the L2 norm of all the image's pixels. Thus, the output should be a tensor with one dimension (i.e. one value per image). The <code>tf.norm()</code> (<a href=""https://www.tensorflow.org/api_docs/python/tf/norm"" rel=""nofollow noreferrer"">documentation</a>) accepts an axis parameter, but it only lets me specify up to two axes over which to take the norm, when I would like to take the norm over axes 1, 2, and 3. How do I do this?</p>

<pre><code>n = tf.norm(X, ord=2, axis=0)          # n.get_shape() is (?, ?, 3), not (?)
n = tf.norm(X, ord=2, axis=[1,2,3])    # ValueError
</code></pre>
",0
44217076,tf.extract_image_patches for 3D images,"<p><a href=""https://www.tensorflow.org/api_docs/python/tf/extract_image_patches"" rel=""nofollow noreferrer"">The documentation of tf.extract_image_patches</a></p>

<p>It is only for 2D image, could it be expand to 3D images, which is useful for the implementation for SSIM loss function?</p>

<p>I cannot find the source code. There is a similar function <code>skimage.util.view_as_windows</code>, however, when I try to use this function with the tensorflow as backend in keras, there are errors. The transition from numpy array to tensor confused me a lot.</p>
",1
44232541,Concept of getter in TensorFlow,"<p>In TensorFlow what is the concept and use of getter ?</p>

<p>The signature of <code>tf.get_variable()</code> is :</p>

<pre><code>get_variable(
    name,
    shape=None,
    dtype=None,
    initializer=None,
    regularizer=None,
    trainable=True,
    collections=None,
    caching_device=None,
    partitioner=None,
    validate_shape=True,
    use_resource=None,
    custom_getter=None
)
</code></pre>

<p>The definition of <code>custom_getter</code> is given in the documentation as follows :</p>

<blockquote>
  <p><strong>custom_getter:</strong> Callable that takes as a first argument the true
  getter, and allows overwriting the internal get_variable method. The
  signature of custom_getter should match that of this method, but the
  most future-proof version will allow for changes: def
  custom_getter(getter, *args, **kwargs). Direct access to all
  get_variable parameters is also allowed: def custom_getter(getter,
  name, *args, **kwargs). A simple identity custom getter that simply
  creates variables with modified names is: python def
  custom_getter(getter, name, *args, **kwargs): return getter(name +
  '_suffix', *args, **kwargs)</p>
</blockquote>

<p>Unfortunately it is not very clear. Could someone please expand on it ?</p>
",1
44314992,Are valid `tf.matmul` arguments described correctly in the TensorFlow documentation?,"<p>Maybe I'm confused about what ""inner"" and ""outer"" tensor dimensions are, but the documentation for <code>tf.matmul</code> puzzles me:</p>

<blockquote>
  <p>The inputs must be matrices (or tensors of rank > 2, representing
  batches of matrices), with matching inner dimensions, possibly after
  transposition.</p>
</blockquote>

<p>Isn't it the case that R-rank arguments need to have matching (or no) R-2 outer dimensions, and that (as in normal matrix multiplication) the Rth, inner dimension of the first argument must match the R-1st dimension of the second. That is, in</p>

<pre><code>A = tf.constant(..., shape=[a, ..., z, p, x])
B = tf.constant(..., shape=[a', ..., z', x', q]) 
C = tf.matmul(A, B)
</code></pre>

<p>The outer dimensions <code>a, ..., z</code> must be identical to <code>a', ..., z'</code> (or not exist), and <code>x</code> and <code>x'</code> must match (while <code>p</code> and <code>q</code> can be anything).</p>

<p>Or put another way, shouldn't the docs say: </p>

<blockquote>
  <p>The inputs must, following any transpositions, be tensors of rank ≥ 2 where the inner 2 dimensions specify valid matrix multiplication arguments, and any further outer dimensions match.</p>
</blockquote>
",1
44339009,Tensorflow: What is the difference between having several feature columns and having one feature column with several dimensions?,"<p>I'm trying to get started with TensorFlow with a simple <code>tf.contrib.learn.LinearRegressor</code>. My data set is a time series where I want to use the steps at <code>T-n, ..., T-1</code> as features and <code>T-0</code> as the label. All values are floats, so naturally I thought I would turn each time step into a <code>real_valued_column</code> feature.</p>

<p>However, these columns have a <code>dimension=</code> attribute, and in an example with the Iris data set on the TF website <a href=""https://www.tensorflow.org/get_started/tflearn#construct_a_deep_neural_network_classifier"" rel=""nofollow noreferrer"">they use</a> a single column with four dimensions as the feature. I would have thought that here too each of the four attributes (sepal width/length, petal width/length) should have become it's own feature and thus it's own <code>real_valued_column</code>. Is my understanding wrong? What's the difference between these approaches?</p>
",0
44357675,Documentation on how to use tf.estimator in TensorFlow,"<p>I understand that we can write custom models and encapsulate it using tf.estimator. But I just can't seem to find any documentation with an example.</p>

<p>I know that you have to define your model inside a 'model_fn' but what exactly should I return from this function. Also am I supposed to put the the loss and the training step within the 'model_fn' or just the network.  How should I modify the code give below to make it work with tf.estimator. Would really appreciate some help.</p>

<pre><code>def test_model(features,labels):
    X = tf.placeholder(tf.float32,shape=(None,1),name=""Data_Input"")
    #Output
    Y = tf.placeholder(tf.float32,shape=(None,1),name=""Target_Labels"")
    W =  tf.Variable(tf.random_normal([0],stddev=stddev0)) 
    b = tf.Variable(tf.random_normal([0],stddev=stddev0))

    Ypredict = W*X + b
    return Ypredict

 estimator = tf.estimator.Estimator(model_fn = test_model)
</code></pre>
",1
44401495,Tensorflow program results in type conversion error,"<p>Tring to master Tensorflow, following documentation of TensorFlow.</p>

<p><strong>Below program results in 'Incompatible type conversion error'</strong></p>

<pre><code>import tensorflow as tf

W = tf.Variable([.3], tf.float32)
b = tf.Variable([-3], tf.float32)
x = tf.placeholder(tf.float32)
linear_model = 1.0
linear_model = W * x + b
#tf.to_float(linear_model, name='ToFloat')

# Global initialization is must
init = tf.global_variables_initializer()
sess.run(init)

print(sess.run(linear_model, {x:[1,2,3,4]}))
</code></pre>

<p><strong>Above program results in this error</strong></p>

<blockquote>
  <p>File ""v-prog3-variables.py"", line 7, in 
      linear_model = W * x + b
  .. .. ..
  ValueError: Incompatible type conversion requested to type 'float32'
  for variable of type 'int32_ref'</p>
</blockquote>

<p><strong>I tried to solve the problem by defining the 'linear_model' variable as float (linear_model = 1.0) or tf.to_float(linear_model = W * x + b)</strong></p>

<p>but nothing works</p>

<p>Im a TensorFlow newbie, please help me out.
Thanks in advance.</p>
",0
44403127,Adding a GPU Op in Tensorflow,"<p>I am trying to add a new op to TensorFlow loosely following <a href=""https://www.tensorflow.org/extend/adding_an_op"" rel=""nofollow noreferrer"">this</a> document. The difference being that I am trying to implement a GPU based op. The op I'm trying to add is the cuda op from <a href=""https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/adding_an_op"" rel=""nofollow noreferrer"">here</a> (cuda_op.py, cuda_op_kernel.cc, cuda_op_kernel.cu.cc). I am trying to compile these outside of tensorflow and the use <code>tf.load_op_library</code> to pull them in. I have made some changes so here are my files:</p>

<p>cuda_op_kernel.cc</p>

<pre><code>#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/framework/op_kernel.h""

using namespace tensorflow;  // NOLINT(build/namespaces)

REGISTER_OP(""AddOne"")
    .Input(""input: int32"")
    .Output(""output: int32"")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      c-&gt;set_output(0, c-&gt;input(0));
      return Status::OK();
    });

void AddOneKernelLauncher(const int* in, const int N, int* out);

class AddOneOp : public OpKernel {
 public:
  explicit AddOneOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor&amp; input_tensor = context-&gt;input(0);
    auto input = input_tensor.flat&lt;int32&gt;();

    // Create an output tensor
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context-&gt;allocate_output(0, input_tensor.shape(),
                                                     &amp;output_tensor));
    auto output = output_tensor-&gt;template flat&lt;int32&gt;();

    // Set all but the first element of the output tensor to 0.
    const int N = input.size();
    // Call the cuda kernel launcher
    AddOneKernelLauncher(input.data(), N, output.data());

  }
};

REGISTER_KERNEL_BUILDER(Name(""AddOne"").Device(DEVICE_GPU), AddOneOp);
</code></pre>

<p>cuda_op_kernel.cu</p>

<pre><code>#define EIGEN_USE_GPU
#include &lt;cuda.h&gt;
#include &lt;stdio.h&gt;

__global__ void AddOneKernel(const int* in, const int N, int* out) {
  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; N;
       i += blockDim.x * gridDim.x) {
    out[i] = in[i] + 1;
  }
}

void AddOneKernelLauncher(const int* in, const int N, int* out) {
  AddOneKernel&lt;&lt;&lt;32, 256&gt;&gt;&gt;(in, N, out);

  cudaError_t cudaerr = cudaDeviceSynchronize();
  if (cudaerr != cudaSuccess)
    printf(""kernel launch failed with error \""%s\"".\n"", cudaGetErrorString(cudaerr));
}
</code></pre>

<p>CMakeLists.txt</p>

<pre><code>cmake_minimum_required(VERSION 3.5)

#found from running python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())'
include_directories(/usr/local/lib/python3.5/dist-packages/tensorflow/include)

find_package(CUDA)

#set flags based on tutorial
set (CMAKE_CXX_FLAGS ""--std=c++11 -fPIC -O2 -D_GLIBCXX_USE_CXX11_ABI=0"")

#pass flags to c++ compiler
SET(CUDA_PROPAGATE_HOST_FLAGS ON)

#create library
cuda_add_library(
    cuda_op SHARED
    src/cuda_op_kernel.cu
    src/cuda_op_kernel.cc
    OPTIONS -gencode=arch=compute_20,code=sm_20)

#copy test file to build folder
configure_file(src/test.py test.py COPYONLY)
</code></pre>

<p>test.py</p>

<pre><code>import tensorflow as tf
mod = tf.load_op_library('./libcuda_op.so')
with tf.Session() as sess:
    start = [5,4,3,2,1]
    print(start)
    print(mod.add_one(start).eval())
</code></pre>

<p>I am able to compile and run <code>test.py</code> successfully, but the output is always <code>[0 0 0 0 0]</code>. If I replace <code>AddOneKernel&lt;&lt;&lt;32, 256&gt;&gt;&gt;(in, N, out);</code> with <code>for (int i = 0; i &lt; N; i++) out[i] = in[i] + 1;</code> and <code>DEVICE_GPU</code> with <code>DEVICE_CPU</code>, the op outputs the right values <code>[6 5 4 3 2]</code> (with exact same <code>CMakeList.txt</code>).</p>

<p>Any idea how to get the correct values to be returned?</p>
",0
44415901,tensorflow using tf.train.string_input_producer,"<p>I'm using tf.train.string_input_producer to read data from tfRecord file. I suppose it create a queue and pipeline and the data will automatically loaded and feed into my model. However, it stuck at the first batch, and show this exception:</p>

<blockquote>
  <p>FailedPreconditionError (see above for traceback): Attempting to use uninitialized value input_producer/limit_epochs/epochs</p>
</blockquote>

<p>my tfrecord was made by tf.train.SequenceExample, instead of tf.train.Example, which don't have clear documentation in the official guide.</p>

<p>here is code snapshot to reproduce my problem. (I believe my problem come from the queue initializing or sth. because it seems that the whole pipeline is hang up)</p>

<pre><code>from config.config import get_config

init = tf.global_variables_initializer()
config = get_config()

filename_queue = tf.train.string_input_producer(['data0.tfrecord,data1.tfrecord'], 5, capacity=16384)
reader = tf.TFRecordReader()

(keys, values) = reader.read_up_to(filename_queue, config.batch_size)

context_features = {
    ""seq_len"": tf.FixedLenFeature([1], dtype=tf.int64),
}
audio_features = {
    ""audio"": tf.FixedLenSequenceFeature([config.num_features], dtype=tf.float32),
    ""label"": tf.FixedLenSequenceFeature([config.num_classes], dtype=tf.float32)
}
audio_list = []
label_list = []
len_list = []

for i in range(config.batch_size):
    print(i)
    context, sequence = tf.parse_single_sequence_example(
        serialized=values[i],
        context_features=context_features,
        sequence_features=audio_features
    )
    audio = sequence['audio']
    label = sequence['label']
    # seq_len = context['seq_len'][0]
    seq_len = tf.shape(audio)[0]
    audio_list.append(audio)
    label_list.append(label)
    len_list.append(seq_len)

audio_tensor = tf.stack(audio_list)
label_tenor = tf.stack(label_list)
len_tensor = tf.stack(len_list)

with tf.Session() as sess:
    sess.run(init)

    threads = tf.train.start_queue_runners(sess=sess)
    for i in range(3):
        x, y, z = sess.run([audio_tensor, label_tenor, len_tensor])
        print(z)
</code></pre>
",1
44437066,`tf.train.shuffle_batch` crashes when reading `TFRecord` files in TensorFlow,"<p>I am trying to use <code>tf.train.shuffle_batch</code> to consume batches of data from a <code>TFRecord</code> file using TensorFlow 1.0. The relevant functions are:</p>

<pre><code>def tfrecord_to_graph_ops(filenames_list):
    file_queue = tf.train.string_input_producer(filenames_list)
    reader = tf.TFRecordReader()
    _, tfrecord = reader.read(file_queue)

    tfrecord_features = tf.parse_single_example(
        tfrecord,
        features={'targets': tf.FixedLenFeature([], tf.string)}
    )
    ## if no reshaping: `ValueError: All shapes must be fully defined` in
    ## `tf.train.shuffle_batch`
    targets = tf.decode_raw(tfrecord_features['targets'], tf.uint8)
    ## if using `strided_slice`, always get the first record
    # targets = tf.cast(
    #     tf.strided_slice(targets, [0], [1]),
    #     tf.int32
    # )
    ## error on shapes being fully defined
    # targets = tf.reshape(targets, [])
    ## get us: Invalid argument: Shape mismatch in tuple component 0.
    ## Expected [1], got [1000]
    targets.set_shape([1])
    return targets


def batch_generator(filenames_list, batch_size=BATCH_SIZE):
    targets = tfrecord_to_graph_ops(filenames_list)
    targets_batch = tf.train.shuffle_batch(
        [targets],
        batch_size=batch_size,
        capacity=(20 * batch_size),
        min_after_dequeue=(2 * batch_size)
    )
    targets_batch = tf.one_hot(
        indices=targets_batch, depth=10, on_value=1, off_value=0
    )
    return targets_batch


def examine_batches(targets_batch):
    with tf.Session() as sess:
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(coord=coord)
        for _ in range(10):
            targets = sess.run([targets_batch])
            print(targets)
        coord.request_stop()
        coord.join(threads)
</code></pre>

<p>The code enters through <code>examine_batches()</code>, having been handed the output of <code>batch_generator()</code>. <code>batch_generator()</code> calls <code>tfrecord_to_graph_ops()</code> and the problem is in that function, I believe.</p>

<p>I am calling </p>

<pre><code>targets = tf.decode_raw(tfrecord_features['targets'], tf.uint8)
</code></pre>

<p>on a file with 1,000 bytes (numbers 0-9). If I call <code>eval()</code> on this in a Session, it shows me all 1,000 elements. But if I try to put it in a batch generator, it crashes.</p>

<p>If I don't reshape <code>targets</code>, I get an error like <code>ValueError: All shapes must be fully defined</code> when <code>tf.train.shuffle_batch</code> is called. If I call <code>targets.set_shape([1])</code>, reminiscent of Google's <a href=""https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_input.py"" rel=""nofollow noreferrer"">CIFAR-10 example code</a>, I get an error like <code>Invalid argument: Shape mismatch in tuple component 0. Expected [1], got [1000]</code> in <code>tf.train.shuffle_batch</code>. I also tried using <code>tf.strided_slice</code> to cut a chunk of the raw data - this doesn't crash but it results in just getting the first event over and over again.</p>

<p>What is the right way to do this? To pull batches from a <code>TFRecord</code> file?</p>

<p>Note, I could manually write a function that chopped up the raw byte data and did some sort of batching - especially easy if I am using the <code>feed_dict</code> approach to getting data into the graph - but I am trying to learn how to use TensorFlow's <code>TFRecord</code> files and how to use their built in batching functions.</p>

<p>Thanks!</p>
",0
44478812,What kind of calculation does tf.nn.dynamic_rnn do with its input parameters?,"<p>What kind of calculation does <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer""><code>tf.nn.dynamic_rnn</code></a> perform? How does it use the parameters <code>cell</code> and <code>inputs</code> (to create the result)? </p>

<p>I have looked up in the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer"">documentation</a>, but I have not found an explanation.</p>
",1
44508335,Is there a way to build a graph that takes input from separate input pipelines for training and testing?,"<p>The <a href=""https://www.tensorflow.org/programmers_guide/reading_data#multiple_input_pipelines"" rel=""nofollow noreferrer"">documentation</a> states that there are two ways of dealing with different input pipelines: Either implementing separate processes and graphs that load the model from a checkpoint file (e.g. <code>infer.py</code> and <code>train.py</code>) or by using a single graph and variable sharing. I am interested in the second case, but the documentation does not give an example how it can be done. It would work by constructing the model twice and accessing the same variables, but I am wondering how to do it using only a single graph. Perhaps one could pass a flag to a <code>tf.placeholder</code> that decides for one or the other input via a <code>tf.cond</code> node, or would that be inefficient because it would make the graph more dynamic between two <code>sess.run</code> calls, thus eliminating in-place guarantees?</p>
",1
44526763,How to perform tf.image.per_image_standardization on a batch of images in tensorflow,"<p>I would like to know how to perform image whitening on a batch of images. </p>

<p>According to the documentation in <a href=""https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization</a>, it is said that <code>tf.image.per_image_standardization</code> takes as input a 3D tensor, that is an image, of shape: <code>[height, width, channels]</code>. </p>

<p>Is it a missing feature or there is a different method?</p>

<p>Any help is much appreciated. </p>
",1
44535119,Example of tensorflow.contrib.learn.ExportStrategy,"<p>Can someone provide examples of full working code for Tensorflow</p>

<p><code>tf.contrib.learn.ExportStrategy</code></p>

<p>The documentation lacks examples.  I also could not find any examples on Github or Stackoverflow for this seemingly obscure Tensorflow operation.</p>

<p>Documentation: <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/learn/ExportStrategy"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/contrib/learn/ExportStrategy</a></p>
",1
44563648,How to effectively use tf.bucket_by_sequence_length in Tensorflow?,"<p>So I'm trying to use tf.bucket_by_sequence_length() from Tensorflow, but can not quite figure out how to make it work.</p>

<p>Basically, it should take sequences (of different lengths) as input and have buckets of sequences as output, but it does not seem to work this way.</p>

<p>From this discussion: 
<a href=""https://github.com/tensorflow/tensorflow/issues/5609"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/5609</a>
I have the impression that it needs a queue in order to feed this function, sequence by sequence. It's not clear though.</p>

<p>Function's documentation can be found here: <a href=""https://www.tensorflow.org/versions/r0.12/api_docs/python/contrib.training/bucketing#bucket_by_sequence_length"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r0.12/api_docs/python/contrib.training/bucketing#bucket_by_sequence_length</a></p>
",1
44637815,Tensorflow: Define a conv2d-like operation,"<p>I want to define a new operation which should work the same way <code>tf.conv2d</code> works. Looking at the documentation, I should implement <code>Compute(OpKernelContext* context)</code> member method of my new operation class. As far as I understood, <code>conv2d</code> is defined in <code>conv_ops.cc</code>. However, the documentation showcases a rather too simple operator (<code>ZeroOutOp</code>) and <code>conv_ops.cc</code> has a lot of dependencies. I couldn't find good documentation as to how the implementation was carried out?
Any ideas where I can find documentation about tensorflow source codes? Any tutorials that show how to implement a rather complicated operation? </p>
",1
44640357,Does Tensorflow's tf.while_loop automatically capture dependencies when executing in parallel?,"<p>I am interested in implementing a Recursive Neural Network in Tensorflow, like what has been done in <a href=""https://stackoverflow.com/questions/37054188/how-can-i-implement-a-recursive-neural-network-in-tensorflow"">How can I implement a recursive neural network in TensorFlow?</a>. </p>

<p>However, in his implementation, the <code>parallel_iterations</code> of the <code>tf.while_loop</code> statement was fixed to be 1. I fear that this might be too slow. Since the tree I am going to feed into tensorflow have parts that are not dependent on each other, I would hope that I could set <code>parallel_iterations</code> to a higher value. However, it is inevitable that there are some dependencies required in the tree I feed in as input to tensorflow, and I am afraid that setting it to higher value may break the dependency property. </p>

<p>So my question is, had Tensorflow's <code>tf.while_loop</code> automatically captured dependencies already, in order to only use paralleism on placed that are not dependent on each other?</p>

<p>The tensorflow documentation says the following:</p>

<blockquote>
  <p>For correct programs, while_loop should return the same result for any
  parallel_iterations > 0.</p>
</blockquote>

<p>But I am not sure what they mean by ""correct programs"".</p>
",1
44689887,Difference Between Keras Input Layer and Tensorflow Placeholders,"<p>I was hoping someone could explain the difference (if any) between the Input Layer in Keras and Placeholders within Tensorflow?</p>

<p>The more I investigate, the more the two appear similar, but I am not convinced 100% either way thus far.</p>

<p><strong>Here is what I have observed in favor of the claim that Input Layers and tf Placeholders are the same:</strong></p>

<p>1) The tensor returned from keras.Input() can be used like a placeholder in the feed_dict of tf.Session's run method. Here is part of a simple example using Keras, which adds two tensors (a and b) and concatenates the result with a third tensor (c):</p>

<pre><code>model = create_graph()

con_cat = model.output[0]
ab_add = model.output[1]

# These values are used equivalently to tf.Placeholder() below
mdl_in_a = model.input[0] 
mdl_in_b = model.input[1]
mdl_in_c = model.input[2]

sess = k.backend.get_session()


a_in = rand_array() # 2x2 numpy arrays
b_in = rand_array()
c_in = rand_array()
a_in = np.reshape( a_in, (1,2,2))
b_in = np.reshape( b_in, (1,2,2))
c_in = np.reshape( c_in, (1,2,2))

val_cat, val_add = sess.run([con_cat, ab_add], 
               feed_dict={  mdl_in_a: a_in, mdl_in_b: b_in, mdl_in_c: c_in})
</code></pre>

<p>2) The docs from the Tensorflow Contrib regarding the Keras <a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/keras/layers/Input"" rel=""noreferrer"">Input Layer</a> mention Placeholders in its argument description:</p>

<blockquote>
  <p>""sparse: A boolean specifying whether the placeholder
   to be created is sparse""</p>
</blockquote>

<p><strong>Here is what I have observed in favor of the claim that Input Layers and tf Placeholders are NOT the same:</strong></p>

<p>1) I have seen people utilize tf.Placeholder's instead of the Input Layer's returned Tensor. Something like:</p>

<pre><code>a_holder = tf.placeholder(tf.float32, shape=(None, 2,2))
b_holder = tf.placeholder(tf.float32, shape=(None, 2,2))
c_holder = tf.placeholder(tf.float32, shape=(None, 2,2))

model = create_graph()

con_cat, ab_add = model( [a_holder, b_holder, c_holder])


sess = k.backend.get_session()


a_in = rand_array() # 2x2 numpy arrays
b_in = rand_array()
c_in = rand_array()
a_in = np.reshape( a_in, (1,2,2))
b_in = np.reshape( b_in, (1,2,2))
c_in = np.reshape( c_in, (1,2,2))


val_cat, val_add = sess.run([con_cat, ab_add], 
               feed_dict={  a_holder: a_in, b_holder: b_in, c_holder: c_in})
</code></pre>
",0
44690363,How to use tf.train.ExponentialMovingAverage in Android/IOS,"<p>I use <code>freeze_graph</code> to export my model to a file named <code>""frozen.pb""</code>. But Found that the accuracy of predictions on <code>frozen.pb</code> is very bad.</p>

<p>I know the problem maybe <code>MovingAverage</code> not included in <code>frozen.pb</code>.</p>

<p>When I use <code>model.ckpt</code> files to restore model for evaluating, if I call <code>tf.train.ExponentialMovingAverage(0.999)</code> , then the accuracy is good as expected, else the accuracy is bad.</p>

<p><strong>So How To export a binary model which performance is the same as the one restored from checkpoint files?</strong>  I want to use <code>"".pb""</code> files in Android Devices.</p>

<p><a href=""https://www.tensorflow.org/versions/r0.12/api_docs/python/train/moving_averages"" rel=""nofollow noreferrer"">The official document</a> doesn't mention this.</p>

<p>Thanks!!</p>

<p>Freeze Command:</p>

<pre><code>~/bazel-bin/tensorflow/python/tools/freeze_graph \
  --input_graph=./graph.pbtxt \
  --input_checkpoint=./model.ckpt-100000 \
  --output_graph=frozen.pb \
  --output_node_names=output  \
  --restore_op_name=save/restore_all \
  --clear_devices
</code></pre>

<p>Evaluate Code:</p>

<pre><code>... ...
logits = carc19.inference(images)
top_k = tf.nn.top_k(logits, k=10)

# Precision: 97%
# Restore the moving average version of the learned variables for eval.
variable_averages = tf.train.ExponentialMovingAverage(carc19.MOVING_AVERAGE_DECAY)
variables_to_restore = variable_averages.variables_to_restore()
for k in variables_to_restore.keys():
  print (k,variables_to_restore[k])
saver = tf.train.Saver(variables_to_restore)

# Precision: 84%
#saver = tf.train.Saver()

#model_path = '/tmp/carc19_train/model.ckpt-9801'
with tf.Session() as sess:
  saver.restore(sess, model_path)
... ...
</code></pre>
",1
44705066,Retraining Inception and Downsampling,"<p>I have followed <a href=""https://www.youtube.com/watch?v=cSKfRcEDGUs"" rel=""nofollow noreferrer"">this Tensorflow tutorial</a> on transfer learning with the Inception model using my own dataset of 640x360 images. My question comes in 2 parts</p>

<p>1) My data set conatains 640x360 images. Is the first operation that happens a downsampling to 299x299? I ask because I have a higher res version of the same dataset and I am wondering if training with the higher resolution images will result in different performance (hopefully better)</p>

<p>2) When running the network (using tf.sess.run()) is my input image down-sampled to 299x299?</p>

<p>Note: I have seen the 299x299 resolution stat listed many places online like <a href=""http://josephpcohen.com/w/visualizing-cnn-architectures-side-by-side-with-mxnet/"" rel=""nofollow noreferrer"">this one</a> and I am confused at exactly which images its referring to; the initial training dataset images (for Inception I think it was imagenet), the transfer learning dataset (my personal one), the input image when running the CNN, or a combination of the 3.</p>

<p>Thanks in advance :)</p>
",0
44753916,How to slice a part of tensor?,"<p>I want to slice [3.0 ,33.0].I have tried to access this slice by following code. I'm not so clear about tf.slice command. I'm not so clear about begin and size mentioned in documentaion about this command. Can someone please make it easy to understand.  </p>

<pre><code>batch = tf.constant([
  [#First image
    [[0.0,10.0],[1.0,11.0]],
    [[3.0,33.0],[4.0,44.0]]
  ],
  [#Second image
    [[5.0,55.0],[6.0,66.0]],
    [[7.0,77.0],[8.0,88.0]]
  ]
])
slice1 = tf.slice(batch,[0,0,0,0], [0,0,1,0]) 
sess = tf.InteractiveSEssion()
sess.run(tf.initialize_all_variables())
print slice1.eval()
</code></pre>
",1
44764887,How to restore trained LinearClassifier from tensorflow high level API and make predictions,"<p>I have trained a logistic regression model model using tensorflow's LinearClassifier() class, and set the model_dir parameter, which specifies the location where to save metagrahps of checkpoints during model training:</p>

<pre><code># Create temporary directory where metagraphs will evenually be saved
model_dir = tempfile.mkdtemp()

logistic_model = tf.contrib.learn.LinearClassifier(
    feature_columns=feature_columns, 
    n_classes=num_labels, model_dir=model_dir)
</code></pre>

<p>I've been reading about restoring models from metagraphs, but have found nothing about how to do so for models created using the high level api.  LinearClassifier() has a predict() function, but I can't find any documentation on how to run prediction using an instance of the model that has been restored via checkpoint metagraph. How would I go about doing this?  Once the model is restored, my understanding is that I am working with a tf.Sess object, which lacks all of the built in functionality of the LinearClassifier class, like this:</p>

<pre><code>with tf.Session() as sess:
  new_saver = tf.train.import_meta_graph('my-save-dir/my-model-10000.meta')
  new_saver.restore(sess, 'my-save-dir/my-model-10000')
  # Run prediction algorithm...
</code></pre>

<p>How do I run the same prediction algorithm used by the high-level api to make predictions on a restored model? Is there a better way to approach this?</p>

<p>Thanks for your input.</p>
",1
44770980,Tensorflow - Retrieve each character in a string tensor,"<p>I'm trying to retrieve the characters in a string tensor for character level prediction. The ground truths are words where each character has an id in dictionary. I have a tensor corresponding to the length of the string. </p>

<p>Now, I have to get each character in the string tensor. After checking the related posts, a simple retrieval can be as follows. Example string is ""This""</p>

<pre><code>a= tf.constant(""This"",shape=[1])
b=tf.string_split(a,delimiter="""").values  #Sparse tensor has the values array which stores characters
</code></pre>

<p>Now I want to make a string with spaces in between the letters ""This"" i.e "" T h i s "". I need spacing at the start and the end too. 
How do I do this?</p>

<p>I have tried to iterate through the characters like below</p>

<pre><code>for i in xrange(b.dense_shape[1]): # b.dense_shape[1] has the length of string
        x=b.values[i]
</code></pre>

<p>But the loop expects an integer rather than a tensor. </p>

<p>Any idea on how to do the above tasks? I couldn't find any documentation related to this (apart from the tf.string_split function). Any suggestions are welcome. Thanks</p>
",1
44853243,tensorflow conv2d diffrent start index between even and odd stride,"<p>To my understanding from <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/conv2d"" rel=""nofollow noreferrer"">tf.nn.conv2d doc</a> for SAME convolution (no matter the stride) The first dot product should be centered around (0,0)
though as you can see bellow when the stride is odd the first dot product seems to be centered around (1,1):
in this toy example</p>
<blockquote>
<p>input shape is [5,5,1]</p>
<p>filer shape is [3,3,1,1]</p>
<p>res = tf.nn.conv2d(X, F, strides=[1,x,x,1], padding='SAME')</p>
</blockquote>
<p>stride 1 result:</p>
<pre><code>array([[ 1.49573362,  2.65084887,  2.96818447,  3.04787111,  1.89275599],
   [ 3.1941781 ,  4.47312069,  4.10260868,  4.13415051,  2.85520792],
   [ 2.65490007,  3.41439581,  2.93415952,  3.65811515,  2.89861989],
   [ 2.22547054,  2.98453856,  2.89428496,  3.29111433,  2.53204632],
   [ 0.52702606,  1.16226625,  1.75986075,  2.20483446,  1.56959426]], dtype=float32)
</code></pre>
<p>stride 2 result:</p>
<pre><code>array([[ 1.49573362,  2.96818447,  1.89275599],
   [ 2.65490007,  2.93415952,  2.89861989],
   [ 0.52702606,  1.75986075,  1.56959426]], dtype=float32)
</code></pre>
<p>stride 3 result:</p>
<pre><code>array([[ 4.47312069,  2.85520792],
   [ 1.16226625,  1.56959426]], dtype=float32)
</code></pre>
<p>Is this a bug or am I missing something?</p>
",0
44873802,"What is tf.bfloat16 ""truncated 16-bit floating point""?","<p>What is the difference between tf.float16 and tf.bfloat16 as listed in <a href=""https://www.tensorflow.org/versions/r0.12/api_docs/python/framework/tensor_types"" rel=""noreferrer"">https://www.tensorflow.org/versions/r0.12/api_docs/python/framework/tensor_types</a> ?</p>

<p>Also, what do they mean by ""quantized integer""?</p>
",1
44887367,"TensorFlow, tf.one_hot why the shape of output is defined by the value of axis?","<p>I read the <a href=""https://www.tensorflow.org/api_docs/python/tf/one_hot"" rel=""nofollow noreferrer"">docs of tf.one_hot</a> and found that </p>

<blockquote>
  <p>... . The new axis is created at dimension axis (default: the new axis is appended at the end).</p>
</blockquote>

<p>What is <code>The new axis</code>?</p>

<blockquote>
  <p>If indices is a vector of length features, the output shape will be:</p>
  
  <blockquote>
    <p>features x depth if axis == -1</p>
    
    <p>depth x features if axis == 0</p>
  </blockquote>
  
  <p>If indices is a matrix (batch) with shape [batch, features], the output shape will be:</p>
  
  <blockquote>
    <p>batch x features x depth if axis == -1</p>
    
    <p>batch x depth x features if axis == 1</p>
    
    <p>depth x batch x features if axis == 0</p>
  </blockquote>
</blockquote>

<p>Why the shape of output is defined by axis? </p>
",0
44895394,"Can't run tensorboard because of "" Symbol not found: _PyBytes_AsString""","<p>I've been trying to run tensoarboard without any success. I can see that the function <strong>tf.summary.FileWriter()</strong> is working since I can see that the logFile is created in the right folder. But when I run in my terminal:</p>

<pre><code>    tensorboard --logdir=/private/tmp/mnist/2 --port=6006
</code></pre>

<p>I get : </p>

<pre><code>**Traceback (most recent call last):**
**File**""/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/tensorboard/tensorboard.py"", line 32, in &lt;module&gt;
**from** tensorflow.python.summary import event_file_inspector as efi
**File** ""/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/summary/event_file_inspector.py"", line 122, in &lt;module&gt;
from tensorflow.python.platform import gfile
**File** ""/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/platform/gfile.py"", line 22, in &lt;module&gt;
**from** tensorflow.python.lib.io.file_io import copy as Copy
  **File** ""/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io.py"", line 27, in &lt;module&gt;
**from** tensorflow.python import pywrap_tensorflow
  **File** ""/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 28, in &lt;module&gt;
_pywrap_tensorflow = swig_import_helper()
**File** ""/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
**ImportError**: dlopen(/usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow.so, 2): Symbol not found: _PyBytes_AsString
**Referenced from**: /usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow.so
**Expected in**: flat namespace
in /usr/local/Cellar/python3/3.5.2_3/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorboard/tensorboard.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow.so
</code></pre>

<p>I don't know how to deal with the <strong>Symbol not found: _PyBytes_AsString</strong> and <strong>Expected in: flat namespace</strong>. There is this <a href=""https://stackoverflow.com/questions/35006614/what-does-symbol-not-found-expected-in-flat-namespace-actually-mean"">link</a> that talks about a similar error I think but it didn't help me.  </p>

<p>The code I used can be found on this <a href=""http://ischlag.github.io/2016/06/04/how-to-use-tensorboard/"" rel=""nofollow noreferrer"">tutorial</a> and I'm on MacOS Sierra 10.12.1</p>
",0
44939540,How to get tensorflow to do a convolution on a 2 x 2 matrix with a 1 x 2 kernel?,"<p>I have the following matrix:</p>

<p><img src=""https://latex.codecogs.com/gif.latex?%5Cbegin%7Bbmatrix%7D&space;0&space;&amp;&space;1%5C%5C&space;2&space;&amp;&space;3&space;%5Cend%7Bbmatrix%7D"" title=""\begin{bmatrix} 0 &amp; 1\\ 2 &amp; 3 \end{bmatrix}"" /></p>

<p>and the following kernel:</p>

<p><img src=""https://latex.codecogs.com/gif.latex?%5Cbegin%7Bbmatrix%7D&space;1&space;&amp;&space;2&space;%5Cend%7Bbmatrix%7D"" title=""\begin{bmatrix} 1 &amp; 2 \end{bmatrix}"" /></p>

<p>If I do a convolution with no padding and slide by 1 row, I should get the following answer:</p>

<p><img src=""https://latex.codecogs.com/gif.latex?%5Cbegin%7Bbmatrix%7D&space;2&space;%5C%5C&space;8&space;%5Cend%7Bbmatrix%7D"" title=""\begin{bmatrix} 2 \\ 8 \end{bmatrix}"" /></p>

<p>Because:</p>

<p><img src=""https://latex.codecogs.com/gif.latex?2&space;=&space;(0%5Ctimes1)&space;&plus;&space;(1%5Ctimes&space;2)"" title=""2 = (0\times1) + (1\times 2)"" /></p>

<p><img src=""https://latex.codecogs.com/gif.latex?8&space;=&space;(2%5Ctimes&space;1)&space;&plus;&space;(3&space;%5Ctimes&space;2)"" title=""8 = (2\times 1) + (3 \times 2)"" /></p>

<p>Based the documentation of  <code>tf.nn.conv2d</code>, I thought this code expresses what I just described above:</p>

<pre><code>import tensorflow as tf

input_batch = tf.constant([
    [
        [[.0], [1.0]],
        [[2.], [3.]]
    ]
])

kernel = tf.constant([
    [
        [[1.0, 2.0]]
    ]
])

conv2d = tf.nn.conv2d(input_batch, kernel, strides=[1, 1, 1, 1], padding='VALID')
sess = tf.Session()

print(sess.run(conv2d))
</code></pre>

<p>But it produces this output:</p>

<pre><code>[[[[ 0.  0.]
   [ 1.  2.]]

  [[ 2.  4.]
   [ 3.  6.]]]]
</code></pre>

<p>And I have no clue how that is computed. I've tried experimenting with different values for the strides padding parameter but still am not able to produce the result I expected.</p>
",1
44951240,TensorFlow: tf.layers vs low-level API,"<p>I am currently in the process of planning my first Conv. NN implementation in Tensorflow, and have been reading many of the tutorials available on Tensorflow's <a href=""https://www.tensorflow.org/tutorials/"" rel=""noreferrer"">website</a> for insight.</p>

<p>It seems that there are essentially two ways to create a custom CNN:</p>

<p>1) Use Tensorflow layers module <code>tf.layers</code>, which is the ""high-level API"". Using this method, you define a model definition function consisting of <code>tf.layers</code> objects, and in the main function, instantiate a <code>tf.learn.Estimator</code>, passing the model definition function to it. From here, the <code>fit()</code> and <code>evaluate()</code> methods can be called on the <code>Estimator</code> object, which train and validate, respectively. Link: <a href=""https://www.tensorflow.org/tutorials/layers"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/layers</a>. Main function below:</p>

<pre><code>def main(unused_argv):
  # Load training and eval data
  mnist = learn.datasets.load_dataset(""mnist"")
  train_data = mnist.train.images  # Returns np.array
  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)
  eval_data = mnist.test.images  # Returns np.array
  eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)

  # Create the Estimator
  mnist_classifier = learn.Estimator(
      model_fn=cnn_model_fn, model_dir=""/tmp/mnist_convnet_model"")

  # Set up logging for predictions
  # Log the values in the ""Softmax"" tensor with label ""probabilities""
  tensors_to_log = {""probabilities"": ""softmax_tensor""}
  logging_hook = tf.train.LoggingTensorHook(
       tensors=tensors_to_log, every_n_iter=50)

  # Train the model
  mnist_classifier.fit(
      x=train_data,
      y=train_labels,
      batch_size=100,
      steps=20000,
      monitors=[logging_hook])

  # Configure the accuracy metric for evaluation
  metrics = {
      ""accuracy"":
          learn.MetricSpec(
              metric_fn=tf.metrics.accuracy, prediction_key=""classes""),
  }

  # Evaluate the model and print results
  eval_results = mnist_classifier.evaluate(
      x=eval_data, y=eval_labels, metrics=metrics)
  print(eval_results)
</code></pre>

<p>Full code <a href=""https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/layers/cnn_mnist.py"" rel=""noreferrer"">here</a></p>

<hr>

<p>2) Use Tensorflow's ""low-level API"" in which layers are defined in a definition function. Here, layers are manually defined, and the user must perform many calculations manually. In the main function, the user starts a <code>tf.Session()</code>, and manually configures training/validation using for loop(s). Link: <a href=""https://www.tensorflow.org/get_started/mnist/pros"" rel=""noreferrer"">https://www.tensorflow.org/get_started/mnist/pros</a>. Main function below:</p>

<pre><code>def main(_):
  # Import data
  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)

  # Create the model
  x = tf.placeholder(tf.float32, [None, 784])

  # Define loss and optimizer
  y_ = tf.placeholder(tf.float32, [None, 10])

  # Build the graph for the deep net
  y_conv, keep_prob = deepnn(x)

  with tf.name_scope('loss'):
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_,
                                                        logits=y_conv)
  cross_entropy = tf.reduce_mean(cross_entropy)

  with tf.name_scope('adam_optimizer'):
    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)

  with tf.name_scope('accuracy'):
    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))
    correct_prediction = tf.cast(correct_prediction, tf.float32)
  accuracy = tf.reduce_mean(correct_prediction)

  graph_location = tempfile.mkdtemp()
  print('Saving graph to: %s' % graph_location)
  train_writer = tf.summary.FileWriter(graph_location)
  train_writer.add_graph(tf.get_default_graph())

  with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(20000):
      batch = mnist.train.next_batch(50)
      if i % 100 == 0:
        train_accuracy = accuracy.eval(feed_dict={
            x: batch[0], y_: batch[1], keep_prob: 1.0})
        print('step %d, training accuracy %g' % (i, train_accuracy))
      train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})

    print('test accuracy %g' % accuracy.eval(feed_dict={
        x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))
</code></pre>

<p>Full code <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_deep.py"" rel=""noreferrer"">here</a></p>

<hr>

<p>My dilemma is, I like the simplicity of defining the neural network using <code>tf.layers</code> (option 1), but I want the customizability of the training that the ""low-level API"" (option 2) provides. Specifically, when using the <code>tf.layers</code> implementation, is there a way to report validation accuracy every n iterations of training? Or more generally, can I train/validate using the <code>tf.Session()</code>, or am I confined to using the <code>tf.learn.Estimator</code>'s <code>fit()</code> and <code>evaluate()</code> methods?</p>

<p>It seems odd that one would want a final evaluation score after all training is complete, as I thought the whole point of validation is to track network progression during training. Otherwise, what would be the difference between validation and testing?</p>

<p>Any help would be appreciated.</p>
",0
45011724,How to use tf.data's initializable iterators within a tf.estimator's input_fn?,"<p>I would like to manage my training with a <a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/estimator/Estimator"" rel=""noreferrer""><code>tf.estimator.Estimator</code></a> but have some trouble to use it alongside the <a href=""https://www.tensorflow.org/api_docs/python/tf/data"" rel=""noreferrer""><code>tf.data</code></a> API.</p>

<p>I have something like this:</p>

<pre><code>def model_fn(features, labels, params, mode):
  # Defines model's ops.
  # Initializes with tf.train.Scaffold.
  # Returns an tf.estimator.EstimatorSpec.

def input_fn():
  dataset = tf.data.TextLineDataset(""test.txt"")
  # map, shuffle, padded_batch, etc.

  iterator = dataset.make_initializable_iterator()

  return iterator.get_next()

estimator = tf.estimator.Estimator(model_fn)
estimator.train(input_fn)
</code></pre>

<p>As I can't use a <code>make_one_shot_iterator</code> for my use case, my issue is that <code>input_fn</code> contains an iterator that should be initialized within <code>model_fn</code> (here, I use <a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/train/Scaffold"" rel=""noreferrer""><code>tf.train.Scaffold</code></a> to initialize local ops).</p>

<p>Also, I understood that we can't only use <code>input_fn = iterator.get_next</code> otherwise the other ops will not be added to the same graph.</p>

<p>What is the recommended way to initialize the iterator?</p>
",0
45028222,Caffe EuclideanLoss reproduce in Tensorflow,"<p>I am trying to reproduce the <code>EuclideanLoss</code> from <code>Caffe</code> in <code>Tensorflow</code>. I found a function called: <code>tf.nn.l2_loss</code> which according to the documents computes the following:</p>

<pre><code>output = sum(t ** 2) / 2
</code></pre>

<p>When looking at the EuclideanLoss in the Python version of caffe it says:</p>

<pre><code>def forward(self, bottom, top):
        self.diff[...] = bottom[0].data - bottom[1].data
        top[0].data[...] = np.sum(self.diff**2) / bottom[0].num / 2.
</code></pre>

<p>In the original docu it says:</p>

<p>To me this is exactly the same computation. However, my loss values for the same net in Tensorflow are around 3000 and in Caffe they are at roughly 300. So where is the difference?</p>
",0
45030619,Detecting out-of-bounds slicing with tf.slice like in numpy,"<p>In tensorflow, I'm trying to use tf.slice, but <a href=""https://www.tensorflow.org/versions/r0.12/api_docs/python/array_ops/slicing_and_joining"" rel=""nofollow noreferrer"">as its documentation states</a>, it requires the slice to fit in the input array. For instance, if you try to slice the first 5 positions of the tensor [1,2,3,4] it will crash. I want to have the same functionality we get with python lists or numpy arrays where slicing gets you the intersection of the original array and the slice you asked for. For instance if you ask for positions 2 to 6 of [1,2,3,4] you'll get [2,3,4].</p>

<p>How can I do that in tensorflow?</p>

<p>Thanks!</p>
",1
45054212,How to initialize evaluation's local variables with tf.train.Scaffold?,"<p>I'm using the high level <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment"" rel=""nofollow noreferrer""><code>tf.contrib.learn.Experiment</code></a> object to interleave training and evaluation. However, I'm facing an issue with the local variables from the evaluation and metrics modules that are reported as non initialized:</p>

<pre class=""lang-none prettyprint-override""><code>Variables not initialized: mean/total, mean/count, eval_step
</code></pre>

<p>I provide a custom <code>local_init_op</code> to <a href=""https://www.tensorflow.org/api_docs/python/tf/train/Scaffold"" rel=""nofollow noreferrer""><code>tf.train.Scaffold</code></a> which basically looks like this:</p>

<pre><code>scaffold = tf.train.Scaffold(
  local_init_op=tf.group(
    iterator.initializer,
    tf.tables_initializer(),
    tf.local_variables_initializer()))
</code></pre>

<p>(where <code>iterator</code> is a <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/data/Iterator"" rel=""nofollow noreferrer""><code>tf.contrib.data.Iterator</code></a>.)</p>

<p>which is then stored in a <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec"" rel=""nofollow noreferrer""><code>tf.estimator.EstimatorSpec</code></a> to be returned by the <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator"" rel=""nofollow noreferrer""><code>tf.estimator.Estimator</code></a>'s <code>model_fn</code> function.</p>

<p>As I don't think <code>tf.local_variables_initializer()</code> operates lazily, it means these variables are not yet created.</p>

<p>So how to initialize them?</p>
",0
45074049,Tensorflow: How does tf.get_variable work?,"<p>I have read about <code>tf.get_variable</code> from this <a href=""https://stackoverflow.com/questions/37098546/difference-between-variable-and-get-variable-in-tensorflow"">question</a> and also a bit from the documentation available at the tensorflow website. However, I am still not clear and was unable to find an answer online.</p>
<p>How does <code>tf.get_variable</code> work? For example:</p>
<pre><code>var1 = tf.Variable(3.,dtype=float64)
var2 = tf.get_variable(&quot;var1&quot;,[],dtype=tf.float64)
</code></pre>
<p>Does it mean that <strong>var2</strong> is <strong>another</strong> variable with initialization similar to <strong>var1</strong>? Or is <strong>var2</strong> an alias for <strong>var1</strong> (I tried and it doesn't seem to)?</p>
<p>How are <strong>var1</strong> and <strong>var2</strong> related?</p>
<p>How is a variable constructed when the variable we are <em>getting</em> doesn't really exist?</p>
",1
45077445,How to use method recover_last_checkpoints of tf.train.Saver()?,"<p>The documentation writes that a list of checkpoint paths should be passed to it, but how to get the list? By hard coding? No, it's a silly practice. By parsing the protocol buffer file (a file named as <code>checkpoint</code> in your model directory)? But tensorflow does not implement a parser, does it? So do I have to implement one by myself? <strong>Do you have a good practice to get the checkpoint paths list?</strong></p>

<p>I raise this question because these days I am troubled by one thing. As you know, a days-long training may crash for some reason, and I have to recover it from the latest checkpoint. Recovering training is easy, since I just need to write the following code:</p>

<pre><code>restorer = tf.train.Saver()
restorer.restore(sess, latest_checkpoint)
</code></pre>

<p>I can hard code <code>latest_checkpoint</code>, or somewhat wiser, use <code>tf.train.latest_checkpoint()</code>.</p>

<p>However, a problem arises after I recover the training. Those old checkpoints files that are created before crash are left there. The Saver only manages the checkpoint files created in one run. I hope it could also manage the previously created checkpoints files so they would be automatically deleted, and I don't have to manually delete them every time. I think such repeating work is really silly.</p>

<p>Then I find the <code>recover_last_checkpoints</code> method in class <code>tf.train.Saver()</code>, which allows Saver to manage old checkpoints. But it's not handy to use. So is there any good solution?</p>
",1
45151015,How does tf.gradients behave when passed a list of `ys` tensors?,"<p>How exactly does <code>tf.gradients</code> behave when passed a list of tensors as its first argument? Take this very small example:</p>

<pre><code>a = tf.constant(5)
b = tf.constant(7)
c = a + 2 * b
</code></pre>

<p>If I compute the gradients of a single tensor, <code>c</code>, with respect to <code>[a,b]</code>, I get the expected answer:</p>

<pre><code>grads = tf.gradients(c, [a, b])
with tf.Session() as sess:
    sess.run(grads) # returns (1, 2)
</code></pre>

<p>According to the Tensorflow documentation, if you pass in a <em>list</em> of tensors as your first argument <code>ys</code>, <code>tf.gradients</code> will sum the gradients over that list, returning <code>sum_over_ys(dy/dx)</code> for each <code>x</code> in your second argument. So I would expect:</p>

<pre><code>tf.gradients([a, b, c], [a, b])
</code></pre>

<p>to behave the same way as:</p>

<pre><code>tf.gradients(a + b + c, [a, b])
</code></pre>

<p>Am I reading the docs wrong? When I test this code, I get the expected result <code>[2, 3]</code> for the second expression (explicitly summing <code>a + b + c</code>), but <code>[2, 1]</code> for the first. Where is this <code>[2, 1]</code> coming from?</p>
",1
45155864,Tensorflow LinearRegressor not converging,"<p>I'm attempting to do a toy linear regression in Python with TensorFlow, using the pre-built estimator tf.contrib.learn.LinearRegressor instead of building my own estimator. 
The inputs I'm using are real-valued numbers between 0 and 1, and the outputs are just 3*inputs. TensorFlow seems to fit the data (no errors raised), but the outputs have no correlation to what they should be.</p>

<p>I'm not sure I'm getting the predictions done correctly- the documentation for the predict() function is pretty sparse.</p>

<p>Any ideas for how to improve the fitting?</p>

<pre><code>import numpy as np
import pandas as pd
import tensorflow as tf
import itertools
import matplotlib.pyplot as plt

#Defining data set
x = np.random.rand(200)
y = 3.0*x
data = pd.DataFrame({'X':x, 'Y':y})
training_data = data[50:]
test_data= data[:50]

COLUMNS = ['Y','X']
FEATURES = ['X']
LABELS = 'Y'

#Wrapper function for the inputs of LinearRegressor
def get_input_fn(data_set, num_epochs=None, shuffle=True):
  return tf.estimator.inputs.pandas_input_fn(
      x=pd.DataFrame(data_set[FEATURES]),
      y=pd.Series(data_set[LABELS]),
      num_epochs=num_epochs,
      shuffle=shuffle)


feature_cols = [tf.feature_column.numeric_column(k) for k in FEATURES]
regressor = tf.contrib.learn.LinearRegressor(feature_columns=feature_cols)
regressor.fit(input_fn=get_input_fn(test_data), steps=100)

results = regressor.predict(input_fn=get_input_fn(test_data, 
num_epochs=1))
predictions = list(itertools.islice(results, 50))

#Visualizing the results
fig = plt.figure(figsize=[8,8])
ax = fig.add_subplot(111)
ax.scatter(test_data[LABELS], predictions)

ax.set_xlabel('Actual')
ax.set_ylabel('Predicted')
plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/IGI8o.png"" rel=""nofollow noreferrer"">Scatter plot of results</a></p>
",1
45159133,Does tf.one_hot() supports SparseTensor as indices parameter?,"<p>I would like to ask whether <a href=""https://www.tensorflow.org/api_docs/python/tf/one_hot"" rel=""nofollow noreferrer"">tf.one_hot()</a> function supports SparseTensor as the ""indices"" parameter. I want to do a multi-label classification (each example has multiple labels) which requires to calculate a cross_entropy loss. </p>

<p>I try to directly put the SparseTensor in the ""indices"" parameter but it raises the following error:</p>

<p>TypeError: Failed to convert object of type  to Tensor. Contents: SparseTensor(indices=Tensor(""read_batch_features/fifo_queue_Dequeue:106"", shape=(?, 2), dtype=int64, device=/job:worker), values=Tensor(""string_to_index_Lookup:0"", shape=(?,), dtype=int64, device=/job:worker), dense_shape=Tensor(""read_batch_features/fifo_queue_Dequeue:108"", shape=(2,), dtype=int64, device=/job:worker)). Consider casting elements to a supported type.</p>

<p>Any suggestion on the possible cause?</p>

<p>Thanks.</p>
",0
45229165,How can I serve the Faster RCNN with Resnet 101 model with tensorflow serving,"<p>I am trying to serve the Faster RCNN with Resnet 101 model with tensorflow serving.</p>

<p>I know I need to use tf.saved_model.builder.SavedModelBuilder to export the model definition as well as variables, then I need a script like inception_client.py provided by tensorflow_serving. </p>

<p>while I am going through the examples and documentation and experimenting, I think someone may have done the same thing. So plase help if you have done the same or know how to get it done. Thanks in advance.</p>
",0
45247909,Tensorflow - How to get the gradients of the output w.r.t the model parameters,"<p>I would like to know if it is possible to compute the gradients of the output of a model with respect to the model parameters.  In other words I would like to compute <code>dy / d theta</code>.</p>

<p>Here is a short example of what I mean:</p>

<pre><code>import keras
import tensorflow as tf

# Dummy input
test = np.random.rand(1, 32, 32, 1)

x = tf.placeholder(tf.float32, shape=(None, 32, 32, 1))

model = keras.layers.Conv2D(16, 5, padding = 'same', activation='elu') (x)
model = keras.layers.Flatten() (model)
model = keras.layers.Dense(128, activation='relu') (model)
predictions = keras.layers.Dense(1) (model)

with tf.Session() as sess:
    init_op = tf.global_variables_initializer()
    sess.run(init_op)
    y = sess.run(predictions, feed_dict={x: test})

    # Get gradients of y w.r.t model parameters.
    gradients = sess.run(tf.gradients(y, model_parameters))
</code></pre>

<p>I have looked at the documentation of <code>tf.gradients()</code> and it states</p>

<blockquote>
  <p><code>ys</code> and <code>xs</code> are each a <code>Tensor</code> or a list of tensors. <code>grad_ys</code> is a list of <code>Tensor</code>, holding the gradients received by the <code>ys</code>. The list must be the same length as <code>ys</code>.</p>
</blockquote>

<p>So I do understand that both args need to be a tensor. However, when I try </p>

<p><code>model_parameters = tf.trainable_variables()</code></p>

<p><code>model_parameters</code> is a list of elements of type <code>tensorflow.python.ops.variables.Variable</code></p>

<p>Is there a way to get the parameters of the model as a tensor to use for differentiation?</p>
",1
45278276,"TensorFlow : lstm dropout implementation, shape problems","<p>I'm working on an prediction project using lstm model in TensorFlow.
The structure of the implementation worked, however, got a bad result which the accuracy of testing set was only 0.5. Thus, I have searched whether there exists some tricks of training a lstm-based model. Then I got ""adding dropout"". </p>

<p>However, following the tutorial by others, some errors occur.</p>

<p>Here's the original version and it worked : </p>

<pre><code>def lstmModel(x, weights, biases):
    x = tf.unstack(x, time_step, 1)

    lstm_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, state_is_tuple=True, forget_bias=1)
    outputs, states = rnn.static_rnn (lstm_cell, x, dtype=tf.float32)rnn.static_rnn)

    return tf.matmul(outputs[-1], weights['out']) + biases['out']
</code></pre>

<p>and after changing to below, it occurs an error :</p>

<blockquote>
  <p><strong><em>ValueError: Shape (90, ?) must have rank at least 3</em></strong></p>
</blockquote>

<pre><code>def lstmModel(x, weights, biases):
    x = tf.unstack(x, time_step, 1)

    lstm_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, state_is_tuple=True, forget_bias=1)
    lstm_dropout = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=0.5)
    lstm_layers = rnn.MultiRNNCell([lstm_dropout]* 3)
    outputs, states = tf.nn.dynamic_rnn(lstm_layers, x, dtype=tf.float32)
    return tf.matmul(outputs[-1], weights['out']) + biases['out']
</code></pre>

<p>I'm confused if my shape of input data went wrong.
Before entering this function, the input <code>x</code> is in the shape <code>(batch_size, time_step, data_size)</code></p>

<pre><code>batch_size = 30 
time_step = 4 #read 4 words 
data_size = 80 # total 80 words, each is in np.shape of [1,80]
</code></pre>

<p>So, the input shape <code>x</code> each batch is <code>[30,4,80]</code>.
And the input word <code>x[0,0,80]</code> is followed by the word <code>x[0,1,80]</code>.
Does the design make sense ?</p>

<p>The whole implementation is actually modified by other tutorial and I also wonder what did the <code>tf.unstack()</code> actually do?</p>

<p>several problems above...
I have putted the code in <a href=""https://github.com/billy0059/LstmTest"" rel=""nofollow noreferrer"">github</a> with ""worked version"" and ""failed version"" mentioned above.
Only the mentioned function differs!
Please check,
thanks!</p>
",0
45313351,Tensorflow Depthwise Convolution Understanding,"<p>I'm currently trying to understand how Tensorflow's <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d"" rel=""nofollow noreferrer"">Depthwise Convolution</a> works. As far as I've understood, each channel in the input image is convolved with it's own set of filters, and then the results are concatenated. I'm going to stick with the parameter <code>depth_multiplier=1</code> for the sake of simplicity in the remainder, so <code>n_inputchannels == n_outputchannels</code>. </p>

<p>So in theory, I could split up the depthwise convolution into <code>N</code> individual, regular <code>Conv2Ds</code>, correct? Why does the following code produce different results then I am wondering - is this a precision issue? I'm following the documentation for the ordering <code>[filter_height, filter_width, in_channels, 1]</code> for the depthwise convolution filters, and <code>[filter_height, filter_width, in_channels, out_channels]</code> for the regular convolutions, and <code>NHWC</code> data format.</p>

<pre><code>import tensorflow as tf
import numpy as np
import random

width = 128
height = 128
channels = 32
kernel_width = 3
kernel_height = 3

with tf.Session() as sess:

    _input = np.float32(np.random.rand(1, height, width, channels))
    _weights =  np.float32(np.random.rand(kernel_height, kernel_width, channels, 1))

    _input_ph = tf.placeholder(tf.float32, shape=(1, height, width, channels))
    _weights_pc = tf.placeholder(tf.float32, shape=(kernel_height, kernel_width, channels, 1))

    feed = { _input_ph: _input, _weights_pc : _weights }

    result = tf.nn.depthwise_conv2d(_input_ph, _weights_pc, [1,1,1,1], 'SAME')

    individual_results = []
    for i in range(channels):
        individual_results.append(tf.nn.conv2d(tf.expand_dims(_input_ph[:,:,:,i],axis=3), tf.expand_dims(_weights_pc[:,:,i,:],axis=3), [1,1,1,1], 'SAME'))

    depth_result = sess.run(result, feed_dict=feed)
    concat_result = sess.run(tf.concat(individual_results, axis=3), feed_dict=feed)

    channel_diff = 0.0
    for i in range(channels):
        channel_diff += np.sum(depth_result[:,:,:,i]-concat_result[:,:,:,i])

    print(channel_diff)
</code></pre>

<p>Here I'm computing first the normal <code>tf.nn.depthwise_conv2d</code> and then slice the input and weights accordingly and do <code>tf.nn.conv2d</code>s individually. For these parameters I get about <code>1e-5</code> difference, but that tends to get higher when I increase the number of channels.</p>

<p>I would be really glad if someone could explain to me what's going on :) 
Thanks!</p>
",0
45373740,Tensorflow ReLU normalizes strangely,"<p>in my opinion the rectified linear unit is supposed to execute the following function:</p>

<pre><code>relu(x) = max(x, 0)
</code></pre>

<p>However, this seems not to be the case with <code>tf.nn.relu</code>:</p>

<pre><code>import tensorflow as tf
import numpy as np
rand_large = np.random.randn(10, 3)*100
X = tf.placeholder(tf.float32, [10, 3])
sess = tf.Session()
sess.run(tf.nn.relu(X), feed_dict={X:rand_large})
</code></pre>

<p>The random matrix looks like this:</p>

<pre><code>&gt;&gt;&gt; rand_large
array([[  21.94064161,  -82.16632876,   16.25152777],
   [  55.54897693,  -93.15235155,  118.99166126],
   [ -13.36452239,   39.36508285,   65.42844521],
   [-193.34041145,  -97.08632376,   99.22162259],
   [  87.02924619,    2.04134891,  -27.29975745],
   [-181.11406687,   43.55952393,   42.29312993],
   [ -29.81242188,   93.5764354 , -165.62711447],
   [  17.78380711, -171.30536766, -197.20709038],
   [ 105.94903623,   34.07995616,   -7.27568839],
   [-100.59533697, -189.88957685,   -7.52421816]])
</code></pre>

<p>And the output from the relu function like this:</p>

<pre><code>&gt;&gt;&gt; sess.run(tf.nn.relu(X), feed_dict={X:rand_large})array([[ 1. ,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5],
   [ 0.5,  0.5,  0.5]], dtype=float32)
</code></pre>

<p><strong>So, if I see it correctly, <code>tf.nn.relu</code> does some sort of normalization, right? If yes, why isn't it mentioned in the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/relu"" rel=""nofollow noreferrer"">docs</a>?</strong></p>

<p>Okay, I found out that the whole issue was related to my tensorflow installtion which seemed to be corrupt. On another machine, I did get the expected results.
Thank you for the help and helpful comments.</p>
",1
45428557,Tensorflow: How to make return value of tf.unique same size as input,"<p>According to <a href=""https://www.tensorflow.org/api_docs/python/tf/unique"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/unique</a>, <code>tf.unique(x)</code> returns a tuple <code>(y, idx)</code>,  The <strong>shape of y is (?, )</strong> is not known during build time. Is there anyway I can pad <code>y</code> to match the input size <code>x</code>?.</p>

<p>For example, </p>

<pre><code># tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]
y, idx = unique(x)
y ==&gt; [1, 2, 4, 7, 8]
idx ==&gt; [0, 0, 1, 2, 2, 2, 3, 4, 4]
</code></pre>

<p>I wanna make y = [1, 2, 4, 7, 8, 0, 0, 0, 0]</p>
",1
45434781,"TensorFlow, when can Python-like negative indexing be used if ever?","<p>I'm new to TensorFlow (version 1.2), but not to Python or Numpy.  I am building a model to predict the shape of a protein molecule.  I need to wrap TensorFlow's standard tf.losses.cosine_distance function in some extra code, because I need to stop the propagation of some NaN values into the loss calculation.</p>

<p>I know exactly which cells will be NaN.  Whatever my machine learning system predicts for those cells does not count.  I plan to turn the NaN part of the output of tf.losses.cosine_distance into zeros before summing up the loss function.</p>

<p>Here's a snippet of working code, using tf.scatter_nd_update for the element assignment:</p>

<pre><code>def custom_distance(predict, actual):
    with tf.name_scope(""CustomDistance""):
        loss = tf.losses.cosine_distance(predict, actual, -1, 
               reduction=tf.losses.Reduction.NONE)
        loss = tf.Variable(loss) # individual elements can be modified
        indices = tf.constant([[0,0,0],[29,1,0],[29,2,0]])
        updates = tf.constant([0., 0., 0.])
        loss = tf.scatter_nd_update(loss, indices, updates)
        return loss
</code></pre>

<p>But, that only works on the one protein that I have that is 30 amino acids long.  What if I have a protein of a different length?  I will have many. 
 In Numpy, I would just use Python's negative indexing, and substitute -1's for the two 29's on the indices line.  Tensorflow will not accept that.  If I make that substitution, I get a long traceback, but I think that the most important part of it is this:</p>

<pre><code>File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Invalid indices: [0,1] = [-1, 1, 0] is not in [0, 30)
</code></pre>

<p>(I could also modify the <em>predict</em> Tensor so that the cells in question exactly match the <em>actual</em> Tensor before calculating the loss, but in each case the challenge is the same: to assign the values of individual elements in a TensorFlow object.)</p>

<p>Should I just forget about negative indexing in TensorFlow?  I am poring through the TensorFlow docs to understand the correct approach to this problem.  I assume that I can retrieve the length of my input Tensors long the primary axis and use that.  But after seeing the strong parallels between TensorFlow and Numpy, I have to wonder whether that's clunky.</p>

<p>Thanks for your suggestions.</p>
",0
45496277,Having trouble understanding lstm use in tensorflow code sample,"<p>Why is the <code>pred</code> variable being calculated before any of the training iterations occur? I would expect that a <code>pred</code> would be generated (through the <code>RNN()</code> function) during <strong>each</strong> pass through of the data for every iteration? </p>

<p>There must be something I am missing. Is <code>pred</code> something like a function object? I have looked at the docs for <code>tf.matmul()</code> and that returns a tensor, not a function.</p>

<p>Full source: <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py"" rel=""nofollow noreferrer"">https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py</a></p>

<p>Here is the code:</p>

<pre><code>def RNN(x, weights, biases):

    # Prepare data shape to match `rnn` function requirements
    # Current data input shape: (batch_size, n_steps, n_input)
    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)

    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)
    x = tf.unstack(x, n_steps, 1)

    # Define a lstm cell with tensorflow
    lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)

    # Get lstm cell output
    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)

    # Linear activation, using rnn inner loop last output
    return tf.matmul(outputs[-1], weights['out']) + biases['out']

pred = RNN(x, weights, biases)

# Define loss and optimizer
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

# Evaluate model
correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

# Initializing the variables
init = tf.global_variables_initializer()
</code></pre>
",0
45542897,CIFAR10 example running time,"<p>I have been learning ML using TensorFlow for a few weeks. I have been following the tutorials given on the TensorFlow website (<a href=""https://www.tensorflow.org/tutorials/deep_cnn"" rel=""nofollow noreferrer"">here</a>). I started with training the model and it has been running on the system with the following specifications(it was taken before training started therefore showing minimal usage)<a href=""https://i.stack.imgur.com/vmEcR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vmEcR.png"" alt=""enter image description here""></a></p>

<p>It has completed more than 200,000 steps so for how long should it be running or is there anything I am missing here.</p>

<p>Also, a similar question was found on the forum <a href=""https://stackoverflow.com/questions/43991501/how-long-to-train-cifar10-in-tensorflow-with-a-gtx-960"">here</a>. I could not find any reference on TensorFlow website where it says that you have to terminate it yourself when you get the desired loss. Even if it is so, how to determine what is the value of loss where you can stop the training?</p>
",0
45553280,TensorArray Initialization from another tensor,"<p>What is the right way to initialize a tensorarray from another tensor in tensorflow. </p>

<p>Suppose I have a tensor</p>

<pre><code>T1 

TensorArr = tf.TensorArray(tf.int32, 1, dynamic_size=True)
</code></pre>

<p>What is way to say that this tensorarray depends on T1?  Looking at the <a href=""https://www.tensorflow.org/api_docs/python/tf/TensorArray"" rel=""nofollow noreferrer"">documentation</a> I cant figure out how to initialize this. </p>

<p>Correct me if my understanding is wrong, T1 is a nested tensor and I want to loop over a dimension using tf.while_loop and hence I want to initialize the TensorArray with it. </p>
",1
45553929,Implementing im2col in TensorFlow,"<p>I wish to implement an operation similar to 2D convolution in TensorFlow. As per my understanding, the most common approach to implementing convolution is by first applying an <code>im2col</code> operation to the image (see <a href=""http://cs231n.github.io/convolutional-networks/#conv"" rel=""noreferrer"">here</a> - subsection ""<em>Implementation as Matrix Multiplication</em>"") - an operation that transforms an image into a 2D matrix with individual ""chunks"" of the image to which the kernel is applied as flattened columns.</p>

<p>In other words, this excerpt from the above linked resource explains what <code>im2col</code> does nicely:</p>

<blockquote>
  <p>[...] For example, if the input is [227x227x3] <em>(in the format height x width x n_channels)</em> and it is to be convolved with 11x11x3 filters at stride 4, then we would take [11x11x3] blocks of pixels in the input and stretch each block into a column vector of size 11*11*3 = 363. Iterating this process in the input at stride of 4 gives (227-11)/4+1 = 55 locations along both width and height, leading to an output matrix <code>X_col</code> of <code>im2col</code> of size [363 x 3025], where every column is a stretched out receptive field and there are 55*55 = 3025 of them in total. Note that since the receptive fields overlap, every number in the input volume may be duplicated in multiple distinct columns.</p>
</blockquote>

<p>As I understand from the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/conv2d"" rel=""noreferrer"">TensorFlow docs</a>, that is what's done internally with <code>tf.nn.conv2d</code> as well.</p>

<p>Now, I would like to implement said <code>im2col</code> operation in TensorFlow separately (as I wish to have access to this intermediary result). As this involves copying of values in a non-trivial way, how would I build a relatively efficient computational graph for this operation myself? Similarly, how would one implement the reverse operation?</p>
",0
45563059,How to get Tensorflow's batch_sequences_with_states to infer sequence lengths automatically,"<p>I'm trying to use Tensorflow's <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/training/batch_sequences_with_states"" rel=""nofollow noreferrer"">tf.contrib.training.batch_sequences_with_states</a> to prepare data for an RNN, specifically batching and segmenting variable length sequences.</p>

<p>The code below is a toy example that I expect to run without error:</p>

<pre><code>import os
import tensorflow as tf

BATCH_SIZE = 10
STATE_SIZE = 5
STEP_SIZE = 20

input_key = tf.placeholder(tf.string, [])

input_sequences = {
    ""inputs"": tf.placeholder(tf.float32, [None]),
    ""labels"": tf.placeholder(tf.float32, [None])
}

input_context = {
    ""length"": tf.placeholder(tf.int32)
}

initial_states = {
    ""cell"": tf.zeros([STATE_SIZE], tf.float32)
}

# https://www.tensorflow.org/api_docs/python/tf/contrib/training/batch_sequences_with_states
batch = tf.contrib.training.batch_sequences_with_states(
    input_key=input_key,
    input_sequences=input_sequences,
    input_context=input_context,
    input_length=None, # infer lengths
    initial_states=initial_states,
    num_unroll=STEP_SIZE,
    batch_size=BATCH_SIZE
)

init = tf.global_variables_initializer()

sess = tf.Session()
sess.run(init)

tf.train.start_queue_runners(sess=sess)
</code></pre>

<p>I'm not feeding any actual data or running anything (other than to initialize the variables). All I am doing is starting the queue runner. I expect this example to run successfully (albeit not doing anything). However, when I run this an exception is thrown:</p>

<pre><code>2017-08-08 01:06:00.789556: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Shape [-1] has negative dimensions
   [[Node: input_sequences_inputs = Placeholder[dtype=DT_FLOAT, shape=[?], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
2017-08-08 01:06:00.796772: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Shape [-1] has negative dimensions
   [[Node: input_sequences_inputs = Placeholder[dtype=DT_FLOAT, shape=[?], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
2017-08-08 01:06:00.798030: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Shape [-1] has negative dimensions
   [[Node: input_sequences_inputs = Placeholder[dtype=DT_FLOAT, shape=[?], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
2017-08-08 01:06:00.805150: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Shape [-1] has negative dimensions
   [[Node: input_sequences_inputs = Placeholder[dtype=DT_FLOAT, shape=[?], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
2017-08-08 01:06:00.810993: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Shape [-1] has negative dimensions
   [[Node: input_sequences_inputs = Placeholder[dtype=DT_FLOAT, shape=[?], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
2017-08-08 01:06:00.812120: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Shape [-1] has negative dimensions
   [[Node: input_sequences_inputs = Placeholder[dtype=DT_FLOAT, shape=[?], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
2017-08-08 01:06:00.818859: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Shape [-1] has negative dimensions
   [[Node: input_sequences_inputs = Placeholder[dtype=DT_FLOAT, shape=[?], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
2017-08-08 01:06:00.822800: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Shape [-1] has negative dimensions
   [[Node: input_sequences_inputs = Placeholder[dtype=DT_FLOAT, shape=[?], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
2017-08-08 01:06:00.823670: E tensorflow/core/common_runtime/executor.cc:644] Executor failed to create kernel. Invalid argument: Shape [-1] has negative dimensions
   [[Node: input_sequences_inputs = Placeholder[dtype=DT_FLOAT, shape=[?], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
</code></pre>

<p>My intention is to provide the <code>input_sequences</code> as a placeholder and provide the actual values during training. The sequences will be variable length and so I do not want to define a fixed size for this placeholder, hence the shape of <code>[None]</code> for <code>input_sequences[""inputs""]</code> and <code>input_sequences[""labels""]</code>.</p>

<p>The documentation suggests that you can provide <code>input_length=None</code> and lengths for the sequences in <code>input_sequences[""inputs""]</code> will be inferred from the actual sequences:</p>

<blockquote>
  <p><code>input_length</code> is a Tensor scalar or an int recording the time dimension prior to padding. It should be between 0 and the time dimension. One reason we want to keep track of it is so that we can take it into consideration when computing the loss. If <code>pad=True</code> then <code>input_length</code> can be <code>None</code> and will be inferred.</p>
  
  <p><a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/training/batch_sequences_with_states"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/contrib/training/batch_sequences_with_states</a></p>
</blockquote>

<p>What am I doing wrong?</p>

<p>I am using Python 3.6.1, Tensorflow 1.2.1.</p>
",0
45569277,How can I efficiently use tensorflow if I have a CPU with 64 cores?,"<p>So I have a facility of 1 CPU with 64 cores. I have installed tensorflow from anaconda. I know that if I had multiple CPUs, I could distribute computation by specifying the CPUids. Like below (adapted from <a href=""https://stackoverflow.com/questions/42069147/implementation-of-model-parallelism-in-tensorflow"">here</a>) :</p>

<pre><code>with tf.device(""/cpu:0""):
    a = tf.Variable(tf.ones(()))
    a = tf.square(a)
with tf.device(""/cpu:1""):
    b = tf.Variable(tf.ones(()))
    b = tf.square(b)
with tf.device(""/cpu:2""):
    loss = a+b
opt = tf.train.GradientDescentOptimizer(learning_rate=0.1)
train_op = opt.minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())
for i in range(10):
    loss0, _ = sess.run([loss, train_op])
    print(""loss"", loss0)
</code></pre>

<p>The above example code assumes three CPUs. But I was wondering if I can efficiently do some kind of efficient deep learning exercises with the present facility (1 CPU, 64 cores)? Can someone help or guide me?</p>

<hr>

<p>UPDATE : </p>

<ul>
<li><p>The cores are <em>Intel Xeon Phi</em> processor model. </p></li>
<li><p>Also please note that I don't have administrator privilege, so cannot compile any libraries. I installed every python libraries via Anaconda.</p></li>
<li><p>My attempt to understand something. I used the Timeline concept (from <a href=""https://stackoverflow.com/questions/34293714/can-i-measure-the-execution-time-of-individual-operations-with-tensorflow/37774470#37774470"">here</a>) in the above given code like below : </p>

<pre><code>import tensorflow as tf
from tensorflow.python.client import timeline


with tf.device(""/cpu:0""):
    a = tf.Variable(tf.ones(()))
    a = tf.square(a)
with tf.device(""/cpu:0""):
    b = tf.Variable(tf.ones(()))
    b = tf.square(b)
with tf.device(""/cpu:0""):
    loss = a+b
opt = tf.train.GradientDescentOptimizer(learning_rate=0.1)
train_op = opt.minimize(loss)

sess = tf.Session()
run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
run_metadata = tf.RunMetadata()
sess.run(tf.global_variables_initializer())
for i in range(10):
    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
    run_metadata = tf.RunMetadata()
    loss0, _ = sess.run([loss, train_op], options=run_options,run_metadata=run_metadata)
    print(""loss"", loss0)

# Create the Timeline object, and write it to a json
tl = timeline.Timeline(run_metadata.step_stats)
ctf = tl.generate_chrome_trace_format()
with open('timeline_execution1.json', 'w') as f:
    f.write(ctf)
</code></pre></li>
</ul>

<p>And then I generated different json files to see the timeline in chrome with <code>config=tf.ConfigProto(intra_op_parallelism_threads=#,inter_op_parallelism_threads=#)</code> line in <code>tf.Session()</code>. And then I got different outputs. But I understood nothing other than one point. This program is using 4 cores, whatever options I give inside <code>tf.Session()</code>. Like below :</p>

<p><a href=""https://i.stack.imgur.com/Ow32S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Ow32S.png"" alt=""enter image description here""></a></p>
",0
45591967,How does Kmeans clustering work in tensorflow?,"<p>I saw that there is an implementation of Kmeans clustering available within tensorflow contrib library. However, I was not able to do the simple operation of estimating cluster centers for 2D points. </p>

<p>Code:</p>

<pre><code>## Generate synthetic data
N,D = 1000, 2 # number of points and dimenstinality

means = np.array([[0.5, 0.0],
                  [0, 0],
                  [-0.5, -0.5],
                  [-0.8, 0.3]])
covs = np.array([np.diag([0.01, 0.01]),
                 np.diag([0.01, 0.01]),
                 np.diag([0.01, 0.01]),
                 np.diag([0.01, 0.01])])
n_clusters = means.shape[0]

points = []
for i in range(n_clusters):
    x = np.random.multivariate_normal(means[i], covs[i], N )
    points.append(x)
points = np.concatenate(points)

## construct model
kmeans = tf.contrib.learn.KMeansClustering(num_clusters = n_clusters)
kmeans.fit(points.astype(np.float32))
</code></pre>

<p>I get the following error : </p>

<pre><code>InvalidArgumentError (see above for traceback): Shape [-1,2] has negative dimensions
     [[Node: input = Placeholder[dtype=DT_FLOAT, shape=[?,2], _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]
</code></pre>

<p>I guess I'm doing something wrong but couldn't figure out what from the documentation. </p>

<p><strong>Edit</strong>:  </p>

<p>I solved it using <code>input_fn</code> but it is really slow (I had to reduce the number of points in each cluster to 10 to see results). Why is that and how can I make it faster? </p>

<pre><code> def input_fn():
    return tf.constant(points, dtype=tf.float32), None

## construct model
kmeans = tf.contrib.learn.KMeansClustering(num_clusters = n_clusters, relative_tolerance=0.0001)
kmeans.fit(input_fn=input_fn)
centers = kmeans.clusters()
print(centers)
</code></pre>

<p><strong>Solved:</strong> </p>

<p>It seems that a relative tolerance should be set. so I changed only one line and it works fine. 
<code>kmeans = tf.contrib.learn.KMeansClustering(num_clusters = n_clusters, relative_tolerance=0.0001)</code></p>
",1
45595419,Is it possible to have multiple conditions defined in tf.while_loop,"<p>Is it possible to define to multiple conditions for termination of a tf.while_loop in tensorflow? For example depending on the two tensor values achieving two specific values. eg. <code>i==2</code> and <code>j==3</code> ?</p>

<p>Also can I have several blocks of code in the body? In all the examples in the documentation, it seems that the body is more like a single statement returning a value or a tuple. I want to execute a set of several ""<strong>sequential</strong>"" statements in the body.</p>
",1
45610004,Disabling tensorflow os level warning.,"<p>Sorry for an inappropriate question.First time installed tensor-flow.While testing if it was installed correctly getting errors/warning in tf.Session().</p>

<p>using python 3.5.</p>

<p><strong>Code</strong></p>

<pre><code>import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
print(sess.run(hello))
</code></pre>

<p><strong>Error</strong></p>

<pre><code>2017-08-10 14:47:51.923532: W c:\tf_jenkins\home\workspace\release-win\m\windows\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-08-10 14:47:51.924625: W c:\tf_jenkins\home\workspace\release-win\m\windows\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-10 14:47:51.925259: W c:\tf_jenkins\home\workspace\release-win\m\windows\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-10 14:47:51.925848: W c:\tf_jenkins\home\workspace\release-win\m\windows\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-10 14:47:51.926445: W c:\tf_jenkins\home\workspace\release-win\m\windows\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-10 14:47:51.926971: W c:\tf_jenkins\home\workspace\release-win\m\windows\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-08-10 14:47:51.927455: W c:\tf_jenkins\home\workspace\release-win\m\windows\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-08-10 14:47:51.928056: W c:\tf_jenkins\home\workspace\release-win\m\windows\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
</code></pre>

<p><strong>Output</strong></p>

<pre><code>b'Hello, tensor-flow'
</code></pre>
",0
45615850,"How to convert byte image from [0,255] to [-1,1] float range in Python for Tensorflow Mobilenet predictions?","<p>I'm an R user, new to both Python and TensorFlow, and have been struggling to get my retrained image classifier to actually make predictions when modifying <a href=""https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image"" rel=""nofollow noreferrer"">label_image.py</a> for use with Mobilenets.  I've identified the problem and know I need to implement the last line from <a href=""https://www.tensorflow.org/versions/r1.3/tutorials/image_retraining#other_model_architectures"" rel=""nofollow noreferrer"">this tutorial</a>, but I can't figure out how.</p>

<blockquote>
  <p>If you're going to be using the Mobilenet models in label_image or
  your own programs, you'll need to feed in an image of the specified
  size converted to a float range into the 'input' tensor. Typically
  24-bit images are in the range [0,255], and you must convert them to
  the [-1,1] float range expected by the model with the formula (image -
  128.)/128..</p>
</blockquote>

<p>In R I'm used to dealing with JPEGs as 3 dimensional arrays.  If it were in that format I would know what to do, but the image type returned from <code>tf.gfile.FastGFile(""fileName.jpg"", 'rb').read()</code> is <code>bytes</code>.  I don't really understand what this is.  Directly applying the formula they give to the image object returns <code>TypeError: unsupported operand type(s) for -: 'bytes' and 'float'</code>.  I assume that after I change the range I'll still need it to be in <code>bytes</code> format to feed it into the network, but I'm not 100% clear on that either.  Any clarifications on what this object type is and how to work with it would be much appreciated.</p>
",0
45634450,What are the advantages of using tf.train.SequenceExample over tf.train.Example for variable length features?,"<p>Recently I read <a href=""http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features"" rel=""noreferrer"">this</a> guide on undocumented featuers in TensorFlow, as I needed to pass variable length sequences as input. However, I found the protocol for <code>tf.train.SequenceExample</code> relatively confusing (especially due to lack of documentation), and managed to build an input pipe using <code>tf.train.Example</code> just fine instead.</p>

<p>Are there any advantages to using <code>tf.train.SequenceExample</code>? Using the standard example protocol when there is a dedicated one for variable length sequences seems like a cheat, but does it bear any consequence?</p>
",1
45666147,Tensorflow: Global step must be from the same graph as loss,"<p>I'm trying to use Tensorflow to do some classification with the tf.contrib.layers package, and I've run into a problem I can't quite figure out. As far as I can tell from examples (e.g. <a href=""https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/examples/tutorials/layers/cnn_mnist.py"" rel=""nofollow noreferrer"">this</a> and it's <a href=""https://www.tensorflow.org/tutorials/layers"" rel=""nofollow noreferrer"">tutorial</a>), everything with the graph is handled by the API. I can download and run the same code in my environment perfectly well.</p>

<p>However, when I run my code, I get the an error that my global step is not from the same graph as my loss, which seems bizarre: <code>ValueError: Tensor(""global_step:0"", shape=(), dtype=int64_ref) must be from the same graph as Tensor(""softmax_cross_entropy_loss/value:0"", shape=(), dtype=float32).</code> The error occurs during the construction of the <code>train_op</code></p>

<p>Here's my tensorflow code (I do have some other code for handling the loading of the data, but it doesn't use anything from tensorflow). Sorry that the code is sort of messy right now: I've been tearing it apart trying to figure this error out.</p>

<pre><code>import numpy as np
import tensorflow as tf
from tensorflow.contrib.learn.python.learn.estimators import model_fn as model_fn_lib

import data # my data loading module


def train(training_file, vocab_path, hidden_units=[10, 20, 10], estimator=tf.contrib.learn.DNNClassifier):
    """"""
    Given a training CSV file, train a Tensorflow neural network
    """"""

    training_set = data.load(training_file)

    vocab = tf.contrib.learn.preprocessing.VocabularyProcessor(data.DOC_LENGTH)
    vocab = vocab.restore(vocab_path)
    training_data = tf.one_hot(training_set.data, len(vocab.vocabulary_._mapping), dtype=tf.float32)
    training_targets = tf.constant(np.array(training_set.targets, dtype=np.int32))

    classifier = tf.contrib.learn.Estimator(model_fn=lambda features, targets, mode, params: model_fn(features, targets, mode, params, hidden_units))

    classifier.fit(input_fn=lambda: (training_data, training_targets), steps=2000)

    return classifier


def model_fn(features, targets, mode, params, hidden_units):
    if len(hidden_units) &lt;= 0:
        raise ValueError(""Hidden units must be a iterable of ints of length &gt;= 1"")

    # Define the network
    network = tf.contrib.layers.relu(features, hidden_units[0])
    for i in range(1, len(hidden_units)):
        network = tf.contrib.layers.relu(network, hidden_units[i])

    # Flatten the network
    network = tf.reshape(network, [-1, hidden_units[-1] * data.DOC_LENGTH])

    # Add dropout to enhance feature use
    network = tf.layers.dropout(inputs=network, rate=0.5, training=mode == tf.contrib.learn.ModeKeys.TRAIN)

    # Calculate the logits
    logits = tf.contrib.layers.fully_connected(network, 15)

    loss = None
    train_op = None

    if mode != tf.contrib.learn.ModeKeys.INFER:
        targets = tf.cast(tf.one_hot(targets, 15, 1, 0), dtype=tf.float32)
        loss = tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=targets)

    if mode == tf.contrib.learn.ModeKeys.TRAIN:
        # This train_op causes the error
        train_op = tf.contrib.layers.optimize_loss(
          loss=loss,
          global_step=tf.train.get_global_step(),
          optimizer='Adam',
          learning_rate=0.01)

    predictions = {
          ""classes"": tf.argmax(input=logits, axis=1),
          ""probabilities"": tf.nn.softmax(logits, name=""softmax_tensor"")
    }

    return model_fn_lib.ModelFnOps(mode=mode, predictions=predictions, loss=loss, train_op=train_op)


def main(unusedargv):

    # ... parses arguments

    classifier = train(args.train_data, args.vocab)
    print(evaluate(classifier, args.train_data))
    print(evaluate(classifier, args.test_data))


if __name__ == ""__main__"":
    tf.app.run()
</code></pre>

<p>Here's the full stack trace:</p>

<pre><code>File ""categorize.py"", line 126, in main
  classifier = train(args.train_data, args.vocab)
File ""categorize.py"", line 39, in train
  classifier.fit(input_fn=lambda: (training_data, training_targets), steps=2000)
File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 280, in new_func
  return func(*args, **kwargs)
File ""/usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 426, in fit
  loss = self._train_model(input_fn=input_fn, hooks=hooks)
File ""/usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 934, in _train_model
  model_fn_ops = self._call_legacy_get_train_ops(features, labels)
File ""/usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1003, in _call_legacy_get_train_ops
  train_ops = self._get_train_ops(features, labels)
File ""/usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1162, in _get_train_ops
  return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
File ""/usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1133, in _call_model_fn
  model_fn_results = self._model_fn(features, labels, **kwargs)
File ""categorize.py"", line 37, in &lt;lambda&gt;
  classifier = tf.contrib.learn.Estimator(model_fn=lambda features, targets, mode, params: model_fn(features, targets, mode, params, hidden_units))
File ""categorize.py"", line 73, in model_fn
  learning_rate=0.01)
File ""/usr/local/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py"", line 152, in optimize_loss
  with vs.variable_scope(name, ""OptimizeLoss"", [loss, global_step]):
File ""/usr/local/Cellar/python3/3.6.0_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py"", line 82, in __enter__
  return next(self.gen)
File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 1410, in variable_scope
  g = ops._get_graph_from_inputs(values)  # pylint: disable=protected-access
File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3968, in _get_graph_from_inputs
  _assert_same_graph(original_graph_element, graph_element)
File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3907, in _assert_same_graph
  ""%s must be from the same graph as %s."" % (item, original_item))
ValueError: Tensor(""global_step:0"", shape=(), dtype=int64_ref) must be from the same graph as Tensor(""softmax_cross_entropy_loss/value:0"", shape=(), dtype=float32).
</code></pre>

<p>Here's my code:</p>
",0
45701681,Determining the Epoch Number with tf.train.string_input_producer in tensorflow,"<p>I have some doubts on how <code>tf.train.string_input_producer</code> works. So suppose I fed filename_list as an input parameter to the <code>string_input_producer</code>. Then, according to the documentation <a href=""https://www.tensorflow.org/programmers_guide/reading_data"" rel=""nofollow noreferrer"">https://www.tensorflow.org/programmers_guide/reading_data</a>, this will create a <code>FIFOQueue</code>, where I can set epoch number, shuffle the file names and so on. Therefore, in my case, I have 4 file names (""db1.tfrecords"", ""db2.tfrecords""...). And I used <code>tf.train.batch</code> to feed the network batch of images. In addition, each file_name/database, contain a set of images for one person. The second database is for the second person and so on. So far I have the following code: </p>

<pre><code>tfrecords_filename_seq = [(common + ""P16_db.tfrecords""), (common + ""P17_db.tfrecords""), (common + ""P19_db.tfrecords""),
                          (common + ""P21_db.tfrecords"")]

filename_queue = tf.train.string_input_producer(tfrecords_filename_seq, num_epochs=num_epoch, shuffle=False, name='queue')
reader = tf.TFRecordReader()

key, serialized_example = reader.read(filename_queue)
features = tf.parse_single_example(
    serialized_example,
    # Defaults are not specified since both keys are required.
    features={
        'height': tf.FixedLenFeature([], tf.int64),
        'width': tf.FixedLenFeature([], tf.int64),
        'image_raw': tf.FixedLenFeature([], tf.string),
        'annotation_raw': tf.FixedLenFeature([], tf.string)
    })

image = tf.decode_raw(features['image_raw'], tf.uint8)
height = tf.cast(features['height'], tf.int32)
width = tf.cast(features['width'], tf.int32)

image = tf.reshape(image, [height, width, 3])

annotation = tf.cast(features['annotation_raw'], tf.string)

min_after_dequeue = 100
num_threads = 4
capacity = min_after_dequeue + num_threads * batch_size
label_batch, images_batch = tf.train.batch([annotation, image],
                                                        shapes=[[], [112, 112, 3]],
                                                        batch_size=batch_size,
                                                        capacity=capacity,
                                                        num_threads=num_threads)
</code></pre>

<p>Finally, when trying to view out the reconstructed image at the output of the autoencoder, I got the first the images from the 1st database, then I start viewing images from the second database and so on. </p>

<p>My question: How can i know if I'm within the same epoch? And if I'm within the sane epoch, how can i merge a batch of images from all the file_names that I have?</p>

<p>Finally, I tried to print out the value of the epoch by evaluating the local variable within the <code>Session</code> as follows: </p>

<pre><code>epoch_var = tf.local_variables()[0]
</code></pre>

<p>Then: </p>

<pre><code>with tf.Session() as sess:
    print(sess.run(epoch_var.eval())) # Here I got 9 as output. don't know y.
</code></pre>

<p>Any help is much appreciated!!</p>
",0
45705070,how to load and use a saved model on tensorflow?,"<p>I have found 2 ways to save a model in Tensorflow: <code>tf.train.Saver()</code> and <code>SavedModelBuilder</code>. However, <strong>I can't find documentation on using the model</strong> after it being loaded  the second way.</p>

<p>Note: I want to use <code>SavedModelBuilder</code> way because I train the model in Python and will use it at serving time in another language (Go), and it seems that <code>SavedModelBuilder</code> is the only way in that case.</p>

<p>This works great with <code>tf.train.Saver()</code> (first way):</p>

<pre><code>model = tf.add(W * x, b, name=""finalnode"")

# save
saver = tf.train.Saver()
saver.save(sess, ""/tmp/model"")

# load
saver.restore(sess, ""/tmp/model"")

# IMPORTANT PART: REALLY USING THE MODEL AFTER LOADING IT
# I CAN'T FIND AN EQUIVALENT OF THIS PART IN THE OTHER WAY.

model = graph.get_tensor_by_name(""finalnode:0"")
sess.run(model, {x: [5, 6, 7]})
</code></pre>

<p><code>tf.saved_model.builder.SavedModelBuilder()</code> is defined in the <a href=""https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/saved_model/"" rel=""noreferrer"">Readme</a>  but after loading the model with <code>tf.saved_model.loader.load(sess, [], export_dir)</code>), I can't find documentation on getting back at the nodes (see <code>""finalnode""</code> in the code above)</p>
",1
45734487,tensorflow: Error multiplying a sparse matrix with a dense matrix using tf.matmul,"<p>In the following code, I want dense matrix <code>B</code> to left multiply a sparse matrix <code>A</code>, but I got errors.</p>

<pre><code>import tensorflow as tf
import numpy as np

A = tf.sparse_placeholder(tf.float32)
B = tf.placeholder(tf.float32, shape=(5,5))
C = tf.matmul(B,A,a_is_sparse=False,b_is_sparse=True)
sess = tf.InteractiveSession()
indices = np.array([[3, 2], [1, 2]], dtype=np.int64)
values = np.array([1.0, 2.0], dtype=np.float32)
shape = np.array([5,5], dtype=np.int64)
Sparse_A = tf.SparseTensorValue(indices, values, shape)
RandB = np.ones((5, 5))
print sess.run(C, feed_dict={A: Sparse_A, B: RandB})
</code></pre>

<p>The error message is as follows:</p>

<pre><code>TypeError: Failed to convert object of type &lt;class 'tensorflow.python.framework.sparse_tensor.SparseTensor'&gt; 
to Tensor. Contents: SparseTensor(indices=Tensor(""Placeholder_4:0"", shape=(?, ?), dtype=int64), values=Tensor(""Placeholder_3:0"", shape=(?,), dtype=float32), dense_shape=Tensor(""Placeholder_2:0"", shape=(?,), dtype=int64)). 
Consider casting elements to a supported type.
</code></pre>

<p>What's wrong with my code?</p>

<p>I'm doing this following the <a href=""https://www.tensorflow.org/api_docs/python/tf/matmul"" rel=""nofollow noreferrer"">documentation</a> and it says we should use <code>a_is_sparse</code> to denote whether the first matrix is sparse, and similarly with <code>b_is_sparse</code>. Why is my code wrong?</p>

<p>As is suggested by vijay, I should use <code>C = tf.matmul(B,tf.sparse_tensor_to_dense(A),a_is_sparse=False,b_is_sparse=True)</code></p>

<p>I tried this but I met with another error saying:</p>

<pre><code>Caused by op u'SparseToDense', defined at:
  File ""a.py"", line 19, in &lt;module&gt;
    C = tf.matmul(B,tf.sparse_tensor_to_dense(A),a_is_sparse=False,b_is_sparse=True)
  File ""/home/fengchao.pfc/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/sparse_ops.py"", line 845, in sparse_tensor_to_dense
    name=name)
  File ""/home/mypath/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/sparse_ops.py"", line 710, in sparse_to_dense
    name=name)
  File ""/home/mypath/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_sparse_ops.py"", line 1094, in _sparse_to_dense
    validate_indices=validate_indices, name=name)
  File ""/home/mypath/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""/home/mypath/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""/home/mypath/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

InvalidArgumentError (see above for traceback): indices[1] = [1,2] is out of order
[[Node: SparseToDense = SparseToDense[T=DT_FLOAT, Tindices=DT_INT64, validate_indices=true, _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_Placeholder_4_0_2, _arg_Placeholder_2_0_0, _arg_Placeholder_3_0_1, SparseToDense/default_value)]]
</code></pre>

<p>Thank you all for helping me!</p>
",1
45784815,How best to implement a matrix mask operation in tensorflow?,"<p>I had a case where I needed to fill some holes (missing data) in an image processing application in tensorflow. The 'holes' are easy to locate as they are zeros and the good data is not zeros. I wanted to fill the holes with random data. This is quite easy to do using python numpy but doing it in tensorflow requires some work. I came up with a solution and wanted to see if there is a better or more efficient way to do the same thing. I understand that tensorflow does not yet support the more advanced numpy type indexing yet but there is a function tf.gather_nd() that seems promising for this. However, I could not tell from the documentation how to us it for what I wanted to do. I would appreciate answers that improve on what I did or especially if someone can show me how to do it using tf.gather_nd(). Also, tf.boolean_mask() does not work for what I am  trying to do because it does not allow you to use the output as an index.  In python what I am trying to do:</p>

<pre><code>a = np.ones((2,2))
a[0,0]=a[0,1] = 0
mask = a == 0
a[mask] = np.random.random_sample(a.shape)[mask]
print('new a = ', a)
</code></pre>

<p>What I ended up doing in Tensorflow to achieve same thing (skipping filling the array steps)</p>

<pre><code>zeros = tf.zeros(tf.shape(a))  
mask = tf.greater(a,zeros)
mask_n = tf.equal(a,zeros)
mask = tf.cast(mask,tf.float32)
mask_n = tf.cast(mask_n,tf.float32
r = tf.random_uniform(tf.shape(a),minval = 0.0,maxval=1.0,dtype=tf.float32)
r_add = tf.multiply(mask_n,r)
targets = tf.add(tf.multiply(mask,a),r_add)
</code></pre>
",1
45869131,All Tensorflow outputs are nan,"<pre><code>import tensorflow as tf

# Model parameters
A = tf.Variable([.3], dtype=tf.float32)
W = tf.Variable([.3], dtype=tf.float32)
b = tf.Variable([-.3], dtype=tf.float32)
# Model input and output
x = tf.placeholder(tf.float32)
q_model = A * (x**2) + W * x + b
y = tf.placeholder(tf.float32)

# loss
loss = tf.reduce_sum(tf.square(q_model - y)) # sum of the squares
# optimizer
optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss)

# training data
x_train = [0, 1, 2, 3, 4]
y_train = [0, 1, 4, 9, 16]
# training loop
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init) # reset values to wrong
for i in range(1000):
  sess.run(train, {x: x_train, y: y_train})

# evaluate training accuracy
curr_A, curr_W, curr_b, curr_loss = sess.run([A, W, b, loss], {x: x_train, y: y_train})
print(""A: %s W: %s b: %s loss: %s""%(curr_A, curr_W, curr_b, curr_loss))
</code></pre>

<p>On their website, tf gives model code to perform linear regression. However, I wanted to play around to see if I could also get it to do quadratic regression. To do so, I added a tf.Variable A, put it into the model and then modified the output to tell me what it got as the value.</p>

<p>Here are the results:</p>

<pre><code>A: [ nan] W: [ nan] b: [ nan] loss: nan
</code></pre>

<p>What do y'all think is the issue here? Is it between the chair and the keyboard?</p>
",0
45879776,TensorFlow how to make results reproducible for `tf.nn.sampled_softmax_loss`,"<p>I would like to get reproducible results for my tensorflow runs. The way I'm trying to make this happen is to set up the numpy and tensorflow seeds:</p>

<pre><code>import numpy as np
rnd_seed = 1
np.random.seed(rnd_seed)

import tensorflow as tf
tf.set_random_seed(rnd_seed)
</code></pre>

<p>As well as make sure that the weights of the neural network, that I initialized with <code>tf.truncated_normal</code> also use that seed: <code>tf.truncated_normal(..., seed=rnd_seed)</code></p>

<p>For reasons that are beyond the scope of this question, I'm using the sampled softmax loss function, <code>tf.nn.sampled_softmax_loss</code>, and unfortunately, I'm not able to control the stochasticity of this function with a random seed.</p>

<p>By a look at the TensorFlow documentation of this function (<a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss</a>), I can see that parameter <code>sampled_values</code> should be the only parameter that affects randomization, but I'm not able to understand how to actually use a seed.</p>

<p>[EDITED]
This is (part of) my script</p>

<pre><code>import numpy as np
# set a seed so that the results are consistent
rnd_seed = 1
np.random.seed(rnd_seed)

import tensorflow as tf
tf.set_random_seed(rnd_seed)

embeddings_ini = np.random.uniform(low=-1, high=1, size=(self.vocabulary_size, self.embedding_size))

with graph.as_default(), tf.device('/cpu:0'):

    train_dataset = tf.placeholder(tf.int32, shape=[None, None])
    train_labels = tf.placeholder(tf.int32, shape=[None, 1])
    valid_dataset = tf.constant(self.valid_examples, dtype=tf.int32)

    # Variables.
    initial_embeddings = tf.placeholder(tf.float32, shape=(self.vocabulary_size, self.embedding_size))
    embeddings = tf.Variable(initial_embeddings)

    softmax_weights = tf.Variable(
        tf.truncated_normal([self.vocabulary_size, self.embedding_size],
                            stddev=1.0 / math.sqrt(self.embedding_size), seed=rnd_seed))
    softmax_biases = tf.Variable(tf.zeros([self.vocabulary_size]))

    # Model.
    # Look up embeddings for inputs.
    if self.model == ""skipgrams"":
        # Skipgram model
        embed = tf.nn.embedding_lookup(embeddings, train_dataset)
    elif self.model == ""cbow"":
        # CBOW Model
        embeds = tf.nn.embedding_lookup(embeddings, train_dataset)
        embed = tf.reduce_mean(embeds, 1, keep_dims=False)

    # Compute the softmax loss, using a sample of the negative labels each time.
    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=softmax_weights,
                                                     biases=softmax_biases,
                                                     inputs=embed,
                                                     labels=train_labels,
                                                     num_sampled=self.num_sampled,
                                                     num_classes=self.vocabulary_size))
</code></pre>
",1
45946489,Tensorflow Object Detection API can't load .records file,"<p>I am trying to train object detection model by following <a href=""https://github.com/tensorflow/models/blob/master/object_detection/g3doc/running_locally.md"" rel=""nofollow noreferrer"">running locally</a> option from tensorflow object detection API. 
After following the documentation:</p>

<ol>
<li>I am able to download the data and have placed pascal_train.record, pascal_eval.record and pascal_label_map.pbtxt in ./object_detection/data/ directory. </li>
<li>I have edited the ""faster_rcnn_resnet101.config"" and placed as : ./models/model/faster_rcnn_resnet101.config. </li>
<li>I did following changes in the .config file: fine_tune_checkpoint : model_path/model.ckpt, input_path = ./object_detection/data/pascal_train.record, label_map_path: ./object_detection/data/pascal_label_map.pbtxt and similarly for eval_input_reader. </li>
<li>Now if I use following command to train the model : python3 object_detection/train.py --pipeline_config_path=./object_detection/data/faster_rcnn_resnet101.config --train_dir=./model/train_logs, it throws this error : </li>
</ol>

<p>INFO:tensorflow:Starting Session.</p>

<p>INFO:tensorflow:Saving checkpoint to path ./model/train_logs/model.ckpt
INFO:tensorflow:Starting Queues.</p>

<p>INFO:tensorflow:Error reported to Coordinator: , ~/Documents/Projects/models/object_detection/data/pascal_train.record</p>

<p>[[Node: parallel_read/ReaderReadV2_2 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/cpu:0""](parallel_read/TFRecordReaderV2_2, parallel_read/filenames)]]</p>

<p>INFO:tensorflow:global_step/sec: 0
2017-08-29 11:41:59.852783: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: FIFOQueue '_5_prefetch_queue' is closed and has insufficient elements (requested 1, current size 0)
         [[Node: prefetch_queue_Dequeue = QueueDequeueV2component_types=[DT_INT32, DT_FLOAT, DT_INT32, DT_BOOL, DT_INT32, DT_BOOL, DT_INT32, DT_INT32, DT_STRING, DT_FLOAT, DT_FLOAT, DT_INT3
2, DT_INT64, DT_STRING, DT_INT32, DT_INT32, DT_INT32, DT_INT64, DT_INT32, DT_INT64, DT_STRING, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""]]
2017-08-29 11:41:59.852819: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: FIFOQueue '_5_prefetch_queue' is closed and has insufficient elements (requested 1, current size 0)
         [[Node: prefetch_queue_Dequeue = QueueDequeueV2component_types=[DT_INT32, DT_FLOAT, DT_INT32, DT_BOOL, DT_INT32, DT_BOOL, DT_INT32, DT_INT32, DT_STRING, DT_FLOAT, DT_FLOAT, DT_INT3
2, DT_INT64, DT_STRING, DT_INT32, DT_INT32, DT_INT32, DT_INT64, DT_INT32, DT_INT64, DT_STRING, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""]]
2017-08-29 11:41:59.852797: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: FIFOQueue '_5_prefetch_queue' is closed and has insufficient elements (requested 1, current size 0)
.
.
.
.</p>

<p>[[Node: prefetch_queue_Dequeue = QueueDequeueV2component_types=[DT_INT32, DT_FLOAT, DT_INT32, DT_BOOL, DT_INT32, DT_BOOL, DT_INT32, DT_INT32, DT_STRING, DT_FLOAT, DT_FLOAT, DT_INT32, DT_INT64, DT_STRING, DT_INT32, DT_INT32, DT_INT32, DT_INT64, DT_INT32, DT_INT64, DT_STRING, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""]]
2017-08-29 11:42:00.353191: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: FIFOQueue '_5_prefetch_queue' is closed and has insufficient elements (requested 1, current size 0)
         [[Node: prefetch_queue_Dequeue = QueueDequeueV2component_types=[DT_INT32, DT_FLOAT, DT_INT32, DT_BOOL, DT_INT32, DT_BOOL, DT_INT32, DT_INT32, DT_STRING, DT_FLOAT, DT_FLOAT, DT_INT32, DT_INT64, DT_STRING, DT_INT32, DT_INT32, DT_INT32, DT_INT64, DT_INT32, DT_INT64, DT_STRING, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""]]
2017-08-29 11:42:00.353105: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: FIFOQueue '_5_prefetch_queue' is closed and has insufficient elements (requested 1, current size 0)
         [[Node: prefetch_queue_Dequeue = QueueDequeueV2component_types=[DT_INT32, DT_FLOAT, DT_INT32, DT_BOOL, DT_INT32, DT_BOOL, DT_INT32, DT_INT32, DT_STRING, DT_FLOAT, DT_FLOAT, DT_INT32, DT_INT64, DT_STRING, DT_INT32, DT_INT32, DT_INT32, DT_INT64, DT_INT32, DT_INT64, DT_STRING, DT_INT32], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""]]</p>

<p>INFO:tensorflow:Caught OutOfRangeError. Stopping Training.</p>

<p>INFO:tensorflow:Finished training! Saving model to disk.</p>

<p>Traceback (most recent call last):
  File ""object_detection/train.py"", line 199, in 
    tf.app.run()</p>

<p>File ""/home/skulhare/.local/lib/python3.5/site-packages/tensorflow/python/platform/app.py"", line 48, in run</p>

<pre><code>_sys.exit(main(_sys.argv[:1] + flags_passthrough))
</code></pre>

<p>File ""object_detection/train.py"", line 195, in main</p>

<pre><code>worker_job_name, is_chief, FLAGS.train_dir)
</code></pre>

<p>File ""/home/skulhare/Documents/Projects/models/object_detection</p>

<p>/trainer.py"", line 296, in train</p>

<pre><code>saver=saver)
</code></pre>

<p>File ""/home/skulhare/.local/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/learning.py"", line 767, in train</p>

<pre><code>sv.stop(threads, close_summary_writer=True)
</code></pre>

<p>File ""/home/skulhare/.local/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py"", line 792, in stop
    stop_grace_period_secs=self._stop_grace_secs)</p>

<p>File ""/home/skulhare/.local/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
    six.reraise(*self._exc_info_to_raise)</p>

<p>File ""/home/skulhare/.local/lib/python3.5/site-packages/six.py"", line 686, in reraise
    raise value
  File ""/home/skulhare/.local/lib/python3.5/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 238, in _run</p>

<pre><code>enqueue_callable()
</code></pre>

<p>File ""/home/skulhare/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1235, in _single_operation_run
    target_list_as_strings, status, None)</p>

<p>File ""/usr/lib/python3.5/contextlib.py"", line 66, in <strong>exit</strong>
    next(self.gen)
  File ""/home/skulhare/.local/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))</p>

<p>tensorflow.python.framework.errors_impl.NotFoundError: ~/Documents/Projects/models/object_detection/data/pascal_train.record
         [[Node: parallel_read/ReaderReadV2_2 = ReaderReadV2[_device=""/job:localhost/replica:0/task:0/cpu:0""](parallel_read/TFRecordReaderV2_2, parallel_read/filenames)]]</p>

<p>I have been using tensorflow 1.3.0 on Ubuntu 16.04 with GTX 1080 ti. 
<a href=""https://github.com/skrealworld/Useful_Scripts/blob/master/faster_rcnn_resnet101.config"" rel=""nofollow noreferrer"">Here</a> is the content of faster_rcnn_resnet101.config file.</p>

<p>Thanks in advance. </p>
",0
45955241,How do I create padded batches in Tensorflow for tf.train.SequenceExample data using the DataSet API?,"<p>For training an <strong>LSTM model</strong> in <strong>Tensorflow</strong>, I have structured my data into a <strong>tf.train.SequenceExample</strong> format and stored it into a <strong>TFRecord file</strong>. I would now like to use the new DataSet API to <strong>generate padded batches for training</strong>. In <a href=""https://www.tensorflow.org/programmers_guide/datasets"" rel=""noreferrer"">the documentation</a> there is an example for using padded_batch, but for my data I can't figure out what the value of <em>padded_shapes</em> should be.</p>

<p>For reading the TFrecord file into the batches I have written the following Python code:</p>

<pre><code>import math
import tensorflow as tf
import numpy as np
import struct
import sys
import array

if(len(sys.argv) != 2):
  print ""Usage: createbatches.py [RFRecord file]""
  sys.exit(0)


vectorSize = 40
inFile = sys.argv[1]

def parse_function_dataset(example_proto):
  sequence_features = {
      'inputs': tf.FixedLenSequenceFeature(shape=[vectorSize],
                                           dtype=tf.float32),
      'labels': tf.FixedLenSequenceFeature(shape=[],
                                           dtype=tf.int64)}

  _, sequence = tf.parse_single_sequence_example(example_proto, sequence_features=sequence_features)

  length = tf.shape(sequence['inputs'])[0]
  return sequence['inputs'], sequence['labels']

sess = tf.InteractiveSession()

filenames = tf.placeholder(tf.string, shape=[None])
dataset = tf.contrib.data.TFRecordDataset(filenames)
dataset = dataset.map(parse_function_dataset)
# dataset = dataset.batch(1)
dataset = dataset.padded_batch(4, padded_shapes=[None])
iterator = dataset.make_initializable_iterator()

batch = iterator.get_next()

# Initialize `iterator` with training data.
training_filenames = [inFile]
sess.run(iterator.initializer, feed_dict={filenames: training_filenames})

print(sess.run(batch))
</code></pre>

<p>The code works well if I use <code>dataset = dataset.batch(1)</code> (no padding needed in that case), but when I use the <code>padded_batch</code> variant, I get the following error:</p>

<blockquote>
  <p>TypeError: If shallow structure is a sequence, input must also be a
  sequence. Input has type: .</p>
</blockquote>

<p>Can you help me figuring out what I should pass for the <em>padded_shapes</em> parameter?</p>

<p>(I know there is lots of example code using threading and queues for this, but I'd rather use the new DataSet API for this project)</p>
",1
46078650,Using exponential decay in tf.contrib.layers.optimize_loss,"<p>The doc for <code>tf.contrib.layers.optimize_loss</code> mentions the possibility to give a <code>learning_rate_decay_fn</code>, e.g. <code>tf.train.exponential_decay</code>. However, for the suggested decay function I did not find out how to pass the additional arguments (<code>decay_steps</code>, <code>decay_rate</code> and <code>staircase</code>).</p>

<p>First attempt: </p>

<pre><code>def my_decay(a, b):
tf.train.exponential_decay(a, b, decay_steps=5000, decay_rate=0.5,
                           staircase=True, name=""LR_decay"")


train_op = tf.contrib.layers.optimize_loss(
    loss,
    global_step=global_counter,
    learning_rate=FLAGS.learning_rate,
    optimizer=optimizer,
    learning_rate_decay_fn=my_decay
)
</code></pre>

<p>This results in a ValueError (Tried to convert 'values' to a tensor and failed. Error: None values not supported). Probably due to the fact that the function is a python function and not a TensorFlow op.</p>

<p>Second attempt: </p>

<pre><code>train_op = tf.contrib.layers.optimize_loss(
    loss,
    global_step=global_counter,
    learning_rate=FLAGS.learning_rate,
    optimizer=optimizer,
    learning_rate_decay_fn=tf.train.exponential_decay(decay_steps=5000, decay_rate=0.5,
                               staircase=True, name=""LR_decay"")
)
</code></pre>

<p>which complains about 2 missing positional arguments (as calling the function with missing parameters does not return a function but results in an attempt at evaluation).</p>

<p>Another attempt of adding the positional arguments to the <code>optimize_loss</code> function directly also failed (as the arguments are not passed down but seen as arguments to the function directly, which must fail.)</p>

<p>How can I pass the necessary arguments?</p>
",0
46089839,Creating a session in a graph that uses another graph and its session,"<p><strong>Versions :</strong> I am using tensorflow (version :  v1.1.0-13-g8ddd727 1.1.0) in python3 (Python 3.4.3 (default, Nov 17 2016, 01:08:31) [GCC 4.8.4] on linux), it is installed from source and GPU-based (name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate (GHz) 1.076).</p>

<p><strong>Context :</strong> Generative adversarial networks (GANs) learn to synthesise new samples from a high-dimensional distribution by passing samples drawn from a latent space through a generative network. When the high-dimensional distribution describes images of a particular data set, the network should learn to generate visually similar image samples for latent variables that are close to each other in the latent space. For tasks such as image retrieval and image classification, it may be useful to exploit the arrangement of the latent space by projecting images into it, and using this as a representation for discriminative tasks.</p>

<p><strong>Context Problem</strong> : I am trying to invert a generator (compute L2 norm between an input image in cifar10 and a image g(z) of the generator, where z is  a parameter to be trained with stochastic gradient descent in order to minimize this norm and find an approximation of the preimage of the input image).</p>

<p><strong>Technical Issue</strong> : Therefore, I am building a new graph in a new session in tensorflow but I need to use a trained gan that was trained in another session, which I cannot import because the two graphs are not the same. That is to say, when I use sess.run(), the variables are not found and therefore there is a Error Message.</p>

<p>The code is </p>

<pre><code>import tensorflow as tf
from data import cifar10, utilities
from . import dcgan
import logging

logger = logging.getLogger(""gan.test"")

BATCH_SIZE = 1


random_z = tf.get_variable(name='z_to_invert', shape=[BATCH_SIZE, 100], initializer=tf.random_normal_initializer())
#random_z = tf.random_normal([BATCH_SIZE, 100], mean=0.0, stddev=1.0, name='random_z')

# Generate images with generator
generator = dcgan.generator(random_z, is_training=True, name='generator')


# Add summaries to visualise output images 
generator_visualisation = tf.cast(((generator / 2.0) + 0.5) * 255.0, tf.uint8)
summary_generator = tf.summary.\
                    image('summary/generator', generator_visualisation,
                     max_outputs=8)


#Create one image to test inverting
test_image = map((lambda inp: (inp[0]*2. -1., inp[1])), 
                     utilities.infinite_generator(cifar10.get_train(), BATCH_SIZE))
inp, _ = next(test_image)
summary_inp = tf.summary.image('input_image', inp)

img_summary = tf.summary.merge([summary_generator, summary_inp])

with tf.name_scope('error'):
    error = inp - generator #generator = g(z)
    # We set axis = None because norm(tensor, ord=ord) is equivalent to norm(reshape(tensor, [-1]), ord=ord)
    error_norm = tf.norm(error, ord=2, axis=None, keep_dims=False, name='L2Norm')
    summary_error = tf.summary.scalar('error_norm', error_norm)

with tf.name_scope('Optimizing'):
    optimizer = tf.train.AdamOptimizer(0.001).minimize(error_norm, var_list=z)

sv = tf.train.Supervisor(logdir=""gan/invert_logs/"", save_summaries_secs=None, save_model_secs=None)

batch = 0
with sv.managed_session() as sess:
    logwriter = tf.summary.FileWriter(""gan/invert_logs/"", sess.graph)
    while not sv.should_stop():
        if batch &gt; 0 and batch % 100 == 0:
            logger.debug('Step {} '.format(batch))
        (_, s) = sess.run((optimizer, summary_error))
        logwriter.add_summary(s, batch)
        print('step %d: Patiente un peu poto!' % batch)
        img = sess.run(img_summary)
        logwriter.add_summary(img, batch)
        batch += 1
        print(batch)
</code></pre>

<p>I understood what is the problem, it is actually that I am trying to run a session which is saved in gan/train_logs but the graph does not have those variables I am trying to run.</p>

<p>Therefore, I tried to implement this instead : </p>

<pre><code>graph = tf.Graph()
tf.reset_default_graph()
with tf.Session(graph=graph) as sess:

    ckpt = tf.train.get_checkpoint_state('gan/train_logs/')
    saver = tf.train.import_meta_graph(ckpt.model_checkpoint_path + '.meta', clear_devices=True)
    saver.restore(sess, ckpt.model_checkpoint_path)
    logwriter = tf.summary.FileWriter(""gan/invert_logs/"", sess.graph)
    #inp, _ = next(test_image)

    BATCH_SIZE = 1

    #Create one image to test inverting
    test_image = map((lambda inp: (inp[0]*2. -1., inp[1])), 
                     utilities.infinite_generator(cifar10.get_train(), BATCH_SIZE))
    inp, _ = next(test_image)
    #M_placeholder = tf.placeholder(tf.float32, shape=cifar10.get_shape_input(), name='M_input')
    M_placeholder = inp
    zmar = tf.summary.image('input_image', inp)
    #Create sample noise from random normal distribution
    z = tf.get_variable(name='z', shape=[BATCH_SIZE, 100], initializer=tf.random_normal_initializer())

    # Function g(z) zhere z is randomly generated
    g_z = dcgan.generator(z, is_training=True, name='generator')
    generator_visualisation = tf.cast(((g_z / 2.0) + 0.5) * 255.0, tf.uint8)
    sum_generator = tf.summary.image('summary/generator', generator_visualisation)

    img_summary = tf.summary.merge([sum_generator, zmar])
    with tf.name_scope('error'):
        error = M_placeholder - g_z
        # We set axis = None because norm(tensor, ord=ord) is equivalent to norm(reshape(tensor, [-1]), ord=ord)
        error_norm = tf.norm(error, ord=2, axis=None, keep_dims=False, name='L2Norm')
        summary_error = tf.summary.scalar('error_norm', error_norm)

    with tf.name_scope('Optimizing'):
        optimizer = tf.train.AdamOptimizer(0.001).minimize(error_norm, var_list=z)

    sess.run(tf.global_variables_initializer())
    for i in range(10000):
        (_, s) = sess.run((optimizer, summary_error))
        logwriter.add_summary(s, i)
        print('step %d: Patiente un peu poto!' % i)
        img = sess.run(img_summary)
        logwriter.add_summary(img, i)

    print('Done Training')
</code></pre>

<p>This script runs, but I have checked on tensorboard, the generator that is used here does not have the trained weights and it only produces noise.</p>

<p>I think I am trying to run a session in a graph that uses another graph and its trained session. I have read thoroughly the Graphs and Session documentation on tensorflow website <a href=""https://www.tensorflow.org/versions/r1.3/programmers_guide/graphs"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r1.3/programmers_guide/graphs</a>, I have found an interesting tf.import_graph_def function :</p>

<ol>
<li><p>You can rebind tensors in the imported graph to tf.Tensor objects in the default graph by passing the optional input_map argument. For example, input_map enables you to take import a graph fragment defined in a tf.GraphDef, and statically connect tensors in the graph you are building to tf.placeholder tensors in that fragment.</p>

<ol start=""2"">
<li>You can return tf.Tensor or tf.Operation objects from the imported graph by passing their names in the return_elements list.</li>
</ol></li>
</ol>

<p>But I don't know how to use this function, no example is given, and also I only found those two links that may help me : 
<a href=""https://github.com/tensorflow/tensorflow/issues/7508"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/7508</a>
<a href=""https://stackoverflow.com/questions/41796965/tensorflow-how-to-use-a-trained-model-in-a-application"">Tensorflow: How to use a trained model in a application?</a></p>

<p>It would be really nice to have your help on this topic. This should be straightforward for someone who has already used the tf.import_graph_def function... What I really need is to get the trained generator to apply it to a new variable z which is to be trained in another session.</p>

<p>Thanks</p>
",1
46131410,How to examine the feature weights of a Tensorflow LinearClassifier?,"<p>I am trying to understand the <a href=""https://www.tensorflow.org/tutorials/linear"" rel=""nofollow noreferrer""><strong>Large-scale Linear Models with TensorFlow</strong></a> documentation. The docs motivate these models as follows:</p>

<blockquote>
  <p>Linear model can be interpreted and debugged more easily than neural
  nets. <strong>You can examine the weights assigned to each feature</strong> to figure
  out what's having the biggest impact on a prediction.</p>
</blockquote>

<p>So I ran the extended code example from the accompanying <a href=""https://www.tensorflow.org/tutorials/wide"" rel=""nofollow noreferrer""><strong>TensorFlow Linear Model Tutorial</strong></a>. In particular, I ran the <a href=""https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/learn/wide_n_deep_tutorial.py"" rel=""nofollow noreferrer""><strong>example code from GitHub</strong></a> with the <code>model-type</code> flag set to <code>wide</code>. This correctly ran and produced <code>accuracy: 0.833733</code>, similar to the <code>accuracy: 0.83557522</code> on the Tensorflow web page.</p>

<p>The example uses a <code>tf.estimator.LinearClassifier</code> to train the weights. However, in contrast to the quoted motivation of being able to examine the weights, I can't find any function to actually extract the trained weights in the <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/LinearClassifier"" rel=""nofollow noreferrer""><strong>LinearClassifier documentation</strong></a>.</p>

<p><strong>Question</strong>: how do I access the trained weights for the various feature columns in a <code>tf.estimator.LinearClassifier</code>? I'd prefer to be able to extract all the weights in a NumPy array.</p>

<p><strong>Note</strong>: I am coming from an R environment where linear regression / classification models have a <code>coefs</code> method to extract learned weights. I want to be able to compare linear models in both R and TensorFlow on the same datasets. </p>
",1
46139202,Tensorflow: TypeError with numpy_input_fn,"<p>I am coding a Convolutional Neural Network to classify images in TensorFlow but there is a problem:</p>

<p>When I try to feed my NumPy array of flattened  images (3 channels with RGB values from 0 to 255) to a tf.estimator.inputs.numpy_input_fn I get the following error:</p>

<pre><code>  TypeError: Failed to convert object of type &lt;class 'dict'&gt; to Tensor. 
  Contents: {'x': &lt;tf.Tensor 'random_shuffle_queue_DequeueMany:1' shape=(8, 
  196608) dtype=uint8&gt;}. Consider casting elements to a supported type.
</code></pre>

<p>My numpy_imput_fn looks like this:</p>

<pre><code>train_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={'x': train_x},
    y=train_y,
    batch_size=8,
    num_epochs=None,
    shuffle=True)
</code></pre>

<p>In the documentation for the function it is said that x should be a dict of NumPy array:</p>

<blockquote>
  <p>x: dict of numpy array object.</p>
</blockquote>
",1
46153978,Tensorflow Slim - model trains but always predicts the same when evaluating,"<p><a href=""https://kwotsin.github.io/tech/2017/02/11/transfer-learning.html"" rel=""nofollow noreferrer"">https://kwotsin.github.io/tech/2017/02/11/transfer-learning.html</a>
I followed the above link to make a image classifier</p>

<p>Training code:</p>

<pre><code>slim = tf.contrib.slim

dataset_dir = './data'
log_dir = './log'
checkpoint_file = './inception_resnet_v2_2016_08_30.ckpt'
image_size = 299
num_classes = 21
vlabels_file = './labels.txt'
labels = open(labels_file, 'r')
labels_to_name = {}
for line in labels:
    label, string_name = line.split(':')
    string_name = string_name[:-1]
    labels_to_name[int(label)] = string_name

file_pattern = 'test_%s_*.tfrecord'

items_to_descriptions = {
    'image': 'A 3-channel RGB coloured product image',
    'label': 'A label that from 20 labels'
}

num_epochs = 10
batch_size = 16
initial_learning_rate = 0.001
learning_rate_decay_factor = 0.7
num_epochs_before_decay = 4

def get_split(split_name, dataset_dir, file_pattern=file_pattern, file_pattern_for_counting='products'):
    if split_name not in ['train', 'validation']:
        raise ValueError(
            'The split_name %s is not recognized. Please input either train or validation as the split_name' % (
            split_name))

    file_pattern_path = os.path.join(dataset_dir, file_pattern % (split_name))

    num_samples = 0
    file_pattern_for_counting = file_pattern_for_counting + '_' + split_name
    tfrecords_to_count = [os.path.join(dataset_dir, file) for file in os.listdir(dataset_dir) if
                          file.startswith(file_pattern_for_counting)]
    for tfrecord_file in tfrecords_to_count:
        for record in tf.python_io.tf_record_iterator(tfrecord_file):
            num_samples += 1

    test = num_samples

    reader = tf.TFRecordReader

    keys_to_features = {
        'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),
        'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),
        'image/class/label': tf.FixedLenFeature(
            [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),
    }

    items_to_handlers = {
        'image': slim.tfexample_decoder.Image(),
        'label': slim.tfexample_decoder.Tensor('image/class/label'),
    }

    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)

    labels_to_name_dict = labels_to_name

    dataset = slim.dataset.Dataset(
        data_sources=file_pattern_path,
        decoder=decoder,
        reader=reader,
        num_readers=4,
        num_samples=num_samples,
        num_classes=num_classes,
        labels_to_name=labels_to_name_dict,
        items_to_descriptions=items_to_descriptions)

    return dataset

def load_batch(dataset, batch_size, height=image_size, width=image_size, is_training=True):
    '''
    Loads a batch for training.

    INPUTS:
    - dataset(Dataset): a Dataset class object that is created from the get_split function
    - batch_size(int): determines how big of a batch to train
    - height(int): the height of the image to resize to during preprocessing
    - width(int): the width of the image to resize to during preprocessing
    - is_training(bool): to determine whether to perform a training or evaluation preprocessing

    OUTPUTS:
    - images(Tensor): a Tensor of the shape (batch_size, height, width, channels) that contain one batch of images
    - labels(Tensor): the batch's labels with the shape (batch_size,) (requires one_hot_encoding).

    '''
    # First create the data_provider object
    data_provider = slim.dataset_data_provider.DatasetDataProvider(
        dataset,
        common_queue_capacity=24 + 3 * batch_size,
        common_queue_min=24)

    # Obtain the raw image using the get method
    raw_image, label = data_provider.get(['image', 'label'])

    # Perform the correct preprocessing for this image depending if it is training or evaluating
    image = inception_preprocessing.preprocess_image(raw_image, height, width, is_training)

    # As for the raw images, we just do a simple reshape to batch it up
    raw_image = tf.expand_dims(raw_image, 0)
    raw_image = tf.image.resize_nearest_neighbor(raw_image, [height, width])
    raw_image = tf.squeeze(raw_image)

    # Batch up the image by enqueing the tensors internally in a FIFO queue and dequeueing many elements with tf.train.batch.
    images, raw_images, labels = tf.train.batch(
        [image, raw_image, label],
        batch_size=batch_size,
        num_threads=4,
        capacity=4 * batch_size,
        allow_smaller_final_batch=True)

    return images, raw_images, labels


def run():
    # Create the log directory here. Must be done here otherwise import will activate this unneededly.
    if not os.path.exists(log_dir):
        os.mkdir(log_dir)

    # ======================= TRAINING PROCESS =========================
    # Now we start to construct the graph and build our model
    with tf.Graph().as_default() as graph:
        tf.logging.set_verbosity(tf.logging.INFO)  # Set the verbosity to INFO level

        # First create the dataset and load one batch
        dataset = get_split('train', dataset_dir, file_pattern=file_pattern)
        images, _, labels = load_batch(dataset, batch_size=batch_size)

        # Know the number steps to take before decaying the learning rate and batches per epoch
        num_batches_per_epoch = int(dataset.num_samples / batch_size)
        num_steps_per_epoch = num_batches_per_epoch  # Because one step is one batch processed
        decay_steps = int(num_epochs_before_decay * num_steps_per_epoch)

        # Create the model inference
        with slim.arg_scope(inception_resnet_v2_arg_scope()):
            logits, end_points = inception_resnet_v2(images, num_classes=dataset.num_classes, is_training=True)

        # Define the scopes that you want to exclude for restoration
        exclude = ['InceptionResnetV2/Logits', 'InceptionResnetV2/AuxLogits']
        variables_to_restore = slim.get_variables_to_restore(exclude=exclude)

        # Perform one-hot-encoding of the labels (Try one-hot-encoding within the load_batch function!)
        one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)

        # Performs the equivalent to tf.nn.sparse_softmax_cross_entropy_with_logits but enhanced with checks
        loss = tf.losses.softmax_cross_entropy(onehot_labels=one_hot_labels, logits=logits)
        total_loss = tf.losses.get_total_loss()  # obtain the regularization losses as well

        # Create the global step for monitoring the learning_rate and training.
        global_step = get_or_create_global_step()

        # Define your exponentially decaying learning rate
        lr = tf.train.exponential_decay(
            learning_rate=initial_learning_rate,
            global_step=global_step,
            decay_steps=decay_steps,
            decay_rate=learning_rate_decay_factor,
            staircase=True)

        # Now we can define the optimizer that takes on the learning rate
        optimizer = tf.train.AdamOptimizer(learning_rate=lr)

        # Create the train_op.
        train_op = slim.learning.create_train_op(total_loss, optimizer)

        # State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.
        predictions = tf.argmax(end_points['Predictions'], 1)
        probabilities = end_points['Predictions']
        accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)
        metrics_op = tf.group(accuracy_update, probabilities)

        # Now finally create all the summaries you need to monitor and group them into one summary op.
        tf.summary.scalar('losses/Total_Loss', total_loss)
        tf.summary.scalar('accuracy', accuracy)
        tf.summary.scalar('learning_rate', lr)
        my_summary_op = tf.summary.merge_all()

        # Now we need to create a training step function that runs both the train_op, metrics_op and updates the global_step concurrently.
        def train_step(sess, train_op, global_step):
            '''
            Simply runs a session for the three arguments provided and gives a logging on the time elapsed for each global step
            '''
            # Check the time for each sess run
            start_time = time.time()
            total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])
            time_elapsed = time.time() - start_time

            # Run the logging to print some results
            logging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)

            return total_loss, global_step_count

        # Now we create a saver function that actually restores the variables from a checkpoint file in a sess
        saver = tf.train.Saver(variables_to_restore)

        def restore_fn(sess):
            return saver.restore(sess, checkpoint_file)

        # Define your supervisor for running a managed session. Do not run the summary_op automatically or else it will consume too much memory
        sv = tf.train.Supervisor(logdir=log_dir, summary_op=None, init_fn=restore_fn)

        # Run the managed session
        with sv.managed_session() as sess:
            for step in xrange(num_steps_per_epoch * num_epochs):
                # At the start of every epoch, show the vital information:
                if step % num_batches_per_epoch == 0:
                    logging.info('Epoch %s/%s', step / num_batches_per_epoch + 1, num_epochs)
                    learning_rate_value, accuracy_value = sess.run([lr, accuracy])
                    logging.info('Current Learning Rate: %s', learning_rate_value)
                    logging.info('Current Streaming Accuracy: %s', accuracy_value)

                    # optionally, print your logits and predictions for a sanity check that things are going fine.
                    logits_value, probabilities_value, predictions_value, labels_value = sess.run(
                        [logits, probabilities, predictions, labels])
                    print 'logits: \n', logits_value
                    print 'Probabilities: \n', probabilities_value
                    print 'predictions: \n', predictions_value
                    print 'Labels:\n:', labels_value

                # Log the summaries every 10 step.
                if step % 10 == 0:
                    loss, _ = train_step(sess, train_op, sv.global_step)
                    summaries = sess.run(my_summary_op)
                    sv.summary_computed(sess, summaries)

                # If not, simply run the training step
                else:
                    loss, _ = train_step(sess, train_op, sv.global_step)

            # We log the final training loss and accuracy
            logging.info('Final Loss: %s', loss)
            logging.info('Final Accuracy: %s', sess.run(accuracy))

            # Once all the training has been done, save the log files and checkpoint model
            logging.info('Finished training! Saving model to disk now.')
            sv.saver.save(sess, sv.save_path, global_step=sv.global_step)
</code></pre>

<p>This code seems to work an I have ran training on some sample data and Im getting 94% accuracy</p>

<p>Evaluation code:</p>

<pre><code>log_dir = './log'
log_eval = './log_eval_test'
dataset_dir = './data'
batch_size = 10
num_epochs = 1

checkpoint_file = tf.train.latest_checkpoint('./')


def run():
    if not os.path.exists(log_eval):
        os.mkdir(log_eval)
    with tf.Graph().as_default() as graph:
        tf.logging.set_verbosity(tf.logging.INFO)
        dataset = get_split('train', dataset_dir)
        images, raw_images, labels = load_batch(dataset, batch_size=batch_size, is_training=False)

        num_batches_per_epoch = dataset.num_samples / batch_size
        num_steps_per_epoch = num_batches_per_epoch

        with slim.arg_scope(inception_resnet_v2_arg_scope()):
            logits, end_points = inception_resnet_v2(images, num_classes=dataset.num_classes, is_training=False)

        variables_to_restore = slim.get_variables_to_restore()
        saver = tf.train.Saver(variables_to_restore)

        def restore_fn(sess):
            return saver.restore(sess, checkpoint_file)

        predictions = tf.argmax(end_points['Predictions'], 1)
        accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)
        metrics_op = tf.group(accuracy_update)

        global_step = get_or_create_global_step()
        global_step_op = tf.assign(global_step, global_step + 1)

        def eval_step(sess, metrics_op, global_step):
            '''
            Simply takes in a session, runs the metrics op and some logging information.
            '''
            start_time = time.time()
            _, global_step_count, accuracy_value = sess.run([metrics_op, global_step_op, accuracy])
            time_elapsed = time.time() - start_time

            logging.info('Global Step %s: Streaming Accuracy: %.4f (%.2f sec/step)', global_step_count, accuracy_value,
                         time_elapsed)

            return accuracy_value

        tf.summary.scalar('Validation_Accuracy', accuracy)
        my_summary_op = tf.summary.merge_all()

        sv = tf.train.Supervisor(logdir=log_eval, summary_op=None, saver=None, init_fn=restore_fn)

        with sv.managed_session() as sess:
            for step in xrange(num_steps_per_epoch * num_epochs):
                sess.run(sv.global_step)
                if step % num_batches_per_epoch == 0:
                    logging.info('Epoch: %s/%s', step / num_batches_per_epoch + 1, num_epochs)
                    logging.info('Current Streaming Accuracy: %.4f', sess.run(accuracy))

                if step % 10 == 0:
                    eval_step(sess, metrics_op=metrics_op, global_step=sv.global_step)
                    summaries = sess.run(my_summary_op)
                    sv.summary_computed(sess, summaries)


                else:
                    eval_step(sess, metrics_op=metrics_op, global_step=sv.global_step)

            logging.info('Final Streaming Accuracy: %.4f', sess.run(accuracy))

            raw_images, labels, predictions = sess.run([raw_images, labels, predictions])
            for i in range(10):
                image, label, prediction = raw_images[i], labels[i], predictions[i]
                prediction_name, label_name = dataset.labels_to_name[prediction], dataset.labels_to_name[label]
                text = 'Prediction: %s \n Ground Truth: %s' % (prediction_name, label_name)
                img_plot = plt.imshow(image)

                plt.title(text)
                img_plot.axes.get_yaxis().set_ticks([])
                img_plot.axes.get_xaxis().set_ticks([])
                plt.show()

            logging.info(
                'Model evaluation has completed! Visit TensorBoard for more information regarding your evaluation.')
</code></pre>

<p>So after training the model and getting 94% accuracy i tried to evaluate the model. On evaluation I get 0-1% accuracy the whole time. I investigated this only to find that it is predicting the same class every time</p>

<pre><code>labels: [7, 11, 5, 1, 20, 0, 18, 1, 0, 7]
predictions: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]
</code></pre>

<p>Can anyone help in where i may be going wrong?</p>

<p>EDIT:</p>

<p>TensorBoard accuracy and loss form training</p>

<p><a href=""https://i.stack.imgur.com/NLiwC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NLiwC.png"" alt=""enter image description here""></a>
<a href=""https://i.stack.imgur.com/QdX6d.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QdX6d.png"" alt=""enter image description here""></a></p>

<p>TensorBoard accuracy from evaluation</p>

<p><a href=""https://i.stack.imgur.com/TNE5B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TNE5B.png"" alt=""enter image description here""></a></p>

<p>EDIT:</p>

<p>Ive still not been able to solve this issues. I thought there might be a problem with how I am restoring the graph in the eval script so I tried using this to restore the model instead</p>

<pre><code>saver = tf.train.import_meta_graph('/log/model.ckpt.meta')

def restore_fn(sess):
    return saver.restore(sess, checkpoint_file)
</code></pre>

<p>instead of</p>

<pre><code>variables_to_restore = slim.get_variables_to_restore()
    saver = tf.train.Saver(variables_to_restore)

def restore_fn(sess):
    return saver.restore(sess, checkpoint_file)
</code></pre>

<p>and just just takes a very long time to start and finally errors. I then tried using V1 of the writer in the saver (<code>saver = tf.train.Saver(variables_to_restore, write_version=saver_pb2.SaveDef.V1)</code>) and retrained and was unable to load this checkpoint at all as it said variables was missing.</p>

<p>I also attempted to run my eval script with the same data it trained on just to see if this may give different results yet I get the same. </p>

<p>Finally I re-cloned the repo from the url and ran a train using the same dataset in the tutorial and I get 0-3% accuracy when I evaluate even after getting it to 84% whilst training. Also my checkpoints must have the correct information as when I restart training the accuracy continues from where it left of. It feels like i'm not doing something correctly when I restore the model. Would really appreciate any suggestions on this as im at a dead end currently :( </p>
",0
46221420,How to use TensorFlow Dataset API in combination with dense layers,"<p>I am trying out the Dataset API for my input pipeline shown in the <a href=""https://www.tensorflow.org/versions/r1.3/programmers_guide/datasets#using_high-level_apis"" rel=""nofollow noreferrer"">TensorFlow documentation</a> and use almost the same code:</p>

<pre><code>tr_data = Dataset.from_tensor_slices((train_images, train_labels))
tr_data = tr_data.map(input_parser, NUM_CORES, output_buffer_size=2000)
tr_data = tr_data.batch(BATCH_SIZE)
tr_data = tr_data.repeat(EPOCHS)

iterator = dataset.make_one_shot_iterator()
next_example, next_label = iterator.get_next()

# Script throws error here
loss = model_function(next_example, next_label)

with tf.Session(...) as sess:
    sess.run(tf.global_variables_initializer())

     while True:
        try:
            train_loss = sess.run(loss)
        except tf.errors.OutOfRangeError:
            print(""End of training dataset."")
            break
</code></pre>

<p>This should be faster since it avoids using the slow feed_dicts. But I can't make it work with my model, which is a simplified LeNet architecture. The <strong>problem</strong> is the <code>tf.layers.dense</code> in my <code>model_function()</code> which expects an known input shape (I guess because it has to know the number of weights beforehand). But <code>next_example</code> and <code>next_label</code> only get their shape by running them in the session. Before evaluating them their shape is just undefined <code>?</code></p>

<p>Declaring the <code>model_function()</code> throws this error:</p>

<blockquote>
  <p>ValueError: The last dimension of the inputs to <code>Dense</code> should be
  defined. Found <code>None</code>.</p>
</blockquote>

<p>Right now, I don't know if I am using this Dataset API in the intended way or if there is a workaround.</p>

<p>Thanks in advance!</p>

<p><strong>Edit 1:</strong> 
Below is my model and it throws the error at the first dense layer</p>

<pre><code>def conv_relu(input, kernel_shape):
    # Create variable named ""weights"".
    weights = tf.get_variable(""weights"", kernel_shape,
        initializer=tf.random_normal_initializer())
    # Create variable named ""biases"".
    biases = tf.get_variable(""biases"", kernel_shape[3],
        initializer=tf.constant_initializer(0.0))
    conv = tf.nn.conv2d(input, weights,
        strides=[1, 1, 1, 1], padding='VALID')
    return tf.nn.relu(conv + biases)

def fully(input, output_dim):
    assert len(input.get_shape())==2, 'Wrong input shape, need flattened tensor as input'
    input_dim = input.get_shape()[1]

    weight = tf.get_variable(""weight"", [input_dim, output_dim],
        initializer=tf.random_normal_initializer())
    bias = tf.get_variable('bias', [output_dim],
        initializer=tf.random_normal_initializer())

    fully = tf.nn.bias_add(tf.matmul(input, weight), bias)
    return fully


def simple_model(x):

    with tf.variable_scope('conv1'):
        conv1 = conv_relu(x, [3,3,1,10])
        conv1 = tf.nn.max_pool(conv1,[1,2,2,1],[1,2,2,1],'SAME')

    with tf.variable_scope('conv2'):
        conv2 = conv_relu(conv1, [3,3,10,10])
        conv2 = tf.nn.max_pool(conv2,[1,2,2,1],[1,2,2,1],'SAME')

    with tf.variable_scope('conv3'):
        conv3 = conv_relu(conv2, [3,3,10,10])
        conv3 = tf.nn.max_pool(conv3,[1,2,2,1],[1,2,2,1],'SAME')

    flat = tf.contrib.layers.flatten(conv3)
    with tf.variable_scope('fully1'):
        fully1 = tf.layers.dense(flat, 1000)
        fully1 = tf.nn.relu(fully1)

    with tf.variable_scope('fully2'):
        fully2 = tf.layers.dense(fully1, 100)
        fully2 = tf.nn.relu(fully2)

    with tf.variable_scope('output'):
        output = tf.layers.dense(fully2, 4)
        fully1 = tf.nn.relu(output)


    return output
</code></pre>

<p><strong>Edit 2:</strong>  </p>

<p>Here you see the print of the tensors. Notice that next_example does not have a shape</p>

<blockquote>
  <p>next_example: Tensor(""IteratorGetNext:0"", dtype=float32)<br>
  next_label: Tensor(""IteratorGetNext:1"", shape=(?, 4), dtype=float32)</p>
</blockquote>
",1
46298583,Tensorflow embeddings,"<p>I know what embeddings are and how they are trained. Precisely, while referring to the tensorflow's documentation, I came across two different articles. I wish to know what exactly is the difference between them.</p>

<p>link 1: <a href=""https://www.tensorflow.org/tutorials/word2vec#building_the_graph"" rel=""nofollow noreferrer"">Tensorflow | Vector Representations of words</a></p>

<p>In the first tutorial, they have explicitly trained embeddings on a specific dataset. There is a distinct session run to train those embeddings. I can then later on save the learnt embeddings as a numpy object and use the </p>

<p><code>tf.nn.embedding_lookup()</code> function while training an LSTM network.</p>

<p>link 2: <a href=""https://www.tensorflow.org/programmers_guide/embedding"" rel=""nofollow noreferrer"">Tensorflow | Embeddings</a></p>

<p>In this second article however, I couldn't understand what is happening. </p>

<pre><code>word_embeddings = tf.get_variable(“word_embeddings”,
[vocabulary_size, embedding_size])
embedded_word_ids = tf.gather(word_embeddings, word_ids)
</code></pre>

<p>This is given under the training embeddings sections. My doubt is: does the gather function train the embeddings automatically? I am not sure since this op ran very fast on my pc.</p>

<p>Generally: What is the right way to convert words into vectors (link1 or link2) in tensorflow for training a seq2seq model? Also, how to train the embeddings for a seq2seq dataset, since the data is in the form of separate sequences for my task unlike (a continuous sequence of words refer: link 1 dataset) </p>
",0
46338074,Tensorflow seq2seq,"<p>The api doc of tensorflow's tf.contrib.legacy_seq2seq.basic_rnn_seq2seq says:</p>

<pre><code>(outputs, states) = basic_rnn_seq2seq(
    encoder_inputs,
    decoder_inputs,
    cell,
    dtype=tf.float32,
    scope=None
)
</code></pre>

<p>where, <br>
encoder_inputs: A list of 2D Tensors [batch_size x input_size]. <br>
decoder_inputs: A list of 2D Tensors [batch_size x input_size].</p>

<p>Question 1: Why are the sequence sizes (input_size) same for both the encoder_inputs and decoder_inputs. In my case, there are seq - to - seq mappings for unequal lengthts. For instance, ""What are you doing"" (length 4) -> ""wie gehts (length 2)"". Is it necessary to pad and make the sequences of equal lengths while using this tensorflow module?</p>

<p>Question 2: If I use the </p>

<pre><code>cell = tf.nn.rnn_cell.LSTMCell(num_units = 128)
</code></pre>

<p>The outputs come out to be of the size (batch_size x 128 [num of hidden units in lstm]) which really confuses me. Shouldn't it be equal to (batch_size x output_size)? What am I missing here? I am really confused how the decoder works here.</p>
",0
46342025,"In TensorFlow, why a m*n matrix can add n * 1 matrix?","<p>I am very new to python and TensorFlow, recent days I met a problem when I study ""MNIST For ML Beginners""(<a href=""https://www.tensorflow.org/get_started/mnist/beginners"" rel=""nofollow noreferrer"">https://www.tensorflow.org/get_started/mnist/beginners</a>). </p>

<p>In this tutorial, we use <code>y = tf.nn.softmax(tf.matmul(X, W) + b)</code> to get our outputs. </p>

<p>My question is, for example, X is a [100,784] matrix, and W is [784,10] matrix, b is a [10] tensor (like a [10,1] matrix?), after we called tf.matmul(X, W) we will get a [100,10] matrix. here is my question, how can a [100,10] matrix add a b[10] tensor here? It does not make any sense to me. </p>

<p>I know why there are biases and I know why the biases need to be added. But I just do not know how  the ""+"" operator worked in this problem.</p>
",0
46372554,When feeding a dictionary to a tensorflow function I get Why do I get TypeError: unhashable type: 'numpy.ndarray',"<p>I am working on a Tensor Flow Coursera Course and I dont understand why I am getting a type mismatch. </p>

<p>This is the function I am defining:</p>

<pre><code>def one_hot_matrix(labels, C):
    """"""
    Creates a matrix where the i-th row corresponds to the ith class number and the jth column
                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) 
                     will be 1. 

Arguments:
labels -- vector containing the labels 
C -- number of classes, the depth of the one hot dimension

Returns: 
one_hot -- one hot matrix
""""""

### START CODE HERE ###

# Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)
C = tf.constant(C, name=""C"")
#labels =tf.placeholder(labels, name=""labels"")

# Use tf.one_hot, be careful with the axis (approx. 1 line)
one_hot_matrix = tf.one_hot(indices=labels, depth=C, axis=0)

# Create the session (approx. 1 line)
sess = tf.Session()

# Run the session (approx. 1 line)
one_hot = sess.run(one_hot_matrix, feed_dict={labels:labels, C:C})

# Close the session (approx. 1 line). See method 1 above.
sess.close()

### END CODE HERE ###

return one_hot
</code></pre>

<p>And when running this: </p>

<pre><code>labels = np.array([1,2,3,0,2,1])
one_hot = one_hot_matrix(labels, C = 4)
print (""one_hot = "" + str(one_hot))
</code></pre>

<p>I get this type error:</p>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-113-2b9d0290645f&gt; in &lt;module&gt;()
      1 labels = np.array([1,2,3,0,2,1])
----&gt; 2 one_hot = one_hot_matrix(labels, C = 4)
      3 print (""one_hot = "" + str(one_hot))

&lt;ipython-input-112-f9f17c86d0ba&gt; in one_hot_matrix(labels, C)
     28 
     29     # Run the session (approx. 1 line)
---&gt; 30     one_hot = sess.run(one_hot_matrix, feed_dict={labels:labels, C:C})
     31 
     32     # Close the session (approx. 1 line). See method 1 above.

TypeError: unhashable type: 'numpy.ndarray'ter code here
</code></pre>

<p>I checked the Tensorflow documentation for tf.one_hot and there shouldn't be a problem with np.arrays.</p>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/one_hot"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/one_hot</a></p>
",1
46381790,How does TensorFlow handle none shape?,"<p>I'm trying to implement a simple computational graph framework and test it with simple neural network, mainly by learning from TensorFlow. Now I would want to be clear how does TensorFlow handle none shape tensors.</p>

<p>In <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/multilayer_perceptron.py"" rel=""nofollow noreferrer"">this example</a>, <code>X</code> has shape <code>[None, n_input]</code>, <code>weights['h1']</code> has shape <code>[n_input, n_hidden_1]</code>, and <code>biases['b1']</code> has shape <code>[n_hidden_1]</code>. When it tries to do this: <code>layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])</code>, <code>tf.matmul(x, weights['h1'])</code> should have shape <code>[None, n_hidden_1]</code>, and how exactly does TensorFlow add it with <code>biases['b1']</code>? Based on the <a href=""https://www.tensorflow.org/api_docs/python/tf/add"" rel=""nofollow noreferrer"">documentation</a>, <code>tf.add</code> only works when the 2 operands have the same shape. If we run with a batch of size 10, <code>tf.matmul(x, weights['h1'])</code> will have shape <code>[10, n_hidden_1]</code>, and it shouldn't be able to be added with <code>biases['b1']</code>.</p>
",1
46386211,When would I want to set a stride in the batch or channel dimension for TensorFlow convolution?,"<p>Tensor flow implements a basic convolution operation with <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/conv2d"" rel=""nofollow noreferrer"" title=""tf.nn.conv2d"">tf.nn.conv2d</a>.</p>
<p>I am specifically interested in the &quot;strides&quot; parameter, which lets you set the stride of the convolution filter -- how far across the image you shift the filter each time.</p>
<p>The example given in <a href=""https://www.tensorflow.org/get_started/mnist/pros"" rel=""nofollow noreferrer"">one of the early tutorials</a>, with an image stride of 1 in each direction, is</p>
<pre><code>def conv2d(x, W):
  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')
</code></pre>
<p>The strides array is explained more in the linked docs:</p>
<blockquote>
<p>In detail, with the default NHWC format...</p>
<p>Must have strides[0] = strides[3] = 1. For the most common case of the same horizontal and vertices strides, strides = [1, stride, stride, 1].</p>
</blockquote>
<p>Note the order of &quot;strides&quot; matches the order of inputs: <code>[batch, height, width, channels]</code> in the NHWC format.</p>
<p>Obviously having a stride of not 1 for <code>batch</code> and <code>channels</code> wouldn't make sense, right? (your filter should always go across every batch and every channel)</p>
<p>But why is it even an option to put something other than 1 in <code>strides[0]</code> and <code>strides[3]</code>, then? (where it being an &quot;option&quot; is in regards to the fact that you could put something other than 1 in the python array you pass in, disregarding the documentation quote above)</p>
<p>Is there a situation where I would have a non-one stride for the <code>batch</code> or <code>channels</code> dimension, e.g.</p>
<pre><code>tf.nn.conv2d(x, W, strides=[2, 1, 1, 2], padding='SAME')
</code></pre>
<p>If so, what would that example even mean in terms of the convolution operation?</p>
",0
46418686,tf.nn.dynamic_rnn shape error in seq2seq,"<p>I am attempting to write my own basic seq2seq classifier. Im doing this by using <code>tf.nn.dynamic_rnn</code> and the code is shown below. However, there seems to be a problem with the shape of the tensor I'm sending to <code>tf.nn.dynamic_rnn</code>. The reason I'm doing this is because tensorflow's documentation when it comes to seq2seq is very much all over the place.</p>

<p>Running </p>

<pre><code>import numpy as np
source_batch = np.random.randint(x_letters, size=[batch_size, x_seq_length])
target_batch = np.random.randint(y_letters, size=[batch_size, y_seq_length+1])

sess.run(tf.global_variables_initializer())
loss = sess.run([loss],
            feed_dict = {inputs: source_batch, 
                         outputs: target_batch[:, :-1], 
                         targets: target_batch[:, 1:]})
</code></pre>

<p>gives me the error: <code>ValueError: Cannot feed value of shape (128, 10) for Tensor 'decoding/rnn/transpose:0', which has shape '(128, 10, 32)'</code>.</p>

<p><strong>The graph</strong> is shown below:</p>

<pre><code>import tensorflow as tf

x_seq_length = 29
y_seq_length = 10

x_letters = 60
y_letters = 13

epochs = 2
batch_size = 128
nodes = 32
embed_size = 10

####################
# Tensorflow Graph
####################
tf.reset_default_graph()
sess = tf.InteractiveSession()

inputs = tf.placeholder(tf.int32, (batch_size, x_seq_length), 'inputs')
outputs = tf.placeholder(tf.int32, (batch_size, y_seq_length), 'output')
targets = tf.placeholder(tf.int32, (batch_size, y_seq_length), 'targets')

input_embedding = tf.Variable(tf.random_uniform((x_letters, embed_size), -1, 1), name='enc_embedding')
output_embedding = tf.Variable(tf.random_uniform((y_letters, embed_size), -1, 1), name='dec_embedding')

date_input_embed = tf.nn.embedding_lookup(input_embedding, inputs)
date_output_embed = tf.nn.embedding_lookup(output_embedding, outputs)

with tf.variable_scope(""encoding"") as encoding_scope:
    lstm_enc = tf.contrib.rnn.BasicLSTMCell(nodes)
    _, last_state = tf.nn.dynamic_rnn(lstm_enc, dtype=tf.float32,inputs=date_input_embed)

with tf.variable_scope(""decoding"") as decoding_scope:
    lstm_dec = tf.contrib.rnn.BasicLSTMCell(nodes)
    outputs, _ = tf.nn.dynamic_rnn(lstm_dec, inputs=date_output_embed, initial_state=last_state)

logits = tf.contrib.layers.fully_connected(outputs, num_outputs=y_letters, activation_fn=None) 

with tf.name_scope(""optimization""):
    loss = tf.contrib.seq2seq.sequence_loss(logits, targets, tf.ones([batch_size, y_seq_length]))
    optimizer = tf.train.AdamOptimizer().minimize(loss)
</code></pre>
",1
46424912,tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq output projection,"<p>The official documentation for <code>tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq</code> has the following explanation for the output_projection argument:
<br></p>

<p><code>output_projection</code>: None or a pair (W, B) of output projection weights and biases; W has shape [output_size x num_decoder_symbols] and B has shape [num_decoder_symbols]; if provided and feed_previous=True, each fed previous output will first be multiplied by W and added B.</p>

<p>I don't understand why the B argument should have the size of <code>[num_decoder_symbols]</code>? Since the output is first multiplied by W and then the biases are added, Shouldn't it be <code>[output_size]</code>? </p>
",1
46444018,"Meaning of buffer_size in Dataset.map , Dataset.prefetch and Dataset.shuffle","<p>As per TensorFlow <a href=""https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset#prefetch"" rel=""noreferrer"">documentation</a> , the <code>prefetch</code> and <code>map</code> methods of <code>tf.contrib.data.Dataset</code> class, both have a parameter called <code>buffer_size</code>.</p>
<p>For <code>prefetch</code> method, the parameter is known as <code>buffer_size</code> and according to documentation :</p>
<blockquote>
<p>buffer_size: A tf.int64 scalar tf.Tensor, representing the maximum
number elements that will be buffered when prefetching.</p>
</blockquote>
<p>For the <code>map</code> method, the parameter is known as <code>output_buffer_size</code> and according to documentation :</p>
<blockquote>
<p>output_buffer_size: (Optional.) A tf.int64 scalar tf.Tensor,
representing the maximum number of processed elements that will be
buffered.</p>
</blockquote>
<p>Similarly for the <code>shuffle</code> method, the same quantity appears and according to documentation :</p>
<blockquote>
<p>buffer_size: A tf.int64 scalar tf.Tensor, representing the number of
elements from this dataset from which the new dataset will sample.</p>
</blockquote>
<p>What is the relation between these parameters ?</p>
<p>Suppose I create a<code>Dataset</code> object as follows :</p>
<pre><code> tr_data = TFRecordDataset(trainfilenames)
    tr_data = tr_data.map(providefortraining, output_buffer_size=10 * trainbatchsize, num_parallel_calls\
=5)
    tr_data = tr_data.shuffle(buffer_size= 100 * trainbatchsize)
    tr_data = tr_data.prefetch(buffer_size = 10 * trainbatchsize)
    tr_data = tr_data.batch(trainbatchsize)
</code></pre>
<p>What role is being played by the <code>buffer</code> parameters in the above snippet ?</p>
",1
46455648,Tensorflow seq2seq Decoder problems?,"<p>I try to write a seq2seq decoder with the tensorflow tf.contrib.seq2seq package. 
I am wondering if my code is correct and if there is better way to rewrite it. The documentation is not easy to read. </p>

<p>Or my question can be: how can I easily debug this kind of code? How can I inspect some intermediate results in tensorflow?</p>

<pre><code>class Decoder:
    def __init__(self, embedding, hidden_size, num_layers=1, max_length=15):
        self.embedding = embedding
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.cell = tf.nn.rnn_cell.GRUCell(hidden_size)  
        self.linear = tf.Variable(tf.random_normal(shape=(self.hidden_size, cn_total_words))*0.1)


    def __call__(self, inputs, state, encoder_outputs, encoder_state, decoder_length, mode=""train""):


        with tf.variable_scope(""decoder"") as scope:

            inputs = tf.nn.embedding_lookup(self.embedding, inputs)
            encoder_state = tf.tile(tf.expand_dims(encoder_state, 1), (1, tf.shape(inputs)[1], 1))

            attention_mechanism = tf.contrib.seq2seq.LuongAttention(self.hidden_size, encoder_outputs)
            attn_cell = tf.contrib.seq2seq.AttentionWrapper(self.cell, attention_mechanism, self.hidden_size)
            if mode == ""train"":
                helper = tf.contrib.seq2seq.TrainingHelper(inputs=inputs, sequence_length=decoder_length)
            elif mode == ""infer"":
                helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding=self.embedding, 
                                                start_tokens=tf.tile([en_dict[""BOS""]], [tf.shape(inputs)[0]]), end_token=en_dict[""EOS""])

            decoder = tf.contrib.seq2seq.BasicDecoder(cell=attn_cell, helper=helper, 
                                                      initial_state=attn_cell.zero_state(tf.shape(inputs)[0], tf.float32))

            outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=decoder)
            outputs = tf.concat([tf.expand_dims(out, 1) for out in outputs], 1)

            outputs = tf.tensordot(outputs, self.linear, axes=[[2], [0]])
            return outputs, state
</code></pre>

<p>I got the following error when running the code</p>

<blockquote>
  <p>--------------------------------------------------------------------------- ValueError                                Traceback (most recent call
  last)
  ~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py
  in apply_op(self, op_type_name, name, **keywords)
      434                 preferred_dtype=default_dtype,
  --> 435                 as_ref=input_arg.is_ref)
      436             if input_arg.number_attr and len(</p>
  
  <p>~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py
  in internal_convert_n_to_tensor(values, dtype, name, as_ref,
  preferred_dtype)
      736             as_ref=as_ref,
  --> 737             preferred_dtype=preferred_dtype))
      738   return ret</p>
  
  <p>~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py
  in internal_convert_to_tensor(value, dtype, name, as_ref,
  preferred_dtype)
      675         if ret is None:
  --> 676           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
      677 </p>
  
  <p>~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py
  in _TensorTensorConversionFunction(t, dtype, name, as_ref)
      548         ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r""
  --> 549         % (dtype.name, t.dtype.name, str(t)))
      550   return t</p>
  
  <p>ValueError: Tensor conversion requested dtype float32 for Tensor with
  dtype int32: 'Tensor(""seq2seq-train/decoder/ExpandDims_2:0"", shape=(?,
  1, ?), dtype=int32)'</p>
  
  <p>During handling of the above exception, another exception occurred:</p>
  
  <p>TypeError                                 Traceback (most recent call
  last)  in ()
        4 emb_en = np.random.uniform(low=-0.1, high=0.1, size=(en_total_words, hidden_size))
        5 emb_cn = np.random.uniform(low=-0.1, high=0.1, size=(cn_total_words, hidden_size))
  ----> 6 model = Seq2Seq(hidden_size, num_layers, emb_en, emb_cn)
        7 sess = tf.Session()
        8 init = tf.global_variables_initializer()</p>
  
  <p> in <strong>init</strong>(self, hidden_size,
  num_layers, embed_words_en, embed_words_cn)
       81             encoder_outputs, encoder_state = self.encoder(self.encoder_inputs, self.encoder_length)
       82             decoder_length = tf.cast(tf.reduce_sum(self.decoder_mask, 1), tf.int32)
  ---> 83             decoder_outputs, decoder_state = self.decoder(self.decoder_inputs, encoder_state, encoder_outputs,
  encoder_state, decoder_length)
       84 
       85             # decoder_outputs.append(decoder_out)</p>
  
  <p> in <strong>call</strong>(self, inputs, state,
  encoder_outputs, encoder_state, decoder_length, mode)
       50 
       51             outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=decoder)
  ---> 52             outputs = tf.concat([tf.expand_dims(out, 1) for out in outputs], 1)
       53 
       54             outputs = tf.tensordot(outputs, self.linear, axes=[[2], [0]])</p>
  
  <p>~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py
  in concat(values, axis, name)    1064   return
  gen_array_ops._concat_v2(values=values,    1065<br>
  axis=axis,
  -> 1066                                   name=name)    1067     1068 </p>
  
  <p>~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py
  in _concat_v2(values, axis, name)
      491   """"""
      492   result = _op_def_lib.apply_op(""ConcatV2"", values=values, axis=axis,
  --> 493                                 name=name)
      494   return result
      495 </p>
  
  <p>~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py
  in apply_op(self, op_type_name, name, **keywords)
      461                                 (prefix, dtype.name))
      462               else:
  --> 463                 raise TypeError(""%s that don't all match."" % prefix)
      464             else:
      465               raise TypeError(""%s that are invalid."" % prefix)</p>
  
  <p>TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have
  types [float32, int32] that don't all match.</p>
</blockquote>
",1
46510363,How to read txt file in tensorflow,"<p>I am trying to read 2 different .txt files in tensorflow using <em>tf.textLineReader</em> specified <a href=""https://www.tensorflow.org/api_guides/python/reading_data"" rel=""nofollow noreferrer"">here</a> but it does not work out. </p>

<p>1st file contains tweets (text only) as a tweet per line. </p>

<p>The 2nd file contains the an associated label (20 classes) for tweets as numeric values. I previously tried to read the files using:</p>

<pre><code>with open('tweets10k.txt', 'r') as infile:
    tweets = np.array(infile.readlines())
</code></pre>

<p>But I have an error when passing <em>tweets</em> to tf. Any help
Thanks in advance</p>
",0
46513923,TensorFlow: How and why to use SavedModel,"<p>I have a few questions regarding the <code>SavedModel</code> API, whose <a href=""https://www.tensorflow.org/programmers_guide/saved_model#overview_of_saving_and_restoring_models"" rel=""noreferrer"">documentation</a> I find leaves a lot of details unexplained.</p>

<p>The first three questions are about what to pass to the arguments of the <code>add_meta_graph_and_variables()</code> method of <code>tf.saved_model.builder.SavedModelBuilder</code>, while the fourth question is about why to use the <code>SavedModel</code> API over <code>tf.train.Saver</code>.</p>

<ol>
<li><p>What is the format of the <code>signature_def_map</code> argument? Do I normally need to set this argument when saving a model?</p></li>
<li><p>Similarly, What is the format of the <code>assets_collection</code> argument?</p></li>
<li><p>Why do you save a list of tags with a metagraph as opposed to just giving it a name (i.e. attaching just one unique tag to it)? Why would I add multiple tags to a given metagraph? What if I try to load a metagrpah from a <code>pb</code> by a certain tag, but multiple metagraphs in that <code>pb</code> match that tag?</p></li>
<li><p>The documentation argues that it is recommended to use <code>SavedModel</code> to save entire models (as opposed to variables only) in self-contained files. But <code>tf.train.Saver</code> also saves the graph in addition to the variables in a <code>.meta</code> file. So what are the advantages of using <code>SavedModel</code>? The documentation says</p></li>
</ol>

<blockquote>
  <p>When you want to save and load variables, the graph, and the graph's
  metadata--basically, when you want to save or restore your model--we
  recommend using SavedModel. SavedModel is a language-neutral,
  recoverable, hermetic serialization format. SavedModel enables
  higher-level systems and tools to produce, consume, and transform
  TensorFlow models.</p>
</blockquote>

<p>but this explanation is quite abstract and doesn't really help me understand what the advantages of <code>SavedModel</code> are. What would be concrete examples where <code>SavedModel</code> (as opposed to <code>tf.train.Saver</code>) would be better to use?</p>

<p>Please note that my question is not a duplicate of <a href=""https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model"">this question</a>. I'm not asking how to save a model, I am asking very specific questions about the properties of <code>SavedModel</code>, which is only one of multiple mechanisms TensorFlow provides to save and load models. None of the answers in the linked question touch on the <code>SavedModel</code> API (which, once again, is not the same as <code>tf.train.Saver</code>).</p>
",1
46516168,Word2Vec Tutorial: Tensorflow TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x',"<p>Version of Tensorflow: 1.2.1<br>
Version of Python: 3.5<br>
Operating System: Windows 10  </p>

<p>Another poster has asked about this same problem on StackOverflow <a href=""https://stackoverflow.com/questions/43786994/in-tensorflow-tf-nn-nce-loss-get-typeerror-input-y-of-mul-op-has-type-float"">here</a>, and he appears to be using code from the same Udacity Word2Vec tutorial. So, maybe I'm dense, but the code of this example is so busy and complex that I can't tell what fixed his problem. </p>

<p>The error occurs when I call <code>tf.reduce_means</code>:</p>

<pre><code>loss = tf.reduce_mean(
    tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,
                               train_labels, num_sampled, vocabulary_size))
</code></pre>

<p>Right before the call to <code>tf.reduce_mean</code> the key variables have the following data types.</p>

<blockquote>
  <p>train_dataset.dtype<br>
  >> tf.int32<br>
  train_labels.dtype<br>
  >> tf.int32<br>
  valid_dataset.dtype<br>
  >> tf.int32<br>
  embeddings.dtype<br>
  >> tf.float32_ref<br>
  softmax_weights.dtype<br>
  >> tf.float32_ref<br>
  softmax_biases.dtype<br>
  >> tf.float32_ref<br>
  embed.dtype<br>
  >> tf.float32   </p>
</blockquote>

<p>I tried every permutation of data type in the definitions of the variables <code>train_dataset.dtype</code>, <code>train_labels.dtype</code> and <code>valid_dataset.dtype</code>: making them all <code>int64</code>, all <code>float32</code>, all <code>float64</code>, and combinations of integer and floating point. Nothing worked. I didn't try altering the data types of <code>softmax_weight</code> and <code>softmax_biases</code>, because I'm afraid that might foul up the optimization algorithm. Don't these need to be floats to support the calculus that is done during backpropagation? (Tensorflow is often a very opaque black box with documentation that verges on completely useless, so I can suspect things but never know for sure.)</p>

<p>Program Flow at Time of Error:   </p>

<p>After the call to <code>reduce_mean</code> program control transfers to <code>sampled_softmax_loss()</code> in file <code>nn_impl.py</code> which in turn calls <code>_compute_sampled_logits()</code>:</p>

<pre><code>  logits, labels = _compute_sampled_logits(
      weights=weights,
      biases=biases,
      labels=labels,
      inputs=inputs,
      num_sampled=num_sampled,
      num_classes=num_classes,
      num_true=num_true,
      sampled_values=sampled_values,
      subtract_log_q=True,
      remove_accidental_hits=remove_accidental_hits,
      partition_strategy=partition_strategy,
      name=name)
</code></pre>

<p>At this point I check the data types of the passed-in parameters and get the following:</p>

<blockquote>
  <p>weights.dtype<br>
  >> tf.float32_ref<br>
  biases.dtype<br>
  >> tf.float32_ref<br>
  labels.dtype<br>
  >> tf.float32<br>
  inputs.dtype<br>
  >> tf.int32   </p>
</blockquote>

<p>On the very next step an exception occurs, and I am thrown into the <code>StreamWrapper</code> class in file <code>ansitowin32.py</code>. Running to the end, I get the following Traceback:</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\framework\op_def_library.py in apply_op(self, op_type_name, name, **keywords)
    489                 as_ref=input_arg.is_ref,
--&gt; 490                 preferred_dtype=default_dtype)
    491           except TypeError as err:

C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\framework\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype)
    740         if ret is None:
--&gt; 741           ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    742 

C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\framework\ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
    613         ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r""
--&gt; 614         % (dtype.name, t.dtype.name, str(t)))
    615   return t

ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(""sampled_softmax_loss/Reshape_1:0"", shape=(?, 1, ?), dtype=float32, device=/device:CPU:0)'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
&lt;ipython-input-7-66d378b94a16&gt; in &lt;module&gt;()
     34     loss = tf.reduce_mean(
     35       tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,
---&gt; 36                                train_labels, num_sampled, vocabulary_size))
     37 
     38     # Optimizer.

C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\ops\nn_impl.py in sampled_softmax_loss(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, remove_accidental_hits, partition_strategy, name)
   1266       remove_accidental_hits=remove_accidental_hits,
   1267       partition_strategy=partition_strategy,
-&gt; 1268       name=name)
   1269   sampled_losses = nn_ops.softmax_cross_entropy_with_logits(labels=labels,
   1270                                                             logits=logits)

C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\ops\nn_impl.py in _compute_sampled_logits(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, subtract_log_q, remove_accidental_hits, partition_strategy, name)
   1005     row_wise_dots = math_ops.multiply(
   1006         array_ops.expand_dims(inputs, 1),
-&gt; 1007         array_ops.reshape(true_w, new_true_w_shape))
   1008     # We want the row-wise dot plus biases which yields a
   1009     # [batch_size, num_true] tensor of true_logits.

C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\ops\math_ops.py in multiply(x, y, name)
    284 
    285 def multiply(x, y, name=None):
--&gt; 286   return gen_math_ops._mul(x, y, name)
    287 
    288 

C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\ops\gen_math_ops.py in _mul(x, y, name)
   1375     A `Tensor`. Has the same type as `x`.
   1376   """"""
-&gt; 1377   result = _op_def_lib.apply_op(""Mul"", x=x, y=y, name=name)
   1378   return result
   1379 

C:\Anaconda3\envs\aind-dog\lib\site-packages\tensorflow\python\framework\op_def_library.py in apply_op(self, op_type_name, name, **keywords)
    524                   ""%s type %s of argument '%s'."" %
    525                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,
--&gt; 526                    inferred_from[input_arg.type_attr]))
    527 
    528           types = [values.dtype]

TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.
</code></pre>

<p>Here's the complete program:</p>

<pre><code># These are all the modules we'll be using later. 
# Make sure you can import them before proceeding further.

# %matplotlib inline

from __future__ import print_function
import collections
import math
import numpy as np
import os
import random
import tensorflow as tf
import zipfile
from matplotlib import pylab
from six.moves import range
from six.moves.urllib.request import urlretrieve
from sklearn.manifold import TSNE

print(""Working directory = %s\n"" % os.getcwd())

def read_data(filename):
    """"""Extract the first file enclosed in a zip file as a list of words""""""
    with zipfile.ZipFile(filename) as f:
        data = tf.compat.as_str(f.read(f.namelist()[0])).split()
    return data

filename = 'text8.zip'

words = read_data(filename)
print('Data size %d' % len(words))

vocabulary_size = 50000

def build_dataset(words):
    count = [['UNK', -1]]
    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))
    dictionary = dict()
    # Loop through the keys of the count collection dictionary
    # (apparently, zeroing out counts)
    for word, _ in count:
        dictionary[word] = len(dictionary)
    data = list()
    unk_count = 0  # count of unknown words
    for word in words:
        if word in dictionary:
            index = dictionary[word]
        else:
            index = 0  # dictionary['UNK']
            unk_count = unk_count + 1
        data.append(index)
    count[0][1] = unk_count
    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))
    return data, count, dictionary, reverse_dictionary


data, count, dictionary, reverse_dictionary = build_dataset(words)
print('Most common words (+UNK)', count[:5])
print('Sample data', data[:10])
del words  # Hint to reduce memory.

data_index = 0

def generate_batch(batch_size, num_skips, skip_window):
    global data_index
    assert batch_size % num_skips == 0
    assert num_skips &lt;= 2 * skip_window
    batch = np.ndarray(shape=(batch_size), dtype=np.int32)
    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)
    span = 2 * skip_window + 1 # [ skip_window target skip_window ]
    buffer = collections.deque(maxlen=span)
    for _ in range(span):
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    for i in range(batch_size // num_skips):
        target = skip_window  # target label at the center of the buffer
        targets_to_avoid = [ skip_window ]
        for j in range(num_skips):
            while target in targets_to_avoid:
                target = random.randint(0, span - 1)
            targets_to_avoid.append(target)
            batch[i * num_skips + j] = buffer[skip_window]
            labels[i * num_skips + j, 0] = buffer[target]
        buffer.append(data[data_index])
        data_index = (data_index + 1) % len(data)
    return batch, labels

print('data:', [reverse_dictionary[di] for di in data[:8]])

for num_skips, skip_window in [(2, 1), (4, 2)]:
    data_index = 0
    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)
    print('\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))
    print('    batch:', [reverse_dictionary[bi] for bi in batch])
    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])

batch_size = 128
embedding_size = 128  # Dimension of the embedding vector.
skip_window = 1  # How many words to consider left and right.
num_skips = 2  # How many times to reuse an input to generate a label.
# We pick a random validation set to sample nearest neighbors. here we limit the
# validation samples to the words that have a low numeric ID, which by
# construction are also the most frequent.
valid_size = 16  # Random set of words to evaluate similarity on.
valid_window = 100  # Only pick dev samples in the head of the distribution.
valid_examples = np.array(random.sample(range(valid_window), valid_size))
num_sampled = 64  # Number of negative examples to sample.

graph = tf.Graph()

with graph.as_default(), tf.device('/cpu:0'):
    # Input data.
    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)

    # Variables.
    embeddings = tf.Variable(
        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
    softmax_weights = tf.Variable(
        tf.truncated_normal([vocabulary_size, embedding_size],
                            stddev=1.0 / math.sqrt(embedding_size)))
    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))

    # Model.
    # Look up embeddings for inputs.
    embed = tf.nn.embedding_lookup(embeddings, train_dataset)
    # Compute the softmax loss, using a sample of the negative labels each time.
    loss = tf.reduce_mean(
        tf.nn.sampled_softmax_loss(softmax_weights, softmax_biases, embed,
                                   train_labels, num_sampled, vocabulary_size))

    # Optimizer.
    # Note: The optimizer will optimize the softmax_weights AND the embeddings.
    # This is because the embeddings are defined as a variable quantity and the
    # optimizer's `minimize` method will by default modify all variable quantities
    # that contribute to the tensor it is passed.
    # See docs on `tf.train.Optimizer.minimize()` for more details.
    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)

    # Compute the similarity between minibatch examples and all embeddings.
    # We use the cosine distance:
    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    normalized_embeddings = embeddings / norm
    valid_embeddings = tf.nn.embedding_lookup(
        normalized_embeddings, valid_dataset)
    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))
</code></pre>
",1
46608489,Multiple categorical input from a column in Tensorflow,"<p>I use tensorflow in version 1.3 and use a Wide' n' Deep Estimator for my network. The API allows the definition of <a href=""https://www.tensorflow.org/api_docs/python/tf/feature_column"" rel=""nofollow noreferrer""><code>tf.feature_columns</code></a>. Now I have single fields, which can have several entries. These are a free combination of words, i.e. a text where the appearance of the words is relevant to me. According to the documentation, there is only the possibility to convert the entire field into a hash---<a href=""https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column"" rel=""nofollow noreferrer""><code>embedding_column</code></a> with the bucket-size or <a href=""https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket"" rel=""nofollow noreferrer""><code>categorial_column_with_hash_bucket</code></a>---however, for me the individual words are relevant as input, not the text itself. </p>

<pre><code>Example: 
3 input of the same field: [hello world] [world is fine] [make software great again]
Categorial inputs I want: [hello, world, is, fine, make, software, great, again]
</code></pre>

<p>Is there a way to pass a set of elements in a column as input, so that tensorflow takes care of each word individually and categorically? In my opinion, the alternative is preprocessing, where I have to create 1000 of sparse columns and distribute the words to these columns individually.</p>

<p>Many thanks for your support</p>
",0
46627477,"using tf.scatter_nd(idx, update, shape) with shape having values that are dynamically allocated","<p>I would like to update the value in a vector at some index. I've found <a href=""https://www.tensorflow.org/api_docs/python/tf/scatter_nd"" rel=""nofollow noreferrer"">tf.scatter_nd</a> to do what I want. But I'm batching the operation such that I have an array <code>batch_size*5</code>, where each row is the vector of size 5 that I'm updating. My <code>batch_size</code> is determined at runtime.</p>

<p>In using <a href=""https://www.tensorflow.org/api_docs/python/tf/scatter_nd"" rel=""nofollow noreferrer"">tf.scatter_nd</a>, the shape argument takes a tensor which is the shape of the tensor to be produced. However if the first dimension is determined at run time, (such as if it is the batch size), then I get an error:</p>

<pre><code>TypeError: Input 'shape' of 'ScatterNd' Op has type int32 that does not match type int64 of argument 'indices'.
</code></pre>

<p>This error is actually due to the shape variable having the values : [None, 5]. ie to produce a tensor that is of size <code>batch_size*5</code> updated with updates of size <code>batch_size</code> and using indicies of size `batch_size'. </p>

<p>How would I correctly use tf.scatter_nd() on a space that is dynamically allocated? </p>
",0
46647805,building a tf.estimator input_fn: feature is not in features dictionary,"<p>I have a corpus of records that represent matchups in a video game. I want to feed this to a <code>tf.estimator.DNNClassifier</code>. </p>

<p>The records contain text representations of the 5 heroes on team 0 and the 5 heroes on team 1, the map the game was played on, and the winner of the game. I want to represent these three features as three sparse vectors.</p>

<p>I'm not using pandas or numpy right now. I would prefer to keep it as simple as possible for the time being, until I can elaborate my tf knowledge. (But no simpler!). </p>

<p>Perhaps the best way to ask the question is to show what I have and ask for help filling in the blank, at <code>make_input_fn</code></p>

<pre><code>import tensorflow as tf
import packunpack as source
import tempfile
from collections import namedtuple

GameRecord = namedtuple('GameRecord', 'team_0 team_1 game_map winner')
def parse(line):
    parts = line.rstrip().split(""\t"")
    return GameRecord(
        game_map = parts[1], 
        team_0 = parts[2].split("",""), 
        team_1 = parts[3].split("",""), 
        winner = int(parts[4]))

def conjugate(record):
    return GameRecord(
        team_0 = record.team_1, 
        team_1 = record.team_0, 
        game_map = record.game_map, 
        winner = 0 if record.winner == 1 else 1)

def sparse_team(team):
    return tf.SparseTensor(indices=team, values = [1] * len(team), dense_shape=[len(source.heroes_array)])

def sparse_map(i):
    return tf.SparseTensor(indices=[i], values = [1], dense_shape=[len(source.maps_array)])

def make_input_fn(filename, shuffle = True, add_conjugate_games = True):
    def _fn():
        records = []
        with open(filename, ""r"") as raw:
            i = 0
            for line in raw:
                record = parse(line)
                records.append(record)
                if add_conjugate_games:
                    # the team_0 and team_1 designations are arbitrary, and so the same inference should be drawn from a game and its ""conjugate"" game
                    records.append(conjugate(record))

        team_0s = map(lambda r: sparse_team(r.team_0), records)
        team_1s = map(lambda r: sparse_team(r.team_1), records)
        maps = map(lambda r: sparse_map(r.game_map), records)
        winners = map(lambda r: tf.constant([r.winner]), records)

        return ({
                    team_0: team_0s,
                    team_1: team_1s,
                    game_map: maps,
                }, 
                winners)
        #Please help me finish this function?

    return _fn

team_0 = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_vocabulary_list(""team_0"", source.heroes_array), 1)
team_1 = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_vocabulary_list(""team_1"", source.heroes_array), 1)
game_map = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_vocabulary_list(""game_map"", source.maps_array), 1)

model_dir = tempfile.mkdtemp()
m = tf.estimator.DNNClassifier(
    model_dir=model_dir,
    hidden_units = [1024, 512, 256], 
    feature_columns=[team_0, team_1, game_map])

def main():
    m.train(input_fn=make_input_fn(""validation.txt""))

if __name__ == ""__main__"":
    main()
</code></pre>

<p>I've been all over the docs today, but all of the code examples I can find show how to feed pandas and numpy data structures into input_fn, and obsure the underlying mechanics of the process by calling out to helper functions that don't work for me.</p>

<p>(e.g.,   <a href=""https://www.tensorflow.org/get_started/input_fn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/get_started/input_fn</a>
and 
<a href=""https://www.tensorflow.org/tutorials/wide"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/wide</a>)</p>

<p>tf version 1.4.0-dev20171008</p>

<p>When I run I get this stack trace. I think it doesn't like the return value from <code>_fn</code>. But that dictionary does have the names of the features that I gave to the model AFAICT.</p>

<pre><code> File ""estimator.py"", line 72, in &lt;module&gt;
    main()
  File ""estimator.py"", line 69, in main
    m.train(input_fn=make_input_fn(""validation.txt""))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 711, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/canned/dnn.py"", line 334, in _model_fn
    config=config)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/canned/dnn.py"", line 190, in _dnn_model_fn
    logits = logit_fn(features=features, mode=mode)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/canned/dnn.py"", line 89, in dnn_logit_fn
    features=features, feature_columns=feature_columns)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column.py"", line 230, in input_lay
er
    trainable=trainable)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column.py"", line 1834, in _get_den
se_tensor
    inputs, weight_collections=weight_collections, trainable=trainable)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column.py"", line 2119, in _get_spa
rse_tensors
    return _CategoricalColumn.IdWeightPair(inputs.get(self), None)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column.py"", line 1533, in get
    transformed = column._transform_feature(self)  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column.py"", line 2087, in _transfo
rm_feature
    input_tensor = _to_sparse_input(inputs.get(self.key))
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/feature_column/feature_column.py"", line 1529, in get
    raise ValueError('Feature {} is not in features dictionary.'.format(key))
ValueError: Feature team_0 is not in features dictionary.
</code></pre>
",0
46682855,tf.contrib.data.DataSet batch size can only set to 1,"<p>I converted pascal voc dataset to tfrecords via code <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/create_pascal_tf_record.py"" rel=""nofollow noreferrer"">create_pascal_tf_record.py</a>. I used <code>tf.contrib.data.Dataset</code> to read data. I used code as follows:</p>

<pre><code>import tensorflow as tf
from tensorflow.contrib.data import Iterator

slim_example_decoder = tf.contrib.slim.tfexample_decoder

flags = tf.app.flags
flags.DEFINE_string('data_dir', '/home/aurora/workspaces/data/tfrecords_data/voc_dataset/trainval.tfrecords',
                'tfrecords file output path')
flags.DEFINE_integer('batch_size', 1, 'training batch size')
flags.DEFINE_integer('capacity', 10000, 'training batch size')
FLAGS = flags.FLAGS

features = {""image/height"": tf.FixedLenFeature((), tf.int64, default_value=1),
        ""image/width"": tf.FixedLenFeature((), tf.int64, default_value=1),
        ""image/filename"": tf.FixedLenFeature((), tf.string, default_value=""""),
        ""image/source_id"": tf.FixedLenFeature((), tf.string, default_value=""""),
        ""image/key/sha256"": tf.FixedLenFeature((), tf.string, default_value=""""),
        ""image/encoded"": tf.FixedLenFeature((), tf.string, default_value=""""),
        ""image/format"": tf.FixedLenFeature((), tf.string, default_value=""jpeg""),
        ""image/object/object_number"": tf.FixedLenFeature((), tf.int64, default_value=1),
        ""image/object/bbox/xmin"": tf.VarLenFeature(tf.float32),
        ""image/object/bbox/xmax"": tf.VarLenFeature(tf.float32),
        ""image/object/bbox/ymin"": tf.VarLenFeature(tf.float32),
        ""image/object/bbox/ymax"": tf.VarLenFeature(tf.float32),
        ""image/object/class/text"": tf.VarLenFeature(tf.string),
        ""image/object/class/label"": tf.VarLenFeature(tf.int64),
        ""image/object/difficult"": tf.VarLenFeature(tf.int64),
        ""image/object/truncated"": tf.VarLenFeature(tf.int64),
        ""image/object/view"": tf.VarLenFeature(tf.string),
      }

items_to_handlers = {
    'image': slim_example_decoder.Image(
        image_key='image/encoded', format_key='image/format', channels=3),
    'height': (
        slim_example_decoder.Tensor('image/height')),
    'width': (
        slim_example_decoder.Tensor('image/width')),
    'source_id': (
        slim_example_decoder.Tensor('image/source_id')),
    'key': (
        slim_example_decoder.Tensor('image/key/sha256')),
    'filename': (
        slim_example_decoder.Tensor('image/filename')),
    # Object boxes and classes.
    'groundtruth_boxes': (
        slim_example_decoder.BoundingBox(
            ['ymin', 'xmin', 'ymax', 'xmax'], 'image/object/bbox/')),
    'groundtruth_classes': (
        slim_example_decoder.Tensor('image/object/class/label')),
    'groundtruth_difficult': (
        slim_example_decoder.Tensor('image/object/difficult')),
    'image/object/truncated': (
        slim_example_decoder.Tensor('image/object/truncated')),
    }

decoder = slim_example_decoder.TFExampleDecoder(features, items_to_handlers)
keys = decoder.list_items()


def _parse_function_train(example):
    serialized_example = tf.reshape(example, shape=[])
    decoder = slim_example_decoder.TFExampleDecoder(features, items_to_handlers)
    keys = decoder.list_items()
    tensors = decoder.decode(serialized_example, items=keys)
    tensor_dict = dict(zip(keys, tensors))
    tensor_dict['image'].set_shape([None, None, 3])
    # tensor_dict['image'] = tf.expand_dims(tensor_dict['image'], 0)
    images = tensor_dict['image']
    float_images = tf.cast(images, tf.uint8)
    tensor_dict['image'] = float_images
    return tensor_dict


def build_pipleline(train_data_dir, test_data_dir, batch_size, capacity):
    train_dataset = tf.contrib.data.TFRecordDataset(train_data_dir)
    train_dataset = train_dataset.map(_parse_function_train)
    train_dataset = train_dataset.repeat(1)
    train_dataset = train_dataset.batch(batch_size)
    train_dataset = train_dataset.shuffle(buffer_size=capacity)

    iterator = Iterator.from_structure(train_dataset.output_types,
                                   train_dataset.output_shapes)
    next_element = iterator.get_next()
    training_init_op = iterator.make_initializer(train_dataset)

    return training_init_op, next_element 


if __name__ == '__main__':
    # TODO: only support batch size 1
    training_init_op, next_element = build_pipleline(FLAGS.data_dir, None, FLAGS.batch_size, FLAGS.capacity)
    sess = tf.Session()
    sess.run(training_init_op)
    counter = 0
    while True:
        try:
            next_element_val = sess.run(next_element)
            print(next_element_val['image'].shape, next_element_val['filename'])
            print(next_element_val['groundtruth_boxes'])
            print('-'*30)
            counter += 1
        except tf.errors.OutOfRangeError:
            print('End of training data in step %d' %counter)
            break
</code></pre>

<p>The code can run correctly when the batch size set to 1, When I change the batch size to larger than 1, the code will have errors.
Errors as flollows:</p>

<pre><code>/usr/software/anaconda3/bin/python3.6 /home/aurora/workspaces/PycharmProjects/object_detection_models/builder/voc_input_pipline_dataset_builder.py
2017-10-11 15:55:05.886856: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-11 15:55:05.886869: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-11 15:55:05.886872: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-10-11 15:55:05.886874: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-11 15:55:05.886876: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-10-11 15:55:05.974850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-10-11 15:55:05.975103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: GeForce GTX 1080 Ti
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:01:00.0
Total memory: 10.90GiB
Free memory: 10.46GiB
2017-10-11 15:55:05.975112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2017-10-11 15:55:05.975114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2017-10-11 15:55:05.975118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0,       name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0)
2017-10-11 15:55:06.027798: W tensorflow/core/framework/op_kernel.cc:1192] Internal: HandleElementToSlice Cannot copy slice: number of elements does not match.  Shapes are: [element]: [1,4], [parent slice]: [5,4]
Traceback (most recent call last):
  File ""/usr/software/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
return fn(*args)
  File ""/usr/software/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1306, in _run_fn
status, run_metadata)
  File ""/usr/software/anaconda3/lib/python3.6/contextlib.py"", line 89, in __exit__
next(self.gen)
  File ""/usr/software/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
pywrap_tensorflow.TF_GetCode(status))
 tensorflow.python.framework.errors_impl.InternalError: HandleElementToSlice Cannot copy slice: number of elements does not match.  Shapes are: [element]: [1,4], [parent slice]: [5,4]
 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?,4], [?,?], [?,?], [?], [?,?,?,3], [?,?], [?], [?], [?]], output_types=[DT_STRING, DT_FLOAT, DT_INT64, DT_INT64, DT_INT64, DT_UINT8, DT_INT64, DT_STRING, DT_STRING, DT_INT64], _device=""/job:localhost/replica:0/task:0/cpu:0""](Iterator)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/aurora/workspaces/PycharmProjects/object_detection_models/builder/voc_input_pipline_dataset_builder.py"", line 98, in &lt;module&gt;
next_element_val = sess.run(next_element)
  File ""/usr/software/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 895, in run
run_metadata_ptr)
  File ""/usr/software/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1124, in _run
feed_dict_tensor, options, run_metadata)
  File ""/usr/software/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
options, run_metadata)
  File ""/usr/software/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: HandleElementToSlice Cannot copy slice: number of elements does not match.  Shapes are: [element]: [1,4], [parent slice]: [5,4]
 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?,4], [?,?], [?,?], [?], [?,?,?,3], [?,?], [?], [?], [?]], output_types=[DT_STRING, DT_FLOAT, DT_INT64, DT_INT64, DT_INT64, DT_UINT8, DT_INT64, DT_STRING, DT_STRING, DT_INT64], _device=""/job:localhost/replica:0/task:0/cpu:0""](Iterator)]]

Caused by op 'IteratorGetNext', defined at:
  File ""/home/aurora/workspaces/PycharmProjects/object_detection_models/builder/voc_input_pipline_dataset_builder.py"", line 92, in &lt;module&gt;
training_init_op, next_element = build_pipleline(FLAGS.data_dir, None, FLAGS.batch_size, FLAGS.capacity)
  File ""/home/aurora/workspaces/PycharmProjects/object_detection_models/builder/voc_input_pipline_dataset_builder.py"", line 84, in build_pipleline
next_element = iterator.get_next()
  File ""/usr/software/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/dataset_ops.py"", line 304, in get_next
name=name))
  File ""/usr/software/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 379, in iterator_get_next
output_shapes=output_shapes, name=name)
  File ""/usr/software/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
op_def=op_def)
  File ""/usr/software/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
original_op=self._default_original_op, op_def=op_def)
  File ""/usr/software/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InternalError (see above for traceback): HandleElementToSlice Cannot copy slice: number of elements does not match.  Shapes are: [element]: [1,4], [parent slice]: [5,4]
 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?], [?,?,4], [?,?], [?,?], [?], [?,?,?,3], [?,?], [?], [?], [?]], output_types=[DT_STRING, DT_FLOAT, DT_INT64, DT_INT64, DT_INT64, DT_UINT8, DT_INT64, DT_STRING, DT_STRING, DT_INT64], _device=""/job:localhost/replica:0/task:0/cpu:0""](Iterator)]]
</code></pre>

<p>How could I change the batch-size to larger than 1?
Thanks</p>
",0
46759271,Image pixel value normalized for tf.image.decode_jpeg and tf.train.shuffle_batch?,"<p>I am trying to use the tf.train.shuffle_batch function from tensorflow, then I need to first load the images using tf.image.decode_jpeg(or other similar functions to load png and jpg). But I just found out that the images are loaded as probability map, which means the max of the value of pixel is 1, and the min of the value of the pixel is 0. Below is my code updated from a github repo. I don't know why the values of pixels are normalized to [0,1], and I don't find related documentation on tensorflow. Could anyone help me? Thanks.  </p>

<pre><code>def load_examples(self, input_dir,  flip, scale_size, batch_size, min_queue_examples):
    input_paths = get_image_paths(input_dir)
    with tf.name_scope(""load_images""):
        path_queue = tf.train.string_input_producer(input_paths)
        reader = tf.WholeFileReader()
        paths, contents = reader.read(path_queue)
        # note this is important for truncated images
        raw_input = tf.image.decode_jpeg(contents,try_recover_truncated = True, acceptable_fraction=0.5)
        raw_input = tf.image.convert_image_dtype(raw_input, dtype=tf.float32)
        raw_input.set_shape([None, None, 3])

        # break apart image pair and move to range [-1, 1]
        width = tf.shape(raw_input)[1]  # [height, width, channels]
        a_images = preprocess(raw_input[:, :width // 2, :])
        b_images = raw_input[:, width // 2:, :]

    inputs, targets = [a_images, b_images]

    def transform(image):
        r = image

        r = tf.image.resize_images(r, [self.image_height, self.image_width], method=tf.image.ResizeMethod.AREA)
        return r
    def transform_gaze(image):
        r = image
        r = tf.image.resize_images(r, [self.gaze_height, self.gaze_width], method=tf.image.ResizeMethod.AREA)
        return r
    with tf.name_scope(""input_images""):
        input_images = transform(inputs)

    with tf.name_scope(""target_images""):
        target_images = transform(targets)
    total_image_count = len(input_paths)
    # target_images = tf.image.per_image_standardization(target_images)
    target_images = target_images[:,:,0]
    target_images = tf.expand_dims(target_images, 2)
    inputs_batch, targets_batch = tf.train.shuffle_batch([input_images, target_images],
                                         batch_size=batch_size,
                                         num_threads=1,
                                         capacity=min_queue_examples + 3 * batch_size,
                                         min_after_dequeue=min_queue_examples)
    # inputs_batch, targets_batch = tf.train.batch([input_images, target_images],batch_size=batch_size)
    return inputs_batch, targets_batch, total_image_count
</code></pre>
",1
46882503,"How to batch to space [n*7,12] to [n,7,12]","<p>I believe <code>tf.batch_to_space_nd</code> is what I need for this task, but I can't seem to wrap my head around this indexing and shape/rank business.</p>

<p>What I'd like to do is , from the input tensor with shape [n*7, 12] take the first 7 rows and put them in a separate dimension. Than take the next 7 rows, etc. The final shape should than be: [n,7,12].</p>

<p>Possibly I could simply add or squeeze a dimension, e.g. input could be [n*7, 12, 1, 1] and output could be [n,7,12,1]. I believe this can be done with batch_to_space_nd or possibly with tf.gather_nd or one of the other <a href=""https://www.tensorflow.org/versions/r1.2/api_guides/python/array_ops#Slicing_and_Joining"" rel=""nofollow noreferrer"">slice and join</a> operations, but haven't found the right combination.</p>

<p>Or do I really have to slice and stack? I know how to do that, but seems unnecessarily ugly.</p>
",0
46885191,tf.nn.conv2d_transpose output_shape dynamic batch_size,"<p>The documentation of tf.nn.conv2d_transpose says:</p>

<pre><code>tf.nn.conv2d_transpose(
    value,
    filter,
    output_shape,
    strides,
    padding='SAME',
    data_format='NHWC',
    name=None
)
</code></pre>

<p>The output_shape argument requires a 1D tensor specifying the shape of the tensor output by this op. Here, since my conv-net part has been built entirely on dynamic batch_length placeholders, I can't seem to device a workaround to the static <code>batch_size</code> requirement of the output_shape for this op. </p>

<p>There are many discussions around the web for this, however, I couldn't find any solid solution to this issue. Most of them are hacky ones with a <code>global_batch_size</code> variable defined. I wish to know the best possible solution to this problem. This trained model is going be shipped as a deployed service.</p>
",1
46929675,Tensorflow make assign op an explicit dependency for computing a tensor,"<p>I want to be able to implicitly run an <code>assign</code> Op every single time I run another tensor which depends on the <code>tf.Variable</code> which is changed during the <code>assign</code> Op. I don't want to run the <code>assign</code> Op manually  every single step. I tried 2 different approaches. Here is a simple example illustration:</p>

<pre><code>target_prob     = tf.placeholder(dtype=tf.float32, shape=[None, 2])
target_var      = tf.Variable(0, trainable=False, dtype=tf.float32)
init_target_var = tf.assign(target_var, tf.zeros_like(target_prob),
                            validate_shape=False)

# First approach
with tf.control_dependencies([init_target_var]):
  result = target_prob + target_var

# Second approach
# [target_var] = tf.tuple([target_var], control_inputs=[init_target_var])
# result = target_prob + target_var

sess = tf.Session()
sess.run(tf.global_variables_initializer())
res1 = sess.run(result, feed_dict={target_prob: np.ones([10, 2], dtype=np.float32)})
res2 = sess.run(result, feed_dict={target_prob: np.ones([12, 2], dtype=np.float32)})
</code></pre>

<p>Both fail with the error <code>InvalidArgumentError (see above for traceback): Incompatible shapes: [12,2] vs. [10,2]</code> when <code>res2</code> is being computed. It all works if I instead do:</p>

<pre><code>res1 = sess.run(result, feed_dict={target_prob: np.ones([10, 2], dtype=np.float32)})
sess.run(init_target_var, feed_dict={target_prob: np.ones([12, 2], dtype=np.float32)})
res2 = sess.run(result, feed_dict={target_prob: np.ones([12, 2], dtype=np.float32)})
</code></pre>

<p>But again, running <code>init_target_var</code> explicitly is exactly what I am trying to avoid.</p>

<p>P.S. The above is just a simplistic example. My final goal is to use the resulting tensor from <a href=""https://www.tensorflow.org/api_docs/python/tf/scatter_add"" rel=""nofollow noreferrer"">tf.scatter_add</a> which unfortunately requires a mutable tensor as input.</p>
",0
46964597,Tensorflow TypeError: Input 'pred' of 'Switch' Op has type float32 that does not match expected type of bool,"<p>I'm currently working on building a deep neural network using Tensorflow, and encountering some issues implementing a regularization technique called dropout (check out the original paper by Geoffrey Hinton <a href=""https://arxiv.org/pdf/1207.0580.pdf"" rel=""nofollow noreferrer"">here</a>).</p>

<p>Tensorflow has a function to take care of this, and I'm following a tutorial by Aurelien Geron's book <em>Hands-On Machine Learning with Scikit-Learn &amp; Tensorflow</em> (which, by the way, is incredible). In it, his sample code to implement dropout consists of declaring a <code>training</code> placeholder:</p>

<pre><code>training = tf.placeholder(tf.float32, shape = (), name = ""training"")
</code></pre>

<p>and then creating the hidden layer dropout object:</p>

<pre><code>hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training = training)
</code></pre>

<p>However, when I execute this, I receive an error pointing to the above line.</p>

<pre><code>TypeError: Input 'pred' of 'Switch' Op has type float32 that does not match expected type of bool
</code></pre>

<p>I looked into the <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/dropout"" rel=""nofollow noreferrer"">Tensorflow documentation regarding dropout</a>, <code>tf.layers.dropout()</code> method's <code>training</code> parameter is defined as </p>

<blockquote>
  <p>Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a
  placeholder). Whether to return the output in training mode (apply
  dropout) or in inference mode (return the input untouched).</p>
</blockquote>

<p>However, in the code above, I'm clearly passing in <code>tf.float32</code>. I suspect this is the cause of my error- it's even stated in the error message itself. So was this simply a typo by the author, or am I not understanding what is happening behind the scenes?</p>

<p>Should I just replace the hidden layer declaration with this line instead?</p>

<pre><code>hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training = True)
</code></pre>

<p>I've also looked into other SO <a href=""https://stackoverflow.com/questions/41842440/tensorflow-input-split-dim-of-split-op-has-type-float32-that-does-not-matc"">posts with similar errors</a>, like this one, but the answers seem to suggest that the error stems from an outdated version of Tensorflow, which is not the case- I only recently installed on my machine a few weeks ago.</p>
",1
46976226,`tf.estimator.RunConfig` vs `tf.contrib.learn.RunConfig`,"<p>I am confused regarding whether I should be using <code>tf.estimator.RunConfig</code> or <code>tf.contrib.learn.RunConfig</code> to pass a <code>RunConfig</code> to an estimator. </p>

<p>using <code>tf.contrib.learn.RunConfig</code> is straightforward:</p>

<pre><code>rc = tf.contrib.learn.RunConfig(save_checkpoints_secs=1,
                                model_dir=model_dir)
</code></pre>

<p>But <code>tf.estimator.RunConfig</code> has some odd syntax:</p>

<pre><code>rc = tf.estimator.RunConfig()
rc = rc.replace(save_checkpoints_secs=1,
                model_dir=model_dir)
</code></pre>

<p>Is there any reason to prefer one <code>RunConfig</code> over the other? The documentation is not clear on this.</p>
",1
46982402,What's the difference between tf.losses and tf.metrics?,"<p>In <a href=""https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/examples/tutorials/estimators/abalone.py"" rel=""noreferrer"">this example</a>, <code>tf.losses.mean_squared_error</code> is used for the <code>loss</code> parameter of <code>EstimatorSpec</code>, while <code>tf.metrics.root_mean_squared_error</code> is used for <code>eval_metric_ops</code> parameter.</p>

<p>Does anyone have ideas what is the main difference between <code>tf.loss</code> and <code>tf.metrics</code>?</p>
",0
46989256,Batch wise batch normalization in TensorFlow,"<p>What is the correct way of performing batch wise batch normalization in TensorFlow? (I.e. I don't want to compute a running mean and variance). My current implementation is based on <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization"" rel=""nofollow noreferrer""><code>tf.nn.batch_normalization</code></a>, where <code>x</code>is the output of a convolutional layer with shape <code>[batch_size, width, height, num_channels]</code>. I want to perform batch norm channel wise.</p>

<pre><code>batch_mean, batch_var = tf.nn.moments(x, axes=[0, 1, 2])
x = tf.nn.batch_normalization(x, batch_mean, batch_var, offset=0, scale=0, variance_epsilon=1e-6)
</code></pre>

<p>But the results of this implementation are very bad. Comparison with <code>tensorflow.contrib.slim.batch_norm</code> shows that it is fare inferior (similarly bad training performance).</p>

<p><a href=""https://i.stack.imgur.com/rxOaF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/rxOaF.png"" alt=""enter image description here""></a></p>

<p>What am I doing wrong, and what can explain this bad performance?</p>
",0
47035862,How does tensorflow's tf.contrib.training.batch_sequences_with_states API work?,"<p>I am dealing with long sequential data which has to be passed to an RNN. To do truncated BPTT and batching, seems like there are two options: </p>

<ol>
<li>Create a batch by combining <em>respective</em> segments from different sequences. Preserve final state of each sequence in a batch and pass it on to next batch.</li>
<li>Consider each sequence as a mini-batch with segments from the sequence becoming members of the batch. Preserve the state of the last time step in one segment and pass it on to the first time step of the next segment.</li>
</ol>

<p>I came across <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/training/batch_sequences_with_states"" rel=""nofollow noreferrer""><code>tf.contrib.training.batch_sequences_with_states</code></a> which seems to be doing one of the two. The documentation is confusing to me and hence I want to be certain which way does it generate the batches. </p>

<p>My guess is it does it the first way.  That’s because, if the batching is being done the second way, then we cannot leverage the benefits of vectorization, since, to preserve the state between the last time step of one segement  to the first time step of the next segment, RNN should process one token at a time sequentially. </p>

<p>Question:</p>

<p>Which of these two batching strategies are implemented in <code>tf.contrib.training.batch_sequences_with_states</code>?</p>
",1
47119604,"In tensorflow, does tf.summary record average values over multiple steps?","<p>By default, <code>RunConfig.save_summary_steps</code> is 100 in <code>tf.estimator.Estimator</code>, so it saves summaries every 100 steps. At each time it saves a summary, does it just save the current summary value computed from the current <code>step/minibatch</code>? Or it saves the average summary values computed from the recent 100 <code>steps/minibatches</code>? I cannot find a clear description for this in the official documentation.</p>
",1
47170879,What is partitioner parameter in Tensorflow variable_scope used for?,"<p><code>tf.variable_scope</code> has a <code>partitioner</code> parameter as mentioned in <a href=""https://www.tensorflow.org/api_docs/python/tf/variable_scope#__init__"" rel=""noreferrer"">documentation</a>.</p>

<p>As I understand it's used for distributed training. Can anyone explain it in more details what is the correct use of it?</p>
",1
47187768,Tensorflow slice operator to strided_slice,"<p>I would like to access the first channel of a tensor, which has a shape of: <code>[batch_size, img_width, img_height, channel_size]</code></p>

<p>Currently I do it like this:</p>

<pre><code>ch1 = X[...,0]
</code></pre>

<p>But I would like to give this operation a name, so I have to use <a href=""https://www.tensorflow.org/api_docs/python/tf/strided_slice"" rel=""nofollow noreferrer""><code>tf.strided_slice</code></a>, but I can not really understand how that works. How is it possible to rewrite <code>X[...,0]</code> using <code>tf.strided_slice</code>?</p>
",0
47205160,Tensorflow v1.4: Layer.input not supported in Eager mode,"<p>I understand that Eager mode is a new alpha feature on the nightly builds and that it is not perfect yet, but I do not know if there are any tf.keras workarounds for this problem.</p>

<p>The error <code>Layer.input not supported in Eager mode.</code> triggers on the block</p>

<pre><code>model = tf.keras.models.Sequential()
model.add(tf.layers.Dense(2, input_shape = (None, 1)))
model.add(tf.layers.Dense(units = 1))
model.compile(optimizer = ""sgd"", loss = ""mean_squared_error"")
</code></pre>

<p>I do not know anything about keras or the keras tensorflow API and I was wondering if there was a way to avoid <code>Layer.input</code> with keras techniques so as to stay within Eager mode. Following a tutorial in the tf.Eager docs I have confirmed that <code>model = tf.layers.Dense(1)</code> works but I don't know how to add another layer.</p>

<p>Any help is very much appreciated.</p>

<p><strong>EDIT</strong>
As of tensorflow v1.10, keras is supported in eager mode.</p>
",1
47216730,install python-tk using apt-get before running tensorflow on Google Cloud-ML,"<p>I’m working on an object detector using Cloud Machine Learning Engine via a Cloud-VM instance. Following the tutorial (<a href=""https://cloud.google.com/blog/big-data/2017/06/training-an-object-detector-using-cloud-machine-learning-engine"" rel=""nofollow noreferrer"">https://cloud.google.com/blog/big-data/2017/06/training-an-object-detector-using-cloud-machine-learning-engine</a>).</p>

<p>I get a module import error on Google Cloud Platform when I submit the training job below:</p>

<pre><code>gcloud ml-engine jobs submit training `whoami`_object_detection_`date +%s` \
    --job-dir=${YOUR_GCS_BUCKET}/train \
    --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz \
    --module-name object_detection.train \
    --region us-central1 \
    --config object_detection/samples/cloud/cloud.yml \
    -- \
    --train_dir=${YOUR_GCS_BUCKET}/train \
    --pipeline_config_path=${YOUR_GCS_BUCKET}/data/faster_rcnn_resnet101_coco.config
</code></pre>

<p>The error is below:</p>

<pre><code>...object_detection/utils/visualization_utils.py"", line 24, in &lt;module&gt;
import matplotlib.pyplot as plt
ImportError: No module named matplotlib.pyplot
</code></pre>

<p>I've installed matplotlib using pip install. This code works fine python2.7 -c 'import matplotlib.pyplot as plt'.</p>

<p>The matplotlib error is resolved by adding package names in the REQUIRED_PACKAGES list inside the setup.py program file.</p>

<p>Also, Please see my setup.py file..</p>

<pre class=""lang-py prettyprint-override""><code>""""""Setup script for object_detection.""""""

from setuptools import find_packages
from setuptools import setup
import subprocess

subprocess.check_call(['apt-get', 'update'])
subprocess.check_call(['apt-get', 'install', 'python-tk'])

REQUIRED_PACKAGES = ['Pillow&gt;=1.0', 'matplotlib']

setup(
    name='object_detection',
    version='0.1',
    install_requires=REQUIRED_PACKAGES,
    include_package_data=True,
    packages=[p for p in find_packages() if p.startswith('object_detection')],
    description='Tensorflow Object Detection Library',
)
</code></pre>

<p>But, even after resolving this, there are some other errors coming in this scenario as matplotlib has a dependency on python-tk package.</p>

<pre><code>ps-replica-0   Could not find a version that satisfies the requirement python-tk (from object-detection==0.1) (from versions: ) ps-replica-0 
ps-replica-0 No matching distribution found for python-tk (from object-detection==0.1) ps-replica-0 
ps-replica-0 Command '['pip', 'install', '--user', u'object_detection-0.1.tar.gz']' returned non-zero exit status 1 ps-replica-0 
ps-replica-0 Module completed; cleaning up. ps-replica-0 
</code></pre>

<p>But python-tk/python3-tk is not available in the pip package. In order to do so, we need to do
    sudo apt-get install python-tk
or
    sudo apt-get install python3-tk</p>

<p>Google Cloud-ML runs python 2.7. Hence, we need install python-tk before running out tensorflow training program.</p>

<p>Now, can someone please help me in order to command Cloud ML to install python-tk using apt-get before running tensorflow.</p>

<p><strong><em>Update_01</em>:*</strong></p>

<p>I'm getting another set of errors. It seems it's caused by the failure of python setup.py egg_info.
Also, there's this..</p>

<pre><code>Command '['apt-get', 'install', 'python-tk']' returned non-zero exit status 1
</code></pre>

<p>The error log is shown below.
Thanks in advance for your help.</p>

<pre><code>ps-replica-2
Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-BhSDtP-build/
ps-replica-2
Command '['pip', 'install', '--user', '--upgrade', '--force-reinstall', '--no-deps', u'object_detection-0.1.tar.gz']' returned non-zero exit status 1
The replica ps 0 exited with a non-zero status of 1. Termination reason: Error. 
Traceback (most recent call last):
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
  File ""/tmp/pip-C3hdCp-build/setup.py"", line 8, in &lt;module&gt;
subprocess.check_call(['apt-get', 'install', 'python-tk'])
  File ""/usr/lib/python2.7/subprocess.py"", line 540, in check_call
raise CalledProcessError(retcode, cmd)
CalledProcessError: Command '['apt-get', 'install', 'python-tk']' returned non-zero exit status 1

The replica ps 1 exited with a non-zero status of 1. Termination reason: Error. 
Traceback (most recent call last):
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
  File ""/tmp/pip-iR0TqP-build/setup.py"", line 8, in &lt;module&gt;
subprocess.check_call(['apt-get', 'install', 'python-tk'])
  File ""/usr/lib/python2.7/subprocess.py"", line 540, in check_call
raise CalledProcessError(retcode, cmd)
CalledProcessError: Command '['apt-get', 'install', 'python-tk']' returned non-zero exit status 1

The replica ps 2 exited with a non-zero status of 1. Termination reason: Error. 
Traceback (most recent call last):
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
  File ""/tmp/pip-BhSDtP-build/setup.py"", line 8, in &lt;module&gt;
subprocess.check_call(['apt-get', 'install', 'python-tk'])
  File ""/usr/lib/python2.7/subprocess.py"", line 540, in check_call
raise CalledProcessError(retcode, cmd)
CalledProcessError: Command '['apt-get', 'install', 'python-tk']' returned non-zero exit status 1

To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=640992742297&amp;resource=ml_job%2Fjob_id%2Froot_object_detection_1510462119&amp;advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%22root_object_detection_1510462119%22"" 
</code></pre>

<p>The job submission code:</p>

<pre><code>gcloud ml-engine jobs submit training `whoami`_object_detection_`date +%s` \
    --job-dir=${YOUR_GCS_BUCKET}/train \
    --packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz \
    --module-name object_detection.train \
    --config object_detection/samples/cloud/cloud.yml \
    -- \
    --train_dir=${YOUR_GCS_BUCKET}/train \
    --pipeline_config_path=${YOUR_GCS_BUCKET}/data/faster_rcnn_resnet101_coco.config
</code></pre>

<p>Thanks in advance..   </p>

<p><strong><em>Update_02: Solution</em></strong>
Thanks to <strong>@Dennis Liu</strong> for the solution. No need to install <code>python-tk</code> package.
Apart from that there will be one more error, which can be solved by changing <code>tf.train.get_or_create_global_step()</code> to <code>tf.contrib.framework.get_or_create_global_step()</code> at line 103 in <code>object_detection/builders/optimizer_builder.py</code>. <a href=""http://by%20changing%20tf.train.get_or_create_global_step()%20to%20tf.contrib.framework%20get_or_create_global_step()%20at%20line%20103%20in%20object_detection/builders/optimizer_builder.py."" rel=""nofollow noreferrer"">Solution Link</a></p>
",0
47223625,Tensorflow: working tf.while_loop does not work as part of Dataset API input pipeline,"<p>My problem is an image keypoint recognition task on images of snails. I have found that although there are many prewritten image augmentation functions for classification tasks (such as Keras' ImageDataGenerator), there are none that I can find suitable for this problem, which requires changes to the output keypoints to match the random transformations of the image. Hence I am writing my own to be mapped onto the dataset as it is read from TFRecord.</p>

<p>The logic I am using involves a while loop which continues to generate random transformations (rotation + shift + zoom etc.) and apply them the real keypoints until it finds a set of transformations where the keypoints fit into the image. This is to avoid transformations that leave part of the snail outside the image. It would then apply those same transformations to the image and return them.</p>

<p>My problem is that, while I have successfully got this augmentation function to work on a single test set of keypoints, when I use the same function as part of my input pipeline, it does not work, throwing the following error: 'Merge can not have more than one valid input' (full trace included at end). I have not been able to find an explanation anywhere. </p>

<pre><code># Defining cond argument to while loop.'ph' are placeholders to match numbers of arguments for tf.while_loop

def not_fit_in_image(landmarks, ph2, ph3, ph4, ph5, ph6):
    # tf logical operators to find if landmarks fit in image
    return landmarks_not_fit_in_image

def augmentation_function(image, original_landmarks):

    def body(ph1, ph2, ph3, ph4, ph5, ph6):

        shift = tf.random_uniform([1, 2], -shift_max, shift_max, tf.float32)
        landmarks = original_landmarks + shift
        # More random transformations generated and applied

        return landmarks, rotation, shift, zoom, y_over_x_proportion_change, shear

    # placeholders to match number of arguments
    ph_a = tf.constant(0, dtype=tf.float32)

    landmarks, rotation, shift, zoom, y_over_x_proportion_change, shear = tf.while_loop(not_fit_in_image, body, [original_landmarks, ph_a, ph_b, ph_a, ph_a, ph_a])

    # In future, would now apply these same transformations to image.

    return image, landmarks


# Setting up input data pipeline using Dataset API
train = tf.data.TFRecordDataset(train_data_tfrecords).map(parse_function)

train = train.map(augmentation_function) # Using the above augmentation function

train = train.repeat().shuffle(buffer_size).batch(batch_size)

# ... Set up handle, iterator, init ops ... all works ...

with tf.Session() as sess:
    train_handle = sess.run(train_iterator.string_handle())
    sess.run(train_init_op)
    train_images, train_landmarks = sess.run(next_batch, feed_dict={handle: train_handle})
</code></pre>

<p>The following error occurs:</p>

<pre><code>2017-11-10 13:08:14.449612: W C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\framework\op_kernel.cc:1192] Internal: Merge can not have more than one valid input.
     [[Node: while/Merge_5 = Merge[N=2, T=DT_FLOAT](while/Enter_5, while/NextIteration_5)]]
Traceback (most recent call last):
  File ""C:\Users\hanne\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py"", line 1323, in _do_call
    return fn(*args)
  File ""C:\Users\hanne\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py"", line 1302, in _run_fn
    status, run_metadata)
  File ""C:\Users\hanne\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InternalError: Merge can not have more than one valid input.
     [[Node: while/Merge_5 = Merge[N=2, T=DT_FLOAT](while/Enter_5, while/NextIteration_5)]]
     [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,384,384], [?,15,2]], output_types=[DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](IteratorFromStringHandle)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/hanne/Documents/Tensorflow Projects/Snails/random_rotations_working_while_loop_experiments.py"", line 143, in &lt;module&gt;
    train_images, train_landmarks = sess.run(next_batch, feed_dict={handle: train_handle})
  File ""C:\Users\hanne\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py"", line 889, in run
    run_metadata_ptr)
  File ""C:\Users\hanne\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\hanne\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""C:\Users\hanne\Anaconda3\envs\tensorflow-gpu\lib\site-packages\tensorflow\python\client\session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: Merge can not have more than one valid input.
     [[Node: while/Merge_5 = Merge[N=2, T=DT_FLOAT](while/Enter_5, while/NextIteration_5)]]
     [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,384,384], [?,15,2]], output_types=[DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](IteratorFromStringHandle)]]
</code></pre>

<p>This is my first time asking a question on stack overflow, so any comments about how to write better questions are also very welcome! I have tried to strip down the code above as much as I can for brevity and it is hence minimal but NOT complete or verifiable - let me know if I should include more code. </p>

<p>EDIT</p>

<p>I was able to figure out what was wrong! tf.while_loop acts like a python while loop, checking the condition before each run of 'body', which includes THE VERY FIRST RUN. The argument 'loop_vars' takes the variables for this first check. I had entered placeholder values of the wrong format to 'loop_vars', which caused the error above. A good way around this, which worked for me, is to enter the result of a first run of 'body' to the loop_vars variable, as this is assured of being of the right form.</p>
",0
47231048,Converting from feed_dict to queues results in increasing loss,"<p>I have a working tensorflow model that I am trying to convert to using queues. It may not be the best function but it works. </p>

<p>The data comes in as a list(dict()) called 'rows' from a processing pipeline outside of TF in the form format [{'y1': 1,  'y2': 0, 'y3':1, 'y4':0, 'x1':...'x1182': 0}] (SPECIAL_FIELD_CHAR is 'y', meaning it's calculated from the 'xN' data). The features_outputs() just returns the xs and the ys as ['y1', 'y2', 'y3', 'y4'] and ['x1', ..., 'x1182']. The idea is that the xs determine the ys. There are 4 independent ys that are calculated per row of xs.</p>

<pre><code>def train_rows(initial_weights, weights_filename, rows):
    (features, outputs ) = features_outputs(rows[0].keys())

    x_true = [ [float(row[feature]) for feature in features] for row in rows]
    try:
        y_true = [ [float(row[output]) for output in outputs] for row in rows ]
    except Exception as e:
        print [row[output] for output in outputs], e

    w_true = np.random.rand(len(features), 1) # init weights
    b_true = np.random.rand(1) # init bias

    x_in = tf.placeholder(tf.float32, [None, len(features)], ""x_in"")
    if initial_weights is None:
        w = tf.Variable(tf.random_normal((len(features), len(outputs))), name=""w"")
        b = tf.Variable(tf.constant(0.1, shape=[len(outputs)]), name=""b"")
    else:
        w = tf.Variable(weights['w'], name=""w"")
        b = tf.Variable(weights['b'], name=""b"")

    h = tf.add(tf.matmul(x_in, w), b, name=""h"")
    y_in = tf.placeholder(tf.float32, [None, len(outputs)], ""y_in"")
    loss_op = tf.reduce_mean(tf.square(tf.subtract(y_in, h)), name=""loss"")
    #train_op = tf.train.AdamOptimizer(0.01).minimize(loss_op)
    train_op = tf.train.GradientDescentOptimizer(0.3).minimize(loss_op)

    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        last_error = 1.7976931348623157e+308
        this_error = 1.7976931348623157e+307
        diff = 1
        iteration = initial_weights['iteration'] if initial_weights is not None and 'iteration' in initial_weights else 0
        while diff &gt; 0:
            iteration += 1
            last_error = this_error
            for step in range(1000):
                sess.run(train_op, feed_dict={
                  x_in: x_true,
                  y_in: y_true
                })

            w_computed = sess.run(w)
            b_computed = sess.run(b)

            pred = tf.add(tf.matmul(x_in, w), b)

            results = sess.run(pred, feed_dict={x_in: x_true})
            error = tf.losses.mean_squared_error(y_true, results)
            this_error = float(error.eval())
            (diff, locs) = compare(y_true, results)
            if locs &lt; 50:
                print ""iteration:"", iteration, ""error:"",this_error, ""diff:"", diff, ""locs:"", locs
</code></pre>

<p>This produces a model that converges. However with the queue based version it does not, and error increases rapidly:</p>

<pre><code>def multithreaded_train_rows(initial_weights, weights_filename, rows):
    (features, outputs ) = features_outputs(rows[0].keys())

    x_true = np.array([ [float(row[feature]) for feature in features] for row in rows])
    y_true = np.array([ [float(row[output]) for output in outputs] for row in rows ])


    #queue
    q = tf.FIFOQueue(capacity=len(rows), dtypes=tf.float32)
    #enq_op = q.enqueue_many(x_true)
    enq_op = q.enqueue_many(np.array( [ [float(row[f]) for f in sorted(row.keys())] for row in rows]  ))
    qr = tf.train.QueueRunner(q, [enq_op] * 1)
    tf.train.add_queue_runner(qr)

    keys = sorted(row.keys())
    x_indices = np.array([[i] for i in range(len(keys)) if not keys[i].startswith(SPECIAL_FIELD_CHAR)])
    y_indices = np.array([[i] for i in range(len(keys)) if     keys[i].startswith(SPECIAL_FIELD_CHAR)])

    input = q.dequeue()

    x_in = tf.transpose(tf.gather(input, x_indices))
    y_in = tf.gather(input, y_indices)

    if initial_weights is None:
        print 'Creating weights', len(x_indices), len(y_indices)
        w = tf.Variable(tf.random_normal((len(x_indices), len(y_indices))), name=""w"")
        b = tf.Variable(tf.constant(0.1, shape=[len(y_indices)]), name=""b"")
    else:
        print 'Using supplied weights', len(weights['w']), len(weights['w'][0])
        w = tf.Variable(weights['w'], name=""w"")
        b = tf.Variable(weights['b'], name=""b"")

    y = tf.add(tf.matmul(x_in, w), b, name=""y"")

    loss_op = tf.reduce_mean(tf.squared_difference(y_in, y), name=""loss"")
    train_op = tf.train.GradientDescentOptimizer(0.3).minimize(loss_op)

    print 'Starting session'
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(coord=coord)
        last_error = 1.7976931348623157e+308
        this_error = 1.7976931348623157e+307
        diff = 1
        iteration = initial_weights['iteration'] if initial_weights is not None and 'iteration' in initial_weights else 0
        while diff &gt; 0:
            iteration += 1
            last_error = this_error
            for step in range(100):
                sess.run([train_op, loss_op])

            w_computed = sess.run(w)
            b_computed = sess.run(b)

            pred = tf.add(tf.matmul(x_in, w), b)

            results = sess.run(y, feed_dict={x_in: x_true})
            error = tf.losses.mean_squared_error(y_true, results)
            this_error = float(error.eval())

            (diff, locs) = compare(y_true, results)
            if locs &lt; 50:
                print ""iteration:"", iteration, ""error:"",this_error, ""diff:"", diff, ""locs:"", locs

        coord.request_stop()
        coord.join(threads)
</code></pre>

<p>They are meant to be the same, but I've had to change a few things:
1. Add a tf.transpose() to the x_in for the matmul()
2. Queue the entire row of xs and ys, then pull apart using tf.gather(). </p>

<p>I've searched a lot for examples that match mine, and I can find no documentation on how to restart a queue and continue the training from the beginning. It'll seemly train forever(not sure why, who is replenishing the queue?) It'll also never stop. </p>

<p>But most of all I have no idea why given the exact same data, the first converges and the second does not?</p>
",1
47235290,In simple multi-layer FFNN only ReLU activation function doesn't converge,"<p>I'm learning tensorflow, deep learning and experimenting various kinds of activation functions.</p>

<p>I created a multi-layer FFNN for the MNIST problem. Mostly based on the tutorial from the official tensorflow website, except that 3 hidden layers were added.</p>

<p>The activation functions I have experimented are: <code>tf.sigmoid</code>, <code>tf.nn.tanh</code>, <code>tf.nn.softsign</code>, <code>tf.nn.softmax</code>, <code>tf.nn.relu</code>. Only <code>tf.nn.relu</code> doesn't converge, the network output random noise (testing accuracy is about 10%). The following are my source code:</p>

<pre><code>import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)

x = tf.placeholder(tf.float32, [None, 784])

W0 = tf.Variable(tf.random_normal([784, 200]))
b0 = tf.Variable(tf.random_normal([200]))
hidden0 = tf.nn.relu(tf.matmul(x, W0) + b0)

W1 = tf.Variable(tf.random_normal([200, 200]))
b1 = tf.Variable(tf.random_normal([200]))
hidden1 = tf.nn.relu(tf.matmul(hidden0, W1) + b1)

W2 = tf.Variable(tf.random_normal([200, 200]))
b2 = tf.Variable(tf.random_normal([200]))
hidden2 = tf.nn.relu(tf.matmul(hidden1, W2) + b2)

W3 = tf.Variable(tf.random_normal([200, 10]))
b3 = tf.Variable(tf.random_normal([10]))
y = tf.matmul(hidden2, W3) + b3

y_ = tf.placeholder(tf.float32, [None, 10])

cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))
train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)
with tf.Session() as session:
    session.run(tf.global_variables_initializer())
    for _ in range(10000):
        batch_xs, batch_ys = mnist.train.next_batch(128)
        session.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
        if _ % 1000 == 0:
            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
            print(_, session.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))

    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    print('final:', session.run(accuracy, feed_dict={x: mnist.test.images,
            y_: mnist.test.labels}))
</code></pre>

<p>The code outputs something like this:</p>

<pre><code>0 0.098
1000 0.098
2000 0.098
3000 0.098
4000 0.098
5000 0.098
6000 0.098
7000 0.098
8000 0.098
9000 0.098
final: 0.098
</code></pre>

<p>If <code>tf.nn.relu</code> is replaced with other activation functions the network accuracy improves gradually (with different final accuracy though), which is expected.</p>

<p>I have read in may textbooks/tutorials that ReLU should be the first candidate as activation function.</p>

<p>My question is why ReLU doesn't work in my network? or my program is simply wrong ?</p>
",0
47283944,tf operation control_inputs vs inputs,"<p>In <a href=""https://www.tensorflow.org/api_docs/python/tf/Operation"" rel=""nofollow noreferrer""><code>tf.Operation</code></a>, what's the difference between <code>control_inputs</code> and <code>inputs</code>? <code>tf.Operation</code> describes them separately, but in the normal course of events I would expect 'every data input is ready' to be exactly the condition for the current operation being able to run. What am I missing?</p>
",0
47361421,Error when using JSON derived data in Tensorflow,"<p>I'm using the following code, derived from the documentation:</p>

<pre><code>import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import json
from pprint import pprint

with open('/root/ml/2017110508.training.json') as text:
    data = json.load(text)
    features = np.array(data['input']['values'])
    labels = np.array(data['output']['values'])
    pprint(features.shape)
    pprint(labels.shape)
    pprint(features[0:3])
    pprint(labels[0:3])

# Assume that each row of `features` corresponds to the same row as `labels`.
assert features.shape[0] == labels.shape[0]

dataset = tf.data.Dataset.from_tensor_slices((features, labels))
</code></pre>

<p>The data in data['input']['values'] and data['output']['values'] are simply rows of floats but I get:</p>

<blockquote>
  <p>TypeError: Expected binary or unicode string, got [0.6, 0.0, 0.6, 0.0,
  0.0, 0.0, 0.0, 0.3, 0.6, 1.5, 0.0, 0.4, 7.7, -8.5, 158.0, 6.2, 55.3, 203.4, 205.7, 156.5, -8.5, 7.3, -8.8, 53.5, -0.9, -31.2, 15.3, -1.9, -87.6, 21.3, -21.6, -34.7, -17.1, -85.0, 28.6, -19.1]</p>
</blockquote>

<p>What format is <strong>from_tensor_slices</strong> expecting?</p>

<p>Thanks.</p>

<p>Output from the <strong>pprint</strong> calls:</p>

<blockquote>
  <p>(58502,)</p>
  
  <p>(58502, 5)</p>
  
  <p>array([ list([0.6, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.3, 0.6, 1.5, 0.0,
  0.4, 7.7, -8.5, 158.0, 6.2, 55.3, 203.4, 205.7, 156.5, -8.5, 7.3, -8.8, 53.5, -0.9, -31.2, 15.3, -1.9, -87.6, 21.3, -21.6, -34.7, -17.1, -85.0, 28.6, -19.1]),
         list([1.3, 0.0, 1.2, 0.0, 0.0, 0.0, 0.0, 0.6, 1.0, 2.3, 0.0, 0.6, 7.7, -8.5, 158.0, 6.2, 55.3, 203.4, 205.7, 156.4, -8.5, 7.5, -8.8, 53.4, -0.9, -31.2, 15.3, -1.9, -87.6, 21.3, -21.6, -34.7, -17.0, -85.0, 28.6, -19.1]),
         list([2.0, 0.0, 1.6, 0.0, 0.0, 0.0, 0.2, 0.8, 1.1, 2.9, 0.0, 0.9, 8.0, -8.5, 158.2, 6.2, 55.3, 203.4, 205.7, 156.3, -8.5, 8.0, -8.8, 53.3, -0.9, -31.2, 15.1, -1.9, -87.6, 21.3, -21.6, -34.8, -16.8, -84.9, 28.6, -19.1])], dtype=object)</p>
  
  <p>array([[0, 0, 0, 0, 0],
         [0, 0, 0, 0, 0],
         [0, 0, 0, 0, 0]])</p>
</blockquote>
",0
47379766,Replacing a node in a frozen Tensorflow model,"<p>I have a <code>frozen inference graph</code> stored in a <code>.pb file</code>, which was obtained from a <code>trained Tensorflow model</code> by the <code>freeze_graph</code> function. </p>

<p><strong>Suppose, for simplicity,</strong> that I would like to change some of the <code>sigmoid activations</code> in the model to <code>tanh activations</code> (and let's not discuss whether this is a good idea). </p>

<p><strong>How can this be done with access only to the frozen graph in the .pb file, and without the possibility to retrain the model?</strong></p>

<p>I am aware of the <code>Graph Editor library in tf.contrib</code>, which should be able to do this kind of job, but I wasn't able to figure out a simple way to do this in the documentation.</p>
",1
47389988,How to control GPU memory size with tf.estimator,"<p>I'm trying to control the size of GPU memory allocated for one tensorflow estimator tf.estimator.Estimator. The purpose is to only allocate half to run other tensorflow net on the same GPU. I found for the contrib version but not for the official. Someone knows if it's possible?</p>
",0
47430801,tf.Print doesn't well with tf.assign,"<p>Using tf.print, we get an error:</p>

<pre><code>In [1]: import tensorflow as tf

In [2]: # using print

In [3]: entcoeff =  tf.Variable([0], dtype=tf.float32, trainable=False)
   ...: entcoeff = tf.Print(entcoeff,[entcoeff,""printing""])

In [4]: tf.assign(entcoeff, [-1.])
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-4-dd57efca5923&gt; in &lt;module&gt;()
----&gt; 1 tf.assign(entcoeff, [-1.])

/nohome/jaan/abhishek/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py in assign(ref, value, validate_shape, use_locking, name)
    270         ref, value, use_locking=use_locking, name=name,
    271         validate_shape=validate_shape)
--&gt; 272   return ref.assign(value)

AttributeError: 'Tensor' object has no attribute 'assign'
</code></pre>

<p>Not using tf.print. It seems to work fine as expected</p>

<pre><code>In [5]: # not using print

In [6]: entcoeff =  tf.Variable([0], dtype=tf.float32, trainable=False)

In [7]: tf.assign(entcoeff, [-1.])
Out[7]: &lt;tf.Tensor 'Assign:0' shape=(1,) dtype=float32_ref&gt;
</code></pre>

<p>Does tf.Print convert the Variable to constant?
Trying to debug:</p>

<pre><code>In [8]: entcoeff = tf.Print(entcoeff,[entcoeff,""printing""])

In [9]: type(entcoeff)
Out[9]: tensorflow.python.framework.ops.Tensor
In [10]: dir(entcoeff)
Out[10]:
['OVERLOADABLE_OPERATORS',
 '__abs__',
 '__add__',
 '__and__',
 '__array_priority__',
 '__bool__',
 '__class__',
 '__delattr__',
 '__dict__',
 '__dir__',
 '__div__',
 '__doc__',
 '__eq__',
 '__floordiv__',
 '__format__',
 '__ge__',
 '__getattribute__',
 '__getitem__',
 '__gt__',
 '__hash__',
 '__init__',
 '__init_subclass__',
 '__invert__',
 '__iter__',
 '__le__',
 '__lt__',
 '__matmul__',
 '__mod__',
 '__module__',
 '__mul__',
 '__ne__',
 '__neg__',
 '__new__',
 '__nonzero__',
 '__or__',
 '__pow__',
 '__radd__',
 '__rand__',
 '__rdiv__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__rfloordiv__',
 '__rmatmul__',
 '__rmod__',
 '__rmul__',
 '__ror__',
 '__rpow__',
 '__rsub__',
 '__rtruediv__',
 '__rxor__',
 '__setattr__',
 '__sizeof__',
 '__str__',
 '__sub__',
 '__subclasshook__',
 '__truediv__',
 '__weakref__',
 '__xor__',
 '_add_consumer',
 '_as_node_def_input',
 '_as_tf_output',
 '_consumers',
 '_dtype',
 '_handle_dtype',
 '_handle_shape',
 '_op',
 '_override_operator',
 '_shape',
 '_shape_as_list',
 '_value_index',
 'consumers',
 'device',
 'dtype',
 'eval',
 'get_shape',
 'graph',
 'name',
 'op',
 'set_shape',
 'shape',
 'value_index']
</code></pre>
",0
47432870,Can tf.contrib.training.batch_sequences_with_states handle input sequences with variable lengths?,"<p>I was trying to use <code>tf.contrib.training.batch_sequences_with_states</code> to create padded batches of variable length input sequences in order to train an LSTM network.</p>
<p>While reading the documentation I stumbled upon contradicting statements, concerning the capabilities of this function.</p>
<p>Specifically the parameter <code>input_sequences</code> is confusing to me.</p>
<blockquote>
<p><code>input_sequence</code> is a dict with values that are tensors with time as first dimension. This time dimension must be the same across those tensors of an example. <strong>It can vary across examples.</strong></p>
<p><code>input_sequences</code>: A dict mapping string names to Tensor values. <strong>The values must all have matching first dimension</strong>, called value_length. They may vary from input to input</p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/training/batch_sequences_with_states"" rel=""nofollow noreferrer"">Source</a></p>
</blockquote>
<p>How do I supply this function with multiple examples?</p>
<h2>My first attempt</h2>
<pre><code>import tensorflow as tf
import numpy as np

batch_size = 10
num_unroll = 4
num_enqueue_threads = 2
lstm_size = 8

seq1 = np.random.rand(8, 8, 2)
seq2 = np.random.rand(16, 8, 2)

sequences = {&quot;seq1&quot;: seq1, &quot;seq2&quot;: seq2}
context = {&quot;seq1&quot;: 0, &quot;seq2&quot;: 1}

initial_states = {&quot;c&quot;: tf.zeros((lstm_size,), dtype=tf.float32),
                  &quot;h&quot;: tf.zeros((lstm_size,), dtype=tf.float32)}

batch = tf.contrib.training.batch_sequences_with_states(
    input_key=&quot;key&quot;,
    input_sequences=sequences,
    input_context=context,
    input_length=None,
    initial_states=initial_states,
    num_unroll=num_unroll,
    batch_size=batch_size,
    num_threads=num_enqueue_threads,
    capacity=batch_size * num_enqueue_threads * 2,
    make_keys_unique=True
)

inputs = batch.sequences[&quot;seq1&quot;]

with tf.Session() as sess:
    tf.train.start_queue_runners()
    print(sess.run([inputs]))
</code></pre>
<p>Executing this code snippet leads to the following error message:</p>
<pre><code>ERROR:tensorflow:Exception in QueueRunner: assertion failed: 
[All sequence lengths must match, but received lengths: 8 
 All sequence lengths must match, but received lengths: 16]
</code></pre>
<p>An example showing how to correctly supply this function with multiple examples of varying input_sequence lengths would be really helpful!</p>
",1
47451126,Tensorflow- How to display accuracy rate for a linear regression model,"<p>I have a linear regression model that seems to work. I first load the <code>data</code> into <code>X</code> and the target column into <code>Y</code>, after that I implement the following...</p>

<pre><code>X_train, X_test, Y_train, Y_test = train_test_split(
    X_data, 
    Y_data, 
    test_size=0.2
)

rng = np.random

n_rows = X_train.shape[0]

X = tf.placeholder(""float"")
Y = tf.placeholder(""float"")


W = tf.Variable(rng.randn(), name=""weight"")
b = tf.Variable(rng.randn(), name=""bias"")

pred = tf.add(tf.multiply(X, W), b)

cost = tf.reduce_sum(tf.pow(pred-Y, 2)/(2*n_rows))

optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning_rate).minimize(cost)



init = tf.global_variables_initializer()
init_local = tf.local_variables_initializer()

with tf.Session() as sess:

    sess.run([init, init_local])

    for epoch in range(FLAGS.training_epochs):

        avg_cost = 0

        for (x, y) in zip(X_train, Y_train):

            sess.run(optimizer, feed_dict={X:x, Y:y})

        # display logs per epoch step
        if (epoch + 1) % FLAGS.display_step == 0:

            c = sess.run(
                cost, 
                feed_dict={X:X_train, Y:Y_train}
            )

            print(""Epoch:"", '%04d' % (epoch + 1), ""cost="", ""{:.9f}"".format(c))

    print(""Optimization Finished!"")

    accuracy, accuracy_op = tf.metrics.accuracy(labels=tf.argmax(Y_test, 0), predictions=tf.argmax(pred, 0))

    print(sess.run(accuracy))
</code></pre>

<p>I cannot figure out how to print out the model's accuracy. For example, in <code>sklearn</code>, it is simple, if you have a model you just print <code>model.score(X_test, Y_test)</code>. But I do not know how to do this in <code>tensorflow</code> or if it is even possible.</p>

<p>I think I'd be able to calculate the <code>Mean Squared Error</code>. Does this help in any way? </p>

<p><strong>EDIT</strong></p>

<p>I tried implementing <code>tf.metrics.accuracy</code> as suggested in the comments but I'm having an issue implementing it. The documentation says it takes 2 arguments, <code>labels</code> and <code>predictions</code>, so I tried the following...</p>

<pre><code>accuracy, accuracy_op = tf.metrics.accuracy(labels=tf.argmax(Y_test, 0), predictions=tf.argmax(pred, 0))

print(sess.run(accuracy))
</code></pre>

<p>But this gives me an error...</p>

<blockquote>
  <p>FailedPreconditionError (see above for traceback): Attempting to use uninitialized value accuracy/count
       [[Node: accuracy/count/read = IdentityT=DT_FLOAT, _class=[""loc:@accuracy/count""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""]]</p>
</blockquote>

<p>How exactly does one implement this?</p>
",0
47555067,Computing set difference in tensorflow between two bidimensional arrays,"<p>How do I compute the set difference of elements of two arrays in tensorflow?</p>

<p>Example: I want to subtract all elements of <code>b</code> from <code>a</code>:</p>

<pre><code>import numpy as np

a = np.array([[1, 0, 1], [2, 0, 1], [3, 0, 1], [0, 0, 0]])
b = np.array([[1, 0, 1], [2, 0, 1]])
</code></pre>

<p>Expected result:</p>

<pre><code>array([[3, 0, 1], 
       [0, 0, 0]])
</code></pre>

<p>It can probably be done with <a href=""https://www.tensorflow.org/api_docs/python/tf/sets/set_difference"" rel=""nofollow noreferrer""><code>tf.sets.set_difference()</code></a>, but I fail to see how.</p>

<p>In numpy, <a href=""https://stackoverflow.com/a/39321661/4528388"">you can do something like this</a>, but I'm after a tensorflow solution to offload this operation to a GPU device, as this operation is computationally expensive for large arrays.</p>
",0
47568998,Tensorflow: Load data in multiple threads on cpu,"<p>I have a python class <code>SceneGenerator</code> which has multiple member functions for preprocessing and a generator function <code>generate_data()</code>. The basic structure is like this:</p>

<pre><code>class SceneGenerator(object):
    def __init__(self):
       # some inits

    def generate_data(self):
        """"""
        Generator. Yield data X and labels y after some preprocessing
        """"""
        while True:
            # opening files, selecting data
            X,y = self.preprocess(some_params, filenames, ...)            

            yield X, y
</code></pre>

<p>I used the class member function sceneGenerator.generate_data() in keras model.fit_generator() function to read the data from disk, preprocess it and yield it. In keras, this is done on multiple CPU threads, if the <code>workers</code> parameter of <code>model.fit_generator()</code> is set to something > 1.</p>

<p>I now want to use the same <code>SceneGenerator</code> class in tensorflow. My current approach is this:</p>

<pre><code>sceneGenerator = SceneGenerator(some_params)
for X, y in sceneGenerator.generate_data():

    feed_dict = {ops['data']: X,
                 ops['labels']: y,
                 ops['is_training_pl']: True
                 }
    summary, step, _, loss, prediction = sess.run([optimization_op, loss_op, pred_op],
                                                  feed_dict=feed_dict)
</code></pre>

<p>This, however, is slow and does not use multiple threads. I found the <a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/data/Dataset"" rel=""noreferrer""><code>tf.data.Dataset</code></a> api with some <a href=""https://www.tensorflow.org/versions/master/programmers_guide/datasets"" rel=""noreferrer"">documentation</a>, but I fail to implement the methods.</p>

<p><strong>Edit:</strong> Notice that I do not work with images so that the image loading mechanisms with file paths etc. do not work here.
My <code>SceneGenerator</code> loads data from hdf5 files. But not complete datasets but - depending on the initialization parameters - only parts of a dataset. I would love to keep the generator function as it is and learn how this generator can be directly used as input for tensorflow and runs on multiple threads on the CPU. Rewriting the data from the hdf5 files to csv is not a good option because it duplicated lots of data.</p>

<p><strong>Edit 2:</strong>: I think something similar to this could help: <a href=""https://stackoverflow.com/questions/47086599/parallelising-tf-data-dataset-from-generator"">parallelising tf.data.Dataset.from_generator</a></p>
",1
47574968,Tensorflow gradients causing contractive autoencoder cost doesn't converge,"<p>To construct a contractive autoencoder, one uses an ordinary autoencoder with the cost function
<a href=""https://i.stack.imgur.com/ALWYG.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ALWYG.gif"" alt=""enter image description here""></a></p>

<p>To implement this with the MNIST dataset, I defined the cost function using using tensorflow as </p>

<pre><code>def cost(X, X_prime):
    grad = tf.gradients(ys=X_prime, xs=X)
    cost = tf.reduce_mean(tf.square(X_prime - X)) + tf.reduce_mean(tf.square(grad))
    return cost
</code></pre>

<p>and used AdamOptimizer for backpropagation. However, the cost doesn't go any lesser than 0.067, which is peculiar. Is my implementation of the cost function incorrect? </p>

<p><strong>Edit:</strong>
After reading the <a href=""https://www.tensorflow.org/versions/r0.12/api_docs/python/train/gradient_computation"" rel=""nofollow noreferrer"">documentation</a> on<code>tf.gradients</code>, the above implementation would have computed 
<a href=""https://i.stack.imgur.com/pd88E.gif"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pd88E.gif"" alt=""enter image description here""></a> instead. So my question is, how do you do derivatives component wise in tensorflow? </p>
",0
47580828,Is making multiple shards of your data with multiple threads minimize the training time?,"<p>My main issue is : I have 204 GB training tfrecord file that has  2 million images, and 28GB for validation tf.record file, of 302900 images. it takes 8 hour to train one epoch and this will take 33 day for training. I want to speed that by using multiple threads and shards but I am little bit confused about couple of things.</p>

<p>In <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""noreferrer"">tf.data.Dataset API</a> there is shard function , So in the documentation they mentioned the following about shard function : </p>

<blockquote>
  <p>Creates a Dataset that includes only 1/num_shards of this dataset.</p>
  
  <p>This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset.</p>
  
  <p>When reading a single input file, you can skip elements as follows:</p>
</blockquote>

<pre><code>d = tf.data.TFRecordDataset(FLAGS.input_file)
d = d.shard(FLAGS.num_workers, FLAGS.worker_index)
d = d.repeat(FLAGS.num_epochs)
d = d.shuffle(FLAGS.shuffle_buffer_size)
d = d.map(parser_fn, num_parallel_calls=FLAGS.num_map_threads)
</code></pre>

<blockquote>
  <p>Important caveats:</p>
  
  <p>Be sure to shard before you use any randomizing operator (such as shuffle).
  Generally it is best if the shard operator is used early in the dataset pipeline. >For example, when reading from a set of TFRecord files, shard before converting >the dataset to input samples. This avoids reading every file on every worker. The >following is an example of an efficient sharding strategy within a complete >pipeline:</p>
</blockquote>

<pre><code>d = Dataset.list_files(FLAGS.pattern)
d = d.shard(FLAGS.num_workers, FLAGS.worker_index)
d = d.repeat(FLAGS.num_epochs)
d = d.shuffle(FLAGS.shuffle_buffer_size)
d = d.repeat()
d = d.interleave(tf.data.TFRecordDataset,
             cycle_length=FLAGS.num_readers, block_length=1)

d = d.map(parser_fn, num_parallel_calls=FLAGS.num_map_threads)
</code></pre>

<p>and these are my question: </p>

<p>1- Is there any relation between the number of tf.records files and number of shards? is number of shards(worker) depend on the number of CPU you have it, or the number of tf.records files you have ? and how I create it, , just by setting the number of shards to specific number?  , or we need to split the files to multiple files and then set specific number of shards. note the number of worker referred to number of shards </p>

<p>2- what is the benefit of creating multiple tf.records file?  some people said <a href=""https://datascience.stackexchange.com/questions/16318/what-is-the-benefit-of-splitting-tfrecord-file-into-shards"">here</a> is is related with when you need to shuffle you tf.records in better way but with Shuufle method exists in tf.Dataset API we don't need to do that, and other people said <a href=""https://github.com/tensorflow/tensor2tensor/issues/190#issuecomment-319565596"" rel=""noreferrer"">here</a> it is only to split your data to smaller sizes parts. My question Do I need to as first step to split my tf.records file to multiple files</p>

<p>3- Now we come to num_threads in the map function  ( num_paralle_calls in new version of tensorflwo) should be the same as number of shards you have it . When I searched I found some people say that if you have 10 shards, and 2 threads, each thread will take 5 shards.</p>

<p>4- what about about the d.interleave function , I know how it works as was mention in this <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave"" rel=""noreferrer"">example</a>. But again  I missed the connection num_threads, cycle length for example</p>

<p>5- If I want to use multiple GPU should I use the shards? as mentioned <a href=""https://stackoverflow.com/questions/46965098/how-does-one-move-data-to-multiple-gpu-towers-using-tensorflows-dataset-api"">in the accepted comment here</a></p>

<p>as a summary I am confused about the relation between ( number of tf.records files, num_shards(workers), cyclic length, num_thread(num_parallel_calls). And what is the better situation to create in order to minimize the training time for both cases ( using multiple GPUs, and using single GPU) </p>
",1
47644412,TensorFlow Dataset API Parsing Error,"<p>I'm using the TensorFlow Dataset API to parse a CSV file and run a logistic regression. I'm following the example from the TF documentation <a href=""https://github.com/tensorflow/models/blob/master/official/wide_deep/wide_deep.py"" rel=""nofollow noreferrer"">here</a>.</p>

<p>The following code snippet shows how I am setting up the model:</p>

<pre><code>def input_fn(path, num_epochs, batch_size):
    dataset = tf.data.TextLineDataset(path)
    dataset = dataset.map(parse_table, num_parallel_calls=12)
    dataset = dataset.repeat(num_epochs)
    dataset.batch(batch_size)

    iterator = dataset.make_one_shot_iterator()
    features, labels = iterator.get_next()
    return features, labels

def parse_table(value):
    cols = tf.decode_csv(value, record_defaults=TAB_COLUMN_DEFAULTS)
    indep_vars = dict(zip(CSV_COLS, cols))
    y = indep_vars.pop('y')
    return indep_vars, y

def build_indep_vars():
    continuous_vars = [
        tf.feature_column.numeric_column(x, shape=1) for x in CONT_COLS]
    categorical_vars = [
        tf.feature_column.categorical_column_with_hash_bucket(
            x, hash_bucket_size=100) for x in CAT_COLS]
    return categorical_vars + continuous_vars
</code></pre>

<p>When calling <code>lr.train(input_fn = lambda: input_fn(data_path, 1, 100))</code> (note: batch size is 100) I'm getting the error </p>

<pre><code>ValueError: Feature (key: V1) cannot have rank 0. Give: Tensor(""IteratorGetNext:0"", shape=(), dtype=float32, device=/device:CPU:0)
</code></pre>

<p>So I'm assuming this means one of the <code>tf.feature_column.numeric_column</code> calls is getting a scalar value which it doesn't like. However, I cannot figure out why this is the case. I've set <code>batch_size</code> to a positive integer and according to the documentation the shape of the NDarray resulting from <code>tf.feature_column.numeric_column</code> should be <code>1Xbatch_size</code> by default. Can anyone explain why TensorFlow is returning this error?</p>

<p>I'm sure this question has a simple answer that will make me feel stupid for not figuring it out, but after spending some time on this I'm still stumped.</p>
",1
47665314,how can we get benefit from sharding the data to speed the training time?,"<p>My main issue is : I have 204 GB training tfrecords for 2 million images, and 28GB for validation tf.records files, of 302900 images. it takes 8 hour to train one epoch and this will take 33 day for training. I want to speed that by using multiple threads and shards but I am little bit confused about couple of things.</p>

<p>In <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">tf.data.Dataset API</a> there is shard function , So in the documentation they mentioned the following about shard function : </p>

<blockquote>
  <p>Creates a Dataset that includes only 1/num_shards of this dataset.</p>
  
  <p>This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset.</p>
  
  <p>When reading a single input file, you can skip elements as follows:</p>
</blockquote>

<pre><code>d = tf.data.TFRecordDataset(FLAGS.input_file)
d = d.shard(FLAGS.num_workers, FLAGS.worker_index)
d = d.repeat(FLAGS.num_epochs)
d = d.shuffle(FLAGS.shuffle_buffer_size)
d = d.map(parser_fn, num_parallel_calls=FLAGS.num_map_threads)
</code></pre>

<blockquote>
  <p>Important caveats:</p>
  
  <p>Be sure to shard before you use any randomizing operator (such as shuffle).
  Generally it is best if the shard operator is used early in the dataset pipeline. >For example, when reading from a set of TFRecord files, shard before converting >the dataset to input samples. This avoids reading every file on every worker. The >following is an example of an efficient sharding strategy within a complete >pipeline:</p>
</blockquote>

<pre><code>d = Dataset.list_files(FLAGS.pattern)
d = d.shard(FLAGS.num_workers, FLAGS.worker_index)
d = d.repeat(FLAGS.num_epochs)
d = d.shuffle(FLAGS.shuffle_buffer_size)
d = d.repeat()
d = d.interleave(tf.data.TFRecordDataset,
             cycle_length=FLAGS.num_readers, block_length=1)

d = d.map(parser_fn, num_parallel_calls=FLAGS.num_map_threads)
</code></pre>

<p>So my question regarding the code above is when I try to makes d.shards of my data using shard function, if I set the number of shards (num_workers)to 10 , I will have 10 splits of my data , then should I set the num_reader in d.interleave function to 10 to guarantee that each reader take one split from the 10 split? </p>

<p>and how I can control which split the function interleave will take? because if I set the shard_index (worker_index) in shard function to 1 it will give me the first split. Can anyone give me an idea how can I perform this distributed training using the above functions? </p>

<p>then what about the num_parallel_call . should I set it to 10 as well? </p>

<p>knowing that I have single tf.records file for training and another one for validation , I don't split the tf.records files into multiple files.</p>
",1
47745397,Why use fixed padding when building resnet model in tensorflow,"<p>Tensorflow has an official realization of resnet in <a href=""https://github.com/tensorflow/models/blob/master/official/resnet/resnet_model.py"" rel=""noreferrer"">github</a>. And it uses <strong>fixed padding</strong> instead of normal tf.layers.conv2d. </p>

<p>Something like this:</p>

<pre><code>def conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format):
  """"""Strided 2-D convolution with explicit padding.""""""
  # The padding is consistent and is based only on `kernel_size`, not on the
  # dimensions of `inputs` (as opposed to using `tf.layers.conv2d` alone).
  if strides &gt; 1:
    inputs = fixed_padding(inputs, kernel_size, data_format)

  return tf.layers.conv2d(
      inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,
      padding=('SAME' if strides == 1 else 'VALID'), use_bias=False,
      kernel_initializer=tf.variance_scaling_initializer(),
      data_format=data_format)
</code></pre>

<p>What's the purpose of doing this? We can get a 16x16 feature map if we input a image of size 32x32 and use tf.layer.conv2d setting padding method to SAME, stride 2. But in the code above, it will pad zero in both side of image and then use padding method VALID.</p>
",0
47783391,Tensorflow Estimator: using predict() function in separate script,"<p>I have successfully (I hope) trained and evaluated a model using the tf.Estimator where I reach a train/eval accuracy of around 83-85%. So now, I would like to test my model on a separate dataset using the predict() function call in the Estimator class. Preferably I would like to do this in a separate script.</p>

<p>I've at <a href=""https://stackoverflow.com/questions/46098863/how-to-import-an-saved-tensorflow-model-train-using-tf-estimator-and-predict-on"">this</a> which says that I need to export as a SavedModel, but is this really necessary? Looking at the <a href=""https://www.tensorflow.org/versions/r1.3/api_docs/python/tf/estimator/Estimator#__init__"" rel=""nofollow noreferrer"">documentation</a> for the Estimator class, it seems like I can just pass the path to my checkpoint and graph files via the <code>model_dir</code> parameter. Has anyone any experience with this? When I run my model on the same dataset I used for validation, I do not obtain the same performance as during the validation phase... :-(</p>
",1
47854014,How to load EMNIST data to Tensorflow,"<p>In all the tutorials i've seen for tensorflow, they've used the MNIST dataset, i've understood the modelling but how do i load this dataset into tensorflow?
<a href=""https://www.nist.gov/itl/iad/image-group/emnist-dataset"" rel=""nofollow noreferrer"">https://www.nist.gov/itl/iad/image-group/emnist-dataset</a></p>
",0
47901034,Usage of tf.GraphKeys,"<p>In tensorflow, there's a class <code>GraphKeys</code>. I came across many codes, where it's been used. But it's not explained very well what's the usage of this class both in tensorflow documentation as well as in the codes, where it has been used.</p>

<p>Can someone please explain what's the usage of <code>tf.GraphKey</code>?</p>

<p>Thank you!</p>
",1
47953242,Tensorflow Batch Normalization: tf.contrib.layers.batch_norm,"<p>I've recently picked up Tensorflow and and have been trying my best to adjust to the environment. It has been nothing but wonderful! However, batch normalization using tf.contrib.layers.batch_norm has been a little tricky. 
Right now, here is the function I'm using:</p>

<pre><code>def batch_norm(x, phase):
    return tf.contrib.layers.batch_norm(x,center = True, scale = True,
                    is_training = phase, updates_collections = None)
</code></pre>

<p>Using this, I followed most documentation (also Q &amp; A) that I've found online and it led me to the following conclusions:</p>

<p>1) is_training should be set to True for training and false for testing. This makes sense! When training, I had convergence (error &lt; 1%, Cifar 10 Dataset). </p>

<p>However during testing, my results are terrible (error > 90%) UNLESS I add (update collections = None) as an argument to the batch norm function above. Only with that as an argument will testing give me the error I expected. </p>

<p>I am also sure to use the following for training:</p>

<pre><code>update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):                                       # Ensures, Updating ops will perform before training
    with tf.name_scope('Cross_Entropy'):
        cross_entropy = tf.reduce_mean(                                         # Implement Cross_Entropy to compute the softmax activation
            tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))  # Cross Entropy: True Output Labels (y_), Softmax output (y_conv)
        tf.summary.scalar('cross_entropy', cross_entropy)                       # Graphical output Cross Entropy

    with tf.name_scope('train'):
        train_step = tf.train.AdamOptimizer(1e-2).minimize(cross_entropy)       # Train Network, Tensorflow minimizes cross_entropy via ADAM Optimization 

    with tf.name_scope('Train_Results'):
        with tf.name_scope('Correct_Prediction'):
            correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))  # Check if prediction is wrong with tf.equal(CNN_result,True_result)
        with tf.name_scope('Accuracy'):
            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))     # Find the percent accuracy, take mean of correct_prediction outputs
            tf.summary.scalar('accuracy', accuracy)                                # Graphical output Classification Accuracy
</code></pre>

<p>This should make sure that the batch normalization parameters are updating during training. </p>

<p>So this leads me to believe that update collections = None is just a nice default to my batch normalization function that during testing function will be sure not to adjust any batch normalization parameters.... Am I correct?</p>

<p>Lastly: Is it normal to have good results (Expected Error) when, during the testing phase, having batch normalization turned on AND off? Using the batch norm function above, I was able to train well (is_training = True) and test well (is_training = False). However, during testing (is_training = True) I was still able to get great results. This is just gives me a bad feeling. Could someone explain why this is happening? Or if it should be happening at all?</p>

<p>Thank you for your time!</p>
",0
47965551,Use tf.train.ExponentialMovingAverage() to Train the model,"<p>I'm learning TensorFlow and trying to apply exponential moving average based gradient descent (instead vanilla gradient descent). Specifically I;m trying to use <a href=""https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage"" rel=""nofollow noreferrer"">tf.train.ExponentialMovingAverage</a> but the document doesn't seem to provide guide for how to use it to build model that drives optimization.</p>

<p>Full code is available at - <a href=""https://github.com/vibhorj/tf/blob/master/so/ema.py"" rel=""nofollow noreferrer"">https://github.com/vibhorj/tf/blob/master/so/ema.py</a>
, but here's what i'm doing and the classifier isn't learning anything (after each epoch, w &amp; b still remain same .. no learning)</p>

<p><strong>STEP1</strong>: define weights &amp; biases</p>

<pre><code>with tf.variable_scope('scp1', reuse=tf.AUTO_REUSE):
w = tf.get_variable(name='weights', initializer = tf.ones(shape=[2,3],dtype=tf.float32))
b = tf.get_variable(name='bias', initializer = tf.ones(shape=[3],dtype=tf.float32))
</code></pre>

<p><strong>STEP2</strong>: define error / loss / optimizer</p>

<pre><code>X = tf.placeholder(tf.float32, shape=[None,2], name='X')
Y = tf.placeholder(tf.float32, shape=[None,3], name='Y')
Ylogits = tf.matmul(X,w) + b
error = -Y*tf.log(Ylogits)
loss = tf.reduce_mean(error, name = 'loss') 
opt = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)
</code></pre>

<p><strong>STEP3</strong>: Created ExponentialMovingAverage object, created training_op that (I expect) to update the moving averages after each training step. </p>

<pre><code>ema = tf.train.ExponentialMovingAverage(decay=0.50,name='EMA')
training_op = ema.apply([w, b])
</code></pre>

<p><strong>STEP4</strong>: Finaly run the iterations</p>

<pre><code>with tf.Session() as sess:
    sess.run(tf.variables_initializer(tf.global_variables()))
    for epoch in range(10): #increase epocs later
        _ = sess.run([training_op], feed_dict=feed_train)
        print(""\n POST:"")
        print(""    {}:\n {}"".format(ema.average_name(w),sess.run(ema.average(w))))
        print(""    {}:\n {}"".format(ema.average_name(b),sess.run(ema.average(b))))
</code></pre>

<p>When I run it, weights &amp; biases stay the same with each successive iterations!</p>

<p>I know there's something i'm missing (to update the parameters!) but unable to identify! The doc isn't much helpful either. Thanks a lot for any clue any guidance on how I can proceed further.</p>

<p>Thanks!</p>
",1
47984876,Tensorflow tf.map_fn parameters,"<p>I'm attempting to structure my parameters so that they will work properly with tf.map_fn() but most of the example documentation only discusses arrays or tensors of the same shape as function arguments.</p>

<p>Links include:</p>

<p><a href=""https://stackoverflow.com/questions/37086098/does-tensorflow-map-fn-support-taking-more-than-one-tensor"">Does tensorflow map_fn support taking more than one tensor?</a></p>

<p>My specific example is this:
I have some tensorflow function that expects [None, 2] and [x,y] as parameter tensor shapes.</p>

<p>Tensor A is of shape [batch_size, x*y, 2]</p>

<p>Tensor B is of shape [batch_size, x, y]</p>

<pre><code>lambdaData = (tensorA, tensorB)
lambdaFunc = lambda x: tensorflowFunc(x[0], x[1])
returnValues = tf.map_fn(lambdaFunc, lambdaData)
</code></pre>

<p>From the tensorflow documentation:</p>

<pre><code>If elems is a (possibly nested) list or tuple of tensors, then each of these 
tensors must have a matching first (unpack) dimension
</code></pre>

<p>Since tensorsA and B only match in dimension 0, I cannot stack or concatenate them; I have also tried creating lambdaData as:</p>

<ol>
<li>A list of two tensors</li>
<li>A tuple of two tensors</li>
<li>A list of tensor pairs</li>
</ol>

<p>All of the above result in varying dimension mismatch errors.  I would follow the recommended use as per documentation of placing all of the data into a single tensor, but because of dimension mismatching between tensorA and tensorB I am unable to.  Has anybody had any luck with tuples or lists of arguments for elems?</p>
",1
48015622,Are the operations in tf.group executed in order?,"<p>Are the operations in <a href=""https://www.tensorflow.org/api_docs/python/tf/group"" rel=""nofollow noreferrer""><code>tf.group()</code></a> executed in order?</p>

<p>If they are not executed in order, is there a similar operation that makes them run in order? Or is there a clean way to run them in order?</p>

<p>My aim is to run operations A and B many times over and over again, i.e.</p>

<pre><code>sess.run(A)
sess.run(B)
sess.run(A)
sess.run(B)
sess.run(A)
sess.run(B)
sess.run(A)
sess.run(B)
...
</code></pre>
",0
48017748,"In tensorflow,when graph is modified, how to use ""MonitoredTrainingSession"" to restore only part of checkpoint?","<p>My purpose is simple and clear: After the graph is partially modified, how to restore the unchanged variables/parameters from previous log's checkpoint file?(Better using MonitoredTrainingSession)</p>

<p>I make a test on the code from here:
<a href=""https://github.com/tensorflow/models/tree/master/research/resnet"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/tree/master/research/resnet</a></p>

<p>In resnet_model.py,line 116-118,the original code(or graph) is:</p>

<pre><code>with tf.variable_scope('logit'):
    logits = self._fully_connected(x, self.hps.num_classes)
    self.predictions = tf.nn.softmax(logits)
with tf.variable_scope('costs'):
    xent = tf.nn.softmax_cross_entropy_with_logits(
    logits=logits, labels=self.labels)
    self.cost = tf.reduce_mean(xent, name='xent')
    self.cost += self._decay()
</code></pre>

<p>after the first training, I obtain checkpoint files.
Then I modified the code to:</p>

<pre><code>with tf.variable_scope('logit_modified'):
    logits_modified = self._fully_connected('fc_1',x, 48)
    #self.predictions = tf.nn.softmax(logits)    
with tf.variable_scope('logit_2'):
    logits_2 = self._fully_connected('fc_2', logits_modified, 
    self.hps.num_classes)
    self.predictions = tf.nn.softmax(logits_2)
with tf.variable_scope('costs'):
    xent = tf.nn.softmax_cross_entropy_with_logits(
    logits=logits_2, labels=self.labels)
    self.cost = tf.reduce_mean(xent, name='xent')
    self.cost += self._decay()
</code></pre>

<p>Then I try to use the latested API tf.train.MonitoredTrainingSession to restore the checkpoint obtained in the first training. I have tried mutiple methods to do this, but none of them works.</p>

<p>Try 1:
If I don't use scaffold in MonitoredTrainingSession:</p>

<pre><code>with tf.train.MonitoredTrainingSession(
    checkpoint_dir=FLAGS.log_root,
    #scaffold=scaffold,
    hooks=[logging_hook, _LearningRateSetterHook()],
    chief_only_hooks=[summary_hook],
    save_checkpoint_secs = 600,
    # Since we provide a SummarySaverHook, we need to disable default
    # SummarySaverHook. To do that we set save_summaries_steps to 0.
    save_summaries_steps=None,
    save_summaries_secs=None,
    config=tf.ConfigProto(allow_soft_placement=True),
    stop_grace_period_secs=120,
    log_step_count_steps=100) as mon_sess:
while not mon_sess.should_stop():
    mon_sess.run(_train_op)
</code></pre>

<p>The error messages are:</p>

<blockquote>
  <p>2017-12-29 10:33:30.699061: W tensorflow/core/framework/op_kernel.cc:1192] Not found: Key logit_modified/fc_1/biases/Momentum not found in checkpoint
  ...</p>
</blockquote>

<p>While It seems that the session trys to restore according to the modified graph but not the variables that exists both in the new graph and the previous checkpoint file(in other words all layers exclude the final 2).</p>

<p>Try 2: 
Inspired by the transfer learning code using tf.train.Supervisor here:
<a href=""https://github.com/kwotsin/transfer_learning_tutorial/blob/master/train_flowers.py"" rel=""nofollow noreferrer"">https://github.com/kwotsin/transfer_learning_tutorial/blob/master/train_flowers.py</a>, from line 251.</p>

<p>First I modified the code in resnet_model.py, add this line:</p>

<pre><code>self.variables_to_restore = tf.contrib.framework.get_variables_to_restore(
exclude=[""logit_modified"", ""logit_2""])
</code></pre>

<p>Then the scaffold in MonitoredTrainingSession is changed to:</p>

<pre><code>saver = tf.train.Saver(variables_to_restore)
def restore_fn(sess):
    return saver.restore(sess, FLAGS.log_root)
scaffold = tf.train.Scaffold(saver=saver, init_fn = restore_fn)
</code></pre>

<p>Unfortunately the followiing error message was shown:</p>

<blockquote>
  <p>RuntimeError: Init operations did not make model ready for local_init.  Init op: group_deps, init fn:  at 0x7f0ec26f4320>, error: Variables not initialized: logit_modified/fc_1/DW, ...</p>
</blockquote>

<p>Seems like the last 2 layers are not restored properly, so that the rest layers are not restored. </p>

<p>Try 3:
I also tried methods that list here:<a href=""https://stackoverflow.com/questions/43336553/how-to-use-tf-train-monitoredtrainingsession-to-restore-only-certain-variables"">How to use tf.train.MonitoredTrainingSession to restore only certain variables</a>, but none of them works.</p>

<p>I know there are others methods to restore such as the code in <a href=""https://github.com/tensorflow/models/blob/6fb14a790c283a922119b19632e3f7b8e5c0a729/research/inception/inception/inception_model.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/blob/6fb14a790c283a922119b19632e3f7b8e5c0a729/research/inception/inception/inception_model.py</a>, but they are nested and not general enough to be applied to other models easily. This is the reason why I want to use ""MonitoredTrainingSession"".</p>

<p>So how to use ""MonitoredTrainingSession"" to restore only part of checkpoint in tensorflow? </p>
",0
48065164,TensorFlow weighted_cross_entropy_with_logits produces wrong result,"<p>I am trying to use tf.nn.weighted_cross_entropy_with_logits API, but I found I just can not get the right result when the weight is not 1.0 (1.0 means no weight).</p>

<pre><code>import tensorflow as tf
import numpy as np

def my_binary_crossentropy_np(labels, output, weight=10.0):
  """"""
  Weighted binary crossentropy between an output tensor 
  and a target tensor.
  """"""
  # transform back to logits
  epsilon = 1e-08
  np.clip(output, epsilon, 1.0 - epsilon, out=output)
  output = np.log(output / (1.0 - output))

  # https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits 
  # l = 1 + (q - 1) * z
  # (1 - z) * x + l * (log(1 + exp(-abs(x))) + max(-x, 0))
  l = 1.0 + (weight - 1.0) * labels
  loss1 = np.multiply(1.0 - labels, output)
  loss2 = np.multiply(l, np.log(1.0 + np.exp(-abs(output))))
  loss3 = np.maximum(-output, 0)
  loss = loss1 + loss2 + loss3

  return np.mean(loss)


def my_binary_crossentropy_tf(labels, output, weight=1.0):
  """"""
  Weighted binary crossentropy between an output tensor 
  and a target tensor.
  """"""
  epsilon = 1e-08
  output = tf.clip_by_value(output, epsilon, 1.0 - epsilon)
  output = tf.log(output / (1.0 - output))

  # compute weighted loss
  #loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=output)
  loss = tf.nn.weighted_cross_entropy_with_logits(targets=labels, logits=output, pos_weight=weight)
  return tf.reduce_mean(loss)


# generate random test data and random label
predict = np.random.rand(10, 8)

label = np.random.rand(10, 8)
label[label &gt;= 0.5] = 1
label[label &lt; 0.5] = 0


loss1 = my_binary_crossentropy_np(label, predict, 1.0)
print('loss1 = ', loss1)

loss1 = my_binary_crossentropy_np(label, predict, 10.0)
print('loss1 = ', loss1)


predict_tf = tf.convert_to_tensor(predict)
loss2 = my_binary_crossentropy_tf(label, predict_tf, 1.0)
loss2 = tf.Session().run(loss2)
print('loss2 = ', loss2)

loss2 = my_binary_crossentropy_tf(label, predict_tf, 10.0)
loss2 = tf.Session().run(loss2)
print('loss2 = ', loss2)
</code></pre>

<p>running result:</p>

<pre><code>loss1 = 1.02193164517
loss1 = 1.96332399324

loss2 = 1.02193164517
loss2 = 4.80529539791
</code></pre>
",0
48101576,TensorFlow - Read video frames from TFRecords file,"<p><strong>TLDR;</strong> <em>my question is on how to load compressed video frames from TFRecords.</em></p>

<p>I am setting up a data pipeline for training deep learning models on a large video dataset (<a href=""https://deepmind.com/research/open-source/open-source-datasets/kinetics/"" rel=""noreferrer"">Kinetics</a>). For this I am using TensorFlow, more specifically the <code>tf.data.Dataset</code> and <code>TFRecordDataset</code> structures. As the dataset contains ~300k videos of 10 seconds, there is a large amount of data to deal with. During training, I want to randomly sample 64 consecutive frames from a video, therefore fast random sampling is important. For achieving this there are a number of data loading scenarios possible during training:</p>

<ol>
<li><strong>Sample from Video.</strong> Load the videos using <code>ffmpeg</code> or <code>OpenCV</code> and sample frames. Not ideal as seeking in videos is tricky, and decoding video streams is much slower than decoding JPG.</li>
<li><strong>JPG Images.</strong> Preprocess the dataset by extracting all video frames as JPG. This generates a huge amount of files, which is probably not going to be fast due to random access.</li>
<li><strong>Data Containers.</strong> Preprocess the dataset to <code>TFRecords</code> or <code>HDF5</code> files. Requires more work getting the pipeline ready, but most likely to be the fastest of those options.</li>
</ol>

<p>I have decided to go for option (3) and use <code>TFRecord</code> files to store a preprocessed version of the dataset. However, this is also not as straightforward as it seems, for example:</p>

<ol>
<li><strong>Compression.</strong> Storing the video frames as uncompressed byte data in TFRecords will require a huge amount of disk space. Therefore, I extract all the video frames, apply JPG compression and store the compressed bytes as TFRecords. </li>
<li><strong>Video Data.</strong> We are dealing with video, so each example in the TFRecords file will be quite large and contains several video frames (typically 250-300 for 10 seconds of video, depending on the frame rate). </li>
</ol>

<p>I have wrote the following code to preprocess the video dataset and write the video frames as TFRecord files (each of ~5GB in size):</p>

<pre><code>def _int64_feature(value):
    """"""Wrapper for inserting int64 features into Example proto.""""""
    if not isinstance(value, list):
        value = [value]
    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))

def _bytes_feature(value):
    """"""Wrapper for inserting bytes features into Example proto.""""""
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))


with tf.python_io.TFRecordWriter(output_file) as writer:

  # Read and resize all video frames, np.uint8 of size [N,H,W,3]
  frames = ... 

  features = {}
  features['num_frames']  = _int64_feature(frames.shape[0])
  features['height']      = _int64_feature(frames.shape[1])
  features['width']       = _int64_feature(frames.shape[2])
  features['channels']    = _int64_feature(frames.shape[3])
  features['class_label'] = _int64_feature(example['class_id'])
  features['class_text']  = _bytes_feature(tf.compat.as_bytes(example['class_label']))
  features['filename']    = _bytes_feature(tf.compat.as_bytes(example['video_id']))

  # Compress the frames using JPG and store in as bytes in:
  # 'frames/000001', 'frames/000002', ...
  for i in range(len(frames)):
      ret, buffer = cv2.imencode("".jpg"", frames[i])
      features[""frames/{:04d}"".format(i)] = _bytes_feature(tf.compat.as_bytes(buffer.tobytes()))

  tfrecord_example = tf.train.Example(features=tf.train.Features(feature=features))
  writer.write(tfrecord_example.SerializeToString())
</code></pre>

<p>This works fine; the dataset is nicely written as TFRecord files with the frames as compressed JPG bytes. My question regards, how to read the TFRecord files during training, randomly sample 64 frames from a video and decode the JPG images. </p>

<p>According to <a href=""https://www.tensorflow.org/programmers_guide/datasets"" rel=""noreferrer"">TensorFlow's documentation</a> on <code>tf.Data</code> we need to do something like: </p>

<pre><code>filenames = tf.placeholder(tf.string, shape=[None])
dataset = tf.data.TFRecordDataset(filenames)
dataset = dataset.map(...)  # Parse the record into tensors.
dataset = dataset.repeat()  # Repeat the input indefinitely.
dataset = dataset.batch(32)
iterator = dataset.make_initializable_iterator()
training_filenames = [""/var/data/file1.tfrecord"", ""/var/data/file2.tfrecord""]
sess.run(iterator.initializer, feed_dict={filenames: training_filenames})
</code></pre>

<p>There are many example on how to do this with images, and that is quite straightforward. However, for video and random sampling of frames I am stuck. The <code>tf.train.Features</code> object stores the frames as <code>frame/00001</code>, <code>frame/000002</code> etc. My first question is how to randomly sample a set of consecutive frames from this inside the <code>dataset.map()</code> function? Considerations are that each frame has a variable number of bytes due to JPG compression and need to be decoded using <code>tf.image.decode_jpeg</code>.</p>

<p>Any help how to best setup reading video sampels from TFRecord files would be appreciated! </p>
",0
48133855,Visualizing weights of trained Tensorflow model,"<p>I trained a model in TF an I would like to visualize its weights. Below is the code that I used to train the model. It looks like a lot of code but it's very straight forward. I just build the network, define the placeholders and train it with the optimize() function.</p>

<pre><code># Create the neural network
  def conv_net(x, reuse=tf.AUTO_REUSE):

# Define a scope for reusing the variables
    with tf.variable_scope('ConvNet', reuse=tf.AUTO_REUSE):

    # Reshape to match picture format [Height x Width x Channel]
    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]
    #x = tf.reshape(x, shape=[-1, height, width, channels])

    # -- FIRST BLOCK --
    # Two convolutional layers with 32 filters, size of 5, padding = 'same'
    # One max-pool layer 2X2
    # One dropout layer
      conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu, padding='SAME', name='conv1')
      conv1 = tf.layers.batch_normalization(conv1,training=training)
      conv2 = tf.layers.conv2d(conv1, 32, 5, activation=tf.nn.relu, padding='SAME', name='conv2')
      conv2 = tf.layers.batch_normalization(conv2, training=training)
      conv2 = tf.layers.max_pooling2d(conv2, 2, 2)
      conv2 = tf.layers.dropout(conv2, rate=dropout)

    # -- SECOND BLOCK --
    # Two convolutional layers with 32 filters, size of 3, padding = 'same'
    # One max-pool layer 2X2
    # One dropout layer with 1.5 dropout rate
      conv3 = tf.layers.conv2d(conv2, 32, 3, activation=tf.nn.relu, padding='SAME', name='conv3')
      conv3 = tf.layers.batch_normalization(conv3, training=training)
      conv4 = tf.layers.conv2d(conv3, 32, 3, activation=tf.nn.relu, padding='SAME', name='conv4')
      conv4 = tf.layers.batch_normalization(conv4, training=training)
      conv4 = tf.layers.max_pooling2d(conv4, 2, 2)
      conv4 = tf.layers.dropout(conv4, rate=dropout*1.25)

    # Flatten the data to a 1-D vector for fully connected layers
      fc1 = tf.contrib.layers.flatten(conv4)

    # -- THIRD BLOCK --
    # Two fully connected layers with 1024 neurons
    # 2x dropout rate on each layer
      fc1 = tf.layers.dense(fc1, 1024)
      fc1 = tf.layers.batch_normalization(fc1, training=training)
      fc1 = tf.layers.dropout(fc1, rate=dropout*2)
      fc2 = tf.layers.dense(fc1, 1024)
      fc2 = tf.layers.batch_normalization(fc2, training=training)
      fc2 = tf.layers.dropout(fc2, rate=dropout*2)

    # Output layer
      out = tf.layers.dense(fc2, num_classes)

  return out

#### Misc stuff tu run train part ####
# Predicted classes
logits_out = conv_net(images_placeholder, reuse=tf.AUTO_REUSE)

y_pred = tf.nn.softmax(logits_out, name='y_pred')
y_pred_cls = tf.argmax(y_pred, axis=1)

y_true_cls = tf.argmax(labels_placeholder, axis=1)

cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits_out,      labels=labels_placeholder)
cost = tf.reduce_mean(cross_entropy)

optimizer =  tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

correct_prediction = tf.equal(y_pred_cls, y_true_cls)
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

#Optimizer
def optimize(epochs):

  start_time = time.time()

  for i in range(epochs):

    indices = np.random.choice(trainX.shape[0], batch_size)
    x_batch, y_true_batch = trainX[indices], trainY[indices]

    batch_norm_update = tf.get_collection(tf.GraphKeys.UPDATE_OPS)

    session.run([optimizer, batch_norm_update],  feed_dict={images_placeholder:x_batch,
                                                            labels_placeholder: y_true_batch,
                                                            training: True})

    saver.save(session, 'C:\\Users\\ggiorcelli\\Tensorflow Workbooks\\animals_model') 


    if (i+1) % 10 == 0:
        train_accuracy = session.run(accuracy, feed_dict={images_placeholder: x_batch, 
                                                          labels_placeholder: y_true_batch,
                                                          training: True})

        indices_test = np.random.choice(testX.shape[0], batch_size)
        images_test = testX[indices_test]
        labels_test = testY[indices_test]

        test_accuracy = session.run(accuracy, feed_dict={images_placeholder: images_test, 
                                                         labels_placeholder: labels_test,
                                                         training: True})

        print('Step {:3d}: training accuracy = {:0.3f}, test accuracy = {:0.3f}'.format(i+1, train_accuracy, test_accuracy))

  end_time = time.time()
  time_dif = end_time - start_time

  print(""Time usage: "" + str(timedelta(seconds=int(round(time_dif)))))

#Train
with tf.Session() as session:
  init = tf.global_variables_initializer()
  session.run(init, {training:True})
  optimize(epochs=200)
</code></pre>

<p>With this code I'm able to train the model successfully.
Now, I would like to get the weights of the convolutional layer. To do so, I use this code snippet:</p>

<pre><code>with tf.variable_scope('conv1', reuse=True):
  w1 = tf.get_variable('weights')
  x_image = tf.reshape(w1, [5, 5, 1, 32])
  img = tf.summary.image('img',x_image)
</code></pre>

<p>However with the code above I only get an image object that I cannot visualize. I'm not even sure if this is the right way to get the weights. I've seen that other people have asked this question but most answers lead to Tensorboard and I want to do this is in an iPython shell.</p>

<p>Below is an example of what I would like to see.</p>

<p><a href=""https://i.stack.imgur.com/8iesP.png"" rel=""nofollow noreferrer"">Conv Layer Weights 1</a></p>

<p><a href=""https://i.stack.imgur.com/gZZLm.png"" rel=""nofollow noreferrer"">Conv Layer Weights 2</a></p>
",0
48174323,Tensorflow 1.4 tf.metrics.auc for AUC calculation,"<p>I am trying to log <strong>AUC</strong> during <strong>training time</strong> of my model.</p>

<p>According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/metrics/auc"" rel=""nofollow noreferrer"">documentation</a>, <code>tf.metric.auc</code> needs a <code>label</code> and <code>predictions</code>, both of same shape.  </p>

<p>But in my case of binary classification, <code>label</code> is a <strong>one-dimensional</strong> tensor, containing just the classes. And <code>prediction</code> is <strong>two-dimensional</strong> containing probability for each class of each datapoint. </p>

<p>How to calculate <strong>AUC</strong> in this case? </p>
",1
48213272,Equivalent API of `tf.norm` of Tensorflow 1.+ for Tensorflow0.11,"<p>I want to run <a href=""https://github.com/rothk/Stabilizing_GANs#js-regularizer--regularized-gan-objective"" rel=""nofollow noreferrer"">this code</a>. It's written in Py3 and TF v1.4+ However it throws a an error. The <a href=""https://www.tensorflow.org/api_docs/python/tf/norm"" rel=""nofollow noreferrer""><code>tf.norm</code></a> in <code>Discriminator_Regularizer</code> function doesn't exist in TF 0.11. I couldn't find the equivalent in TF v0.11. I can't upgrade for some CUDA/Cudnn build/install issues. </p>

<p>I used <a href=""https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.norm.html"" rel=""nofollow noreferrer""><code>numpy.linalg.norm</code></a> instead but it gives a conjugate error, which is perhaps because of the type error of the Graph nodes. </p>

<p>How can I implement <code>tf.norm</code> in the earlier versions?</p>
",0
48216183,Modifying a restored TensorFlow model,"<p>I have trained a TensorFlow (TF) CNN model on the MNIST dataset and stored the model after training using <em>tf.train.Saver()</em>. Now, I'd like to restore this model and add an operation between two layers before retraining it, e.g., adding <em>tf.stop_gradient()</em> between the output of the first convolution layer (named 'conv1') and the first pooling layer (named 'pool1'). Is that possible? If so, how could I do it?</p>

<p>Here is my code:</p>

<pre><code>import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets(""/home/frr/MNIST_data"", one_hot=True)

learning_rate = 0.001
training_iters = 100000
batch_size = 64
display_step = 20
ImVecDim = 784
NumOfClasses = 10
dropout = 0.8

g = tf.get_default_graph()

with tf.Session() as sess:
   LoadMod = tf.train.import_meta_graph('simple_mnist.ckpt.meta')
   LoadMod.restore(sess, tf.train.latest_checkpoint('./'))

#############################################################################
#     Adding a tf.stop_gradient() operation between 'conv1' and 'pool1'     #
#############################################################################

   x = g.get_tensor_by_name('ImageIn:0')
   y = g.get_tensor_by_name('LabelIn:0')
   keep_prob = g.get_tensor_by_name('KeepProb:0')
   accuracy = g.get_tensor_by_name('NetAccuracy:0')
   optimizer = g.get_operation_by_name('Adam')
   step = 1
   while step * batch_size &lt; training_iters:
       batch_xs, batch_ys = mnist.train.next_batch(batch_size)

       sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout})
       if step % display_step == 0:
          acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})
          print(""Iter "" + str(step*batch_size) + "", Training Accuracy= "" + ""{:.5f}"".format(acc))
   step += 1
   print(""Optimization Finished!"")
   print(""Testing Accuracy:"",
   sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256], keep_prob: 1.}))
</code></pre>
",0
48281583,Tensorflow manual evaluation with tf.estimators,"<p>I'm using Tensorflow api <code>tf.estimator</code> to train and evaluate models, building a custom estimator as explained <a href=""https://developers.googleblog.com/2017/12/creating-custom-estimators-in-tensorflow.html"" rel=""nofollow noreferrer"">in this blog</a>, and running the process with this convenient way:</p>
<pre><code>run_config = tf.estimator.RunConfig(save_checkpoints_secs=save_interval_secs,
                                    keep_checkpoint_max=keep_checkpoint_max,
                                    save_summary_steps=save_summary_steps,
                                    model_dir=logdir)
estimator = tf.estimator.Estimator(model_fn=model_fn,
                                   config=run_config)
train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=train_steps)
eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, steps=eval_steps, throttle_secs=eval_interval_secs)
tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
</code></pre>
<p>This process is very efficient to train and monitor <code>tf.metrics</code> metrics into Tensorboard (e.g. with <code>tf.metrics.accuracy</code>).</p>
<p>For one of my projects, I would like to add some more (complicated) metrics coming from a standalone python function, and visualize them in the Tensorboard. This function would take as an argument the training logdir (to load the model and run it on data) and would return some scalar (or tensor) to be monitored into the Tensorboard.</p>
<p>Last but not least, the function will need the GPU resources, so I expect this function to be called after evaluation:</p>
<pre><code>Train xx steps =&gt; Evaluation xx steps =&gt; Custom metrics =&gt; Train xx steps =&gt; ...
</code></pre>
<p>I was thinking about using <code>tf.summary.FileWriter</code> and/or Training Hooks (see <a href=""https://web.archive.org/web/20180925064834/https://www.tensorflow.org/api_guides/python/train#Training_Hooks"" rel=""nofollow noreferrer"">Tensorflow documentation</a>) but I don't know how to use them in a straightforward way with <code>tf.estimator</code> api.</p>
<p>Thank you very much for your help!</p>
<p>M.</p>
",0
48299597,How to efficiently shuffle a large tf.data.Dataset when using tf.estimator.train_and_evaluate?,"<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate"" rel=""nofollow noreferrer""><code>tf.estimator.train_and_evaluate</code></a> documentation makes it clear that the input dataset must be properly shuffled for the training to see all examples:</p>

<blockquote>
  <p>Overfitting: In order to avoid overfitting, it is recommended to set up the training input_fn to shuffle the training data properly. It is also recommended to train the model a little longer, say multiple epochs, before performing evaluation, as the input pipeline starts from scratch for each training. It is particularly important for local training and evaluation.</p>
</blockquote>

<p>In my application, I would like to uniformly sample examples from the full <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer""><code>tf.data.Dataset</code></a> with arbitrary evaluation frequency and <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle"" rel=""nofollow noreferrer""><code>shuffle()</code></a>'s buffer size. Otherwise, the training can at most see the first:</p>

<pre><code>(steps_per_second * eval_delay * batch_size) + buffer_size
</code></pre>

<p>elements, effectively discarding the rest. Is there an efficient way to work around that without loading the complete dataset in the system memory?</p>

<p>I considered <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shard"" rel=""nofollow noreferrer"">sharding</a> the dataset based on the buffer size, but if the evaluation does not occur frequently, it will iterate on the same shard multiple times (a <code>repeat()</code> closes the pipeline). Ideally, I would like to move to another shard after a complete iteration over the dataset, is that possible?</p>

<p>Thanks for any pointers!</p>
",0
48309631,TensorFlow - tf.data.Dataset reading large HDF5 files,"<p>I am setting up a TensorFlow pipeline for reading large HDF5 files as input for my deep learning models. Each HDF5 file contains 100 videos of variable size length stored as a collection of compressed JPG images (to make size on disk manageable). Using <code>tf.data.Dataset</code> and a map to <code>tf.py_func</code>, reading examples from the HDF5 file using custom Python logic is quite easy. For example:</p>

<pre><code>def read_examples_hdf5(filename, label):
    with h5py.File(filename, 'r') as hf:
        # read frames from HDF5 and decode them from JPG
    return frames, label

filenames = glob.glob(os.path.join(hdf5_data_path, ""*.h5""))
labels = [0]*len(filenames) # ... can we do this more elegantly?

dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))
dataset = dataset.map(
    lambda filename, label: tuple(tf.py_func(
        read_examples_hdf5, [filename, label], [tf.uint8, tf.int64]))
)

dataset = dataset.shuffle(1000 + 3 * BATCH_SIZE)
dataset = dataset.batch(BATCH_SIZE)
iterator = dataset.make_one_shot_iterator()
next_batch = iterator.get_next()
</code></pre>

<p>This example works, however the problem is that it seems like <code>tf.py_func</code> can only handle one example at a time. As my HDF5 container stores 100 examples, this limitation causes significant overhead as the files constantly need to be opened, read, closed and reopened. It would be much more efficient to read all the 100 video examples into the dataset object and then move on with the next HDF5 file (preferably in multiple threads, each thread dealing with it's own collection of HDF5 files).</p>

<p>So, what I would like is a number of threads running in the background, reading video frames from the HDF5 files, decode them from JPG and then feed them into the dataset object. Prior to the introduction of the <code>tf.data.Dataset</code> pipeline, this was quite easy using the <code>RandomShuffleQueue</code> and <code>enqueue_many</code> ops, but it seems like there is currently no elegant way of doing this (or the documentation is lacking). </p>

<p>Does anyone know what would be the best way of achieving my goal? I have also looked into (and implemented) the pipeline using <code>tfrecord</code> files, but taking a random sample of video frames stored in a <code>tfrecord</code> file seems quite impossible (see <a href=""https://stackoverflow.com/questions/48101576/tensorflow-read-video-frames-from-tfrecords-file"">here</a>). Additionally, I have looked at the <code>from_generator()</code> inputs for <code>tf.data.Dataset</code> but that is definitely not going to run in multiple threads it seems. Any suggestions are more than welcome.</p>
",1
48316625,"In a two class issue with one-hot label, why tf.losses.softmax_cross_entropy outputs very large cost","<p>I am training a mobilenet for semantic segmentation on tf. The targets has two classes: <em>foreground(1)</em> or <em>background(0)</em>. So this is a two class classification issue.
I choose softmax cross entropy as the loss, using python code like: </p>

<pre><code>tf.losses.softmax_cross_entropy(self.targets, logits)
</code></pre>

<p>Size of targets and logits is <code>[batch_size, 224, 224, 2]</code>. However, the loss becomes very big after 100 batches, curve is like this:</p>

<p><a href=""https://i.stack.imgur.com/N2tXw.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/N2tXw.jpg"" alt=""enter image description here""></a></p>

<p>From tf's api docus, I know that <code>tf.losses.softmax_cross_entropy</code> has [batch_size, num_classes] target one-hot-encoded labels, which coincides with my label with size of <code>[batch_size,224,224,2]</code>, either 1 or 0 (which is exclusive).
So, softmax_cross_entropy can't be used in a two class one-hot label case like this and why? If it can be used, where is my problem?</p>

<p>If I use <code>tf.losses.sigmoid_cross_entropy</code> or <code>tf.losses.sparse_softmax_cross_entropy</code>(only give label size: <code>[batch_size, 224,224,1]</code>), the loss will converge.</p>
",0
48427269,What's the efficient way to feed elements from Iterator (from tf.data.Dataset) into TensorFlow model?,"<p>I'm using TensrFlow's new API for importing data via <code>tf.data.Dataset</code> and iterators. It is working fine, but I'm not sure if what I do is efficient. </p>

<p>What I'm doing at the moment is evaluating an iterator's <code>get_next()</code> method, which gives me a bunch of elements like the actual image, its label, filename, etc. I then feed the image into my model using the <code>feed_dict</code>. </p>

<p>I know that <code>feed_dict</code> is very slow, so am I losing benefits of <code>Dataset</code> and Iterators and having serialised dataset in <code>TFRecord</code>s by evaluating the entries and feeding them into the graph via <code>feed_dict</code>? I haven't found any examples in TF's documentation which shows how one's expected to use Iterator's <code>get_next()</code> to feed elements into the model. Is it better to unpack <code>get_next()</code> and use the result directly in my graph? </p>
",1
48435145,What does tf.contrib.slim.prefetch_queue do in tensorflow,"<p>I saw an example of training cifar10 data using tensorflow:
<a href=""https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10</a></p>

<p>the code generate a batch of images from several single image using tf.train.batch
and create a queue of batchs using prefetch_queue. I understand it is necessary to 
use queues to pre-fetch data when training data is large. I guess tf.train.batch maintains
a queue internally (because it has a capacity parameter). Since a queue of batches is already maintained in tf.train.battch, is it necessary to create
another queue with tf.contrib.slim.prefetch_queue? what does tf.contrib.slim.prefetch_queue do exactly?</p>

<p>the key parts of the cifar-10 example code is shown below:</p>

<pre><code>import tensorflow as tf

images, labels = tf.train.batch(
    [image, label],
    batch_size=...,
    num_threads=...,
    capacity=...,
    min_after_dequeue=...)

batch_queue = tf.contrib.slim.prefetch_queue.prefetch_queue(
    [images, labels], 
    capacity=...)
</code></pre>
",0
48445751,Keras: Constrained dictionary search with CTC decode,"<p>I'm trying to constrain the CTC decoding to a specific (external) dictionary in Keras with a the tensorflow backend. In the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/backend/ctc_decode"" rel=""nofollow noreferrer"">tensorflow documentation for Keras' ctc_decode</a>, it is written that when <code>greedy=False</code> a dictionary will be used. Here is the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/ctc_beam_search_decoder"" rel=""nofollow noreferrer"">documentation for  tf.nn.ctc_beam_search_decoder</a>, which will be called by this option as far as I understand.</p>

<p>Since there is no way to pass an external dictionary or language model (to constrain the search), I assume that with <code>greedy=False</code> it creates its own dictionary from the training data. Is this correct? Is there a way to constrain the search to a specific (external) dictionary?</p>
",1
48452372,Invalid argument error when using tf.reshape(),"<p>I am using some MNIST tutorials on convolutional neural networks to develop my own which can classify 15x15 into one of two classes.</p>

<p>When defining the convolutional network I have encountered an invalid argument error but I can't figure out where I am going wrong. Here is the code I am using to define the conv net:</p>

<pre><code>def convolutional_neural_network(x):
weights = {'W_conv1':tf.Variable(tf.random_normal([5,5,1,32])),
           'W_conv2':tf.Variable(tf.random_normal([5,5,32,64])),
           'W_fc':tf.Variable(tf.random_normal([3*3*64,1024])),
           'out':tf.Variable(tf.random_normal([1024, n_classes]))}

biases = {'b_conv1':tf.Variable(tf.random_normal([32])),
           'b_conv2':tf.Variable(tf.random_normal([64])),
           'b_fc':tf.Variable(tf.random_normal([1024])),
           'out':tf.Variable(tf.random_normal([n_classes]))}

x = tf.reshape(x, shape=[-1, 15, 15, 1])

conv1 = tf.nn.relu(conv2d(x, weights['W_conv1']) + biases['b_conv1'])
conv1 = maxpool2d(conv1)

conv2 = tf.nn.relu(conv2d(conv1, weights['W_conv2']) + 
biases['b_conv2'])
conv2 = maxpool2d(conv2)

fc = tf.reshape(conv2,[-1, 3*3*64])
fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])
fc = tf.nn.dropout(fc, keep_rate)

output = tf.matmul(fc, weights['out'])+biases['out']

return output
</code></pre>

<p>The error it throws looks like this:</p>

<pre><code>Traceback (most recent call last):
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1323, in _do_call
    return fn(*args)
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1302, in _run_fn
    status, run_metadata)
  File ""/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Input to reshape is a tensor with 4096 values, but the requested shape requires a multiple of 576
     [[Node: Reshape_1 = Reshape[T=DT_FLOAT, Tshape=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](MaxPool_1, Reshape_1/shape)]]
</code></pre>

<p>I can see where the <code>tf.reshape()</code> is requesting a tensor of size 576, however I don't understand where the tensor of size 4096 is coming from.</p>
",0
48471674,tf.layers.batch_normalization throws data_type error while using Tensorflow v 1.4 but not 1.0,"<p>The following statement,
 bn1 = tf.contrib.layers.batch_normalization(inputs=conv1, axis=1, training = is_training)
Throws an error: InternalError: The CPU implementation of FusedBatchNorm only supports NHWC tensor format for now.
on my CPU using tensorflow v:1.4
However, I've ensured that the code uses data in NHWC format. The same piece of code works on my friend's CPU, the only difference being he's using Tensorflow v.1.0 and the code runs smoothly without issues.</p>

<p>I tried to look up tensorflow documentation,
<a href=""https://www.tensorflow.org/performance/performance_guide"" rel=""nofollow noreferrer"">https://www.tensorflow.org/performance/performance_guide</a>
It suggests feeding in two extra arguments: fused=True, data_format='NHWC'.</p>

<p>However, as per
<a href=""https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization</a>
there is no such provision for the 2 above mentioned arguments. And in fact, the code throws an error saying batch_normalization received an unexpected argument.</p>

<p>Any responses on the potential reason behind the issue and how I could get around it without rolling back my Tensorflow version (because that would be absurd) are most welcome.
Thank you so uch for your time and effort.</p>
",0
48471926,In Tensorflow's Dataset API how do you map one element into multiple elements?,"<p>In the tensorflow <code>Dataset</code> pipeline I'd like to define a custom map function which takes a single input element (data sample) and returns multiple elements (data samples).</p>

<p>The code below is my attempt, along with the desired results. </p>

<p>I could not follow the documentation on <code>tf.data.Dataset().flat_map()</code> well enough to understand if it was applicable here or not.</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

input = [10, 20, 30]

def my_map_func(i):
  return [[i, i+1, i+2]]       # Fyi [[i], [i+1], [i+2]] throws an exception

ds = tf.data.Dataset.from_tensor_slices(input)
ds = ds.map(map_func=lambda input: tf.py_func(
  func=my_map_func, inp=[input], Tout=[tf.int64]
))
element = ds.make_one_shot_iterator().get_next()

with tf.Session() as sess:
  for _ in range(9):
    print(sess.run(element))
</code></pre>

<p>Results:</p>

<pre><code>(array([10, 11, 12]),)
(array([20, 21, 22]),)
(array([30, 31, 32]),)
</code></pre>

<p>Desired results:</p>

<pre><code>(10)
(11)
(12)
(20)
(21)
(22)
(30)
(31)
(32)
</code></pre>
",1
48507778,keep_prob value in dropout and getting worst results with dropout,"<p>According to this link, the value of keep_prob has to be between (0,1]:
<a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dropout"" rel=""nofollow noreferrer"">Tensorflow manual</a></p>

<p>Otherwise I'll get value error:</p>

<pre><code>ValueError: If keep_prob is not in (0, 1] or if x is not a floating point tensor.
</code></pre>

<p>I'm using the following code for a simple neural network with one hidden layer:</p>

<pre><code>n_nodes_input = len(train_x.columns) # number of input features
n_nodes_hl = 30     # number of units in hidden layer
n_classes = len(np.unique(Y_train_numeric)) 
lr = 0.25
x = tf.placeholder('float', [None, len(train_x.columns)])
y = tf.placeholder('float')
dropout_keep_prob = tf.placeholder(tf.float32)

def neural_network_model(data, dropout_keep_prob):
    # define weights and biases for all each layer
    hidden_layer = {'weights':tf.Variable(tf.truncated_normal([n_nodes_input, n_nodes_hl], stddev=0.3)),
                      'biases':tf.Variable(tf.constant(0.1, shape=[n_nodes_hl]))}
    output_layer = {'weights':tf.Variable(tf.truncated_normal([n_nodes_hl, n_classes], stddev=0.3)),
                    'biases':tf.Variable(tf.constant(0.1, shape=[n_classes]))}
    # feed forward and activations
    l1 = tf.add(tf.matmul(data, hidden_layer['weights']), hidden_layer['biases'])
    l1 = tf.nn.sigmoid(l1)
    l1 = tf.nn.dropout(l1, dropout_keep_prob)
    output = tf.matmul(l1, output_layer['weights']) + output_layer['biases']

    return output

def main():
    prediction = neural_network_model(x, dropout_keep_prob)
    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y,logits=prediction))
    optimizer = tf.train.AdamOptimizer(lr).minimize(cost)

    sess = tf.InteractiveSession()

    tf.global_variables_initializer().run()
    for epoch in range(1000):
        loss = 0
        _, c = sess.run([optimizer, cost], feed_dict = {x: train_x, y: train_y, dropout_keep_prob: 4.})
        loss += c

        if (epoch % 100 == 0 and epoch != 0):
            print('Epoch', epoch, 'completed out of', 1000, 'Training loss:', loss)
    correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))
    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='op_accuracy')

    writer = tf.summary.FileWriter('graph',sess.graph)
    writer.close()

    print('Train set Accuracy:', sess.run(accuracy, feed_dict = {x: train_x, y: train_y, dropout_keep_prob: 1.}))
    print('Test set Accuracy:', sess.run(accuracy, feed_dict = {x: test_x, y: test_y, dropout_keep_prob: 1.}))
    sess.close()


if __name__ == '__main__':
     main()
</code></pre>

<p>If I use a number in range (0,1] for dropout_keep_prob in the sess.run, the accuracy drops drastically. If I use a number bigger than 1, like 4, the accuracy goes beyond 0.9. 
Once I use shift+tab in front of tf.nn.dropout(), this is written as part of description:</p>

<pre><code>With probability `keep_prob`, outputs the input element scaled up by
`1 / keep_prob`, otherwise outputs `0`.  The scaling is so that the expected
sum is unchanged.
</code></pre>

<p>which seems to me that keep_prob has to be greater than 1 otherwise nothing would be dropped!</p>

<p>Bottom line, I'm confused. Which part of dropout have I implemented wrong that my results are getting worst and what is a good number for keep_drop?</p>

<p>Thank you</p>
",0
48529169,Which dim to use on tf.metrics.mean_cosine_distance?,"<p>I'm confused about which <code>dim</code> refers to which actual dimension in Tensorflow in general, but concretely, when using <a href=""https://www.tensorflow.org/api_docs/python/tf/metrics/mean_cosine_distance"" rel=""nofollow noreferrer"">tf.metrics.mean_cosine_distance</a></p>

<p>Given</p>

<pre><code>x = [
   [1, 2, 3, 4, 5],
   [0, 2, 3, 4, 5],
]
</code></pre>

<p>I'd like to calculate the distance column-wise. In other words, which dimension resolves to (pseudo code):</p>

<pre><code>mean([
    cosine_distance(x[0][0], x[1][0]),
    cosine_distance(x[0][1], x[1][1]),
    cosine_distance(x[0][2], x[1][2]),
    cosine_distance(x[0][3], x[1][3]),
    cosine_distance(x[0][4], x[1][4]),
])
</code></pre>
",0
48621769,Attentive Convolution with keras,"<p>I've implemented an attentive convolution layer in keras, as described in this <a href=""https://arxiv.org/pdf/1710.00519.pdf"" rel=""nofollow noreferrer"">paper</a>.</p>

<p>You can see the code for it at this <a href=""https://gist.github.com/stoney95/9c71bb7f7008b7d01dcf8f6bf3afaeff"" rel=""nofollow noreferrer"">gist</a></p>

<p>I'm new to implementing custom layers and it is still very slow. I'm using a lot of <code>tf.map_fn</code> and I think this is the reason why it's so slow, but I don't know a different way to do this.
It would be nice if someone has some tips how to improve the layer or general tips on implementing custom layers like how to avoid backend (tensorflow) functions.</p>

<p>I'm using keras 2.1.3 and tensorflow 1.5 as backend.</p>

<p>Thanks</p>
",0
48679622,Restoring a model trained with tf.estimator and feeding input through feed_dict,"<p>I trained a resnet with tf.estimator, the model was saved during the training process. The saved files consist of <code>.data</code>, <code>.index</code> and <code>.meta</code>. I'd like to load this model back and get predictions for new images. The data was fed to the model during training using <code>tf.data.Dataset</code>.  I have closely followed the resnet implementation given <a href=""https://github.com/deepaksuresh/models/blob/master/official/resnet/resnet.py"" rel=""nofollow noreferrer"">here</a>.</p>

<p>I would like to restore the model and feed inputs to the nodes using a feed_dict. </p>

<p><strong>First attempt</strong> </p>

<pre><code>  #rebuild input pipeline
  images, labels = input_fn(data_dir, batch_size=32, num_epochs=1)

  #rebuild graph
  prediction= imagenet_model_fn(images,labels,{'batch_size':32,'data_format':'channels_first','resnet_size':18},mode = tf.estimator.ModeKeys.EVAL).predictions 

  saver  = tf.train.Saver()
  with tf.Session() as sess:
    ckpt = tf.train.get_checkpoint_state(r'./model')
    saver.restore(sess, ckpt.model_checkpoint_path)
    while True:
    try:
        pred,im= sess.run([prediction,images])
        print(pred)
    except tf.errors.OutOfRangeError:
      break
</code></pre>

<p>I fed a dataset which was evaluated on the same model using <code>classifier.evaluate</code>, but the above method gives wrong predictions. The model gives same class and probability, 1.0, for all images.</p>

<p><strong>Second attempt</strong></p>

<pre><code>saver = tf.train.import_meta_graph(r'.\resnet\model\model-3220.meta')
sess = tf.Session()
saver.restore(sess,tf.train.latest_checkpoint(r'.\resnet\model'))
graph = tf.get_default_graph()
inputImage = graph.get_tensor_by_name('image:0')
logits= graph.get_tensor_by_name('logits:0')

#Get prediction
print(sess.run(logits,feed_dict={inputImage:newimage}))
</code></pre>

<p>This also gives wrong predictions compared to <code>classifier.evaluate</code>. I can even run <code>sess.run(logits)</code> without a <code>feed_dict</code>!</p>

<p><strong>Third attempt</strong></p>

<pre><code>def serving_input_fn():
  receiver_tensor = {'feature': tf.placeholder(shape=[None, 384, 256, 3], dtype=tf.float32)}
  features = {'feature': receiver_tensor['images']}
return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)
</code></pre>

<p>It fails with </p>

<pre><code>Traceback (most recent call last):
  File ""imagenet_main.py"", line 213, in &lt;module&gt;
    tf.app.run(argv=[sys.argv[0]] + unparsed)
  File ""C:\Users\Photogauge\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""imagenet_main.py"", line 204, in main
    resnet.resnet_main(FLAGS, imagenet_model_fn, input_fn)
  File ""C:\Users\Photogauge\Desktop\iprings_images\models-master\models-master\official\resnet\resnet.py"", line 527, in resnet_main
    classifier.export_savedmodel(export_dir_base=r""C:\Users\Photogauge\Desktop\iprings_images\models-master\models-master\official\resnet\export"", serving_input_receiver_fn=serving_input_fn)
  File ""C:\Users\Photogauge\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 528, in export_savedmodel
    config=self.config)
  File ""C:\Users\Photogauge\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 725, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""imagenet_main.py"", line 200, in imagenet_model_fn
    loss_filter_fn=None)
  File ""C:\Users\Photogauge\Desktop\iprings_images\models-master\models-master\official\resnet\resnet.py"", line 433, in resnet_model_fn
    tf.argmax(labels, axis=1), predictions['classes'])
  File ""C:\Users\Photogauge\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\deprecation.py"", line 316, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\Photogauge\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 208, in argmax
    return gen_math_ops.arg_max(input, axis, name=name, output_type=output_type)
  File ""C:\Users\Photogauge\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 508, in arg_max
    name=name)
  File ""C:\Users\Photogauge\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 528, in _apply_op_helper
    (input_name, err))
ValueError: Tried to convert 'input' to a tensor and failed. Error: None values not supported.
</code></pre>

<p>The code I used for training and building the model is as below:</p>

<p>Specification for parsing the dataset:</p>

<pre><code>def parse_record(raw_record, is_training):
  keys_to_features = {
      'image/encoded':
          tf.FixedLenFeature((), tf.string, default_value=''),
      'image/class/label':
          tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),
  }
  parsed = tf.parse_single_example(raw_record, keys_to_features)
  image = tf.image.decode_image(
      tf.reshape(parsed['image/encoded'], shape=[]),3)
  image = tf.image.convert_image_dtype(image, dtype=tf.float32)
  label = tf.cast(
      tf.reshape(parsed['image/class/label'], shape=[]),
      dtype=tf.int32)
  return image, tf.one_hot(label,2)
</code></pre>

<p>The following function parses the data and creates batches for training</p>

<pre><code>def input_fn(is_training, data_dir, batch_size, num_epochs=1):
  dataset = tf.data.Dataset.from_tensor_slices(
      filenames(is_training, data_dir))
  if is_training:
     dataset = dataset.shuffle(buffer_size=_FILE_SHUFFLE_BUFFER)
  dataset = dataset.flat_map(tf.data.TFRecordDataset)
  dataset = dataset.map(lambda value: parse_record(value, is_training),
                        num_parallel_calls=5)
  dataset = dataset.prefetch(batch_size)
  if is_training:
      dataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)
  dataset = dataset.repeat(num_epochs)
  dataset = dataset.batch(batch_size)

  iterator = dataset.make_one_shot_iterator()
  images, labels = iterator.get_next()
  return images, labels
</code></pre>

<p>A classifier is created as below for training on train set and evaluation on validation set</p>

<pre><code>classifier = tf.estimator.Estimator(
      model_fn=model_function, model_dir=flags.model_dir, config=run_config,
      params={
          'resnet_size': flags.resnet_size,
          'data_format': flags.data_format,
          'batch_size': flags.batch_size,
      })

    #Training cycle
     classifier.train(
         input_fn=lambda: input_function(
             training_phase=True, flags.data_dir, flags.batch_size, flags.epochs_per_eval),
         hooks=[logging_hook])
    # Evaluate the model 
    eval_results = classifier.evaluate(input_fn=lambda: input_function(
        training_phase=False, flags.data_dir, flags.batch_size))
</code></pre>

<p>This is how I tried to load and get predictions from the model.</p>

<p>What is the right way to restore a saved model and perform inference on it. I want to feed images directly without using <code>tf.data.Dataset</code>. </p>

<p><strong>Update</strong></p>

<ol>
<li><p>The value of <code>ckpt</code> is after running <code>ckpt = tf.train.get_checkpoint_state(r'./model')</code> is </p>

<p>model_checkpoint_path: ""./model\model.ckpt-5980""
all_model_checkpoint_paths: ""./model\model.ckpt-5060""
all_model_checkpoint_paths: ""./model\model.ckpt-5061""
all_model_checkpoint_paths: ""./model\model.ckpt-5520""
all_model_checkpoint_paths: ""./model\model.ckpt-5521""
all_model_checkpoint_paths: ""./model\model.ckpt-5980""</p></li>
<li><p>The output is same when I try `saver.restore(sess, tf.train.latest_checkpoint(r'.\resnet\model'))</p></li>
<li><p>Passing in full path to <code>saver.restore</code> gives the same output
In all cases the same model, <code>model.ckpt-5980</code> was restored</p></li>
</ol>
",0
48682703,How to set parameter weights in tf.losses.sigmoid_cross_entropy?,"<p>I'm now trying to use <code>tf.losses.sigmoid_cross_entropy</code> on an unbalanced dataset. However, I'm a little confused on the parameter weights. Here are the comments in the documentation:</p>

<blockquote>
  <p>weights: Optional Tensor whose rank is either 0, or the same rank as
  labels, and must be broadcastable to labels (i.e., all dimensions must
  be either 1, or the same as the corresponding losses dimension).</p>
</blockquote>

<p>I know in <code>tf.losses.softmax_cross_entropy</code> the parameter weights can be a rank 1 tensor with weight for each sample. Why must the weights in <code>tf.losses.sigmoid_cross_entropy</code> have the same rank as labels? </p>

<p>Can anybody answer me? Better with an example.</p>
",1
48697799,Tensorflow feature column for variable list of values,"<p>From the TensorFlow docs it's clear how to use <code>tf.feature_column.categorical_column_with_vocabulary_list</code> to create a feature column which takes as input some string and outputs a one-hot vector. For example</p>

<pre><code>vocabulary_feature_column =
    tf.feature_column.categorical_column_with_vocabulary_list(
        key=""vocab_feature"",
        vocabulary_list=[""kitchenware"", ""electronics"", ""sports""])
</code></pre>

<p>Let's say <code>""kitchenware""</code> maps to <code>[1,0,0]</code> and <code>""electronics""</code> maps to <code>[0,1,0]</code>. My question is related to having a <strong>list of strings</strong> as a feature. For example, if the feature value was <code>[""kitchenware"",""electronics""]</code> then the desired output would be <code>[1,1,0]</code>. The input list length is not fixed but the output dimension is.</p>

<p>The use case is a straight bag-of-words type model (obviously with a much larger vocabulary list!).</p>

<p>What is the correct way to implement this?</p>
",1
48773872,How to interpret tf.layers.dropout training arg,"<p>For <code>tf.layers.dropout()</code> the documentation for the <code>training</code> arg is not clear to me.</p>
<p>The <a href=""https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/python/layers/core.py#L340"" rel=""noreferrer"">documentation</a> states:</p>
<pre><code>training: Either a Python boolean, or a TensorFlow boolean scalar tensor
      (e.g. a placeholder). Whether to return the output in training mode
      (apply dropout) or in inference mode (return the input untouched).
</code></pre>
<p>My interpretation is that depending on if <code>training = True</code> or <code>training = False</code> the dropout will be applied. However, it's not clear to me if <code>True</code> or <code>False</code> will apply the dropout (ie. which is in training mode). Given that this is an optional argument, I expected that tf.layers.dropout() would apply by default, but the default is <code>False</code> which intuitively <code>training=False</code> would suggest that the default is not training.</p>
<p>It appears that in order for tf.layers.dropout() to actually apply, one would need something like:</p>
<p><code>tf.layers.dropout(input, 0.5, training = mode == Modes.TRAIN)</code></p>
<p>This is not very obvious to me from the documentation as <code>training</code> is an optional argument.</p>
<p>Does this appear to be the correct implementation of <code>tf.layers.dropout</code>? Why would the <code>training</code> flag just not automatically be tied to <code>Modes.TRAIN</code> as the default and then need to be adjusted for other cases? The default being <code>training=False</code> seems to be very misleading</p>
",1
48789532,How to create a serving input function to provide a prediction for images on google cloud platform?,"<p>From <a href=""https://cloud.google.com/ml-engine/docs/online-predict#binary_data_in_prediction_input"" rel=""nofollow noreferrer"">this</a> google cloud doc and <a href=""https://cloud.google.com/ml-engine/docs/v1/predict-request#multiple-input-tensors"" rel=""nofollow noreferrer"">this one</a>, 
<a href=""https://stackoverflow.com/questions/46216095/using-gcloud-ml-serving-for-large-images"">and the stackoverflow answer</a> in this post by 
rhaertel80, I think the recommended format of a json request to send images to a model for prediction on Google cloud is:</p>

<p>{'instances':  {'image_bytes': {'b64': base64.b64encode(jpeg_data)}}, {'image_bytes':...}}</p>

<p>The next step is to create the serving_input_fn() (<a href=""https://cloud.google.com/ml-engine/docs/deploying-models#creating_serving_input_functions"" rel=""nofollow noreferrer"">described in the google cloud docs</a> and <a href=""https://cloud.google.com/blog/big-data/2018/02/easy-distributed-training-with-tensorflow-using-tfestimatortrain-and-evaluate-on-cloud-ml-engine#step-3-define-training-and-eval-specifications"" rel=""nofollow noreferrer"">this GCP tutorial</a>), which can cope with the nested dictionary that the request will send. </p>

<p>To do this I need to create 'features' and a 'receiver_tensor' to pass into <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/export/ServingInputReceiver"" rel=""nofollow noreferrer"">the ServingInputReciever function</a> which the serving_input_fn() needs to return. </p>

<p>However, I do not see how the requirement of the receiver_tensor to be a dictionary with keys and tensors as values can fit the nested format of the json request. (As I understand it, the receiver_tensors are placeholders for the request).</p>

<p>If the request does not contain nested dictionaries the approach seems to be fairly simple as shown in the tutorials and <a href=""https://stackoverflow.com/questions/48510264/in-tensorflow-for-serving-a-model-what-does-the-serving-input-function-supposed"">this answer</a>.</p>

<h2>Question</h2>

<p>So, how can the serving_input_fn() be formatted to receive the image request in the described form and create features and receiver_tensors which fill the requirements of the ServingInputReceiver function?</p>

<p>Part of the difficulty may be that I do not understand what the serving_input_fn() will need to process. Will it be the entire request in one go? Or will each instance be passed one at a time? Or is there some other way to understand what the function will be processing</p>

<h3>More details</h3>

<p>For more context, I am using tf.Estimator and the <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate"" rel=""nofollow noreferrer"">train_and_evaluate function</a> to train a model and deploy it to google cloud. The input to the model is a tf.dataset containing tuples of ({'spectrogram': image}, label) where image is a tensor.</p>

<p>My attempt to create the input_fn assumes one element of the instances list is passed at a time:</p>

<pre><code>def serving_input_fn():
    feature_placeholders = {'image_bytes': {'b64': tf.placeholder(dtype=tf.string,
                                                                  shape=[None],
                                                                  name='source')}}
    input_images = convert_bytestrings_to_images(feature_placeholders['image_bytes']['b64'])
    features = {'spectrogram': image for image in input_images}
    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)
</code></pre>

<p>Which leads to the error</p>

<pre><code>ValueError: receiver_tensor image_bytes must be a Tensor.
</code></pre>

<p>And I am not sure if features would be in the correct form either.</p>
",0
48821218,tf.image.random_brightness giving negative values randomly in TensorFlow,"<p>'I am trying image data augmentation in TensorFlow using various methods like rotation, random brightness, random saturation. What I observe that the output of tf.image.random_brightness is not consistent - sometimes it produces negative values. I understand the randomness, but is it correct to produce negative values? When I try to plot the image using matplotlib.pyplot, it fails saying ValueError: Floating point image RGB values must be in the 0..1 range
Below is some sample of the code:'</p>

<pre><code># Function which reads file and converts to image array
def read_images_from_file (input_queue):
    label = input_queue[1]

    file_content = tf.read_file(input_queue[0])
    image = tf.image.decode_jpeg(file_content, channels=NUM_CHANNELS)
    image = tf.image.convert_image_dtype(image, dtype=tf.float32, saturate=True)
    image = tf.image.resize_images(image, [IMAGE_HEIGHT, IMAGE_WIDTH])

.....
    #inside a function which applies various augmentations - code shown only for brightness

    X_init = tf.placeholder(tf.float32, shape=images.shape)
    X = tf.Variable(X_init)

    sess.run(tf.variables_initializer([X]), feed_dict={X_init: images})
    aug_images, aug_labels = (sess.run(tf.map_fn(lambda params: (tf.image.random_brightness(params[0], 0.8, 1), params[1]), (X, labels))))

    #inside a loop after calling above function - output of function is returned to aug_train_images

    print (aug_train_images[i])


'Some sample output:'
    [[[-0.18852733 -0.27872342 -0.31009597]
      [-0.18059228 -0.2786315  -0.3060825 ]
      [-0.1765788  -0.27461803 -0.302069  ]
      ...

      [-0.20366213 -0.19974056 -0.18405429]
      [-0.22792684 -0.22437292 -0.20458125]
      [-0.24324547 -0.23166458 -0.21205674]]
</code></pre>

<p>'I am using Jupyter notebook with Python 3.5.3 and TensorFlow CPU version 1.5.0-rc0 on Ubuntu 16.10.'</p>
",0
48869256,Distributed Tensorflow: Unable to run evaluation-only worker,"<p>I'm trying to distribute training and evaluation over two machines:</p>
<ul>
<li>PC1: training on GPU and parameter server</li>
<li>PC2: evaluation-only, whenever a new checkpoint is available</li>
</ul>
<p>To do so, I'm trying to adapt the <code>tf.contrib.learn.Experiment</code> framework, but I can't seem to get the cluster specification right.</p>
<p>This is a stripped-down version of my code:</p>
<pre><code>def get_schedule(run_config):
  if run_config.task_type == 'ps':
    return 'run_std_server'
  if run_config.task_type == 'worker':
    return 'train'
  if run_config.task_type == 'evaluator':
    return 'continuous_eval'
  if run_config.task_type == 'master':
    return 'train'
  raise ValueError('Unknown task type &quot;{}&quot;'.format(run_config.task_type))

def deeplpr_model_fn(features, labels, mode, cluster_spec={}):
  with tf.device(tf.train.replica_device_setter(cluster=cluster_spec)):
    logits = build_model(features['images'], mode)
  #[...] Standard Estimator setup for training &amp; evaluation
  return tf.estimator.EstimatorSpec(mode=mode, 
                                    predictions=predictions, 
                                    export_outputs=export_outputs,
                                    loss=loss,
                                    train_op=train_op,
                                    eval_metric_ops=metrics)

def get_experiment(run_config=None, hparams=None):
  # Create the Estimator
  estimator = tf.estimator.Estimator(
    model_fn=lambda features, labels, mode : my_model_fn(features, labels, mode, run_config.cluster_spec),
    model_dir=FLAGS.model_dir,
    config=run_config)
  
  # Set up input functions for training and evaluation
  train_input_fn = lambda : input_fn(tf.estimator.ModeKeys.TRAIN, FLAGS.batch_size)
  eval_input_fn = lambda : input_fn(tf.estimator.ModeKeys.EVAL, FLAGS.batch_size)
  
  # Set up the experiment
  experiment = tf.contrib.learn.Experiment(
    estimator=estimator,
    train_input_fn=train_input_fn,
    eval_input_fn=eval_input_fn,
    train_steps=FLAGS.steps,
    eval_steps=None,
    eval_delay_secs=20, # time to wait before running the first evaluation
    train_steps_per_iteration=2000)
  return experiment
</code></pre>
<p>The main function looks as follows:</p>
<pre><code>def distributed_main(unused_argv):
  import json
  # Set up environment variables according to the parameters passed to the process
  TF_CONFIG = {
    'cluster': {
        &quot;worker&quot;: [
            &quot;pc1:2222&quot;,
        ],
        &quot;ps&quot;: [
            &quot;pc1:2223&quot;,
        ],
        &quot;evaluator&quot;: [
            &quot;pc2:2224&quot;,
        ]
    },
    'environment': 'cluster',    
    'task': {
        'type': unused_argv[1].strip(),
        'index': unused_argv[2].strip() if len(unused_argv) &gt; 2 else 0
        }
  }
  os.environ['TF_CONFIG'] = json.dumps(TF_CONFIG)
  
  session_config = tf.ConfigProto(device_filters=device_filters, 
                                  allow_soft_placement=True)
  config = tf.contrib.learn.RunConfig(model_dir=FLAGS.model_dir, 
                                      session_config=session_config)
  schedule = get_schedule(config)
  tf.logging.info('Beginning task {}:{}'.format(config.task_type, config.task_id))
  
  # Run the function
  tf.contrib.learn.learn_runner.run(get_experiment, schedule=schedule, run_config=config)
</code></pre>
<p>where <code>unused_argv</code> contains a job name and optionally an index (defaults to 0).</p>
<p>Running three processes with the appropriate job names and task ids, I can't get the worker go past the session initialization step because it expects <code>evaluator</code> to communicate with the chief worker (which it doesn't, if <code>continuous_eval</code> is called, apparently).</p>
<p>Researching the issue, I found <a href=""https://stackoverflow.com/questions/46429766/distributed-tensorflow-createsession-still-waiting"">this answer</a> where they suggest to add a <code>device_filter</code>, so I tried adding:</p>
<pre><code>  device_filters=[&quot;/job:ps&quot;, &quot;/job:worker&quot;]
  if unused_argv[1] != 'worker':
    device_filters += ['/job:evaluator']
  session_config = tf.ConfigProto(device_filters=device_filters)
  config = tf.contrib.learn.RunConfig(model_dir=FLAGS.model_dir, session_config=session_config)
</code></pre>
<p>This effectively unlocks the worker and ps, but the evaluator then crashes when attempting to restore the newest checkpoint:</p>
<pre><code>Traceback (most recent call last):
  File &quot;deeplpr.py&quot;, line 431, in &lt;module&gt;
    tf.app.run(main=distributed_main, argv=[sys.argv[0]] + unparsed)
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py&quot;, line 126, in run
    _sys.exit(main(argv))
  File &quot;deeplpr.py&quot;, line 420, in distributed_main
    tf.contrib.learn.learn_runner.run(get_experiment, schedule=schedule, run_config=config)
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_runner.py&quot;, line 218, in run
    return _execute_schedule(experiment, schedule)
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_runner.py&quot;, line 46, in _execute_schedule
    return task()
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\learn\python\learn\experiment.py&quot;, line 573, in continuous_eval
    continuous_eval_predicate_fn=continuous_eval_predicate_fn)
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\learn\python\learn\experiment.py&quot;, line 533, in _continuous_eval
    hooks=self._eval_hooks)
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\contrib\learn\python\learn\experiment.py&quot;, line 894, in _call_evaluate
    hooks=hooks)
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\estimator\estimator.py&quot;, line 414, in evaluate
    name=name)
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\estimator\estimator.py&quot;, line 949, in _evaluate_model
    config=self._session_config)
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\evaluation.py&quot;, line 209, in _evaluate_once
    session_creator=session_creator, hooks=hooks) as session:
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\monitored_session.py&quot;, line 795, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\monitored_session.py&quot;, line 518, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\monitored_session.py&quot;, line 981, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\monitored_session.py&quot;, line 986, in _create_session
    return self._sess_creator.create_session()
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\monitored_session.py&quot;, line 675, in create_session
    self.tf_sess = self._session_creator.create_session()
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\monitored_session.py&quot;, line 446, in create_session
    init_fn=self._scaffold.init_fn)
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\session_manager.py&quot;, line 275, in prepare_session
    config=config)
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\session_manager.py&quot;, line 191, in _restore_checkpoint
    saver.restore(sess, checkpoint_filename_with_path)
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\training\saver.py&quot;, line 1760, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py&quot;, line 905, in run
    run_metadata_ptr)
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py&quot;, line 1137, in _run
    feed_dict_tensor, options, run_metadata)
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py&quot;, line 1355, in _do_run
    options, run_metadata)
  File &quot;C:\Users\1\Anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\client\session.py&quot;, line 1374, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'save/RestoreV2_1': Operation was explicitly assigned to /job:ps/task:0/device:CPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.
         [[Node: save/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=&quot;/job:ps/task:0/device:CPU:0&quot;](save/Const, save/RestoreV2_1/tensor_names, save/RestoreV2_1/shape_and_slices)]]
</code></pre>
<p>What is the proper way to designate a worker for evaluation only?
In the logs from Tensorflow, I see that the <code>RunConfig</code> used has a parameter <code>'_evaluation_master': ''</code>, but I can't find any documentation about it. Is this somehow related? Are there any working examples that show how to distribute experiments separating training and evaluation?</p>
<h2>Update:</h2>
<p>As suggested, I added <code>log_device_placement=True</code> when defining <code>session_config</code>.
The log output, however, seems to crash before logging the device placements:</p>
<pre><code>INFO:tensorflow:Waiting 20.000000 secs before starting eval.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2018-02-19-15:35:04
INFO:tensorflow:Graph was finalized.
2018-02-19 16:35:04.888165: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
INFO:tensorflow:Restoring parameters from models\test_distributed\model.ckpt-2373
# Here starts the traceback of the error, same as above
</code></pre>
<p>This is slightly confusing, shouldn't I see the placement log at session creation? And doesn't the <code>restore</code> op need the session to run?</p>
<p>Setting <code>allow_device_placement=True</code> also didn't change anything in the log and error.</p>
<h2>Update 2:</h2>
<p>setting <code>log_device_placement=True</code> for <em>all</em> machines only logs it in <code>worker:0</code> (aka the master), which I assume is the expected behaviour</p>
<h2>Edit:</h2>
<p>Updated the code above to reflect how I set <code>allow_device_placement=True</code> (Only the main function changed).</p>
",1
48874558,Tensorflow `tf.layers.batch_normalization` doesn't add update ops to `tf.GraphKeys.UPDATE_OPS`,"<p>The following code (copy/paste runnable) illustrates using <code>tf.layers.batch_normalization</code>. </p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
bn = tf.layers.batch_normalization(tf.constant([0.0]))
print(tf.get_collection(tf.GraphKeys.UPDATE_OPS))

&gt; []     # UPDATE_OPS collection is empty
</code></pre>

<p>Using TF 1.5, the documentation (quoted below) clearly states that <strong>UPDATE_OPS should not be empty</strong> in this case (<a href=""https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization</a>):</p>

<blockquote>
  <p>Note: when training, the moving_mean and moving_variance need to be
  updated. By default the update ops are placed in
  <code>tf.GraphKeys.UPDATE_OPS</code>, so they need to be added as a dependency to
  the train_op. For example:</p>
</blockquote>

<pre><code>update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    train_op = optimizer.minimize(loss)
</code></pre>
",0
48910416,"Explain why does Tensorflow throw ""Attempting to use uninitialized value"" during linear transform","<p>I am trying to apply <code>tf.layers.dense</code> on a tensor while running jupyter notebook. The code I am using raises <code>FailedPreconditionError</code>:</p>

<pre><code>FailedPreconditionError: Attempting to use uninitialized value dense_9/bias
     [[Node: dense_9/bias/read = Identity[T=DT_INT32, _class=[""loc:@dense_9/bias""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](dense_9/bias)]]
</code></pre>

<p>Jupyter notebook code:</p>

<pre><code>import tensorflow as tf
import numpy as np
tf.InteractiveSession()
batch_size = 3
seqlen = 5
nfeats = 3
mini_batch = np.random.randint(1,10, (batch_size, seqlen, nfeats))
d = dict((n, t.eval()) for n,t in enumerate(tf.unstack(mini_batch, 3)))
d0 = tf.convert_to_tensor(d[0],dtype=tf.int32)
d0 = tf.reshape(d0, [seqlen, batch_size])
d0_lin = tf.layers.dense(inputs=d0, units=100)
d0_lin.eval()
</code></pre>

<p>I am new to Tensorflow and the idea I am playing with is how to apply linear transformation on a <code>5x3</code> tensor and transform it into <code>5x100</code> tensor when you have a <code>3x5x3</code> input tensor. So that we could convert <code>3x5x3</code> tensor to <code>3x5x100</code> tensor.</p>
",0
48914952,num_buckets as a parameter in a tensorflow feature column,"<p>Currently Tensorflow documentation define a categorical vocabulary column this way:</p>

<pre><code>vocabulary_feature_column =
tf.feature_column.categorical_column_with_vocabulary_list(
    key=""feature_name_from_input_fn"",
    vocabulary_list=[""kitchenware"", ""electronics"", ""sports""]) 
</code></pre>

<p>However this suppose that we input manually the vocabulary list.
In case of large dataset with many columns and many unique values I would like to automate the process this way:</p>

<pre><code>for k in categorical_feature_names:
    vocabulary_feature_column =
        tf.feature_column.categorical_column_with_vocabulary_list(
        key=""feature_name_from_input_fn"",
        vocabulary_list=list_of_unique_values_in_the_column) 
</code></pre>

<p>To do so I need to retrieve the parameter <code>list_of_unique_values_in_the_column</code>.
Is there anyway to do that with Tensorflow? </p>

<p>I know there is tf.unique that could return unique values in a tensor but I don't get how I could feed the column to it so it returns the right vocabulary list.</p>
",1
48976538,Tensorflow AttributeError: 'module' object has no attribute 'manip',"<p>I try to roll a tensor and in the Tensorflow documentation i found a function called tf.manip.roll() but when i use it i get the error message:</p>

<p>AttributeError: 'module' object has no attribute 'manip'</p>

<p>Has someone an idea where this function has moved?</p>
",1
49008288,Apply dilation rate individually to each dimension with tf.nn.atrous_conv2d?,"<p>I have a tensor with shape <code>B x H x W x C</code> and I like to apply dilation to only <code>H</code>. Do you know any way to have this small trick with <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/atrous_conv2d"" rel=""nofollow noreferrer""><code>tf.nn.atrous_conv2d</code></a>?</p>
",0
49080605,Subtracting matrix elements,"<p>Suppose I have one <code>4x3</code> matrix. I want to subtract every element in that matrix from each other. </p>

<p>I've looked around TensorFlow documentation extensively (and on SO) and noticed that there is a <code>tf.subtract</code> operator. In adding, I know that there is <code>tf.add_n</code> operator which adds all input tensors. I'm new to TensorFlow and was wondering: is there such a subtraction operator which subtracts all input tensors and if not, can you please provide an example of the fastest way to do so?</p>

<p>Example matrix tensor: <code>[[0.10, 0.20], [0.20, 0.40]]</code> so expanded as:</p>

<pre><code>0.10 0.20
0.20 0.40
</code></pre>

<p>Desired subtraction: <code>0.10 - 0.20 - 0.20 - 0.40</code> with desired output as: <code>-.7</code></p>
",0
49166819,"KeyError : The tensor variable , Refer to the tensor which does not exists","<p>Using <strong>LSTMCell</strong> i  trained a model to do text generation . I started the tensorflow session and save all the tensorflow varibles using <strong>tf.global_variables_initializer()</strong> . </p>

<pre><code>import tensorflow as tf
sess = tf.Session()
//code blocks
run_init_op = tf.global_variables_intializer()
sess.run(run_init_op)
saver = tf.train.Saver()
#varible that makes prediction
prediction = tf.nn.softmax(tf.matmul(last,weight)+bias)
#feed the inputdata into model and trained
#saved the model
#save the tensorflow model
save_path= saver.save(sess,'/tmp/text_generate_trained_model.ckpt')
print(""Model saved in the path : {}"".format(save_path))
</code></pre>

<p>The model get trained and saved all its session . Link to review the whole code  <a href=""https://github.com/Madhivarman/DR/blob/master/neuralnetwork/lstm_rnn.py"" rel=""nofollow noreferrer"">lstm_rnn.py</a></p>

<p>Now i loaded the stored model and tried to do text generation for the document . So,i restored the model with following code</p>

<pre><code>tf.reset_default_graph()
imported_data = tf.train.import_meta_graph('text_generate_trained_model.ckpt.meta')
with tf.Session() as sess:
    imported_meta.restore(sess,tf.train.latest_checkpoint('./'))

    #accessing the default graph which we restored
    graph = tf.get_default_graph()

    #op that we can be processed to get the output
    #last is the tensor that is the prediction of the network
    y_pred = graph.get_tensor_by_name(""prediction:0"")
    #generate characters
    for i in range(500):
        x = np.reshape(pattern,(1,len(pattern),1))
        x = x / float(n_vocab)
        prediction = sess.run(y_pred,feed_dict=x)
        index = np.argmax(prediction)
        result = int_to_char[index]
        seq_in = [int_to_char[value] for value in pattern]
        sys.stdout.write(result)
        patter.append(index)
        pattern = pattern[1:len(pattern)]

    print(""\n Done...!"")
sess.close()
</code></pre>

<p>I came to know that the prediction variable does not exist in the graph.</p>

<blockquote>
  <p>KeyError: ""The name 'prediction:0' refers to a Tensor which does not
  exist. The operation, 'prediction', does not exist in the graph.""</p>
</blockquote>

<p>Full code is available here <a href=""https://github.com/Madhivarman/DR/blob/master/neuralnetwork/text_generation.py"" rel=""nofollow noreferrer"">text_generation.py</a></p>

<p>Though i saved all tensorflow varibles , the prediction tensor is not saved in the tensorflow computation graph . whats wrong in my <strong>lstm_rnn.py</strong> file . </p>

<p>Thanks!</p>
",0
49177486,MonitoredTrainingSession save and restore model,"<p>I'm trying to extend the example <a href=""https://www.tensorflow.org/deploy/distributed"" rel=""nofollow noreferrer"">https://www.tensorflow.org/deploy/distributed</a> outlined here but I'm having trouble saving the model. I'm running this in docker container available at <code>gcr.io/tensorflow/tensorflow:1.5.0-gpu-py3</code>. I started two processes one for 'ps' and one for 'worker' and the ps process is simply this code:</p>

<pre><code>import tensorflow as tf
def main(_):
   cluster = tf.train.ClusterSpec({""ps"":[""localhost:2222""],""worker"":[""localhost:2223""]})
   server = tf.train.Server(cluster, job_name=""ps"", task_index=0)
   server.join()
if __name__ == ""__main__"":
   tf.app.run()
</code></pre>

<p>The worker code is the following and is based on the mnist examples and the distributed article above:</p>

<pre><code>import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

data_dir = ""/data""
checkpoint_dir = ""/tmp/train_logs""

def main(_):
   cluster = tf.train.ClusterSpec({""ps"":[""localhost:2222""],""worker"":[""localhost:2223""]})
   server = tf.train.Server(cluster, job_name=""worker"", task_index=0)
   mnist = input_data.read_data_sets(data_dir, one_hot=True)

   with tf.device(tf.train.replica_device_setter(worker_device=""/job:worker/task:0"", cluster=cluster)):
     x = tf.placeholder(tf.float32, [None,784], name=""x_input"")
     W = tf.Variable(tf.zeros([784,10]))
     b = tf.Variable(tf.zeros([10]))
     y = tf.placeholder(tf.float32, [None,10])
     model = tf.matmul(x, W) + b
     cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=model))
     global_step = tf.train.get_or_create_global_step()
     train_op = tf.train.GradientDescentOptimizer(0.5).minimize(cost, global_step=global_step)
     prediction = tf.equal(tf.argmax(model,1), tf.argmax(y,1))
     accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))

  hooks = [tf.train.StopAtStepHook(last_step=101)]
  with tf.train.MonitoredTrainingSession(master=server.target, is_chief=True, checkpoint_dir=checkpoint_dir, hooks=hooks) as sess:
     while not sess.should_stop():
        batch_xs, batch_ys = mnist.train.next_batch(1000)
        sess.run(train_op, feed_dict={x: batch_xs, y: batch_ys})

  latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)
  #saver = tf.train.Saver()
  saver = tf.train.import_meta_graph(latest_checkpoint+"".meta"", clear_devices=True)
  with tf.Session() as sess:
     saver.restore(sess,latest_checkpoint) # ""/tmp/train_logs/model.ckpt""
     acc = sess.run(accuracy, feed_dict={x: mnist.test.images,y: mnist.test.labels});
     print(""Test accuracy = ""+""{:5f}"".format(acc))

if __name__ == ""__main__"":
   tf.app.run()
</code></pre>

<p>The examples I've found all seem to end without showing how to use the model. The above code fails on the <code>saver.restore()</code> line with the following error:</p>

<pre><code>InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'save/RestoreV2_2': 
Operation was explicitly assigned to /job:ps/task:0/device:CPU:0 
but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0 ].
Make sure the device specification refers to a valid device.
</code></pre>

<p>Also, as shown above I tried both <code>saver = tf.train.Saver()</code> and <code>saver = tf.train.import_meta_graph(latest_checkpoint+"".meta"", clear_devices=True)</code> with no success. Same error is shown in either case.</p>

<p>I don't really understand the <code>with tf.device(...):</code> statement. In one iteration I commented out this line (and unindented the statements below it) and the code ran without errors. But I think this is not correct and would like to understand the correct way for this to work.</p>
",0
49201832,How to use TensorBoard and summary operations with the tf.layers module,"<p>I have followed the <a href=""https://www.tensorflow.org/tutorials/layers"" rel=""nofollow noreferrer"">TensorFlow Layers tutorial</a> to create a CNN for MNIST digit classification using TensorFlow's tf.layers module. Now I'm trying to learn how to use TensorBoard from <a href=""https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard"" rel=""nofollow noreferrer"">TensorBoard: Visualizing Learning</a>. Perhaps this tutorial hasn't been updated recently, because it says its example code is a modification of that tutorial's and links to it, but the code is completely different: it manually defines a single-hidden-layer fully-connected network.</p>

<p>The TensorBoard tutorial shows how to use tf.summary to attach summaries to a layer by creating operations on the layer's weights tensor, which is directly accessible because we manually defined the layer, and attaching tf.summary objects to those operations. To do this if I'm using tf.layers and its tutorial code, I believe I'd have to:</p>

<ol>
<li>Modify the Layers tutorial's example code to use the non-functional interface (Conv2D instead of conv2d and Dense instead of dense) to create the layers</li>
<li>Use the layer objects' trainable_weights() functions to get the weight tensors and attach tf.summary objects to those</li>
</ol>

<p>Is that the best way to use TensorBoard with tf.layers, or is there a way that's more directly compatible with tf.layers and the functional interface? If so, is there an updated official TensorBoard tutorial? It would be nice if the documentation and tutorials were more unified. </p>
",1
49223976,Default Initialization for Tensorflow LSTM states and weights?,"<p>I am using the LSTM cell in Tensorflow.</p>

<pre><code>lstm_cell = tf.contrib.rnn.BasicLSTMCell(lstm_units)
</code></pre>

<p>I was wondering how the weights and states are initialized or rather what the default initializer is for LSTM cells (states and weights) in Tensorflow?</p>

<p>And is there an easy way to manually set an Initializer?</p>

<p>Note: For <code>tf.get_variable()</code> the glorot_uniform_initializer is used as far as I could find out from the <a href=""https://www.tensorflow.org/api_docs/python/tf/get_variable"" rel=""nofollow noreferrer"">documentation</a>.</p>
",1
49253848,load tensorflow CUDNNRNNRelu checkpoints for cpu inference,"<p>I am currently training a bidirectional RNN model using the CudnnRNNRelu on tensorflow. I want to use these checkpoints for inference on a CPU machine. The documentation suggests that models trained with Cudnn specific cells (CudnnRNNRelu, CudnnRNNTanh, CudnnLSTM etc ..) cannot be restored directly using platform independent rnn cells ( tf.contrib.rnn.BasicRNNCell etc.. ). While there are separate cells to restore LSTM and GRU cells ( CudnnComaptibleLSTMCell and CudnnCompatibleGRUCell) , there is no indication of how to use CudnnRNNRelu trained checkpoints with platform independent rnn cells.</p>

<p>Restoring works properly on a GPU machine. Is there a way to restore the CudnnRNNRelu checkpoints? </p>
",0
49267730,In Tensorflow what is GraphKeys.INIT_OP for?,"<p>Looking at the documentation for GraphKeys: <a href=""https://www.tensorflow.org/api_docs/python/tf/GraphKeys"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/GraphKeys</a></p>

<p>There is a <code>GraphKeys.INIT_OP</code> listed that has no documentation.</p>

<p>What is this collection for exactly?</p>

<p>I'm looking for the best way to add a few necessary assign OPs to the graph such that they will be run once at initialization time only. My initial thought was to add them to <code>GraphKeys.GLOBAL_VARIABLES</code> which are run at the time <code>sess.run(tf.global_variables_initializer())</code> is run. When I saw <code>GraphKeys.INIT_OP</code> I wondered if it perhaps offered a more robust option?</p>
",1
49281676,Tensorflow: reduce_mean of subtensors and concatenating the results,"<p>I have a 3D tensor with shape <code>(X,Y,Z)</code>. Only Z is known in advance, everything else is determined at running time. I have an integer <code>N</code> s.t N divides X. I would like to do an operation similar to <code>tf.reduce_mean</code> on one axis without completely getting rid of the axis I'm taking the mean along. Another way of seeing this is as a <code>tf.reduce_mean</code> operation on subtensors of dimension <code>(X/N, Y, Z)</code> and concatenating the results back. So the final shape of my 3D tensor would be <code>(N,Y,Z)</code></p>

<p>So given N=2 and the following 3D tensor</p>

<pre><code>          [[[1., 2.],
            [3., 4.],
            [5., 6.]],

           [[7., 8.],
            [9., 0.],
            [1., 2.]],

           [[3., 4.],
            [5., 6.],
            [7., 8.]],

           [[9., 0.],
            [1., 2.],
            [3., 4.]]

           [[5., 6.],
            [7., 8.],
            [9., 0.]]

           [[1., 2.],
            [3., 4.],
            [5., 6.]]]        
</code></pre>

<p>the new 3D tensor should be</p>

<pre><code>          [[[3.66, 4.66],
            [5.66, 2.66],
            [4.33, 5.33]],

           [[5 , 2.66],
            [3.66 , 4.66],
            [5.66 , 3.33]]]      
</code></pre>

<p>I've looked around for a bit in both the tensorflow documentation and on this website, but couldn't find anything. Any insights on what the simplest/most efficient way of doing this is?</p>
",1
49297608,"Keep ""best"" checkpoints instead of latest ones","<p>When using <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator"" rel=""nofollow noreferrer""><code>tf.estimator.Estimator</code></a>, you can only set how many recent checkpoint files you'de like to keep using <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/learn/RunConfig#keep_checkpoint_max"" rel=""nofollow noreferrer""><code>keep_checkpoint_max</code></a></p>

<p>Is there a way to keep the ""best"" checkpoint files instead of the recent ones?</p>

<p>""best"" has to be defined based on the training/eval metrics.</p>

<p>Example: if ""best"" is defined as ""<em>the lowest loss on validation set</em>"", then one should keep this value for each checkpoint saved so far to be able to compare when a new checkpoint arrives.</p>

<p>How can I do this?</p>
",0
49303136,tensorflow tfrecords for batch with variable length data in each example?,"<p>I am trying to use tfrecords to read batches that have a field that is a  variable length list in each example. The data might be</p>

<pre><code>example 1: [1,    2,  3] 
example 2: [10,  11]
example 3: [100,200,300,400]
</code></pre>

<p>I have been using </p>

<pre><code>tf.train.Feature(int64_list=tf.Int64List(value=x))
</code></pre>

<p>to store each of the above, that is I'll make 3 different tfrecords, where x is <code>[1,2,3]</code> then <code>[10,11]</code> then <code>[100,200,300,400]</code></p>

<p>Those three records got their <code>SerializeToString()</code> method called, and each appended to a file via the <code>TFRecordWriter</code></p>

<p>Reading back was tricky, I couldn't use <code>tf.FixedLenFeature</code>, so then I found <code>tf.VarLenFeature</code>. It seemed to be very nice, when I read the data batch in a batch size of 3, it looked like I was getting a  <code>tf.SparseVectorValue</code> back where column 0 of the indices was the example number in the batch, and column 1 was the value within the list, that is, it looked like I was getting (suppose the batch size is 3):</p>

<pre><code>indices=[[0,0],
         [0,1],
         [0,2],
         [1,0],
         [1,1],
         [2,0],
         [2,1],
         [2,2],
         [2,3]]
 values = [1,2,3,10,11,100,200,300,400]
</code></pre>

<p>but now that I'm working with more data, I don't think this is what I'm getting. </p>

<p>My question is, what does VarLenFeature return when you batch up variable length lists like this? Should it do what I explained? Then maybe I have a bug to find. </p>

<p>But if it does something different, then what should I do to read back a batch of data with variable length lists? I need to know the example number in the batch for each list, I could add another field to the tfrecord with the length of each of these lists.</p>

<p>-- EDIT --</p>

<p>I've done more testing, and I think it works like I think. I must have a problem in my bigger program. It would be nice if there was documentation saying exactly what <code>tf.VarLenFeature</code> is supposed to return for batched datasets, so I could be sure my above interpretation is correct.</p>

<p>Below is some test code I'm trying:</p>

<pre><code>import numpy as np
import tensorflow as tf
import random


def make_original_data(N):
    data = []
    for uid in range(N):
        varlen = random.randint(2, 20)
        varx = [random.randint(0,100) for kk in range(varlen)] 
        rec = {'uid': uid,
               'A':random.randint(0,100),
               'B':random.randint(0,100),
               'V':varx,
               'nV':varlen
            }
        data.append(rec)
    return data


def rec2tfrec_example(rec):
    def _int64_feat(value):
        arr_value = np.empty([1], dtype=np.int64)
        arr_value[0] = value
        return tf.train.Feature(int64_list=tf.train.Int64List(value=arr_value))

    def _int64list_feat(values):
        arr_values = np.empty([len(values)], dtype=np.int64)
        arr_values[:] = values[:]
        return tf.train.Feature(int64_list=tf.train.Int64List(value=arr_values))

    feat = {
        'uid': _int64_feat(rec['uid']),
        'A':   _int64_feat(rec['A']),
        'B':   _int64_feat(rec['B']),
        'nV':  _int64_feat(rec['nV']),
        'V':   _int64list_feat(rec['V'])
        }

    return tf.train.Example(features=tf.train.Features(feature=feat))


def parse_example(tfrec_serialized_string):
    feat = {
        'uid': tf.FixedLenFeature([], tf.int64),
        'A': tf.FixedLenFeature([], tf.int64),
        'B': tf.FixedLenFeature([], tf.int64),
        'nV': tf.FixedLenFeature([], tf.int64),
        'V': tf.VarLenFeature(tf.int64)
    }
    return tf.parse_example(tfrec_serialized_string, feat)


def to_serialized_tfrecs(data):
    serialized = []
    for rec in data:
        example = rec2tfrec_example(rec)
        serialized.append(example.SerializeToString())
    return serialized


def write_tfrecs_to_file(fname, recs):
        recwriter = tf.python_io.TFRecordWriter(fname)
        for rec in recs:
            recwriter.write(bytes(rec))
        recwriter.close()


def check_batch(data_batch, tfres):
    for ky in ['A', 'uid', 'B', 'nV']:
        orig_data = np.array([rec[ky] for rec in data_batch], dtype=np.int64)
        assert np.all(orig_data == tfres[ky]), ""batch_idx=%d ky=%s orig=%s tf=%s"" % \
            (batch_idx, ky, orig_data, tfres[ky])
    spTensorValue = tfres['V']
    tf_example_in_batch = spTensorValue.indices[:,0]
    tf_V = spTensorValue.values
    tf_sum_nV = np.sum(tfres['nV'])
    assert tf_sum_nV == len(tf_V), ""tf_sum_nV=%d != len(tf_V)=%d"" % (tf_sum_nV, len(tf_V))
    ex_example_in_batch = np.empty((tf_sum_nV,), np.int64)
    ex_V = np.empty((tf_sum_nV,), np.int64)
    idx = 0
    for example_in_batch, tf_num_this_example in enumerate(tfres['nV']):
        num_this_example = data_batch[example_in_batch]['nV']
        assert num_this_example == tf_num_this_example
        ex_example_in_batch[idx:idx+num_this_example] = example_in_batch
        ex_V[idx:idx+num_this_example] = np.array(data_batch[example_in_batch]['V'])
        idx += num_this_example
    assert np.all(ex_example_in_batch == tf_example_in_batch), ""example in batch wrong, expected=%s != tf=%s"" % (ex_example_in_batch, tf_example_in_batch)
    assert np.all(ex_V == tf_V), ""example in batch wrong, expected=%s != tf=%s"" % (ex_V, tf_V)


def check_tfrecs(sess, tfrec_output_filename, data, N, batch_size):
    dataset = tf.data.TFRecordDataset(tfrec_output_filename) \
                     .batch(batch_size) \
                     .map(parse_example, num_parallel_calls=2)
    tf_iter = dataset.make_initializable_iterator()
    get_next = tf_iter.get_next()

    sess.run(tf_iter.initializer)
    num_batches = N//batch_size
    nextIdx = 0
    for batch_idx in range(num_batches):
        data_batch = [data[idx] for idx in range(nextIdx, nextIdx + batch_size)]
        nextIdx += batch_size
        tfres = sess.run(get_next)
        check_batch(data_batch, tfres)


def main(N=1000, batch_size=5, tfrec_output_filename='tfrec_testing.tfrecords'):
    tf.reset_default_graph()
    data = make_original_data(N)
    tfrec_strings = to_serialized_tfrecs(data)
    write_tfrecs_to_file(tfrec_output_filename, tfrec_strings)
    with tf.Session() as sess:
        check_tfrecs(sess, tfrec_output_filename, data, N, batch_size)

if __name__ == '__main__':
    main()
</code></pre>
",1
49306601,How to export a Tensorflow model with exogenous features,"<p>I try to export a Tensorflow model but I can not find the best way to add the exogenous feature to the <code>tf.contrib.timeseries.StructuralEnsembleRegressor.build_raw_serving_input_receiver_fn</code>. </p>

<p>I use the sample from the Tensorflow contrib: <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/timeseries/examples/known_anomaly.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/timeseries/examples/known_anomaly.py</a> and I just try to save the model.</p>

<pre><code># this is the exogenous column 
string_feature = tf.contrib.layers.sparse_column_with_keys(
      column_name=""is_changepoint"", keys=[""no"", ""yes""])

one_hot_feature = tf.contrib.layers.one_hot_column(
      sparse_id_column=string_feature)

estimator = tf.contrib.timeseries.StructuralEnsembleRegressor(
      periodicities=12,    
      cycle_num_latent_values=3,
      num_features=1,
      exogenous_feature_columns=[one_hot_feature],
      exogenous_update_condition=
      lambda times, features: tf.equal(features[""is_changepoint""], ""yes""))

reader = tf.contrib.timeseries.CSVReader(
      csv_file_name,

      column_names=(tf.contrib.timeseries.TrainEvalFeatures.TIMES,
                    tf.contrib.timeseries.TrainEvalFeatures.VALUES,
                    ""is_changepoint""),

      column_dtypes=(tf.int64, tf.float32, tf.string),

      skip_header_lines=1)

train_input_fn = tf.contrib.timeseries.RandomWindowInputFn(reader, batch_size=4, window_size=64)
estimator.train(input_fn=train_input_fn, steps=train_steps)
evaluation_input_fn = tf.contrib.timeseries.WholeDatasetInputFn(reader)
evaluation = estimator.evaluate(input_fn=evaluation_input_fn, steps=1)

export_directory = tempfile.mkdtemp()

###################################################### 
# the exogenous column must be provided to the build_raw_serving_input_receiver_fn. 
# But How ?
######################################################

input_receiver_fn = estimator.build_raw_serving_input_receiver_fn()
# -&gt; error missing 'is_changepoint' key    

#input_receiver_fn = estimator.build_raw_serving_input_receiver_fn({'is_changepoint' : string_feature}) 
# -&gt; cast exception

export_location = estimator.export_savedmodel(export_directory, input_receiver_fn)
</code></pre>

<p>According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/timeseries/StructuralEnsembleRegressor"" rel=""nofollow noreferrer"">documentation</a>, build_raw_serving_input_receiver_fn <strong>exogenous_features</strong> parameter : <em>A dictionary mapping feature keys to exogenous features (either Numpy arrays or Tensors). Used to determine the shapes of placeholders for these features</em>.</p>

<p>So what is the best way to transform the <em>one_hot_column</em> or <em>sparse_column_with_keys</em> to a <em>Tensor</em> object ?</p>
",0
49309679,Feeding numpy uint8 into tensorflow float32 placeholder,"<p>I recently found a proof of concept implementation, which prepares features in a one-hot encoding using <code>numpy.zeros</code>:</p>

<pre><code>data = np.zeros((len(raw_data), n_input, vocab_size),dtype=np.uint8)
</code></pre>

<p>As could be seen above, the single ones are typed as <code>np.uint8</code>.
After inspecting the model, I realized that the input placeholder of the tensorflow model is defined as <code>tf.float32</code>:</p>

<pre><code>x = tf.placeholder(tf.float32, [None, n_input, vocab_size], name=""onehotin"")
</code></pre>

<p><strong>My particular question:</strong>
How does tensorflow deal with this ""mismatch"" of input types. Are those values <code>(0/1)</code> correctly interpreted or casted by tensorflow. If so, is this somewhere mentioned in the docs. After googling I could not found a answer. It should be mentioned that the model runs and values seems plausible. However, typing the input numpy features as <code>np.float32</code> would cause a significant amount of memory needed.</p>

<p><strong>Relevance:</strong>
A running but falsely trained model would behave differently after adopting the input pipeline / rolling out a model into production.</p>
",0
49310720,Batch Norm - Extract Running Mean & Running Variance in TensorFlow,"<p>I am trying to look at the running mean and running variance of a trained tensorflow model that is exported via GCMLE (<code>saved_model.pb</code>, <code>assets/*</code> &amp; <code>variables/*</code>). Where are these values kept in the graph? I can access gamma/beta values from <code>tf.GraphKeys.TRAINABLE_VARIABLES</code> but I have not been able to find the running mean and running variance in any of the <code>tf.GraphKeys.MODEL_VARIABLES</code>. Are the running mean and running variance stored somewhere else?</p>

<p>I know that at test time (ie. <code>Modes.EVAL</code>), the running mean and running variance are used to normalize the incoming data, then the normalized data is scaled and shifted using gamma and beta. I am trying to look at all of the variables that I need at inference time, but I cannot find the running mean and running variance. Are these only used at test time and not at inference time (<code>Modes.PREDICT</code>)? If so, that would explain why I can't find them in the exported model, but I am expecting them to be there.</p>

<p>Based on <a href=""https://www.tensorflow.org/api_docs/python/tf/GraphKeys"" rel=""nofollow noreferrer"">tf.GraphKeys</a> I have tried other things like <code>tf.GraphKeys.MOVING_AVERAGE_VARIABLES</code> but they are also empty. I also saw this line in the batch_normalization documentation ""Note: when training, the moving_mean and moving_variance need to be updated. By default the update ops are placed in <code>tf.GraphKeys.UPDATE_OPS</code>, so they need to be added as a dependency to the train_op."" so I then tried looking at <code>tf.GraphKeys.UPDATE_OPS</code> from my saved model and they contain an assign op <code>batch_normalization/AssignMovingAvg:0</code> but still not clear where I would get the value from.</p>
",0
49346599,tf.feature_column.input_layer returning wrong shape of tensor,"<p>When I use <code>tf.feature_column.input_layer</code>, it seems to be returning a tensor with shape <code>[number of features, batch size]</code> when it should be returning the opposite - <code>[batch size, number of features]</code>. The code is:</p>

<pre><code>## Generate the input functions
def create_train_input_fn(x_train, y_train): 
    return tf.estimator.inputs.pandas_input_fn(
        x=x_train,
        y=y_train, 
        batch_size=32,
        num_epochs=500,
        shuffle=True)
p = create_train_input_fn(df, df.reward_next)

## Create the custom estimator
deep_q = tf.estimator.Estimator(
    model_fn=deep_q_model_test,
    params={
        'feature_columns_current_state': feature_columns_results['current'],
        'feature_columns_next_state': feature_columns_results['next'],
        'gamma': GAMMA,
        'n_classes':ACTION_DIM,
        'hidden_units':[256, 256],
        'batch_size':32
    })

# Train the Model.
deep_q.train(p)
</code></pre>

<p>The documentation clearly states </p>

<blockquote>
  <p>Its shape is (batch_size, first_layer_dimension) </p>
</blockquote>

<p>I am using my own custom estimator function (<code>deep_q_model_test</code> in the code above), and within that I have the first line as:</p>

<pre><code>net = tf.feature_column.input_layer(features, feature_columns)
print('shape of input to forward pass: ' + str(net.get_shape()))
</code></pre>

<p>And the shape shown in my print (and also after inspecting the tensor board) is:</p>

<pre><code>shape of input to forward pass: (?, 32)
shape of hidden layer: (?, 256)
shape of hidden layer: (?, 256)
</code></pre>

<p>I am using the prebuilt pandas input function as well to be fed into it: tf.estimator.inputs.pandas_input_fn. The feature columns were built with it being:</p>

<pre><code>[_NumericColumn(key='cpc', shape=(1,), default_value=None, 
dtype=tf.float32, normalizer_fn=None),
 _NumericColumn(key='comp_win', shape=(1,), default_value=None, 
dtype=tf.float32, normalizer_fn=None),
 _NumericColumn(key='impressions', shape=(1,), default_value=None, 
dtype=tf.float32, normalizer_fn=None),
 _NumericColumn(key='clicks', shape=(1,), default_value=None, 
dtype=tf.float32, normalizer_fn=None),
 _NumericColumn(key='cost', shape=(1,), default_value=None, 
dtype=tf.float32, normalizer_fn=None),
 _NumericColumn(key='transactions', shape=(1,), default_value=None, 
dtype=tf.float32, normalizer_fn=None),
 _NumericColumn(key='cpo', shape=(1,), default_value=None, 
dtype=tf.float32, normalizer_fn=None),
 _NumericColumn(key='reward', shape=(1,), default_value=None, 
dtype=tf.float32, normalizer_fn=None),
 _IndicatorColumn(categorical_column=_IdentityCategoricalColumn(key='hrs', 
num_buckets=24, default_value=None))]
</code></pre>

<p>Also the neural net is actually trains and inspecting the tensorboard it does show the shape is flipped. The problem with this is when I run a batch size which is different, lets say for prediction where i want to only predict 1 observation, it wont work.</p>

<p><strong>EDITED</strong>
Adding in the actual code for the model</p>

<pre><code>def deep_q_model(features, labels, mode, params):
""""""deep q learning model""""""

## Create the models/
def nn1_forward(features, feature_columns, hidden_units, n_classes):
    net = tf.feature_column.input_layer(features, feature_columns)
    print('shape of input to forward pass: ' + str(net.get_shape()))

    # Hidden layers (batch size, hidden_nodes_dim)
    layer_number = 0
    for units in hidden_units:
        net = tf.layers.dense(net, units=units,
                              activation=tf.nn.relu,
                             name='layer' + str(layer_number),
                             reuse=tf.AUTO_REUSE)
        print('shape of hidden layer: ' + str(net.get_shape()))
        layer_number += 1

    # Logits layer with no activation (batch size, output dims)
    logits = tf.layers.dense(net, n_classes,
                             activation=None,
                            name='layerOutput',
                            reuse=tf.AUTO_REUSE)
    print('shape of output layer: ' + str(logits.get_shape()))

    return logits

## Current state forward pass
logits_current = nn1_forward(features,
                            params['feature_columns_current_state'],
                            params['hidden_units'],
                            params['n_classes'])
tf.summary.histogram('logits_current', logits_current)

# Reshape the action tensor
action_num = tf.reshape(tf.cast(features['action_num'], tf.int32), [params['batch_size'], 1])
# Generate a counter
counter = tf.reshape(tf.range(params['batch_size']), [params['batch_size'], 1])
actions = tf.concat([counter,
                     action_num], axis=1)
print('shape of actions ' + str(actions.get_shape()))

# Get the Q values based on the actual action taken for each observation - flatten to (batch size, 1)
predicted_q_values = tf.transpose(tf.gather_nd(logits_current, [actions]))
print('shape of predicted q values: ' + str(predicted_q_values.get_shape()))


## next state forward pass
logits_next = nn1_forward(features,
                          params['feature_columns_next_state'],
                        params['hidden_units'],
                        params['n_classes'])
tf.summary.histogram('logits_next', logits_next)

# Get the maximum Q value possible for each observation - flatten to (batch size, 1)
max_next_q_values = tf.reshape(tf.reduce_max(logits_next, axis=1), [params['batch_size'], 1])
print('shape of max q values: ' + str(max_next_q_values.get_shape()))


### Bellman equation
# Rewards is what was fed in as ""labels""
rewards = tf.cast(tf.reshape(labels,  [params['batch_size'], 1]), tf.float64)
expected_q_values = tf.add(rewards,tf.multiply(tf.cast(params['gamma'], tf.float64),
                                               tf.cast(max_next_q_values, tf.float64)))
tf.summary.histogram('expected_q_values', expected_q_values)
print('shape of expected q value: '+ str(expected_q_values.get_shape()))

## Compute predictions - predictions will calculate the Q values without activation
predicted_classes = tf.argmax(logits_current, 1)
if mode == tf.estimator.ModeKeys.PREDICT:
    predictions = {
        'class_ids': predicted_classes[:, tf.newaxis],
        'probabilities': tf.nn.softmax(logits_current),
        'logits': logits_current,
    }
    return tf.estimator.EstimatorSpec(mode, predictions=predictions)


# Compute loss
loss = tf.losses.mean_squared_error(labels=tf.cast(expected_q_values, tf.float32),
                                    predictions=tf.cast(predicted_q_values, tf.float32))


# Compute evaluation metrics.
if mode == tf.estimator.ModeKeys.EVAL:
    return tf.estimator.EstimatorSpec(
        mode, loss=loss, eval_metric_ops=loss)

# Create training op.
assert mode == tf.estimator.ModeKeys.TRAIN

optimizer = tf.train.AdamOptimizer(learning_rate=0.01)
train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())
return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)
</code></pre>
",0
49389898,Tensorflow model prediction produce results more than the data provided,"<p>I have created a tensorflow model using tensorflow High level API tf.estimator.Estimator based on this <a href=""https://www.tensorflow.org/tutorials/layers"" rel=""nofollow noreferrer"">Tutorial</a>. Instead of downloading MNIST data from internet I read it from a local CSV file. and it was working fine. However, when I migrated the code to use tf.contrib.learn.Estimator I had a problem with the number of predictions. My test_data is 28000 images, but I get back 28032 predictions as a result of model prediction.</p>

<p>Prediction code:</p>

<pre><code>predict_input_fn = tf.contrib.learn.io.numpy_input_fn(
  x={""x"": test_data},
  num_epochs=1,
  shuffle=False)

predictions = list(mnist_classifier.predict(input_fn=predict_input_fn))
</code></pre>

<p>test_data is array of shape (28000,28,28,1)
but len(predictions) = 28032
Anyone know what can be the problem? </p>
",0
49390211,Tensorflow WarmStartSettings embedding shape mismatch,"<p>I am using the new <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/WarmStartSettings"" rel=""nofollow noreferrer""><code>tf.estimator.WarmStartSettings</code></a> to initialize my network from a previous checkpoint. I now want to run the same network on a new data source, with other vocabs to use for the embeddings.</p>

<p>This snippet from the <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/WarmStartSettings"" rel=""nofollow noreferrer"">documentation page of WarmStartSettings</a> seems to describe my use case:</p>

<blockquote>
  <p>Warm-start all weights but the embedding parameters corresponding to
  sc_vocab_file have a different vocab from the one used in the current
  model:</p>

<pre><code>vocab_info = ws_util.VocabInfo(
    new_vocab=sc_vocab_file.vocabulary_file,
    new_vocab_size=sc_vocab_file.vocabulary_size,
    num_oov_buckets=sc_vocab_file.num_oov_buckets,
    old_vocab=""old_vocab.txt""
)
ws = WarmStartSettings(
    ckpt_to_initialize_from=""/tmp"",
    var_name_to_vocab_info={
        ""input_layer/sc_vocab_file_embedding/embedding_weights"": vocab_info
    })
</code></pre>
</blockquote>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/VocabInfo"" rel=""nofollow noreferrer""><code>tf.estimator.VocabInfo</code></a> allows to specify the old and new vocab with their respective sizes. However, when I try to use the WarmStartSettings as shown above with 2 vocabs of different sizes, I get the following error:</p>

<blockquote>
  <p>ValueError: Shape of variable input_layer/sc_vocab_file_embedding/embedding_weights
  ((1887, 30)) doesn't match with shape of tensor
  input_layer/sc_vocab_file_embedding/embedding_weights ([537, 30]) from checkpoint reader.</p>
</blockquote>

<p>Why does VocabInfo allow to provide separate sizes for the vocabs if their size has to match anyway?</p>
",0
49393659,tf.Data: what are stragglers in parallel interleaving?,"<p><a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/data/Dataset#interleave"" rel=""noreferrer""><code>interleave</code></a> is a <code>tf.Data.Dataset</code> method that can be used to interleave together elements from multiple datasets. <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/data/parallel_interleave"" rel=""noreferrer""><code>tf.contrib.data.parallel_interleave</code></a> provides a parallel version of the same functionality with the help of <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply"" rel=""noreferrer""><code>apply</code></a>.</p>

<p>I can see that reading from many datasets in parallel and having buffers for them as allowed by the parallel version will improve throughput. But the <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/data/parallel_interleave"" rel=""noreferrer"">documentation</a> also has this to say about how <code>parallel_interleave</code> can increase data throughput:</p>

<blockquote>
  <p>Unlike tf.data.Dataset.interleave, it gets elements from cycle_length
  nested datasets in parallel, which increases the throughput,
  especially in the presence of stragglers.</p>
</blockquote>

<p>What exactly are stragglers, and why does <code>parallel_interleave</code> work especially well in terms of throughput in their presence?</p>
",1
49416931,What does the tensorflow.python.eager.tape do in the implementation of tf.contrib.eager.custom_gradient?,"<p>I am going through TensorFlow Eager Execution from <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/g3doc/guide.md"" rel=""nofollow noreferrer"">here</a> and find it difficult to understand the customizing gradients part. </p>

<pre><code>@tfe.custom_gradient
def logexp(x):
    e = tf.exp(x)
    def grad(dy):
        return dy * (1 - 1/(1 + e))
    return tf.log(1 + e), grad
</code></pre>

<p>First, it is difficult to make sense what does dy do in the gradient function.</p>

<p>When I read the implementation of tf.contrib.eager.custom_gradient.
I can't really make sense the working mechanism behind tape. Following is the code I borrow from the implementation of tf.contrib.eager.custom_gradient. Can anybody explain what does tape do here?</p>

<pre><code>from tensorflow.python.eager import tape
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import gen_array_ops
from tensorflow.python.util import nest
from tensorflow.python.framework import ops as tf_ops

def my_custom_gradient(f):
    def decorated(*args, **kwargs):
        for x in args:
            print('args {0}'.format(x))
        input_tensors = [tf_ops.convert_to_tensor(x) for x in args]

        with tape.stop_recording():
            result, grad_fn = f(*args, **kwargs)
            flat_result = nest.flatten(result)
            flat_result = [gen_array_ops.identity(x) for x in flat_result]

        def actual_grad_fn(*outputs):
            print(*outputs)
            return nest.flatten(grad_fn(*outputs))

       tape.record_operation(
            f.__name__, # the name of f, in this case logexp
            flat_result,
            input_tensors,
            actual_grad_fn) # backward_function
       flat_result = list(flat_result)
       return nest.pack_sequence_as(result, flat_result)
return decorated 
</code></pre>

<p>Even though I found the implementation of tape from <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/tape.py"" rel=""nofollow noreferrer"">here</a>. But I can't really get much out of it due the poor documentation.</p>
",1
49465418,Keras: Implementation of sparse softmax cross entropy,"<p>I am working on a Keras implementation of <a href=""https://github.com/salesforce/awd-lstm-lm"" rel=""nofollow noreferrer"">this</a> model. I have put together a data set with around 5 million sequences of length 35 to train the model. With a dictionary size of 50 000 words, the categorical crossentropy implementation in Keras (based on <code>tf.nn.softmax_cross_entropy_with_logits</code>) is very slow, one epoch takes around 12 hours on a Tesla P100 GPU. The most obvious way to speed this up would be to use the sparse softmax cross entropy implementation in Tensorflow. </p>

<p>I wrote a simple custom loss function for this based on a <a href=""https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/21_Machine_Translation.ipynb"" rel=""nofollow noreferrer"">tutorial</a>. </p>

<p>The new custom loss function does speed up the training by a factor of 4, which is fantastic. However, the loss is not decreasing/decreasing very slowly compared to the same number of steps taken with the slow cost function. I was able to reproduce this with some toy data, link below. </p>

<p><a href=""https://erikbrorson.github.io/2018/03/24/Figure-out-problem-with-sparse-cross-softmax/"" rel=""nofollow noreferrer"">Link to blogpost/notebook</a></p>

<p>It seems that Keras isn't calculating the gradients correctly. I have looked around online but cannot seem to find the reason for this annoying issue. Has anyone faced a similar problem before?</p>
",0
49472402,Tensorflow tf.nn.softmax() function performs much better than hand-written softmax,"<p>I'm writing a simple logistic regression with tensorflow. I found out that when using tf.nn.softmax, the algorithm converges much quicker, and in the end the accuracy is higher.
If switched to my own implementation of softmax, the network converges slower, and the end accuracy is not as good.</p>
<p>Here's the code:</p>
<pre><code>SEED = 1025
W = tf.Variable(tf.truncated_normal([image_size * image_size, num_labels], seed=SEED))
b = tf.Variable(tf.zeros([num_labels]))
logits = tf.matmul(train_dataset, W) + b

# My softmax:
y_ = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis=0)
# Tensorflow softmax: 
y_ = tf.nn.softmax(logits)

y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)
loss = -tf.reduce_mean(tf.reduce_sum(train_labels * tf.log(y_clipped), axis=1))
</code></pre>
<p>Using my softmax:</p>
<pre><code>Loss at step 0: 22.213934
Training accuracy: 12.7%
Validation accuracy: 13.2%
Loss at step 100: 12.777291
Training accuracy: 45.3%
Validation accuracy: 45.5%
Loss at step 200: 11.361242
Training accuracy: 48.2%
Validation accuracy: 47.4%
Loss at step 300: 10.658278
Training accuracy: 51.4%
Validation accuracy: 49.7%
Loss at step 400: 9.297832
Training accuracy: 59.2%
Validation accuracy: 56.8%
Loss at step 500: 8.902699
Training accuracy: 62.0%
Validation accuracy: 59.2%
Loss at step 600: 8.681184
Training accuracy: 64.2%
Validation accuracy: 61.0%
Loss at step 700: 8.529438
Training accuracy: 65.8%
Validation accuracy: 62.3%
Loss at step 800: 8.416442
Training accuracy: 66.8%
Validation accuracy: 63.3%
Test accuracy: 70.4%
</code></pre>
<p>Using tensorflow's softmax:</p>
<pre><code>Loss at step 0: 13.555875
Training accuracy: 12.7%
Validation accuracy: 14.5%
Loss at step 100: 2.194562
Training accuracy: 72.5%
Validation accuracy: 72.0%
Loss at step 200: 1.808641
Training accuracy: 75.5%
Validation accuracy: 74.5%
Loss at step 300: 1.593390
Training accuracy: 76.8%
Validation accuracy: 75.0%
Loss at step 400: 1.442661
Training accuracy: 77.7%
Validation accuracy: 75.2%
Loss at step 500: 1.327751
Training accuracy: 78.2%
Validation accuracy: 75.4%
Loss at step 600: 1.236314
Training accuracy: 78.5%
Validation accuracy: 75.6%
Loss at step 700: 1.161479
Training accuracy: 78.9%
Validation accuracy: 75.6%
Loss at step 800: 1.098717
Training accuracy: 79.4%
Validation accuracy: 75.8%
Test accuracy: 83.3%
</code></pre>
<p>From the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/softmax"" rel=""nofollow noreferrer"">documentation</a>, in theory tensorflow's softmax should be exact the same as I implemented, no?</p>
<blockquote>
<p>This function performs the equivalent of</p>
<p>softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)</p>
</blockquote>
<p><strong>EDIT:</strong> I added a seed when initializing from normal distribution, now I can reproduce the accuracy results.
When setting axis value in &quot;My softmax&quot; line, only axis=0 doesn't result in error. Setting axis=1 or axis=-1 both results in this error:</p>
<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 10 and 10000 for 'truediv' (op: 'RealDiv') with input shapes: [10000,10], [10000].
</code></pre>
",1
49505986,How do i get the VALUES of trainable variables from a restored graph & checkpoint in tensorflow,"<p>I want to get the values of the variables from a trained model.  I have a check point file and I can restore graphs and checkpoints and do inference with them just fine.</p>

<p>However, I'm finding it extremely difficult to figure out how to get the trainable variable values (like the weight and bias values, not names...i want the VALUES) after I restore the checkpoint and graph.  I've read through Tensorflow documentation and there's lots of suggestions regarding ""with variable_scope"", ""reuse = True"", and ""tf.get_variable(""myvar"") within the scope...etc, but I get errors stating either the variable already exists or it hasn't been initialized.  tf.graphkeys only returns names...not values.</p>
",1
49564318,Issue with fine-tuning inceptionv3 in slim tensorflow and tf record batches,"<p>I am trying to fine-tune inceptionv3 model using slim tensorflow library. 
I am unable to understand certain things while writing the code for it. I tried to read source code (no proper documentation) and figured out few things and I am able to fine-tune it and save the check point. Here are the steps I followed 
 1. I created a tf.record for my training data which is fine, now I am reading the data using the below code. </p>

<pre><code>import tensorflow as tf
import tensorflow.contrib.slim.nets as nets
import tensorflow.contrib.slim as slim
import matplotlib.pyplot as plt
import numpy as np

# get the data and labels here

data_path = '/home/sfarkya/nvidia_challenge/datasets/detrac/train1.tfrecords'

# Training setting
num_epochs = 100
initial_learning_rate = 0.0002
learning_rate_decay_factor = 0.7
num_epochs_before_decay = 5
num_classes = 5980

# load the checkpoint
model_path = '/home/sfarkya/nvidia_challenge/datasets/detrac/inception_v3.ckpt'

# log directory
log_dir = '/home/sfarkya/nvidia_challenge/datasets/detrac/fine_tuned_model'

with tf.Session() as sess:
    feature = {'train/image': tf.FixedLenFeature([], tf.string),
               'train/label': tf.FixedLenFeature([], tf.int64)}

    # Create a list of filenames and pass it to a queue
    filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)

    # Define a reader and read the next record
    reader = tf.TFRecordReader()
    _, serialized_example = reader.read(filename_queue)

    # Decode the record read by the reader
    features = tf.parse_single_example(serialized_example, features=feature)

    # Convert the image data from string back to the numbers
    image = tf.decode_raw(features['train/image'], tf.float32)

    # Cast label data into int32
    label = tf.cast(features['train/label'], tf.int32)

    # Reshape image data into the original shape
    image = tf.reshape(image, [128, 128, 3])

    # Creates batches by randomly shuffling tensors
    images, labels = tf.train.shuffle_batch([image, label], batch_size=64, capacity=128, num_threads=2,
                                            min_after_dequeue=64)
</code></pre>

<p>Now I am finetuning the model using slim and this is the code. </p>

<pre><code>  init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
    sess.run(init_op)

    # Create a coordinator and run all QueueRunner objects
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)

    # load model

    # load the inception model from the slim library - we are using inception v3
    #inputL = tf.placeholder(tf.float32, (64, 128, 128, 3))

    img, lbl = sess.run([images, labels])
    one_hot_labels = slim.one_hot_encoding(lbl, num_classes)

    with slim.arg_scope(slim.nets.inception.inception_v3_arg_scope()):
        logits, inceptionv3 = nets.inception.inception_v3(inputs=img, num_classes=5980, is_training=True,
                                                          dropout_keep_prob=.6)

    # Restore convolutional layers:

    variables_to_restore = slim.get_variables_to_restore(exclude=['InceptionV3/Logits', 'InceptionV3/AuxLogits'])
    init_fn = slim.assign_from_checkpoint_fn(model_path, variables_to_restore)

    # loss function
    loss = tf.losses.softmax_cross_entropy(onehot_labels=one_hot_labels, logits = logits)
    total_loss = tf.losses.get_total_loss()

    # train operation
    train_op = slim.learning.create_train_op(total_loss + loss, optimizer= tf.train.AdamOptimizer(learning_rate=1e-4))

    print('Im here')
    # Start training.
    slim.learning.train(train_op, log_dir, init_fn=init_fn, save_interval_secs=20, number_of_steps= 10)
</code></pre>

<p>Now I have few questions about the code, which I am quite unable to figure out. Once, the code reaches <strong>slim.learning.train</strong> I don't see anything printing however, it's training, I can see in the log. Now, 
1. How do I give the number of epochs to the code? Right now it's running step by step with each step has batch_size = 64.<br>
2. How do I make sure that in the code <strong>tf.train.shuffle_batch</strong> I am not repeating my images and I am training over the whole dataset? 
3. How can I print the loss values while it's training?</p>
",1
49615689,Tensorflow returns too many predictions,"<p>I am working on a TensorFlow CNN Model for mnist, modifying this example: <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network_raw.py"" rel=""nofollow noreferrer"">https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network_raw.py</a>. </p>

<p>When running the testing 256 mnist images, it returns 784 predictions instead of 256. I am guessing the 784 comes from the mnist image size (28 pixels x 28 pixels = 784), however I am unclear where the axis alignment could be going wrong, if it is in fact an axis alignment issue. </p>

<p>Specifically I get the following error from this line in the code <code>correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1)</code>):</p>

<pre><code>InvalidArgumentError (see above for traceback): Incompatible shapes: [784] vs. [256]
     [[Node: Equal = Equal[T=DT_INT64, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](ArgMax, ArgMax_1)]]
</code></pre>

<p><strong>Where things may be going wrong</strong>: </p>

<ul>
<li>My weight and bias matrix dimensions may not be aligning like I expect. I am attempting to use manually defined weights/biases, and while I think the dimensions align, I am clearly missing something. </li>
<li><p>I may not be using the dense layer correctly. I added the dense layer to go from the weights (4x4x50x500) to the correct size for the classification layer (500x10). The image below shows debugging info and these dimensions. </p>

<p><a href=""https://i.stack.imgur.com/zvYe5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zvYe5.png"" alt=""Screen shot of debugging""></a></p></li>
</ul>

<p><strong>Code</strong></p>

<pre><code>import tensorflow as tf
import pickle

input_size = 28  # e.g. 28x28 input
model = pickle.load(open(""model.p"", ""rb"" ))

weights = {
    'wc1': tf.Variable(model[0]['weights']),  # 5x5x20
    'wc2': tf.Variable(model[2]['weights']),  # 5x5x20x50
    'wd1': tf.Variable(model[4]['weights']),  # 4x4x50x500
    'out': tf.Variable(model[5]['weights'])  # 500x10
}

biases = {
    'bc1': tf.Variable(model[0]['bias']),  # 20
    'bc2': tf.Variable(model[2]['bias']),  # 50
    'bd1': tf.Variable(model[4]['bias']),  # 500
    'out': tf.Variable(model[5]['bias']),  # 10
    }


# Import MNIST data
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""./MNIST-data/"", one_hot=True)

# tf Graph input
X = tf.placeholder(tf.float32, [None, 784])
Y = tf.placeholder(tf.float32, [None, 10])
keep_prob = tf.placeholder(tf.float32)  # dropout (keep probability)



# Create some wrappers for simplicity
def conv2d(x, W, b, strides=1):
    # Conv2D wrapper, with bias and relu activation
    x = tf.nn.conv2d(x, W,strides=[1, strides, strides, 1], padding='SAME')
    x = tf.nn.bias_add(x, b)
    return tf.nn.relu(x)

def maxpool2d(x, k=2):
    # MaxPool2D wrapper
    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],
                          padding='SAME')

# Create model
def conv_net(x, weights, biases):
    x = tf.reshape(x, shape=[-1, input_size, input_size, 1])
    conv1 = conv2d(x, tf.reshape(weights['wc1'], shape=[5, 5, 1, 20]), biases['bc1'])
    pool1 = maxpool2d(conv1, k=2)
    conv2 = conv2d(pool1, weights['wc2'], biases['bc2'])
    pool2 = maxpool2d(conv2, k=2)

    # Fully connected layer
    # Reshape conv2 output to fit fully connected layer input
    pool2_flat = tf.reshape(pool2, [-1, 4 * 4 * 50])
    dense = tf.layers.dense(inputs=pool2_flat, units=500, activation=tf.nn.relu)

    # Output, class prediction
    out = tf.add(tf.matmul(dense, weights['out']), biases['out'])  # shape = (?, 10)
    return out


logits = conv_net(X, weights, biases)
prediction = tf.nn.softmax(logits)

# # Evaluate model
correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    # Calculate accuracy for 256 MNIST test images
    print(""Testing Accuracy:"", \
          sess.run(accuracy, {X: mnist.test.images[:256], Y: mnist.test.labels[:256], keep_prob: 1.0}))
</code></pre>
",0
49619995,How to control when to compute evaluation vs training using the Estimator API of tensorflow?,"<p>As stated in <a href=""https://stackoverflow.com/questions/45952149/tensorflow-estimator-periodic-evaluation-on-eval-dataset"">this question</a>:</p>

<blockquote>
  <p>The tensorflow documentation does not provide any example of how to perform a periodic evaluation of the model on an evaluation set</p>
</blockquote>

<p>The accepted answer suggested the use of Experiment (which is deprecated according to <a href=""https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/contrib/learn/README.md"" rel=""noreferrer"">this README</a>).</p>

<p>All I found on online points towards using the <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate"" rel=""noreferrer"">train_and_evaluate</a> method. However, I still do not see how to switch between the two processes (train and evaluate). I have tried the following:</p>

<pre><code>estimator = tf.estimator.Estimator(
    model_fn=model_fn,
    params=hparams,
    model_dir=model_dir,
    config = tf.estimator.RunConfig(
        save_checkpoints_steps = 2000,
        save_summary_steps = 100,
        keep_checkpoint_max=5
    )
)

train_input_fn = lambda: input_fn(
    train_file, #a .tfrecords file
    train=True,
    batch_size=70,
    num_epochs=100
)

eval_input_fn = lambda: input_fn(
    val_file, # another .tfrecords file
    train=False,
    batch_size=70,
    num_epochs=1
)
train_spec = tf.estimator.TrainSpec(
    train_input_fn,
    max_steps=125
)    

eval_spec = tf.estimator.EvalSpec(
    eval_input_fn,
    steps=30,
    name='validation',
    start_delay_secs=150,
    throttle_secs=200
)

tf.logging.info(""start experiment..."")
tf.estimator.train_and_evaluate(
    estimator,
    train_spec,
    eval_spec
)
</code></pre>

<p>Here is what I think my code should be doing:</p>

<blockquote>
  <p>Train the model for 100 epochs using a batch size of 70; save checkpoints every 2000 batches; save summaries every 100 batches; keep at most 5 checkpoints; after 150 batches on the training set, compute the validation error using 30 batches of validation data</p>
</blockquote>

<p>However, I get the following logs:</p>

<pre><code>INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 1 into /output/model.ckpt.
INFO:tensorflow:loss = 39.55082, step = 1
INFO:tensorflow:global_step/sec: 178.622
INFO:tensorflow:loss = 1.0455043, step = 101 (0.560 sec)
INFO:tensorflow:Saving checkpoints for 150 into /output/model.ckpt.
INFO:tensorflow:Loss for final step: 0.8327793.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2018-04-02-22:49:15
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from /projects/MNIST-GCP/output/model.ckpt-150
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Evaluation [3/30]
INFO:tensorflow:Evaluation [6/30]
INFO:tensorflow:Evaluation [9/30]
INFO:tensorflow:Evaluation [12/30]
INFO:tensorflow:Evaluation [15/30]
INFO:tensorflow:Evaluation [18/30]
INFO:tensorflow:Evaluation [21/30]
INFO:tensorflow:Evaluation [24/30]
INFO:tensorflow:Evaluation [27/30]
INFO:tensorflow:Evaluation [30/30]
INFO:tensorflow:Finished evaluation at 2018-04-02-22:49:15
INFO:tensorflow:Saving dict for global step 150: accuracy = 0.8552381, global_step =150, loss = 0.95031387
</code></pre>

<p>From the logs, it seems that the training stops after the first evaluation step. What am I missing from the documentation? Could you explain me how I should have implemented what I think my code is doing? </p>

<p>Additional info I am running everything using the MNIST dataset, which has 50,000 images in the training set, so (I think) the model should run for *num_epochs*50,000/batch_size ≃ 7,000 steps* </p>

<p>I sincerely appreciate your help!</p>

<p>EDIT: after running experiments I realize that max_steps controls the number of steps of the whole training procedure, not just the amount of steps before computing the metrics on the test set. Reading tf.estimator.Estimator.train, I see it has a steps argument, which works incrementally and is bounded by max_steps; however, tf.estimator.TrainSpec does not have the steps argument, which means I cannot control the number of steps to take before computing metrics on the validation set.</p>
",0
49636845,TensorFlow split integer sequence everywhere that a given int occurs?,"<p>I am training a character-level model, currently experimenting with adding more features to the character embeddings. Namely, concatenating the embedding of the word it is in. So I need to be able to detect, <strong>using TensorFlow ops</strong>, where spaces occur and unstack.</p>

<p>I.e. suppose the integer for a space is 2, and the tensor is <code>[3,5,2,3,2,1,1,1]</code>. Then I need to unstack this to <code>[3,5], [2], [3], [2], [1,1,1]</code>. I can't find anything in the documentation to do this; help is much appreciated.</p>
",1
49642343,Why does the TensorFlow Estimator API take inputs as a lambda?,"<p>The <code>tf.estimator</code> API takes input ""input functions"" that return <code>Dataset</code>s. For example, <code>Estimator.train()</code> takes an <code>input_fn</code> (<a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#train"" rel=""noreferrer"">documentation</a>).</p>

<p>In the examples I've seen, whenever this function is supplied manually, it is an argumentless lambda.</p>

<p>Doesn't that mean that the function always returns the same value? Or is it invoked multiple times with no arguments? I wasn't able to find documentation about this. Why don't functions like <code>train()</code> just take input as a <code>Dataset</code> explicitly?</p>
",1
49662470,Tensorflow global_step TypeError,"<p>I'm adding Tensorboard to an existing small Tensorflow project that I know works to practice working with Tensorboard but I get a type error that global_step must be a string or tensor, however I have assigned global_step to a <code>tf.Variable(0, name='global_step', trainable=False)</code> just like the documentation and every example I see online. Any idea of what I'm missing would be super appreciated. </p>



<p>---> 70             [summary_merge, tf.train.global_step(sess, global_step_tensor) ,update_model, weights], feed_dict)</p>

<p>TypeError: Fetch argument 0 has invalid type , must be a string or Tensor. (Can not convert a int into a Tensor or Operation.)</p>

<pre class=""lang-python prettyprint-override""><code>import gym
import numpy as np
import random
import ipdb
import matplotlib.pyplot as plt
import tensorflow as tf
%matplotlib inline

tf.reset_default_graph()
env = gym.make('FrozenLake-v0')

global_step_tensor = tf.Variable(0, name='global_step', trainable=False)
saver = tf.train.Saver()

with tf.name_scope('policy-network'):
    observations = tf.placeholder(shape=[1, env.observation_space.n], dtype=tf.float32, name='input')
    weights = tf.Variable(tf.random_uniform([16,4], 0, 0.01))
    Qout = tf.matmul(observations, weights)
    predict = tf.argmax(Qout, 1)

    nextQ = tf.placeholder(shape=[1, 4], dtype=tf.float32)
    loss = tf.reduce_sum(tf.square(nextQ - Qout))
    trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
    update_model = trainer.minimize(loss, global_step= global_step_tensor)

with tf.name_scope('summaries'):
    summary_merge = tf.summary.merge([
        tf.summary.histogram('loss', loss),
        tf.summary.histogram('weights', weights)
    ])


init = tf.global_variables_initializer()

gamma = .99
epsilon = 0.1
num_episodes = 2000

j_list = []
r_list = []

with tf.Session() as sess:
    # Create a writer for summary data
    writer = tf.summary.FileWriter('/tmp/display', sess.graph)
    sess.run(init)
    for i in range(num_episodes):
        s = env.reset()
        reward_all = 0
        done = False
        j = 0

        while j &lt; 99:
            j+= 1

            a, allQ = sess.run([predict, Qout], feed_dict={observations:np.identity(16)[s:s+1]})
            if np.random.rand(1) &lt; epsilon:
                a[0] = env.action_space.sample()

            s1, reward, done, _ = env.step(a[0])
            Q1 = sess.run(Qout, feed_dict={observations: np.identity(16)[s1:s1+1]}) 

            maxQ1 = np.max(Q1)
            targetQ = allQ

            targetQ[0, a[0]] = reward + gamma*maxQ1
            feed_dict = {observations: np.identity(16)[s:s+1], nextQ:targetQ}

            # Run prediction operation, evaluate summary_merge and assign to summaries
            summaries, global_step_tensor, _, W1 = sess.run(
            [summary_merge, tf.train.global_step(sess, global_step_tensor) ,update_model, weights], feed_dict)

            # Write summaries to writer
            writer.add_summary(summaries, global_step)

            reward_all += reward
            s = s1

            if done == True:
                epsilon = 1./((i/50) + 10)
                break

        if (i+1) % 1000 == 0:
            # Save the graph every 1000 episodes
            saver.save(sess, '/tmp/checkpoint', global_step=global_step)

        j_list.append(j)
        r_list.append(reward_all)

print(""Percent of succesful episodes: {} %"".format((sum(r_list)/num_episodes)))
</code></pre>
",1
49747331,"In Tensorflow, what is the difference between all trace_level in the member of tf.RunOptions","<p>When i use the following code to generate timeline, I select <code>trace_level</code> as FULL_TRACE. But I want to know what is the difference between all the Class Members.</p>

<pre><code>sess.run(predict , feed_dict=xxx ,options= tf.RunOptions(trace_level=FULL_TRACE ))
</code></pre>

<p>Class Members in tf.RunOptions is at the following link:
<a href=""https://www.tensorflow.org/api_docs/python/tf/RunOptions"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/RunOptions</a></p>

<pre><code>FULL_TRACE
HARDWARE_TRACE
NO_TRACE
SOFTWARE_TRACE
</code></pre>
",0
49761611,"In tensorflow estimator, what does it mean for num_epochs to be None?","<p>I am really confused about the documentation of tensorflow estimator <code>tf.estimator.inputs.numpy_input_fn</code> <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/numpy_input_fn"" rel=""noreferrer"">here</a>, and specifically about the line on <code>num_epochs</code>:</p>

<p><code>num_epochs: Integer, number of epochs to iterate over data. If None will run forever.</code></p>

<p>If I set <code>num_epochs</code> to <code>None</code>, the training would run forever??<br>
What does it even mean for it to run forever?? </p>

<p>It doesn't make sense to me since I cannot imagine people would design the program in such a way that it might run forever. </p>

<p>Could someone explain? </p>

<hr>

<p>ANSWER MY OWN Question: 
I think I've found the answer in here: <a href=""https://www.tensorflow.org/versions/r1.3/get_started/input_fn#evaluating_the_model"" rel=""noreferrer"">https://www.tensorflow.org/versions/r1.3/get_started/input_fn#evaluating_the_model</a></p>

<p>Specifically, in the part <code>Building the input_fn</code>:</p>

<p><code>Two additional arguments are provided: num_epochs: controls the number of epochs to iterate over data. For training, set this to None, so the 
input_fn keeps returning data until the required number of train steps is reached. For evaluate and predict, set this to 1, so the input_fn will iterate over the data once and then raise OutOfRangeError. That error will signal the Estimator to stop evaluate or predict.</code></p>
",1
49765164,How to choose a RNN cell from a list of RNN cells during runtime in TensorFlow,"<p>My network requires a number of independent <code>MultiRNNCells</code>, stored as a python list <code>multirnn_cells</code> right now. And during runtime, based on the value of a scalar placeholder <code>i</code>, I need to do step through the respective RNN cell <code>multirnn_cells[i]</code>. How can I do this?</p>

<p>I noticed in the TensorFlow control flow documentation about <code>tf.case</code>. This is not suitable since <code>tf.case</code> expects the tuple functions to return tensors and not anything else.</p>
",1
49770934,Prevent a Variable from being converted into a Const when freezing a TensorFlow graph,"<p>I'm trying to use the freeze_graph.py tool to save a model, but have run into an issue.</p>

<p>There is a variable in my tensorflow graph which I either assign to using tf.assign, or feed before each inference. I need it to remain a variable because tf.assign requires a mutable tensor and you also can't feed to a const, but the freeze_graph script converts all variables into constants.</p>

<p>I've noticed that freeze_graph has whitelist and blacklist parameters, but I can't for the life of me find any documentation on what these are or how to use them. What can I do here?</p>

<p><em>edit:</em></p>

<p><code>single_c</code> and <code>single_h</code> are the variables I'd like to preserve:</p>

<pre><code>single_c = tf.Variable(tf.random_uniform([num_lstm_cells], 0, 1), trainable=True)
expanded_c = tf.reshape(single_c, [1, num_lstm_cells])
batched_c = tf.tile(expanded_c, tiling_shape, name='c')

single_h = tf.Variable(tf.random_uniform([num_lstm_cells], 0, 1), trainable=True)
expanded_h = tf.reshape(single_h, [1, num_lstm_cells])
batched_h = tf.tile(expanded_h, tiling_shape, name='h')

state = tf.contrib.rnn.LSTMStateTuple(batched_c, batched_h)
</code></pre>

<p>because I assign them using:</p>

<pre><code>restore_c = tf.assign(single_c, c_holder)
restore_h = tf.assign(single_h, h_holder)
</code></pre>

<p>and I feed <code>state</code> using</p>

<pre><code>_, er, new = sess.run([train_nn_step, error, new_state], feed_dict={batch_ph: batch_size, prob: training_dropout, state: new})
</code></pre>

<p>I can't assign or feed if <code>single_c</code> and <code>single_h</code> become constants</p>
",1
49785239,TensorFlow.constant() throws TypeError with placeholder parameter (TF v1.4),"<p>Python host language. TF v1.4.</p>

<p><a href=""https://www.tensorflow.org/versions/r1.4/api_docs/python/tf/constant"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r1.4/api_docs/python/tf/constant</a></p>

<p>Nearly just like docs example (above), I need to make a constant 2-D tensor populated with scalar value, in my case some mean value, which is mean of r, but r is a placeholder, not a variable, NOT a numpy array. As such, feed_dict needs to be used to fill-in placeholder r in my application.  Now a python ndarray or some python list was used in the docs example, which won't work in my application, so that's the difference.</p>

<p>In my application, global_bias (below) needs to be a tf.constant not a tf.variable -- because what I'm saying is, the optimizer needs to not learn it. Just leave it be, please.</p>

<p>Note: In my application, r is a placeholder not a variable.</p>

<pre><code>#global_bias_value = tf.reduce_mean(r)    # tf.constant() FAILS (below)
global_bias_value = 87. # tf.constant() runs OK (below)
global_bias = tf.constant(global_bias_value, shape=(I,J)) #TypeError: List of Tensors when single Tensor expected
rhat = global_bias + fm
</code></pre>

<p>So, as I said, what TF code would be used to make a constant 2-D tensor populated with scalar value, in my case some mean value, which is mean of r, but r is a placeholder, not a variable, NOT a numpy array?</p>

<p>Hey I tried .eval() but TF said you need a session to do that. So are you TF folks saying I need 2 sessions, run sequentially, just to do this little thing?  Or else are you saying I should bypass feed_dict to feed in my r placeholder actual data via numpy arrays in through the side door (in app memory as a python list or ndarray) plus the front door (feed_dict), which would be janky?  </p>

<p>Thanks for clues!   I need to get one.</p>
",1
49808973,Tensorflow estimator: custom input_fn not working,"<p>I am reading Tensorflow's <a href=""https://www.tensorflow.org/get_started/custom_estimators#write_an_input_function"" rel=""nofollow noreferrer"">sample code</a> to build my first CNN. The original code worked very well. However, when I tried to use my own input function rather than original <code>tf.estimator.inputs.numpy_input_fn</code>, I got a problem.</p>
<p>My code is as following, which is similar to <a href=""https://www.tensorflow.org/get_started/premade_estimators#create_input_functions"" rel=""nofollow noreferrer"">this tutorial</a>:</p>
<pre><code>def train_input_fn(x, y):
    dataset = tf.data.Dataset.from_tensor_slices(({&quot;x&quot;: x}, y))
    dataset = dataset.shuffle(100000).repeat().batch(100)
    return dataset


my_classifier.train(
    input_fn=lambda: train_input_fn(train_data, train_labels),
    steps=200,
    hooks=[logging_hook],
)
</code></pre>
<p>However, when I run that code, Tensorflow stuck, after printing the following to console:</p>
<pre><code>INFO:tensorflow:Using config: {'_is_chief': True, '_task_type': 'worker', '_save_checkpoints_secs': 600, '_task_id': 0, '_save_summary_steps': 10, '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_evaluation_master': '', '_service': None, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x00000217B16B64E0&gt;, '_tf_random_seed': None, '_session_config': None, '_log_step_count_steps': 100, '_keep_checkpoint_max': 5, '_global_id_in_cluster': 0, '_model_dir': './model', '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_save_checkpoints_steps': None}
WARNING:tensorflow:Estimator's model_fn (&lt;function cnn_model_fn at 0x000002179C3231E0&gt;) includes params argument, but params are not passed to Estimator.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
2018-04-13 11:23:31.303380: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-04-13 11:23:32.071305: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1344] Found device 0 with properties: 
name: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.0975
pciBusID: 0000:01:00.0
totalMemory: 2.00GiB freeMemory: 1.65GiB
2018-04-13 11:23:32.071834: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1423] Adding visible gpu devices: 0
2018-04-13 11:23:33.064563: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-04-13 11:23:33.064904: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:917]      0 
2018-04-13 11:23:33.065131: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:930] 0:   N 
2018-04-13 11:23:33.065460: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1041] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1420 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)
INFO:tensorflow:Restoring parameters from ./model\model.ckpt-800
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
</code></pre>
<p>After that, Tensorflow neither did the training nor quitted, which is so annoying.</p>
<p>Now my guess is that at every training step, it'll call <code>train_input_fn</code> once. Since the function will shuffle the entire MNIST dataset, it might be computationally inefficient.</p>
<p>To validate my idea, I tried to shrink the size of the dataset from 60000 to 100, and the code worked perfectly.</p>
<p>So what is the solution to this? How can I write my custom input function without such problems? I hope someone could provide me with some guidance on this.</p>
",0
49852162,Train my model on GCP TPU,"<p>What architecture of program can run in TPU?</p>

<p>Is it must to use <code>tf.contrib.tpu.TPUEstimator</code>?</p>

<p>Is there any tpu-program example for reference except the tensorflow official model?</p>
",0
49874387,What's the name arguments in string_to_hash_bucket_fast for?,"<p>Just new to tensorflow. When checking impl for <code>feature_column.categorical_column_with_hash_bucket</code>, I found this code:</p>

<pre><code>sparse_id_values = string_ops.string_to_hash_bucket_fast(sparse_values, 
self.hash_bucket_size, name='lookup')
</code></pre>

<p>Not sure why use <code>name='lookup'</code> here, is it related with <code>lookup_ops.py</code>? Documentation specified in <code>tf.string_to_hash_bucket_fast</code>:</p>

<blockquote>
  <p>name: A name for the operation (optional).</p>
</blockquote>

<p>But not quite understand, trying to go deeper in source code, found it's wrapped in go/wrapper in a interface, even can't find a detailed algo impl. Any suggestions?</p>
",1
49890477,Log accuracy metric while training a tf.estimator,"<p>What's the simplest way to print accuracy metrics along with the loss when training a pre-canned estimator?</p>

<p>Most tutorials and documentations seem to address the issue of when you're creating a custom estimator -- which seems overkill if the intention is to use one of the available ones.</p>

<p>tf.contrib.learn had a few (now deprecated) Monitor hooks. TF now suggests using the hook API, but it appears that it doesn't actually come with anything that can utilize the labels and predictions to generate an accuracy number. </p>
",1
49899526,Tensorflow input pipeline where multiple rows correspond to a single observation?,"<p>So I've just started using Tensorflow, and I'm struggling to properly understand input pipelines. </p>

<p>The problem I'm working on is sequence classification.
I'm trying to read in a CSV file with shape (100000, 4). First 3 columns are features, 4th column is the label. BUT - the data represents sequences of length 10 i.e. rows 1-10 are sequence 1, rows 11-20 are sequence 2 etc. This also means each label is repeated 10 times.</p>

<p>So at some point in this input pipeline, I'll need to reshape my feature tensor like tf.reshape(features, [batch_size_, rows_per_ob, input_dim]). 
And only take every 10th row of my label tensor like label[::rows_per_ob]</p>

<p>Another thing I should point out is that my actual dataset is in the billions of rows so I have to think about performance.</p>

<p>I've put together the below code from documentation and other posts on here, but I don't think I fully understand this because I'm seeing the following error:</p>

<blockquote>
  <p>INFO:tensorflow:Error reported to Coordinator: , Attempting to use uninitialized value input_producer_2/limit_epochs/epochs</p>
</blockquote>

<p>There seems to be an out of range error.</p>

<p>I also can't figure out what to do with these batches once I get them working. Initially, I thought I would reshape them then just feed them into ""feed_dict"", but then I read that this is really bad, and I should be using a tf.data.Dataset object. But I'm not sure how to feed these batches into a Dataset. I'm also not entirely sure when would be the optimal time in this process to reshape my data?</p>

<p>And a final point of confusion - when you use an Iterator with a Dataset object, I see that we use the get_next() method. Does this mean that each element in the Dataset represent a full batch of data? And does this then mean that if we want to change the batch size, we need rebuild the entire Dataset object?</p>

<p>I'm really struggling to fit all the pieces together. If anyone has any pointers for me, it would be very much appreciated! Thanks!</p>

<pre><code># import
import tensorflow as tf

# constants
filename = ""tensorflow_test_data.csv""
num_rows = 100000
rows_per_ob = 10
batch_size_ = 5
num_epochs_ = 2
num_batches = int(num_rows * num_epochs_ / batch_size_ / rows_per_ob)

# read csv line
def read_from_csv(filename_queue):
    reader = tf.TextLineReader(skip_header_lines=1)
    _, value = reader.read(filename_queue)
    record_defaults = [[0.0], [0.0], [0.0], [0.0]]
    a, b, c, d = tf.decode_csv(value, record_defaults=record_defaults)
    features = tf.stack([a, b, c])
    return features, d

def input_pipeline(filename=filename, batch_size=batch_size_, num_epochs=num_epochs_):
    filename_queue = tf.train.string_input_producer([filename],
                                                    num_epochs=num_epochs,
                                                    shuffle=False)
    x, y = read_from_csv(filename_queue)
    x_batch, y_batch = tf.train.batch([x, y],
                                      batch_size = batch_size * rows_per_ob,
                                      num_threads=1,
                                      capacity=10000)
    return x_batch, y_batch

###
x, y = input_pipeline(filename, batch_size=batch_size_,
                      num_epochs = num_epochs_)

# I imagine using lists is wrong here - this was more just for me to
# see the output
x_list = []
y_list = []
with tf.Session() as sess:
    coord = tf.train.Coordinator()
    threads = tf.train.start_queue_runners(coord=coord)
    for _ in range(num_batches):
        x_batch, y_batch = sess.run([x, y])
        x_list.append(x_batch)
        y_list.append(y_batch)
    coord.request_stop()
    coord.join(threads)
</code></pre>
",1
49997294,Moving away from tf.contrib.learn: distributed training with dedicated evaluator process,"<p>In TF 1.8's upcoming release, <code>tf.contrib.learn.*</code> will be deprecated.
The <code>tf.contrib.learn.Experiment</code> class recommends switching to <code>tf.estimator.train_and_evaluate</code> instead, so I'm trying to port my code to that framework.</p>

<p>What I want to do is set up distributed training on two machines' GPUs, plus a third CPU-only process that does continuous evaluation on a small validation set.</p>

<p>Following the examples in <a href=""https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/estimator/train_and_evaluate"" rel=""nofollow noreferrer"">the documentation of <code>train_and_evaluate</code></a> and the <a href=""https://www.tensorflow.org/deploy/distributed"" rel=""nofollow noreferrer"">Distributed Tensorflow</a> guide, I managed to set up the training half of my desired architecture, but I can't find a way to set up an estimator.</p>

<p>So far, what I have looks as follows:</p>

<pre><code>def input_fn(mode, num_classes, batch_size):  
  # [...] build input pipeline
  return {'input': images}, labels

def model_fn(features, labels, num_classes, mode):
  # [...] build model
  return tf.estimator.EstimatorSpec(
    mode=mode,
    predictions=predictions,
    loss=total_loss,
    train_op=train_op,
    eval_metric_ops=metrics,
    export_outputs=export_outputs)

def distributed_main_v2(unused_argv):
  """"""Expects `unused_argv` to be a list ['&lt;task_type&gt;', '&lt;task_id&gt;']""""""  
  import json
  # Set up environment variables according to the parameters passed to the process
  TF_CONFIG = {
    'cluster': {
        ""ps"": [
            ""host1:2222"",
        ],
        ""chief"": [
            ""host1:2223"",
            ],
        ""worker"": [
            ""host2:2224""
            ]
    },
    'environment': 'cluster',    
    'task': {
        'type': unused_argv[1].strip(),
        'index': unused_argv[2].strip() if len(unused_argv) &gt; 2 else 0
        }
  }
  os.environ['TF_CONFIG'] = json.dumps(TF_CONFIG)
  if unused_argv[1].strip() not in ['worker', 'chief']:
    os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # leave the GPU to the worker process

  # create the estimator
  # define warm start configuration
  regex = '^(?!.*final_layer*|.*aux_logits*)'
  ws_settings = tf.estimator.WarmStartSettings('checkpoint_path', regex)

  gpu_opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.95) # fix for cuDNN fatal memory error with tf.contrib.learn.Experiment (TODO: still necessary?)
  sess_conf = tf.ConfigProto(gpu_options=gpu_opts)
  run_conf = tf.estimator.RunConfig(session_config=sess_conf)

  # Create the Estimator
  estimator = tf.estimator.Estimator(
    model_fn=lambda features, labels, mode: model_fn(features, labels, NUM_CLASSES, mode),
    model_dir=model_dir,
    config=run_conf,
    warm_start_from=ws_settings)

  # Set up input functions for training and evaluation
  train_input_fn = lambda : input_fn(tf.estimator.ModeKeys.TRAIN, NUM_CLASSES, batch_size)
  eval_input_fn = lambda : input_fn(tf.estimator.ModeKeys.EVAL, NUM_CLASSES, batch_size)

  train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=steps)
  eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)

  # start distributed training
  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)

if __name__ == '__main__':
  # set up globals and parse known arguments
  distributed_main_v2(unused_argv)
</code></pre>

<p>This code works, although my understanding of it is still pretty limited. I get what the PS and workers do, but from the specification of <code>chief</code> I understand this should be the ""master"" worker that also logs summaries and saves checkpoints. What I'm missing now is the periodic evaluation... and I'm at a loss. From the <code>train_and_evaluate</code> codebase I see there's some ""evaluator"" support but I don't understand how to set it up properly.</p>
",1
50029121,How to use tf.layers classes instead of functions,"<p>It seems that tf.Layer modules come in two flavours: functions and classes. I normally use the functions directly (e.g, tf.layers.dense) but I'd like to know how to use classes directly (tf.layers.<strong>D</strong>ense). I've started experimenting with the new eager execution mode in tensorflow and I think using classes are going to be useful there as well but I haven't seen good examples in the documentation. Is there any part of TF documentation that shows how these are used? </p>

<p>I guess it would make sense to use them in a class where these layers are instantiated in the <code>__init__</code> and then they're linked in the <code>__call__</code> method when the inputs and dimensions are known?</p>

<p>Are these tf.layer classes related to <code>tf.keras.Model</code>? Is there an equivalent wrapper class for using <code>tf.layers</code>?</p>

<p><strong>Update:</strong> for eager execution there's <code>tfe.Network</code> that must be inherited. There's an example <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/neural_network_eager_api.py"" rel=""noreferrer"">here</a></p>
",1
50054453,Tensorflow shape inference static RNN compiler error,"<p>I am working on OCR software optimized for phone camera images. </p>

<p>Currently, each 300 x 1000 x 3 (RGB) image is reformatted as a 900 x 1000 numpy array. I have plans for a more complex model architecture, but for now I just want to get a baseline working. I want to get started by training a static RNN on the data that I've generated.</p>

<p>Formally, I am feeding in n_t at each timestep t for T timesteps, where n_t is a 900-vector and T = 1000 (similar to reading the whole image left to right). Here is the Tensorflow code in which I create batches for training:</p>

<pre><code>sequence_dataset = tf.data.Dataset.from_generator(example_generator, (tf.int32, 
tf.int32))
sequence_dataset = sequence_dataset.batch(experiment_params['batch_size'])
iterator = sequence_dataset.make_initializable_iterator() 
x_batch, y_batch = iterator.get_next()
</code></pre>

<p>The tf.nn.static_bidirectional_rnn documentation claims that the input must be a ""length T list of inputs, each a tensor of shape [batch_size, input_size], or a nested tuple of such elements."" So, I go through the following steps in order to get the data into the correct format.</p>

<pre><code># Dimensions go from [batch, n , t] -&gt; [t, batch, n]
x_batch = tf.transpose(x_batch, [2, 0, 1])

# Unpack such that x_batch is a length T list with element dims [batch_size, n]
x_batch = tf.unstack(x_batch, experiment_params['example_t'], 0)
</code></pre>

<p>Without altering the batch any further, I make the following call:</p>

<pre><code>output, _, _ = tf.nn.static_rnn(lstm_fw_cell, x_batch, dtype=tf.int32)
</code></pre>

<p>Note that I do not explicitly tell Tensorflow the dimensions of the matrices (this could be the problem). They all have the same dimensionality, yet I am getting the following bug:</p>

<pre><code>ValueError: Input size (dimension 0 of inputs) must be accessible via shape 
inference, but saw value None.
</code></pre>

<p>At which point in my stack should I be declaring the dimensions of my input? Because I am using a Dataset and hoping to get its batches directly to the RNN, I am not sure that the ""placeholder -> feed_dict"" route makes sense. If that in fact is the method that makes the most sense, let me know what that looks like (I definitely do not know). Otherwise, let me know if you have any other insights to the problem. Thanks!</p>
",0
50113009,How to parse strings with newlines from a csv file in tensorflow?,"<p>The official tensorflow tutorial suggests parsing csv files by using a <code>tf.TextLineReader</code> to read the file line by line and then using <code>tf.decode_csv</code> (<a href=""https://www.tensorflow.org/api_guides/python/reading_data#file_formats"" rel=""nofollow noreferrer"">source</a>). This does however not work with csv records containing strings with newlines, since this causes a single csv record to be split up by the reader.</p>

<p>What is the best way to parse these types of files?</p>
",0
50149953,Using tf.keras within Tensorflow,"<p>What is the correct way of using the <code>tf.keras</code> API. Can <code>tf.layers.*</code> be directly replaced with <code>tf.keras.layers</code>(Similarly activations or loss functions)? Is it necessary to import <code>tf.keras.backend</code> and do <code>set_learning_phase</code>? This doesnt seem to be explained on the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras"" rel=""nofollow noreferrer"">official TF docs</a> but is mentioned in this <a href=""https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html"" rel=""nofollow noreferrer"">relatively old blog post</a>. </p>
",1
50171872,Tensorboard Visualization in Deep Reinforcement Learning Atari (MsPacman) Example,"<p>In my thesis I am describing a Deep Reinforcement Learning (DRL) example which is written in python. I did not write the code, I just got it running an training on a linux server an it all works fine. </p>

<p>Now I am on the point where I want to <strong>visualize</strong> the accuracy/prediction, loss, learning stability and so on with <strong>tensorboard</strong>. 
I am working in a conda virutal environment where I have installed  gym, atari-py, Pillow and PyOpenGL. On the server TensorFlow-GPU is installed. <a href=""https://github.com/ageron/tiny-dqn"" rel=""nofollow noreferrer"">This</a> is the link to the repository where I got the code from. 
I have watched tutorials about tensorboard and I get it, but I can not include those variables, like <code>tf.summary.histogram()</code> into my code. I do not know where exactly to put them and what variables might be important to visualize. </p>

<p>I already made it that tensorboard visualizes the whole network with the file writer as a graph which looks like <a href=""https://i.stack.imgur.com/4yLMi.png"" rel=""nofollow noreferrer"">this</a>.</p>

<p>But now I am stuck. Every time I want to try including histograms of variables the code throws errors. It seems like I put them into the wrong positions, or I am trying to visualize the wrong variables. (I am not so familiar with python code, maybe I get the syntax wrong.) It would be super nice if someone could help me. </p>

<p><a href=""https://www.youtube.com/watch?v=eBbEDRsCmv4&amp;list=PLqc8fSF0LNnDUFC8wlgHyr-GMEUID8RFR&amp;index=2&amp;t=622s"" rel=""nofollow noreferrer"">This</a> is the link to the tensorboard tutorial I oriented towards.</p>

<p>Below you see the code. The example consists only of this file. </p>

<pre><code>from __future__ import division, print_function, unicode_literals

# Handle arguments (before slow imports so --help can be fast)
import argparse

parser = argparse.ArgumentParser(
    description=""Train a DQN net to play MsMacman."")
parser.add_argument(""-n"", ""--number-steps"", type=int, default=4000000,
                    help=""total number of training steps"")
parser.add_argument(""-l"", ""--learn-iterations"", type=int, default=4,
                    help=""number of game iterations between each training step"")
parser.add_argument(""-s"", ""--save-steps"", type=int, default=1000,
                    help=""number of training steps between saving checkpoints"")
parser.add_argument(""-c"", ""--copy-steps"", type=int, default=10000,
                    help=""number of training steps between copies of online DQN to target DQN"")
parser.add_argument(""-r"", ""--render"", action=""store_true"", default=False,
                    help=""render the game during training or testing"")
parser.add_argument(""-p"", ""--path"", default=""my_dqn.ckpt"",
                    help=""path of the checkpoint file"")
parser.add_argument(""-t"", ""--test"", action=""store_true"", default=False,
                    help=""test (no learning and minimal epsilon)"")
parser.add_argument(""-v"", ""--verbosity"", action=""count"", default=0,
                    help=""increase output verbosity"")
args = parser.parse_args()

from collections import deque
import gym
import numpy as np
import os
import tensorflow as tf

writer = tf.summary.FileWriter(""/home/maggie/tbfiles/1"")

env = gym.make(""MsPacman-v0"")
done = True  # env needs to be reset

# First let's build the two DQNs (online &amp; target)
input_height = 88
input_width = 80
input_channels = 1
conv_n_maps = [32, 64, 64]
conv_kernel_sizes = [(8, 8), (4, 4), (3, 3)]
conv_strides = [4, 2, 1]
conv_paddings = [""SAME""] * 3
conv_activation = [tf.nn.relu] * 3

# TESTING ACTIVATION VIS TENSORBOARD
#tf.summary.histogram(""activation"", conv_activation)
n_hidden_in = 64 * 11 * 10  # conv3 has 64 maps of 11x10 each
n_hidden = 512
hidden_activation = tf.nn.relu
n_outputs = env.action_space.n  # 9 discrete actions are available
initializer = tf.contrib.layers.variance_scaling_initializer()


def q_network(X_state, name):
    prev_layer = X_state
    with tf.variable_scope(name) as scope:
        for n_maps, kernel_size, strides, padding, activation in zip(
                conv_n_maps, conv_kernel_sizes, conv_strides,
                conv_paddings, conv_activation):
            prev_layer = tf.layers.conv2d(
                prev_layer, filters=n_maps, kernel_size=kernel_size,
                strides=strides, padding=padding, activation=activation,
                kernel_initializer=initializer)

        last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, n_hidden_in])
        hidden = tf.layers.dense(last_conv_layer_flat, n_hidden,
                                 activation=hidden_activation,
                                 kernel_initializer=initializer)
        outputs = tf.layers.dense(hidden, n_outputs,
                                  kernel_initializer=initializer)
    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
                                       scope=scope.name)
    trainable_vars_by_name = {var.name[len(scope.name):]: var
                              for var in trainable_vars}
    return outputs, trainable_vars_by_name


X_state = tf.placeholder(tf.float32, shape=[None, input_height, input_width,
                                            input_channels], name=""x_state"")
online_q_values, online_vars = q_network(X_state, name=""q_networks/online"")
target_q_values, target_vars = q_network(X_state, name=""q_networks/target"")

# We need an operation to copy the online DQN to the target DQN
copy_ops = [target_var.assign(online_vars[var_name])
            for var_name, target_var in target_vars.items()]
copy_online_to_target = tf.group(*copy_ops)

# Now for the training operations
learning_rate = 0.001
momentum = 0.95

with tf.variable_scope(""train""):
    X_action = tf.placeholder(tf.int32, shape=[None], name=""x_action"")
    y = tf.placeholder(tf.float32, shape=[None, 1], name=""labels"")
    q_value = tf.reduce_sum(online_q_values * tf.one_hot(X_action, n_outputs),
                            axis=1, keep_dims=True)
    error = tf.abs(y - q_value)
    clipped_error = tf.clip_by_value(error, 0.0, 1.0)
    linear_error = 2 * (error - clipped_error)
    loss = tf.reduce_mean(tf.square(clipped_error) + linear_error, name=""loss"")

    global_step = tf.Variable(0, trainable=False, name='global_step')
    optimizer = tf.train.MomentumOptimizer(
        learning_rate, momentum, use_nesterov=True, name=""xent"")
    training_op = optimizer.minimize(loss, global_step=global_step)

init = tf.global_variables_initializer()
saver = tf.train.Saver()

# Let's implement a simple replay memory
replay_memory_size = 20000
replay_memory = deque([], maxlen=replay_memory_size)


def sample_memories(batch_size):
    indices = np.random.permutation(len(replay_memory))[:batch_size]
    cols = [[], [], [], [], []]  # state, action, reward, next_state, continue
    for idx in indices:
        memory = replay_memory[idx]
        for col, value in zip(cols, memory):
            col.append(value)
    cols = [np.array(col) for col in cols]
    return (cols[0], cols[1], cols[2].reshape(-1, 1), cols[3],
            cols[4].reshape(-1, 1))


# And on to the epsilon-greedy policy with decaying epsilon
eps_min = 0.1
eps_max = 1.0 if not args.test else eps_min
eps_decay_steps = args.number_steps // 2


def epsilon_greedy(q_values, step):
    epsilon = max(eps_min, eps_max - (eps_max - eps_min) * step / eps_decay_steps)
    if np.random.rand() &lt; epsilon:
        return np.random.randint(n_outputs)  # random action
    else:
        return np.argmax(q_values)  # optimal action


# We need to preprocess the images to speed up training
mspacman_color = np.array([210, 164, 74]).mean()


def preprocess_observation(obs):
    img = obs[1:176:2, ::2]  # crop and downsize
    img = img.mean(axis=2)  # to greyscale
    img[img == mspacman_color] = 0  # Improve contrast
    img = (img - 128) / 128 - 1  # normalize from -1. to 1.
    return img.reshape(88, 80, 1)


# TensorFlow - Execution phase
training_start = 10000  # start training after 10,000 game iterations
discount_rate = 0.99
skip_start = 90  # Skip the start of every game (it's just waiting time).
batch_size = 50
iteration = 0  # game iterations
done = True  # env needs to be reset

# We will keep track of the max Q-Value over time and compute the mean per game
loss_val = np.infty
game_length = 0
total_max_q = 0
mean_max_q = 0.0

with tf.Session() as sess:
    if os.path.isfile(args.path + "".index""):
        saver.restore(sess, args.path)
    else:
        init.run()
        copy_online_to_target.run()
    while True:
        step = global_step.eval()
        if step &gt;= args.number_steps:
            break
        iteration += 1
        if args.verbosity &gt; 0:
            print(""\rIteration {}   Training step {}/{} ({:.1f})%   ""
                  ""Loss {:5f}    Mean Max-Q {:5f}   "".format(
                iteration, step, args.number_steps, step * 100 / args.number_steps,
                loss_val, mean_max_q), end="""")
        if done:  # game over, start again
            obs = env.reset()
            for skip in range(skip_start):  # skip the start of each game
                obs, reward, done, info = env.step(0)
            state = preprocess_observation(obs)

        if args.render:
            env.render()

        # Online DQN evaluates what to do
        q_values = online_q_values.eval(feed_dict={X_state: [state]})
        action = epsilon_greedy(q_values, step)

        # Online DQN plays
        obs, reward, done, info = env.step(action)
        next_state = preprocess_observation(obs)

        # Let's memorize what happened
        replay_memory.append((state, action, reward, next_state, 1.0 - done))
        state = next_state

        if args.test:
            continue

        # Compute statistics for tracking progress (not shown in the book)
        total_max_q += q_values.max()
        game_length += 1
        if done:
            mean_max_q = total_max_q / game_length
            total_max_q = 0.0
            game_length = 0

        if iteration &lt; training_start or iteration % args.learn_iterations != 0:
            continue  # only train after warmup period and at regular intervals

        # Sample memories and use the target DQN to produce the target Q-Value
        X_state_val, X_action_val, rewards, X_next_state_val, continues = (
            sample_memories(batch_size))
        next_q_values = target_q_values.eval(
            feed_dict={X_state: X_next_state_val})
        max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)
        y_val = rewards + continues * discount_rate * max_next_q_values

        # Train the online DQN
        _, loss_val = sess.run([training_op, loss], feed_dict={
            X_state: X_state_val, X_action: X_action_val, y: y_val})

        # Regularly copy the online DQN to the target DQN
        if step % args.copy_steps == 0:
            copy_online_to_target.run()

        # And save regularlys
        if step % args.save_steps == 0:
            saver.save(sess, os.path.join(os.getcwd(), 'my_dqn.ckpt'))

        merged_summary = tf.summary.merge_all()
        writer.add_graph(sess.graph)
</code></pre>

<p>This the error I get from the terminal when I comment the line <code>#tf.summary.histogram(""activation"", conv_activation)</code> in. (line 48)</p>

<pre><code>(pacman) maggie@neuronalresearch:~/Documents/AI/my_project_folder/my_project$ python tiny_dqn.py -v --number-steps 1000
Traceback (most recent call last):
  File ""/opt/anaconda3/envs/pacman/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 468, in make_tensor_proto
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/opt/anaconda3/envs/pacman/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 468, in &lt;listcomp&gt;
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/opt/anaconda3/envs/pacman/lib/python3.5/site-packages/tensorflow/python/util/compat.py"", line 65, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got &lt;function relu at 0x7f8999cca048&gt;

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tiny_dqn.py"", line 48, in &lt;module&gt;
    tf.summary.histogram(""activation"", conv_activation)
  File ""/opt/anaconda3/envs/pacman/lib/python3.5/site-packages/tensorflow/python/summary/summary.py"", line 192, in histogram
    tag=tag, values=values, name=scope)
  File ""/opt/anaconda3/envs/pacman/lib/python3.5/site-packages/tensorflow/python/ops/gen_logging_ops.py"", line 188, in _histogram_summary
    ""HistogramSummary"", tag=tag, values=values, name=name)
  File ""/opt/anaconda3/envs/pacman/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 513, in _apply_op_helper
    raise err
  File ""/opt/anaconda3/envs/pacman/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""/opt/anaconda3/envs/pacman/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 926, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/opt/anaconda3/envs/pacman/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 229, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/opt/anaconda3/envs/pacman/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 208, in constant
    value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/opt/anaconda3/envs/pacman/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 472, in make_tensor_proto
    ""supported type."" % (type(values), values))
TypeError: Failed to convert object of type &lt;class 'list'&gt; to Tensor. Contents: [&lt;function relu at 0x7f8999cca048&gt;, &lt;function relu at 0x7f8999cca048&gt;, &lt;function relu at 0x7f8999cca048&gt;]. Consider casting elements to a supported type.
(pacman) maggie@neuronalresearch:~/Documents/AI/my_project_folder/my_project$ 
</code></pre>
",0
50210594,the function of 'bounding_boxes' and 'min_object_covered' in tf.image.sample_distorted_bounding_box?,"<p>How parameters 'bounding_boxes' and 'min_object_covered' control the generation of a single randomly distorted bounding box for an image in tf.image.sample_distorted_bounding_box? </p>

<p>I have read the <a href=""https://www.tensorflow.org/api_docs/python/tf/image/sample_distorted_bounding_box"" rel=""nofollow noreferrer"">function</a> in tensorflow api, but I still can not understand the proplem. Maybe I need a intuitive example.</p>
",1
50230620,How do insert multivariate dict in Tensorflow's NumpyReader?,"<p>the answer must be really simple, but I cannot wrap my head around the exact implementation of this:</p>

<blockquote>
  <p>Multivariate; TIMES is a vector of shape [series length], VALUES has shape [series length x number of features].</p>
</blockquote>

<p>in <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/timeseries/NumpyReader"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/contrib/timeseries/NumpyReader</a></p>

<p>It seems to work fine in univariate as:</p>

<pre><code>            import numpy as np

            status = []
            time = []

            #sensorstatus provides the values for the lists
            for ss in sensorstatus:
                status.append(int(ss.status))
                time.append(ss.time.timestamp())

            status = np.array(status)
            time = np.array(time)

            data = {
                tf.contrib.timeseries.TrainEvalFeatures.TIMES: time,
                tf.contrib.timeseries.TrainEvalFeatures.VALUES: status,
            }

            reader = NumpyReader(data)
</code></pre>

<p>but how could I add another list into the data as tf.contrib.timeseries.TrainEvalFeatures.VALUES ?</p>
",0
50243230,Unable to understand tf.nn.raw_rnn,"<p>In the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn"" rel=""nofollow noreferrer"">official documentation</a> of <code>tf.nn.raw_rnn</code> we have emit structure as the third output of <code>loop_fn</code> when the <code>loop_fn</code> is run for the first time.</p>

<p>Later on the emit_structure is used to copy <code>tf.zeros_like(emit_structure)</code> to the minibatch entries that are finished by <code>emit = tf.where(finished, tf.zeros_like(emit_structure), emit)</code>.</p>

<p>my lack of understanding or lousy documentation on google's part is: emit structure is <code>None</code> so <code>tf.where(finished, tf.zeros_like(emit_structure), emit)</code> is going to throw a ValueError as <code>tf.zeros_like(None)</code> does so. Can somebody please fill in what I am missing here?</p>
",1
50245039,record_defaults of tf.decode_csv in tensorflow,"<p>I used tf.decode_csv in tensorflow as decoder to parse training examples in a tab-delimited file into cnn models. For every training example, the features are 2 dimensions (100 columns, 2000 rows). After reading the document in tensorflow official site, I still have two questions. </p>

<ol>
<li>how to create record_defaults? The following is my code to do that, but I
am not sure if it is right.</li>
</ol>

<p>code</p>

<pre><code>filename_queue = tf.train.string_input_producer([file], num_epochs)

key, value = tf.TextLineReader().read(filename_queue)

record_defaults = [[1.0 for col in range(0, 100)] for row in range(0, 2000)]

content = tf.decode_csv(value, record_defaults = record_defaults, field_delim = '\t')

features = tf.pack(content[0:1999])
</code></pre>

<ol start=""2"">
<li>I am doing binary (0, 1) classification. Where do I put the labels for training examples? in the 2001th row? (For every training example, the first 2000 rows for features, and the 2001th row for label)</li>
</ol>

<p>Thanks for your time!</p>
",1
50246535,Tensorflow estimator input function: defining each feature or not?,"<p>With <code>x</code> is a 120 x 4 feature matrix of Iris data (4 features) and <code>y</code> is a label, I can make an input function for <code>tf.estimator</code> like below </p>

<pre><code>def input_function(x, y):
    dict_x = {
        ""sepal_length"" : x[:,0],
        ""sepal_width"" :  x[:,1],
        ""petal_length"" : x[:,2],
        ""petal_width"" :  x[:,3]
    }

    dataset = tf.data.Dataset.from_tensor_slices((
        dict_x, y
    ))

    return dataset
</code></pre>

<p>then define the feature column like below:</p>

<pre><code>feature_columns = [
    tf.feature_column.numeric_column(key=""sepal_length""),
    tf.feature_column.numeric_column(key=""sepal_width""),
    tf.feature_column.numeric_column(key=""petal_length""),
    tf.feature_column.numeric_column(key=""petal_width"")
]
</code></pre>

<p>But, I found in the internet (I forget the source, still searching) that I also can define the input function like below. The difference with previous method is all four features now defined with only one key, <code>""x""</code>.</p>

<pre><code>def input_function(x, y):
    dict_x = {
        ""x"" : x,
    }

    dataset = tf.data.Dataset.from_tensor_slices((
        dict_x, y
    ))

    return dataset
</code></pre>

<p>then define the feature column like below:</p>

<pre><code>feature_columns = [
    tf.feature_column.numeric_column(key=""x"",shape=4),
]
</code></pre>

<p>I've run both method and both give almost same result. <strong>My question</strong>: I can't find any documentation that explain the difference between both method, because at a glance <code>dict_x</code> have different shape. Are they still treated equally at input layer on neural networks?</p>

<p>I'm new using <code>tf.estimator</code>, Thank You</p>

<p>My estimator code if needed:</p>

<pre><code>classifier = tf.estimator.DNNClassifier(
    feature_columns=feature_columns,
    hidden_units=[10],
    n_classes=3,
    optimizer=tf.train.GradientDescentOptimizer(0.001),
    activation_fn=tf.nn.relu
)

# Train the model
classifier.train(
    input_fn=lambda:input_function(xtrain, ytrain, True)
)
</code></pre>
",1
50277910,Tensorflow Multi-GPU reusing vs. duplicating?,"<p>To train a model on multiple GPUs one can create one set of variables on first GPU and reuse them (by <code>tf.variable_scope(tf.get_variable_scope(), reuse=device_num != 0)</code>) on other GPUs as in <a href=""https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py#L169"" rel=""nofollow noreferrer"">cifar10_multi_gpu_train</a>.</p>

<p>But I came across the <a href=""https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/benchmark_cnn.py"" rel=""nofollow noreferrer"">official CNN benchmarks</a> where in local replicated setting they use new variable scope for each GPU (by <code>tf.variable_scope('v%s' % device_num)</code>). Since all variables are initialized randomly, <a href=""https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/variable_mgr.py#L319"" rel=""nofollow noreferrer""><em>post init op</em></a> is used to copy values from <code>GPU:0</code> to others.</p>

<p>Both implementations then average gradients on CPU and back-propagate back the result (at least that is what I think since the benchmarks code is cryptic:)) - probably resulting in the same outcome.</p>

<p>What is then the difference between these two approaches and more importantly what is faster?</p>

<p>Thank you.</p>
",0
50282420,Tensorflow confusion matrix for multiclass classification,"<p>Thanks for your help. I am coding a multiclass binary classifier for facial actions (such as raised eyebrow, parted lips), and I want to make a confusion matrix. There are 6 facial actions and 593 samples. I'm getting this error: I'm getting this error: ""Shape (?, 2, 6) must have rank 2"". From documentation, tf.confusion_matrix takes 1-D vectors, but I think there should be a way to shape the input data from the feed_dict so that it works, based on <a href=""https://stackoverflow.com/questions/41617463/tensorflow-confusion-matrix-in-tensorboard"">Tensorflow Confusion Matrix in TensorBoard</a>. The labels and predictions look like:</p>

<pre><code># Rows are samples, columns are classes, and the classes shows a facial
# action which is either 1 for detection or 0 for no detection. 
[[0, 0, 1, 0, 1, 0],
[1, 0, 0, 0, 1, 0],
[0, 1, 0, 0, 1, 1],...]
</code></pre>

<p>I'm using a feed-forward MLP and the variable 'pred' is the prediction, with a threshold forcing a choice of 0 or 1. I tried multiplying predictions and labels by np.arange(1,7) to have the positive values match the indices but I got stuck on the shape of the arguments. </p>

<p>There's more code, but I'm showing what I think is relevant. </p>

<pre><code>sess = tf.Session()

x = tf.placeholder(tf.float32, [None, n_input], name = ""x"")
y = tf.placeholder(tf.float32, [None, n_output], name = ""labels"")

#2 fully connected layers
fc1 = fc_layer(x, n_input, n_hidden_1, ""fc1"")
relu = tf.nn.relu(fc1)
tf.summary.histogram(""fc1/relu"", relu)
logits = fc_layer(relu, n_hidden_1, n_output, ""fc2"")

# Calculate loss function
with tf.name_scope(""xent""):
    xent = tf.reduce_mean(
        tf.nn.sigmoid_cross_entropy_with_logits(
            logits=logits, labels=y, name=""xent""))

with tf.name_scope(""train""):
    train_step = tf.train.AdamOptimizer(learning_rate).minimize(xent)


# Choose between 0 and 1
onesMat = tf.ones_like(logits)
zerosMat = tf.zeros_like(logits)   
pred = tf.cast(tf.where(logits&gt;=zero,onesMat,zerosMat),dtype=tf.float32, name = ""op_to_restore"")

# Problem occurs when I add this line. 
confusion = tf.confusion_matrix(predictions = pred*np.arange(1,7), labels = y*np.arange(1,7), num_classes = n_output, name = ""confusion"")

# Save and visualize results
saver = tf.train.Saver()
init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
sess.run(init)

writer = tf.summary.FileWriter(LOGDIR + hparam + '/train')
writer.add_graph(sess.graph)


# Train
for i in range(2001):
    if i % 5 == 0:
      [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: train_x, y: train_y})
      writer.add_summary(s, i)
    if i % 50 == 0:
      [acc,s] = sess.run([accuracy, summ],feed_dict={x: test_x, y: test_y})
    sess.run(train_step, feed_dict={x: train_x, y: train_y})
</code></pre>

<p>Thank you!</p>
",1
50308951,Understanding input/output tensors from tf.layers.conv2d,"<p>I'm trying to understand the transformation performed by <code>tf.layers.conv2d</code>.</p>
<p>The mnist tutorial code from the TensorFlow website includes the convolution layer:</p>
<pre><code># Computes 64 features using a 5x5 filter.
# Padding is added to preserve width and height.
# Input Tensor Shape: [batch_size, 14, 14, 32]
# Output Tensor Shape: [batch_size, 14, 14, 64]
conv2 = tf.layers.conv2d(
    inputs=pool1,
    filters=64,
    kernel_size=[5, 5],
    padding=&quot;same&quot;,
    activation=tf.nn.relu)
</code></pre>
<p>However, my expectation is that the 32 input images would be multiplied by the number of filters, as each filter is applied to each image, to give an output tensor of <code>[batch_sz, 14, 14, 2048]</code>. Clearly this is wrong, but I don't know why. How does the transformation work? The API documentation tells me nothing about how it works. What would be the output if the input tensor was <code>[batch_size, 14, 14, 48]</code>?</p>
",1
50315140,"In tensorflow.metrics, difference between precision_at_k and precision_at_top_k?","<p>In the tensorflow python API, <code>tf.metrics</code> features a few metrics for Information Retrieval.</p>

<p>In particular:</p>

<ul>
<li><code>tf.precision_at_k</code> and <code>tf.precision_at_top_k</code></li>
<li><code>tf.recall_at_k</code> and <code>tf.recall_at_top_k</code></li>
</ul>

<p>What is the difference between the <code>_at_k</code> and <code>_at_top_k</code> metrics?</p>

<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/metrics/precision_at_k"" rel=""noreferrer"">API documentation</a> does not seem to give information on this.</p>
",1
50352777,Shape ValueError in LSTM network using Tensorflow,"<p>I want to train a LSTM model with Tensorflow. I have a text data as input and I get doc2vec of each paragraph of the text and pass it to the lstm layers but I get ValueError because of inconsistency of shape rank.
I've searched through Stackoverflow for similar questions and some tutorials, but I couldn't solve this error. Do you have any idea what should I do?
Here is the error:</p>

<blockquote>
  <p>Traceback (most recent call last):
    File ""writeRNN.py"", line 97, in 
      outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)
    File ""myven/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 627, in dynamic_rnn
      dtype=dtype)
    File ""myven/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 690, in _dynamic_rnn_loop
      for input_ in flat_input)
    File ""myven/lib/python3.5/site-packages/tensorflow/python/ops/rnn.py"", line 690, in 
      for input_ in flat_input)
    File ""myven/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 761, in with_rank_at_least
      raise ValueError(""Shape %s must have rank at least %d"" % (self, rank))
  ValueError: Shape (?, ?) must have rank at least 3</p>
</blockquote>

<p>And below is the code:</p>

<pre><code>lstm_size = 128
lstm_layers = 1
batch_size = 50
learning_rate = 0.001

# Create the graph object
graph = tf.Graph()
# Add nodes to the graph
with graph.as_default():
    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')
    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')
    keep_prob = tf.placeholder(tf.float32, name='keep_prob')
with graph.as_default():
    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)
    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)
    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)
    initial_state = cell.zero_state(batch_size, tf.float32)

with graph.as_default():
    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs_, initial_state=initial_state)


with graph.as_default():
    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid)
    cost = tf.losses.mean_squared_error(labels_, predictions)
    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)


with graph.as_default():
    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)
    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
with tf.Session(graph=graph) as sess:
    sess.run(tf.global_variables_initializer())
    for e in range(epochs):
        state = sess.run(initial_state)
        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):
            feed = {inputs_: x, labels_: y[:, None], keep_prob: 0.5, initial_state: state}
            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)       
</code></pre>

<p>I got error on outputs, final_state = tf.nn.dynamic_rnn(cell, inputs_, initial_state=initial_state) Line as the error I described.
doc2vec model is trained on gensim and converts each sentence into a vector with 100 value.
I tried to change inputs_ shape and labels_ shape but also I get same error!
I really don't know what should I do?!</p>

<p>I really thank if you could answer my question.</p>
",0
50383462,how to randomly initialize weights in tensorflow?,"<p>in tensorflow, I learned from the tutorial that one would initialize the variables with something like
<code>
sess.run(tf.global_variables_initializer())
</code></p>

<p>however I found that every time I run this with the same input dataset, the loss value starts with the same value.</p>

<p>I presume this is due to the fact that the initialization is always setting up the variables with the same values. (probably zero)</p>

<p>I wish to randomize the values of weights. I've tried searching for this but  tensorflow docs doesn't give a clear answer if the initialization is done with zero values by default or random values.</p>

<p>How can I specify the initializaing to setup random values?</p>

<hr>

<p><strong>update</strong></p>

<p>my network is first a bunch of CNNs and pooling layers like below:
```
conv1 = tf.layers.conv2d(inputs=input_layer, filters=32, kernel_size=[3,3], padding=""same"", activation=tf.nn.relu, name=""conv_chad_1"")</p>

<pre><code>    pool1 = tf.layers.max_pooling2d(inputs=conv1,pool_size=[2,2],strides=2)

    conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[3,3], padding=""same"", activation=tf.nn.relu, name=""conv_chad_2"")

    pool2 = tf.layers.max_pooling2d(inputs=conv2,pool_size=[2,2],strides=2, name=""pool_chad_2"")
</code></pre>

<p>```</p>

<p>AFAIK, the weights are defined inside these predefined layers. How do I specify these layers to initialize their weight variables randomly??</p>
",1
50433094,What do the return values of tf.metrics mean?,"<p>I've been trying to add some hopefully useful intermediate calculations used to derive my loss function to the <code>eval_metric_ops</code> dictionary for my evaluation <code>EstimatorSpec</code>. I have wrapped these in a call to <a href=""https://www.tensorflow.org/api_docs/python/tf/metrics/mean"" rel=""nofollow noreferrer""><code>tf.metrics.mean</code></a> as it seemed to fit my needs.</p>

<p>The return type of this function is a <code>tuple</code> of <code>(mean, update_op)</code>, where <code>mean</code> is ostensibly the current mean and <code>update_op</code> is an operation that computes the new mean and returns it.</p>

<p>However, when I try to evaluate it I see that the <code>value</code> and <code>update_op</code> fields seem to be different. The documentation doesn't provide an explanation for this as far as I can see.</p>

<p>For example, take the following snippet of code:</p>

<pre><code>    test_tensor = tensorflow.constant([[1, 2, 3], [4, 5, 6]])
    test_mean = tensorflow.metrics.mean(test_tensor)

    sess = tensorflow.Session()
    sess.run(tensorflow.global_variables_initializer())
    sess.run(tensorflow.local_variables_initializer())

    print sess.run(test_mean)
    print sess.run(test_mean)
    print sess.run(test_mean)
    print sess.run(test_mean)

    print sess.run(test_mean[0])
    print sess.run(test_mean)[1]
</code></pre>

<p>This returns the following:</p>

<pre><code>(0.0, 3.5)
(1.75, 3.5)
(2.3333333, 3.5)
(2.625, 3.5)
3.5
3.5
</code></pre>

<p>The second value of the tuple is obviously an overall average of the input value, but the left hand side values seem to be asymptoting towards <code>3.5</code>, while the taking the zero-index of test_mean and evaluating it results in <code>3.5</code> directly, as opposed to the value that I get by evaluating the whole operation and then taking the index.</p>

<p>What is happening here?</p>
",1
50442156,Loading a model from tensorflow SavedModel onto mutliple GPUs,"<p>Let's say someone hands me a TF SavedModel and I would like to replicate this model on the 4 GPUs I have on my machine so I can run inference in parallel on batches of data. Are there any good examples of how to do this? </p>

<p>I can load a saved model in this way:</p>

<pre><code>def load_model(self, saved_model_dirpath):
    '''Loads a model from a saved model directory - this should 
       contain a .pb file and a variables directory'''

    signature_key = tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY
    input_key = 'input'
    output_key = 'output'

    meta_graph_def = tf.saved_model.loader.load(self.sess, [tf.saved_model.tag_constants.SERVING],
                                                saved_model_dirpath)
    signature = meta_graph_def.signature_def

    input_tensor_name = signature[signature_key].inputs[input_key].name
    output_tensor_name = signature[signature_key].outputs[output_key].name

    self.input_tensor = self.sess.graph.get_tensor_by_name(input_tensor_name)
    self.output_tensor = self.sess.graph.get_tensor_by_name(output_tensor_name)
</code></pre>

<p>..but this would require that I have a handle to the session. For models that I have written myself, I would have access to the inference function and I could just call it and wrap it using <code>with tf.device()</code>, but in this case, I'm not sure how to extract the inference function out of a Saved Model. Should I load 4 separate sessions or is there a better way? Couldn't find much documentation on this, but apologies in advance if I missed something. Thanks!</p>
",1
50486241,Tensorflow: tf.while_loop() with vector condition,"<p>The function tf.while_loop() (<a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/while_loop</a>) repeats the body ""b"" while the condition ""c"" is true. For example:</p>

<pre><code>import tensorflow as tf    
i = tf.constant(0)
c = lambda i: tf.greater(10,i)
b = lambda i: tf.add(i, 1)
r = tf.while_loop(c, b, [i])
</code></pre>

<p>How can I adjust this, when the condition is a vector, e.g.</p>

<pre><code>c = lambda i: tf.greater([10,10],[i,i])
</code></pre>

<p>?</p>

<p>The problem is that the above returns a vector (instead of True or False), same as e.g.</p>

<pre><code>tf.greater([2,2],[1,1])
</code></pre>

<p>I need something that returns true when all the vector elements are true, and false otherwise. I would suggest</p>

<pre><code>i = tf.constant(0)
c = lambda i: True if all(item == True for item in tf.greater([10,10],[i,i]))==True else False
b = lambda i: tf.add(i, 1)
r = tf.while_loop(c, b, [i])
</code></pre>

<p>But this does not work, with the following error:</p>

<pre><code>TypeError: `Tensor` objects are not iterable when eager execution is not enabled. To iterate over this tensor use `tf.map_fn`.
</code></pre>

<p>Any ideas?</p>
",0
50500579,what is the difference between tf.nn.max_pool() and tf.layers.max_pooling2d(),"<p>I am a beginner of tensorflow,i have met two methods about create max_pool layer these days. one is ""tf.nn.max_pool()"" and the other is ""tf.layers.max_pooling2d()"".i want to learn about its difference,and when to use them suitably.based on this ,i have read its official document and searched in google,it is not any help at all.i have searched it in stackoverflow , there is a similar answer(<a href=""https://stackoverflow.com/questions/42785026/tf-nn-conv2d-vs-tf-layers-conv2d"">tf.nn.conv2d vs tf.layers.conv2d</a>）but it didn't solve my problem.does any one can help me? thanks in advance.</p>
",1
50501777,Why does TensorFlow use `None` as the default activation?,"<p>In the TensorFlow Python API, the default value for the <code>activation</code> kwarg of <code>tf.layers.dense</code> is <code>None</code>, then in the documentation it says:</p>

<blockquote>
  <p>activation: Activation function to use. If you don't specify anything, no activation is applied (ie. ""linear"" activation: a(x) = x).</p>
</blockquote>

<p>Why not just use the identity function as default value when defining the function? like this:</p>

<pre><code>def dense(..., activation=lambda x: x, ...):
    pass
</code></pre>

<p>This way you don't have to worry about the inconsistency between the documentation and the code.</p>

<p>Is this (using None to represent a default function) just a coding style, or is there some caveat for using function as the default value of a kw argument?</p>

<p>It's not there to avoid unnecessary function calls, since an identity function is still created and called even <code>None</code> is passed to <code>activation</code>. Besides, since this happens at graph construction time, there is no point to do optimization like this - assuming this indeed is an optimization.</p>

<p>Correction:</p>

<p>As pointed out by @y-luo, the <code>tf</code> implementation doesn't actually create an identity function. But the <code>tf.keras</code> implementation does.</p>
",1
50514454,End of Sequence Error when using tf.estimator and tf.data,"<p>I am using <code>tf.estimator.train_and_evaluate</code> and <code>tf.data.Dataset</code> to feed data to the estimator:</p>

<p>Input Data function:</p>

<pre><code>    def data_fn(data_dict, batch_size, mode, num_epochs=10):
        dataset = {}
        if mode == tf.estimator.ModeKeys.TRAIN:
            dataset = tf.data.Dataset.from_tensor_slices(data_dict['train_data'].astype(np.float32))
            dataset = dataset.cache()
            dataset = dataset.shuffle(buffer_size= batch_size * 10).repeat(num_epochs).batch(batch_size)
        else:
            dataset = tf.data.Dataset.from_tensor_slices(data_dict['valid_data'].astype(np.float32))
            dataset = dataset.cache()
            dataset = dataset.batch(batch_size)

        iterator = dataset.make_one_shot_iterator()
        next_element = iterator.get_next()

    return next_element
</code></pre>

<p>Train Function:</p>

<pre><code>def train_model(data):
    tf.logging.set_verbosity(tf.logging.INFO)
    config = tf.ConfigProto(allow_soft_placement=True,
                            log_device_placement=False)
    config.gpu_options.allow_growth = True
    run_config = tf.contrib.learn.RunConfig(
        save_checkpoints_steps=10,
        keep_checkpoint_max=10,
        session_config=config
    )

    train_input = lambda: data_fn(data, 100, tf.estimator.ModeKeys.TRAIN, num_epochs=1)
    eval_input = lambda: data_fn(data, 1000, tf.estimator.ModeKeys.EVAL)
    estimator = tf.estimator.Estimator(model_fn=model_fn, params=hps, config=run_config)
    train_spec = tf.estimator.TrainSpec(train_input, max_steps=100)
    eval_spec = tf.estimator.EvalSpec(eval_input,
                                      steps=None,
                                      throttle_secs = 30)

    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
</code></pre>

<p>The training goes fine, but when it comes to evaluation I get this error:</p>

<pre><code>OutOfRangeError (see above for traceback): End of sequence 
</code></pre>

<p>If I don't use <code>Dataset.batch</code> on evaluation dataset (by omitting the line <code>dataset[name] = dataset[name].batch(batch_size)</code> in <code>data_fn</code>) I get the same error but after a much longer time.</p>

<p>I can only avoid this error if I don't batch the data and use <code>steps=1</code> for evaluation, but does that perform the evaluation on the whole dataset?</p>

<p>I don't understand what causes this error as the documentation suggests I should be able to evaluate on batches too.</p>

<p>Note: I get the same error when using <code>tf.estimator.evaluate</code> on data batches.</p>
",1
50524778,Tensorflow L2 loss definition,"<p>I was wondering why <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss"" rel=""noreferrer"">tf.nn.l2_loss</a> was set to compute half the L2 norm. Is there any special meaning to divide the L2 norm by 2?</p>
",0
50532850,TensorFlow: 'tf.image.rgb_to_yuv' not working,"<p>Currently I'm working on an image encoder implementation with tensorflow, and in order to transform from RGB- to YUV-colorspace, I wanted to use the tensorflow function <code>tf.image.rgb_to_yuv</code>, which appears not to exist (although it's documented here: <a href=""https://www.tensorflow.org/api_docs/python/tf/image/rgb_to_yuv"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/image/rgb_to_yuv</a>), since I get the error message: </p>

<blockquote>
  <p>AttributeError: module 'tensorflow.python.ops.image_ops' has no attribute 'rgb_to_yuv'</p>
</blockquote>

<p>I'm using Tensorflow 1.8. For example, the analogue HSV-transformation  <code>tf.image.rgb_to_hsv</code> works properly, so it appears to be a special issue related only to the <code>rgb_to_yuv</code> transform. Maybe it has something to do with the transition to the 1.x versions of Tensorflow, in which certain functions got new names, but I could not find anything about it. Does someone know, what's going on here? </p>

<p>Thanks in advance</p>
",1
50560013,"Tensorflow, multi-label confusion matrix","<p>I am trying to figure out how to the generate a confusion matrix for a multi-label classification task using neural networks. I previously managed to calculate the accuracy using the function &quot;intersection&quot;, since for that I did not care about any ordering.</p>
<pre><code>intersection = tf.sets.set_intersection(predictions, labels)
</code></pre>
<p>However, in order to calculate the confusion matrix, I do care about the indexing order of the predictions/labels. And since the labels have always the same value (<code>1,1</code> or <code>0.5,0.5</code>) there is no possible sorting according to higher/lower value.</p>
<p>I wonder:</p>
<p><strong>1) Is it possible to calculate a confusion matrix for the multi-label classification task?</strong></p>
<p><strong>2) How would that be implemented ?</strong></p>
<p><strong>3) How can you handle the case of failure in predicting both labels? Since it is not possible to know which confusion belongs to which prediction.</strong></p>
<p><strong>4) What is the logic behind the sorting of the function tf.nn.top_k()</strong></p>
<p>Below I show an example of the code that I was trying to use.</p>
<pre><code>import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

Z = np.array([[7.0, 3.0, 5.0, 1.0, 0.0, 6.0],[2.0, 3.0, 4.0, 1.0, 3.25, 2.2], [2.0 , 5.0, 1.0, 7.0, 0.0, 8.0]])
Y = np.array([[0.5, 0, 0, 0.0, 0, 0.5],[0, 0.0, 0.5, 0, 0.5, 0], [0,0,0,0.5,0,0.5]])

_, predicted_softmax = tf.nn.top_k(tf.nn.softmax(Z), k = 2, sorted = False)
_ , labels = tf.nn.top_k(Y, k = 2, sorted = False)

with tf.Session() as sess:
    # reshape to (6,1) because there is 2 correct values per sample(2*3)
    print(predicted_softmax.eval().reshape(6,1))
    print(labels.eval().reshape(6,1))
    predicted = predicted_softmax.eval().reshape(6,1)
    labels_idx = labels.eval().reshape(6,1)

class_labels = np.arange(6)
cnf_matrix_train = confusion_matrix(labels_idx, predicted, labels = class_labels)

print(cnf_matrix_train)
</code></pre>
<p>I don't really get why the output of predicted_softmax is:</p>
<pre><code>[[5] [0] [4] [2] [3] [5]] , 
</code></pre>
<p>I was expecting [5] [3] for the last two terms. There is no any logic to this output. In the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/top_k"" rel=""nofollow noreferrer"">documentation</a> they don't specify anything about the ordering in the case that <code>sorted = False</code> thought, but I was expecting some consistent behavior.</p>
<p>Thanks for any help!</p>
",1
50596369,train_and_evaluate() batch size with TPU on GCMLE,"<p>I am trying to use <code>TPUEstimator</code> with <code>train_and_evaluate()</code> for an experiment on GCMLE. The <code>TPUEstimator</code> has a required argument <code>train_batch_size</code> that obviously specifies the batch size. However, for <code>train_and_evaluate()</code> I also specify a batch size through the TrainSpec:</p>

<pre><code>train_input = lambda: input_fn(
    filenames = hparams.train_files,
    batch_size = hparams.train_batch_size,
    hparams = hparams,
    num_epochs = hparams.num_epochs, 
    shuffle=True,
    skip_header_lines=1
    )

train_spec = tf.estimator.TrainSpec(train_input, max_steps = hparams.train_steps)

estimator = tpu_estimator.TPUEstimator(
    use_tpu=True,
    model_fn=model_fn,
    config=run_config,
    train_batch_size = hparams.train_batch_size,
    eval_batch_size = hparams.eval_batch_size,
    )
tf.estimator.train_and_evaluate(tpu_estimator, train_spec, eval_spec)
</code></pre>

<p>In this example, consider that <code>train_input</code> within train_spec has it's own batch_size specified (for something like tf.train.batch() or tf.datasets.batch()) and also <code>train_batch_size</code> is a requirement of a TPUEstimator. </p>

<p>This seems very sloppy to me to have <code>train_batch_size</code> passed in two different places -- is the recommendation just to make sure that the same batch size is passed to both TPUEstimator and the TrainSpec? If the batch_size in TPUEstimator differed from the batch_size in the TrainSpec passed to <code>train_and_evaluate()</code> what would take preference? Is there a better way to use train_and_evaluate() with a TPUEstimator and not need to pass this batch_size in two different places?</p>

<p>Additionally, it appears that TPUEstimator automatically creates params['batch_size'] which appears to be the ""effective batch size"" according to documentation. How does the effctive batch size related to train_batch_size? If my train_batch_size is 1024, is the 
""effective batch size"" 128 (because of the 8 cores)?</p>
",0
50606178,TensorFlow tf.data.Dataset and bucketing,"<p>For an LSTM network, I've seen great improvements with bucketing.</p>

<p>I've come across the <a href=""https://www.tensorflow.org/api_guides/python/contrib.training#Bucketing"" rel=""noreferrer"">bucketing section in the TensorFlow docs</a> which (tf.contrib).</p>

<p>Though in my network, I am using the <code>tf.data.Dataset</code> API, specifically I'm working with TFRecords, so my input pipeline looks something like this</p>

<pre><code>dataset = tf.data.TFRecordDataset(TFRECORDS_PATH)
dataset = dataset.map(_parse_function)
dataset = dataset.map(_scale_function)
dataset = dataset.shuffle(buffer_size=10000)
dataset = dataset.padded_batch(batch_size, padded_shapes={.....})
</code></pre>

<p>How can I incorporate the bucketing method into a the <code>tf.data.Dataset</code> pipeline?</p>

<p>If it matters, in every record in the TFRecords file I have the sequence length saved as an integer.</p>
",1
50660763,Printing inside jupyter notebook custom loss function with Keras/TF,"<p>In Keras, if you make a custom loss function in a Jupyter notebook, you can not print anything. For instance if you have:</p>

<pre><code>def loss_func(true_label, NN_output):
        true_cat = true_label[:,0]
        pred_cat = NN_output[:,0]
        indicator = NN_output[:,1]
        print(""Hi!"")
        custom_term = K.mean(K.abs(indicator))
        return binary_crossentropy(true_cat, pred_cat) + custom_term
</code></pre>

<p>Nothing will print when the function is evaluated.</p>

<p>As a workaround, in case I am doing some debugging, I have found that I can write to a file in a cost function, which can be useful if I want to print something standard like an int or a string.</p>

<p>However, trying to write out a tensor like <code>indicator</code> to a file gives the <em>unbelievably helpful</em> output:</p>

<pre><code>Tensor(""loss_103/model_105_loss/Print:0"", shape=(512,), dtype=float32)
</code></pre>

<p>I know TF provides a <code>tf.Print()</code> method to print the value of a tensor, but I don't understand how that plays with Jupyter. Other answers have said that <code>tf.Print()</code> writes to std. err, which means trying</p>

<pre><code>sys.stderr = open('test.txt', 'w')
</code></pre>

<p>should theoretically allow me to get my output from a file, but unfortunately this doesn't work (at least in Jupyter). </p>

<p>Is there any general method to get a representation of my tensor as a string? How do people generally get around this barrier to seeing what your code does? If I come up with something more fancy than finding a mean, I want to see exactly what's going on in the steps of my calculation to verify it works as intended.</p>

<p>Thanks!</p>
",0
50690865,Overfitting Issue On a variation of ZF-net,"<p>I am training a CNN on imagenet-2012 dataset, but the model keeps overfitting(Valiation Error rate: top1: 49%, top5: 25%, Training Error rate: top1:25%, top5: 8%. trained on GTX1080ti after 600k training steps (about 5 days)). the architecture is based on ZF-net but adds batch norm:</p>

<pre><code>     x_input = feed_key['input']
bn_training = tf.placeholder(dtype=tf.bool, shape=(), name='bn_training')
with tf.name_scope('ZF_conv1'):
    w_conv1 = tf.get_variable(name='conv1_kernel', shape=[7, 7, 3, 96], dtype=tf.float32)
    h_conv1 = tf.nn.conv2d(x_input, w_conv1, strides=[1, 2, 2, 1], padding='SAME')
with tf.name_scope('ZF_bn1'):
    bn1 = tf.layers.batch_normalization(h_conv1, training=bn_training)
with tf.name_scope('ZF_relu1'):
    h_active1 = tf.nn.relu(bn1)

with tf.name_scope('ZF_pool1'):
    h_pool1 = tf.nn.max_pool(h_active1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')

with tf.name_scope('ZF_conv2'):
    w_conv2 = tf.get_variable(name='conv2_kernel', shape=[5, 5, 96, 256], dtype=tf.float32)
    h_conv2 = tf.nn.conv2d(h_pool1, w_conv2, strides=[1, 2, 2, 1], padding='SAME')
with tf.name_scope('ZF_bn2'):
    bn2 = tf.layers.batch_normalization(h_conv2, training=bn_training)
with tf.name_scope('ZF_relu2'):
    h_active2 = tf.nn.relu(bn2)

with tf.name_scope('ZF_pool2'):
    h_pool2 = tf.nn.max_pool(h_active2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')

with tf.name_scope('ZF_conv3'):
    w_conv3 = tf.get_variable(name='conv3_kernel', shape=[3, 3, 256, 384], dtype=tf.float32)
    h_conv3 = tf.nn.conv2d(h_pool2, w_conv3, strides=[1, 1, 1, 1], padding='SAME')
with tf.name_scope('ZF_bn3'):
    bn3 = tf.layers.batch_normalization(h_conv3, training=bn_training)
with tf.name_scope('ZF_relu3'):
    h_active3 = tf.nn.relu(bn3)

with tf.name_scope('ZF_conv4'):
    w_conv4 = tf.get_variable(name='conv4_kernel', shape=[3, 3, 384, 384], dtype=tf.float32)
    h_conv4 = tf.nn.conv2d(h_active3, w_conv4, strides=[1, 1, 1, 1], padding='SAME')
with tf.name_scope('ZF_bn4'):
    bn4 = tf.layers.batch_normalization(h_conv4, training=bn_training)
with tf.name_scope('ZF_relu4'):
    h_active3 = tf.nn.relu(bn4)

with tf.name_scope('ZF_conv5'):
    w_conv5 = tf.get_variable(name='conv5_kernel', shape=[3, 3, 384, 256], dtype=tf.float32)
    h_conv5 = tf.nn.conv2d(h_active3, w_conv5, strides=[1, 1, 1, 1], padding='SAME')

with tf.name_scope('ZF_bn5'):
    bn5 = tf.layers.batch_normalization(h_conv5, training=bn_training)
with tf.name_scope('ZF_relu5'):
    h_active5 = tf.nn.relu(bn5)

feed_key['bn_training'] = bn_training
</code></pre>

<p>Followed by two FC layer:</p>

<pre><code>fc1 = tf.layers.dense(low_out_flat,
                      units=4096,
                      activation=tf.nn.relu,
                      name='zffc1')

keep_prob1 = tf.placeholder(tf.float32, name='keep_prob1')
dropout1 = tf.nn.dropout(fc1, keep_prob1, name='zfdrop1')

fc2 = tf.layers.dense(dropout1,
                      units=4096,
                      activation=tf.nn.relu,
                      name='zffc2')

keep_prob2 = tf.placeholder(tf.float32, name='keep_prob2')
dropout2 = tf.nn.dropout(fc2, keep_prob2, name='zfdrop2')

feed_key['keep_prob1'] = keep_prob1
feed_key['keep_prob2'] = keep_prob2
</code></pre>

<p>Finally compute the cross entropy:</p>

<pre><code> gt_labels = tf.placeholder(dtype=tf.int64, shape=[None])
logits = tf.layers.dense(model.last_layer, units=1000, name='imagenet_logits')

with tf.name_scope('imagenet_cross_entropy'):
    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=gt_labels, logits=logits)
with tf.name_scope('imagenet_loss'):
    loss = tf.reduce_mean(entropy)
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    train_op = training_methods.optimizer.minimize(loss=loss, global_step=training_methods.global_step)
</code></pre>

<p>For The Data Preprocessing at training, I followed the paper:<a href=""https://arxiv.org/abs/1311.2901"" rel=""nofollow noreferrer"">Visualizing and Understanding Convolutional Networks</a></p>

<blockquote>
  <p>Each RGB image was preprocessed by resizing the smallest dimension to 256, cropping the center 256x256 region, subtracting the per-pixel mean (across all images) and then using 10 different sub-crops of size 224x224 (corners + center with(out) horizontal flips).</p>
</blockquote>

<p>(remark: I calculated the image mean across the whole training dataset by first resizing it to 224*224, not 256*256. so I subtract the image mean after sub-cropping image to 224*224. I thought it is not a problem)
When Testing, I just resize the image to 224*224(Is that a problem?)</p>

<p>Optimizer:Adam with initial learning rate 0.001 epsilon 0.1, 
drop out rate was set to 0.5.
lastly, I use <code>tf.variance_scaling_initializer()</code> to initialize all weights</p>

<p>The ZF-net paper reported their result was Testing error rate: top1:36.7% top5:15.3% so this is way off my result, but I can't find where is wrong</p>
",0
50724495,Train and validate using tensorflow estimator,"<p>I have created a shallow NN using tf.estimator API. I would like to something similar to the hyperparameter search explained in here <a href=""https://www.youtube.com/watch?time_continue=948&amp;v=eBbEDRsCmv4"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?time_continue=948&amp;v=eBbEDRsCmv4</a> at TensorFlow Dev Summit.</p>

<p>I could not find any updated documentation about how can you do this. I have the following code (I will try to simplify as much as possible):</p>

<pre><code># Define nn architecture
def neural_net(features):
    input_layer = tf.cast(features['x'], tf.float32)
    hidden_layer = nn_layer(input_layer, 10, 'hidden_layer', act=tf.nn.relu)
    out_layer = nn_layer(hidden_layer, 2, 'out_layer', act=tf.nn.relu)
    return out_layer

# Define model function
def model_fn(features, labels, mode):
    # Build the neural network
    logits = neural_net(features&lt;9


    with tf.name_scope('loss'):
    # Define loss and optimizer
        loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels)

    # Configure the Training
    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
        train_op = optimizer.minimize(
        loss=loss,
        global_step=tf.train.get_global_step())

        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)


nn_classifier = tf.estimator.Estimator(model_fn=model_fn)


train_input_fn = tf.estimator.inputs.numpy_input_fn(
            x={""x"": train_data},
            y=train_labels,
            batch_size=100,
            num_epochs=None,
            shuffle=True)

nn_classifier.train(
        input_fn=train_input_fn,
        steps=20000
        )
</code></pre>

<p>Executing this code I can obtain the summary for the loss and observe it in Tensorboard. But imagine I want to obtain different curves. Let's say that I want to see how the loss evolves with the number of samples, so I would train two models with different sample size. Or two models with a different architecture... whatever.</p>

<p>How can I get these two curves in Tensorboard?</p>
",0
50774683,How is Nesterov's Accelerated Gradient Descent implemented in Tensorflow?,"<p>The documentation for <a href=""https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer"" rel=""noreferrer""><code>tf.train.MomentumOptimizer</code></a> offers a <code>use_nesterov</code> parameter to utilise Nesterov's Accelerated Gradient (NAG) method.</p>

<p>However, NAG requires the gradient at a location other than that of the current variable to be calculated, and the <code>apply_gradients</code> interface only allows for the current gradient to be passed. So I don't quite understand how the NAG algorithm could be implemented with this interface.</p>

<p>The documentation says the following about the implementation:</p>

<blockquote>
  <p><code>use_nesterov</code>: If True use Nesterov Momentum. See <a href=""http://proceedings.mlr.press/v28/sutskever13.pdf"" rel=""noreferrer"">Sutskever et al.,
  2013</a>. This
  implementation always computes gradients at the value of the
  variable(s) passed to the optimizer. Using Nesterov Momentum makes the
  variable(s) track the values called <code>theta_t + mu*v_t</code> in the paper.</p>
</blockquote>

<p>Having read through the paper in the link, I'm a little unsure about whether this description answers my question or not. How can the NAG algorithm be implemented when the interface doesn't require a gradient function to be provided?</p>
",0
50789693,How do I inspect the contents of tf.estimator.inputs.numpy_input_fn?,"<p>I want to train my tensorflow graph over a set of data repeatedly, and I think <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/inputs/numpy_input_fn"" rel=""nofollow noreferrer""><code>tf.estimator.inputs.numpy_input_fn</code></a> might be what I'm looking for. I find the distinction between batch sizes, repeats, epochs and iterators to be incredibly confusing, so I started trying to inspect the contents of my datasets to try to figure out what's actually going on. However, whenever I try to to do this my program just hangs.</p>

<p>Here is the smallest test case I came up with to reproduce this:</p>

<pre><code>import tensorflow as tf
import numpy

class TestMock(tf.test.TestCase):
    def test(self):
        inputs = numpy.array(range(10))
        targets = numpy.array(range(10,20))

        input_fn = tf.estimator.inputs.numpy_input_fn(
            x=inputs,
            y=targets,
            batch_size=1,
            num_epochs=2,
            shuffle=False)

        print input_fn()
        with self.test_session() as sess:
            # sess.run(input_fn()[0]) # it'll hang if I run this
            pass

if __name__ == '__main__':
    tf.test.main()
</code></pre>

<p>This program outputs</p>

<pre><code>(&lt;tf.Tensor 'fifo_queue_DequeueUpTo:1' shape=(?,) dtype=int64&gt;, &lt;tf.Tensor 'fifo_queue_DequeueUpTo:2' shape=(?,) dtype=int64&gt;)
</code></pre>

<p>Which seems reasonable, but as soon as I try to run that <code>sess.run</code> line, my program freezes and I have to kill the process. What am I doing wrong here?</p>

<p>What I want to do is make sure that the data I'm feeding into my process is actually what I think it is, but I don't think I can do that without the ability to inspect the data.</p>
",0
50820781,quesion about the axis of tf.stack(),"<p>I read the doc of <code>tf.stack()</code> on <a href=""https://www.tensorflow.org/api_docs/python/tf/stack"" rel=""nofollow noreferrer"">tensorflow stack </a>. There is an example on the page:</p>

<pre><code>&gt;&gt;&gt; x = tf.constant([1, 4])
&gt;&gt;&gt; y = tf.constant([2, 5])
&gt;&gt;&gt; z = tf.constant([3, 6])
&gt;&gt;&gt; sess=tf.Session()
&gt;&gt;&gt; sess.run(tf.stack([x, y, z]))
array([[1, 4],
       [2, 5],
       [3, 6]], dtype=int32)
&gt;&gt;&gt; sess.run(tf.stack([x, y, z], axis=1))
array([[1, 2, 3],
       [4, 5, 6]], dtype=int32)
</code></pre>

<p>what I don't understand is the second example where <code>axis=1</code>.</p>

<p>From the result it seems it converts the three inputs rows to columns first </p>

<p>and then put them toghter along the <code>axis=1</code>, but </p>

<p>I think the result should be </p>

<pre><code>array([[1,4, 2, 5, 3, 6 ]] dtype=int32 )
</code></pre>

<p>can anyone help explain this?</p>

<p>Thanks!</p>
",1
50840759,"Incorrect name returned in Tensorflow causes ""Tensor which does not exist"" error while invoking get_tensor_by_name","<p>As per the <a href=""https://www.tensorflow.org/programmers_guide/graphs#naming_operations"" rel=""nofollow noreferrer"" title=""description"">documentation</a> TensorFlow would append ""_1"", ""_2"", and so on to the name in tf.Graph namespace, in order to make it unique.  Here I define two convolutional operations.  It is expected that the first one will be named as ""conv2d"" and second one ""conv2d_1"".  But when I try to obtain the name of the second convolution it returns ""conv2d_2"".  I causes error when I try to invoke get_tensor_by_name. Here is the code:</p>

<pre><code>import numpy as np
import tensorflow as tf
import os

x = tf.constant(np.random.randn(1,2,2,1), dtype=tf.float32)
kernel_size = (1,1)
no_of_out = 20
strides = (1,1)
conv_out1 = tf.layers.conv2d(x, 10, (1,1), (1,1))
conv_out2 = tf.layers.conv2d(x, 10, (1,1), (1,1))

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print conv_out1.name # conv2d/BiasAdd:0 .  This value is correct
    print conv_out2.name # conv2d_2/BiasAdd:0 .  This value is incorrect.  It should be conv2d_1/BiasAdd:0
    conv_weights1 = tf.get_default_graph().get_tensor_by_name(os.path.split(conv_out1.name)[0] + '/kernel:0')
    conv_weights2 = tf.get_default_graph().get_tensor_by_name('conv2d_1/kernel:0')  
    conv_weights2 = tf.get_default_graph().get_tensor_by_name(os.path.split(conv_out2.name)[0] + '/kernel:0')
</code></pre>

<p>I could not understand why conv_out2.name returns ""conv2d_2"" instead of ""conv2d_1""</p>
",1
50967885,"tf.gradients, how can I understand `grad_ys` and use it?","<p>In <code>tf.gradients</code>,  there is a keyword argument <code>grad_ys</code></p>

<blockquote>
  <p><code>grad_ys</code> is a list of tensors of the same length as <code>ys</code> that holds the initial gradients for each <code>y</code> in <code>ys</code>. When <code>grad_ys</code> is None, we fill in a tensor of ‘1’s of the shape of <code>y</code> for each <code>y</code> in <code>ys</code>. A user can provide their own initial <code>grad_ys</code> to compute the derivatives using a different initial gradient for each y (e.g., if one wanted to weight the gradient differently for each value in each y).</p>
</blockquote>

<p>Why is <code>grads_ys</code> needed here? The docs here is implicit. Could you please give some specific purpose and code?</p>

<p>And my example code for <code>tf.gradients</code> is</p>

<pre class=""lang-py prettyprint-override""><code>In [1]: import numpy as np

In [2]: import tensorflow as tf

In [3]: sess = tf.InteractiveSession()

In [4]: X = tf.placeholder(""float"", shape=[2, 1])

In [5]: Y = tf.placeholder(""float"", shape=[2, 1])

In [6]: W = tf.Variable(np.random.randn(), name='weight')

In [7]: b = tf.Variable(np.random.randn(), name='bias')

In [8]: pred = tf.add(tf.multiply(X, W), b)

In [9]: cost = 0.5 * tf.reduce_sum(tf.pow(pred-Y, 2))

In [10]: grads = tf.gradients(cost, [W, b])

In [11]: sess.run(tf.global_variables_initializer())

In [15]: W_, b_, pred_, cost_, grads_ = sess.run([W, b, pred, cost, grads], 
                                    feed_dict={X: [[2.0], [3.]], Y: [[3.0], [2.]]})
</code></pre>
",1
50974905,"""Undefined variable from import"" in Contrib in TensorFlow 1.8","<p>Running TensorFlow 1.8 in Eclipse with PyDev, and Eclispe is showing ""Undefined variable from import: layers"" on the following:</p>

<pre><code>import tensorflow as tf
tf.contrib.layers.xavier_initializer()
</code></pre>

<p>It doesn't seem to be specifically limited to <code>layers</code>, but it is specific to <code>contrib</code>. The only things I've seen successfully fill in the <code>tf.contrib._____</code> blank are <code>stat_summarizer</code>, <code>__dict__</code>, <code>__file__</code>, <code>__name__</code>, and <code>__path__</code></p>

<p>I previously had a similar issue caused by some code having been moved from <code>tf.contrib</code> a few versions back to <code>tf.nn</code>, but best I can tell from <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/layers/xavier_initializer"" rel=""nofollow noreferrer"">the 1.8 documentation</a> a lot of things (including<code>layers.xavier_initializer()</code>) are still in <code>contrib</code>.</p>
",0
50976348,how to 'reset' random sequence in tensorflow,"<p>I have a graph with GBs of variables, and a random function (e.g. VAE). I'd like to be able to run a function, and always use the same random sequence (e.g. feed a large number of x, and always get the exact same z and y). </p>

<p>I can achieve this using random seeds, such that I always get the same sequence every time I run the script from scratch (i.e. initialise the session). However I'd like to be able to reset the random sequence without destroying the session, so I can call my function over and over (and get the same sequence). Destroying and reinitialising session isn't really ideal as I have GBs of variables which I lose, and reloading each time is a waste. Setting the random seed again (tf.set_random_seed) doesn't seem to have an affect (I presume the seed from tf.set_random_seed is somehow combined with the op seed and baked into the op when it's created?) </p>

<p>Is there any way around this?</p>

<p>I've read the <a href=""https://www.tensorflow.org/api_docs/python/tf/set_random_seed"" rel=""noreferrer"">documentation</a> and a ton of posts regarding random seeds in tensorflow (e.g. <a href=""https://stackoverflow.com/questions/47491262/tensorflow-resetting-the-seed-to-a-constant-value-does-not-yield-repeating-resu"">TensorFlow: Resetting the seed to a constant value does not yield repeating results</a>, <a href=""https://stackoverflow.com/questions/36096386/tensorflow-set-random-seed-not-working"">Tensorflow `set_random_seed` not working</a>, <a href=""https://stackoverflow.com/questions/38469632/tensorflow-non-repeatable-results"">TensorFlow: Non-repeatable results</a>, <a href=""https://stackoverflow.com/questions/38293895/how-to-get-reproducible-result-in-tensorflow"">how to get reproducible result in Tensorflow</a>, <a href=""https://stackoverflow.com/questions/36288235/how-to-get-stable-results-with-tensorflow-setting-random-seed"">How to get stable results with TensorFlow, setting random seed</a>) however I can't get the behaviour I'm after. </p>

<p>E.g. toy code</p>

<pre><code>import tensorflow as tf
tf.set_random_seed(0)
a = tf.random_uniform([1], seed=1)

def foo(s, a, msg):
    with s.as_default(): print msg, a.eval(), a.eval(), a.eval(), a.eval()


s = tf.Session()
foo(s, a, 'run1 (first session):')

# resetting seed does not reset sequence. is there anything else I can do?
tf.set_random_seed(0)
foo(s, a, 'run2 (same session) :')

# reinitialising session works, but how to do this without closing session and reinitializing?
# and losing GBs of variables which I'd rather not reload
s.close()
s = tf.Session()
foo(s, a, 'run3 (new session)  :')
</code></pre>

<p>gives results:</p>

<pre><code>run1 (first session): [ 0.53973019] [ 0.54001355] [ 0.43089259] [ 0.30245078]
run2 (same session) : [ 0.112077] [ 0.99792898] [ 0.17628896] [ 0.13141966]
run3 (new session)  : [ 0.53973019] [ 0.54001355] [ 0.43089259] [ 0.30245078]
</code></pre>
",0
50997293,tf.reduce_mean prints strange results when reduction_indices is specified,"<p>tf.reduce_mean command is used to take mean along specified axis. But if don't specify axis and use reduction indices zero then it prints middle row. Why is it so? How this reduction indices effects output values? If I print sum.eval() command why it prints [[4,5,6]]?
I also go through the documentation but it doesn't give any explanation for reduction_indices.Please clear my concept about this command.</p>

<pre><code>a = tf.range(0,12)
b = tf.reshape(a,(4,3))
sum = tf.reduce_mean(b,reduction_indices=[0],keep_dims=True)   
sess = tf.InteractiveSession()
print (b.eval())
print (b.get_shape())
print (sum.eval())
print (sum.get_shape())
</code></pre>
",1
50997477,Keras and Tensorflow: Saving non-sequential model weights,"<p>(I'm using Tensorflow 1.8.0 ...)</p>

<p>The <a href=""https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model"" rel=""nofollow noreferrer"">documentation from Keras on how to save a model</a> mentions no difference between saving a sequential model vs. one created from the functional API. But, all of the following blocks of code fail:</p>

<pre><code>import tensorflow as tf
net = tf.keras.models.Model()
net.save('file')
</code></pre>

<p>or</p>

<pre><code>import tensorflow as tf
net = tf.keras.models.Model()
print(net.to_json())
</code></pre>

<p>or</p>

<pre><code>import tensorflow as tf
net = tf.keras.models.Model()
print(net.to_yaml())
</code></pre>

<p>or</p>

<pre><code>import tensorflow as tf
net = tf.keras.models.Model()
print(net.get_config())
</code></pre>

<p>They raise a <code>NotImplementedError</code>. In the Keras module, the relevant lines are</p>

<pre><code>if not self._is_graph_network:
  raise NotImplementedError
</code></pre>

<p>which shows up in <code>.save</code> and <code>get_config</code> (the latter is also called by <code>to_json</code> and <code>to_yaml</code>.</p>

<p>The only thing that DOES work is the following</p>

<pre><code>import tensorflow as tf
net = tf.keras.models.Model()
net.save_weights('file')
</code></pre>

<p>in which case the weights are saved successfully and can be successfully loaded with <code>net.load_weights</code>.</p>

<p>However, replacing the second line of the above blocks of code, <code>net = tf.keras.models.Model()</code>, with <code>net = tf.keras.models.Sequential()</code>, making net a sequential model, allows everything above to work.</p>

<p>Is it really not possible to save the structure of a Keras model made with the functional API (using <code>Model</code> rather than <code>Sequential</code>)? Right now, can we only save weights?</p>
",0
51000187,Panel data set in Tensorflow,"<p>I'm using Tensorflow for DNN regression with a panel data set. The data set is a pool of 12 months of cross sectional data, each month has <code>n</code> records and each record has <code>m</code> dimensions (i.e., <code>m</code> fields). So the dataset theoretically look like <code>t</code> x <code>n</code> x <code>m</code> and from the documentation, I should be using <code>tf.data.Dataset.from_tensor_slices</code>. The trouble is, <code>n</code> can vary month-by-month (i.e., sometimes there are 700 records that month, sometimes 800). So how would you feed in such a tensor?</p>

<p>Cheers,
Steve</p>
",0
51016069,Tensorflow tf.train.Saver not saving all variables,"<p>I thought that Tensorflow saver will save all variables as stated here</p>

<blockquote>
  <p>If you do not pass any arguments to tf.train.Saver(), the saver
  handles all variables in the graph. Each variable is saved under the
  name that was passed when the variable was created.</p>
</blockquote>

<p><a href=""https://www.tensorflow.org/programmers_guide/saved_model"" rel=""nofollow noreferrer"">https://www.tensorflow.org/programmers_guide/saved_model</a></p>

<p>However, the variable epochCount in my code below does not seem to get saved. This variable is used to keep track of the total epoches the model has trained over the data. </p>

<p>When I restore a graph it resets to it's initializer value, not the value it was when it the check point was last saved. </p>

<p>It appears to me that it's only saving variables used in calculating the loss. </p>

<p>Here's my code.</p>

<p>This is where I declare my graph:</p>

<pre><code>graph = tf.Graph()

with graph.as_default(): 

  valid_examples = np.array(random.sample(range(1, valid_window), valid_size)) #put inside graph to get new words each time

  train_dataset = tf.placeholder(tf.int32, shape=[batch_size, cbow_window*2 ])
  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])
  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)
  valid_datasetSM = tf.constant(valid_examples, dtype=tf.int32)

  epochCount = tf.get_variable( 'epochCount', initializer= 0) #to store epoch count to total # of epochs are known

  embeddings = tf.get_variable( 'embeddings', 
    initializer= tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))

  softmax_weights = tf.get_variable( 'softmax_weights',
    initializer= tf.truncated_normal([vocabulary_size, embedding_size],
                         stddev=1.0 / math.sqrt(embedding_size)))
  softmax_biases = tf.get_variable('softmax_biases', 
    initializer= tf.zeros([vocabulary_size]),  trainable=False )

  embed = tf.nn.embedding_lookup(embeddings, train_dataset) #train data set is
  embed_reshaped = tf.reshape( embed, [batch_size*cbow_window*2, embedding_size] )
  segments= np.arange(batch_size).repeat(cbow_window*2)
  averaged_embeds = tf.segment_mean(embed_reshaped, segments, name=None)

  loss = tf.reduce_mean(
    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds,
                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))

  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss) #Original learning rate was 1.0

  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))
  normalized_embeddings = embeddings / norm
  valid_embeddings = tf.nn.embedding_lookup(
    normalized_embeddings, valid_dataset) 
  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings)) 

  saver = tf.train.Saver()
</code></pre>

<p>If I restore the graph from checkpoint, the embeddings, and softmax_biases are restored, but epochCount is reset to its initializer value.  (Note that I am not calling the tf.global_variables_initializer().run() line, which is a common cause of variables mistakenly being reset after a checkpoint has been restored)</p>

<p>Here is the code where the graph is run </p>

<pre><code>num_steps = 1000001

with tf.Session(graph=graph) as session:

  saver.restore(session, './checkpointsBook2VecCbowWindow2Downloaded/bookVec.ckpt' )
  average_loss = 0
  saveIteration = 1
  for step in range(1, num_steps):

    batch_data, batch_labels = generate_batch(
      batch_size, cbow_window)
    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}
    _, l = session.run([optimizer, loss], feed_dict=feed_dict) 

    if step % 20000 == 0:
      recEpoch_indexA =  epoch_index - recEpoch_indexA
      epochCount = tf.add(  epochCount, recEpoch_indexA, name=None )
      recEpoch_indexA = epoch_index

      save_path = saver.save(session, ""checkpointsBook2VecCbowWindow2/bookVec.ckpt"") 
      chptName = 'B2VCbowW2Embed256ckpt'+str(saveIteration)
      zipfolder(chptName, 'checkpointsBook2VecCbowWindow2')
      uploadModel.SetContentFile(chptName+"".zip"")
      uploadModel.Upload()

      print(""Checkpoint uploaded to Google Drive"")
      saveIteration += 1
</code></pre>

<p>This is the code I use to print out all the variables saved in a checkpoint after training. I restore the graph and print out all the variables saved. </p>

<pre><code>with tf.Session() as sess:
  saver = tf.train.import_meta_graph('./MODEL/bookVec.ckpt.meta')
  saver.restore(sess, './MODEL/bookVec.ckpt' )
  for v in tf.get_default_graph().get_collection(""variables""):
    print('From variables collection ', v)
</code></pre>

<p>And this is the output from the code above</p>

<pre><code>From variables collection  &lt;tf.Variable 'embeddings:0' shape=(10001, 256) dtype=float32_ref&gt;
From variables collection  &lt;tf.Variable 'softmax_weights:0' shape=(10001, 256) dtype=float32_ref&gt;
From variables collection  &lt;tf.Variable 'softmax_biases:0' shape=(10001,) dtype=float32_ref&gt;
</code></pre>

<p>As seen, epochCount has not been saved. </p>
",0
51058476,why the parameter 'scale' of tf.layers.batch_normalization is disabled when next layer is relu?,"<p>In the tensorflow documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"" rel=""nofollow noreferrer"">tf.layers.batch_normalization</a>,it is said"" <strong>When the next layer is linear (also e.g. nn.relu), this(the parameter of 'scale' ) can be disabled since the scaling can be done by the next layer.</strong>"" ? It seems wrong because when the next layer is nn.relu, the linear coefficient is an invariant constant(1), and the value won't be sacled.</p>
",1
51069173,What exactly qualifies as a 'Tensor' in TensorFlow?,"<p>I am new to TensorFlow and just went through the eager execution tutorial and came across the tf.decode_csv function. Not knowing about it, I read the documentation.  <a href=""https://www.tensorflow.org/api_docs/python/tf/decode_csv"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/decode_csv</a></p>

<p>I don't really understand it. </p>

<p>The documentation says 'records: A Tensor of type string.' 
<strong>So, my question is: What qualifies as a 'Tensor'?</strong> </p>

<p>I tried the following code:</p>

<pre><code>dec_res = tf.decode_csv('0.1,0.2,0.3', [[0.0], [0.0], [0.0]])
print(dec_res, type(dec_res))



l = [[1,2,3],[4,5,6],[7,8,9]]
r = tf.reshape(l, [9,-1])
print(l, type(l))
print(r, type(r))
</code></pre>

<p>So the list <code>dec_res</code> contains tf.tensor objects. That seems reasonable to me. But is an ordinary string also a 'Tensor' according to the documentation?</p>

<p>Then I tried something else with the <code>tf.reshape</code> function. In the documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/reshape"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/reshape</a> it says that 'tensor: A Tensor.' So, <code>l</code> is supposed to be a tensor. But it is not of type <code>tf.tensor</code> but simply a python <code>list</code>. This is confusing.</p>

<p>Then the documentation says </p>

<blockquote>
  <p>Returns:</p>
  
  <p>A Tensor. Has the same type as tensor.</p>
</blockquote>

<p>But the type of <code>l</code> is <code>list</code> where the type of <code>r</code> is <code>tensorflow.python.framework.ops.Tensor</code>. So the types are not the same. </p>

<p>Then I thought that TensorFlow is very generous with things being a tensor. So I tried:</p>

<pre><code>class car(object):
def __init__(self, color):
    self.color = color


red_car = car('red')
#test_reshape = tf.reshape(red_car, [1, -1])
print(red_car.color) # to check, that red_car exists.
</code></pre>

<p>Now, the line in comments results in an error. </p>

<p>So, can anyone help me to find out, what qualifies as a 'Tensor'?</p>

<p>P.S.: I tried to read the source code of <code>tf.reshape</code> as given in the documentation </p>

<blockquote>
  <p>Defined in tensorflow/python/ops/gen_array_ops.py.</p>
</blockquote>

<p>But this file does not exist in the Github repo. Does anyone know how to read it?</p>
",1
51077930,tf.image.resize_bilinear()-when align_corners=False,"<p>I am using Tensorflow 1.4.0</p>

<p>The Tensorflow tf.image.resize_bilinear() has an argument called 'align_corners' and I am confused with the behavior when we set it to be False. In the <a href=""https://www.tensorflow.org/api_docs/python/tf/image/resize_bilinear"" rel=""noreferrer"">official document</a>, it says:</p>

<blockquote>
  <p>align_corners: An optional bool. Defaults to False. If true, the centers of the 4 corner pixels of the input and output tensors are aligned, preserving the values at the corner pixels. Defaults to false.</p>
</blockquote>

<p>When I use tf.image.resize_bilinear() with align_corners=True in the following program:</p>

<pre><code>import tensorflow as tf
sess = tf.Session()
x = tf.Variable(tf.Variable([[[[1],[2]],[[3],[4]]]]))
pooling_output_size = [4, 4]
pool_output = tf.image.resize_bilinear(x, pooling_output_size,align_corners=True)
sess.run(tf.global_variables_initializer())
print pool_output.eval(session=sess)
</code></pre>

<p>it outputs</p>

<pre><code>[[[[1.       ]
   [1.3333334]
   [1.6666667]
   [2.       ]]

  [[1.6666667]
   [2.       ]
   [2.3333335]
   [2.6666667]]

  [[2.3333335]
   [2.6666665]
   [3.       ]
   [3.3333335]]

  [[3.       ]
   [3.3333333]
   [3.6666667]
   [4.       ]]]]
</code></pre>

<p>which corners are correctly aligned.</p>

<p>However when I set the align_corners=False, I got the following weird outputs</p>

<pre><code>[[[[1. ]
   [1.5]
   [2. ]
   [2. ]]

  [[2. ]
   [2.5]
   [3. ]
   [3. ]]

  [[3. ]
   [3.5]
   [4. ]
   [4. ]]

  [[3. ]
   [3.5]
   [4. ]
   [4. ]]]]
</code></pre>

<p>Is there anyone who understand why Tensorflow will use this weird implementation? I didn't find any explanation anywhere.</p>

<p>Actually PyTorch's bilinear upsampling has the align_corner argument too, when you set it to True, it works well. But if you set it to False, it performs a differnet behaviour to Tensorflow's. I am totally confused with their implementations now (maybe just use align_corners=True will be fine).</p>
",1
51113982,TensorFlow: How to use 'tf.data' instead of 'load_csv_without_header'?,"<p>2 years ago I wrote code in TensorFlow, and as part of the data loading I used the function 'load_csv_without_header'.
Now, when I'm running the code, I get the message:</p>

<pre><code>WARNING:tensorflow:From C:\Users\Roi\Desktop\Code_Win_Ver\code_files\Tensor_Flow\version1\build_database_tuple.py:124: load_csv_without_header (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.data instead.
</code></pre>

<p>How do I use 'tf.data' instead of the current function? How can I can the same dtype, at the same format, without the csv header with tf.data? I'm using TF version 1.8.0 over Python 3.5.</p>

<p>Appreciate your help!</p>
",0
51118565,Tensorflow Dataset API - .from_tensor_slices() / .from_tensor() - cannot create a tensor proto whose content is larger than 2gb,"<p>So I want to use Dataset API for batching my large dataset (~8GB) as I am suffering from large idle times when using my GPU as I am passing data from python to Tensorflow using feed_dict.</p>

<p>When I follow the tutorial as mentioned here:</p>

<p><a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/5_DataManagement/tensorflow_dataset_api.py"" rel=""noreferrer"">https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/5_DataManagement/tensorflow_dataset_api.py</a></p>

<p>When running my simple code:</p>

<pre><code>one_hot_dataset = np.load(""one_hot_dataset.npy"")
dataset = tf.data.Dataset.from_tensor_slices(one_hot_dataset)
</code></pre>

<p>I am getting the error message with TensorFlow 1.8 and Python 3.5:</p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-17-412a606c772f&gt;"", line 1, in &lt;module&gt;
    dataset = tf.data.Dataset.from_tensor_slices((one_hot_dataset))

  File ""/anaconda2/envs/tf/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 235, in from_tensor_slices
    return TensorSliceDataset(tensors)

  File ""/anaconda2/envs/tf/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1030, in __init__
    for i, t in enumerate(nest.flatten(tensors))

  File ""/anaconda2/envs/tf/lib/python3.5/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1030, in &lt;listcomp&gt;
    for i, t in enumerate(nest.flatten(tensors))

  File ""/anaconda2/envs/tf/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1014, in convert_to_tensor
    as_ref=False)

  File ""/anaconda2/envs/tf/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1104, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)

  File ""/anaconda2/envs/tf/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 235, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)

  File ""/anaconda2/envs/tf/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 214, in constant
    value, dtype=dtype, shape=shape, verify_shape=verify_shape))

  File ""/anaconda2/envs/tf/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py"", line 496, in make_tensor_proto
    ""Cannot create a tensor proto whose content is larger than 2GB."")

ValueError: Cannot create a tensor proto whose content is larger than 2GB.
</code></pre>

<p>How can I solve this? I think the cause is obvious but what did the tf developers think by limiting the input data to 2GB ?!? I really cannot understand this rational and what is the workaround when dealing with larger datasets?</p>

<p>I googled quite a lot but I could not find any similar error message. When I use a FITFH of the numpy dataset, the steps above work without any issues. </p>

<p>I somehow need to tell TensorFlow that I actually will be loading the data batch by batch and probably want to prefetch a few batches in order to keep my GPU busy. But it seems as if it is trying to load the whole numpy dataset at once. So what is the benefit of using the Dataset API, as I am able to reproduce this error by simply trying to load my numpy dataset as a tf.constant into the TensorFlow graph, which is obviously does not fit and I get OOM errors.</p>

<p>Tips and troubleshooting hints appreciated!</p>
",0
51158399,Using stop_gradient with AdamOptimizer in TensorFlow,"<p>I am trying to implement a training/finetuning framework when in each backpropagation iteration a certain set of parameters stay fixed. I want to be able to change the set of updating or fixed parameters from iteration to iteration. TensorFlow method <a href=""https://www.tensorflow.org/api_docs/python/tf/stop_gradient"" rel=""nofollow noreferrer""><code>tf.stop_gradient</code></a>, which apparently forces gradients of some parameters to stay zero, is very useful for this purpose and it works perfectly fine with different optimizers if the set of updating or fixed parameters do not change from iterations to iterations. It can also handle varying set of updating or fixed parameters if it is used with <a href=""https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer"" rel=""nofollow noreferrer"">stochastic gradient descent</a>. My problem is that <code>tf.stop_gradient</code> cannot handle such cases when being used with <a href=""https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer"" rel=""nofollow noreferrer"">Adam optimizer</a>. More specifically, it does keep the gradients of the fixed parameters at zero in the output of <code>tf.compute_gradients</code>, but when applying the gradients (<code>tf.apply_gradients</code>), value of the fixed parameters does change. I suppose this is because the optimiaztion step in Adam optimizer is not zero even if the gradient is zero (based on algorithm 1 in <a href=""https://arxiv.org/pdf/1412.6980.pdf"" rel=""nofollow noreferrer"">Kingma and Ba's paper</a>). Is there a cheap way of freezing a variable set of parameters in each Adam iteration, without explicitly saving the previous iteration's values of the fixed parameters?</p>
<h2>More Details:</h2>
<p>Suppose I have a single-layer network with weight matrix variable <code>W</code> and a binary mask matrix placeholder <code>MW</code> that specifies which elements of <code>W</code> should get updated in each iteration (value 1 in the ). Instead of using <code>W</code> to write the input/output relationship of this layer, I modify it as below</p>
<pre><code>masked_W = MW*W + tf.stop_gradient(tf.abs(1-MW)*W)
</code></pre>
<p>to mask certain elements of <code>W</code> from having non-zero gradients. Then I use <code>masked_W</code> to form the output of the layer and consequently the loss of the network depends on this masked variable. The point is that <code>MW</code> changes in each iteration. Suppose <code>W</code> is a vector of 4 elements initialized to all-zero vector. Here is what happens:</p>
<pre><code>opt=tf.AdamOptimizer(1e-5)
sess.run(tf.global_variables_initializer())
grads_vars=opt.compute_gradients(loss, W)

# initial value of W=[0,0,0,0]

# first iteration:
MW_val = [0,1,1,0]
feed_dict={MW:MW_val, x: batch_of_data, y_:batch_of_labels}
sess.run(opt.apply_gradients(grads_vars), feed_dict=feed_dict))
# gradient of  W=[0,xx,xx,0]
# new value of W=[0,a,b,0]
</code></pre>
<p>where <code>xx</code> are some non-zero gradient values, and <code>a</code> and <code>b</code> are new values of updating elements of <code>W</code>. In the second iteration, we change the value assigned to the binary mask matrix <code>MW</code> to [1,0,0,1], hence we expect to have fixed values for <code>W[1]</code> and <code>W[2]</code> and updating values for <code>W[0]</code> and <code>W[3]</code>. But this is what happens:</p>
<pre><code># second iteration
MW_val = [1,0,0,1]
feed_dict={MW:MW_val, x: batch_of_data, y_:batch_of_labels}
sess.run(opt.apply_gradients(grads_vars), feed_dict=feed_dict))
# gradient of  W=[xx,0,0,xx]
# new value of W=[c,aa,bb,d]
</code></pre>
<p>That is, although the gradients of <code>W[1]</code> and <code>W[2]</code> are zero, they get new values (<code>aa != a</code> and <code>bb != b</code>). When changing the optimizer from Adam to SGD, the values of fixed parameters stay the same as expected.</p>
",0
51219219,How do I store and retrieve Tensors in a global namespace in Tensorflow?,"<p>I am trying to read a big Tensorflow project. For a project that the nodes of the computation graph are scattered around the project, I wonder if there is a way to store a Tensor node of the computation graph and add that node to the fetch list in sess.run? </p>

<p>For example, if I want to add probs at line 615 of 
<a href=""https://github.com/allenai/document-qa/blob/master/docqa/nn/span_prediction.py"" rel=""nofollow noreferrer"">https://github.com/allenai/document-qa/blob/master/docqa/nn/span_prediction.py</a> to a global namespace, is there a method like tf.add_node(probs, ""probs""), and later I could get tf.get_node(""probs""), just for the sake of conveniently passing node around the project. </p>

<p>A more general question would be, what will be a better idea to structure the tensorflow code and improve the efficiency of experimenting with different models. </p>
",0
51248061,Eval mode for Tensorflow Eager,"<p>I Pytorch there is this concept of <a href=""https://pytorch.org/docs/stable/nn.html#torch.nn.Module.eval"" rel=""nofollow noreferrer""><code>eval()</code></a> mode which, when set, affect some layers of the model such as deactivating dropout.</p>

<p>I'm using Tensorflow 1.9 with eager/<code>tf.data.Dataset</code>/Keras model subclassing and I was wondering if there is an equivalent for it as I don't want to dropout/batchnorm during the validation or test phase.</p>

<p>Thank you.</p>
",0
51248442,Behavior of the parameter 'throttle_secs' in tf.estimator.EvalSpec for use in tf.estimator.train_and_evaluate,"<p>I am using tensorflow's train_and_eval function as in the <a href=""https://www.tensorflow.org/versions/master/api_docs/python/tf/estimator/train_and_evaluate"" rel=""nofollow noreferrer"">example</a>. Therefore i create an instance of tf.estimator.EvalSpec, according to </p>

<pre><code>eval_spec = tf.estimator.EvalSpec(input_fn=...,throttle_secs=60).
</code></pre>

<p>According to its <a href=""http://eval_spec%20=%20tf.estimator.EvalSpec(input_fn=lambda:%20tf_data_utils.monuseg_input_func(mdl_info,train=False,normalize=True),throttle_secs=60*5)"" rel=""nofollow noreferrer"">documentation</a> the explanation of the parameter throttle_secs states that </p>

<p>""Of course, evaluation does not occur if no new checkpoints are available, hence, this is the minimum.""</p>

<p>However, i observe a different behavior. If there is no new checkpoint and evaluation should be triggered according to the passed parameter a new checkpoint is created and evaluation is performed. </p>

<p>Is this a bug or am i missing something here?</p>
",1
51259459,DataGenerator error in Keras only after import from different file,"<p>I am using a jupyter notebook to run python code with the following file structure:</p>

<blockquote>
  <h2>File structure</h2>

<pre><code>Main_jupyter_file.ipynb
dataloader.py
</code></pre>
  
  <h2>Main_jupyter_file.ipynb</h2>

<pre><code>from dataloader import DataGenerator
training_generator = DataGenerator(...)
model = ...
model.compile(loss=...optimizer='adam')
model.fit_generator(generator=training_generator,\
use_multiprocessing=True,workers=8, epochs=2)
</code></pre>
  
  <h2>dataloader.py</h2>

<pre><code>class DataGenerator(tf.keras.utils.Sequence):
    def __init__(self, ...):

    def __len__(self):
    'Denotes the number of batches per epoch'
        return ...

    def __getitem__(self, index):
        return ...
</code></pre>
</blockquote>

<p>After running <code>model.fit_generator</code> I receive a warning and an error:</p>

<h3>Warning:</h3>

<p>WARNING:tensorflow:Using a generator with <code>use_multiprocessing=True</code> and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.</p>

<h3>Error:</h3>

<p><code>steps_per_epoch=None</code> is only valid for a generator based on the <code>keras.utils.Sequence</code> class. Please specify <code>steps_per_epoch</code> or use the <code>keras.utils.Sequence</code> class.</p>

<p><strong>I get the same results with tf.keras and native Keras.</strong></p>

<h1>Weird finding</h1>

<p>When I copy the code of the DataGenerator class from dataloader.py into the jupyter notebook document, then I don't get any errors or warnings and everything works fine.</p>
",0
51273177,TensorFlow conv1d_transpose Filter Values,"<p>The <code>conv1d_transpose</code> in <code>tf.contrib.nn</code> needs to take a '<em>filter</em>' as a parameter. Unlike the transposed convolution functions for other dimensions this function doesn't automatically create a tensor from a list so it seems to require the user to create a filter as a tensor.</p>

<p>I therefore can't create a filter with a list such as <code>[filter_width, output_channels, input_channels]</code> but instead need to create the tensor and then pass that to '<em>filter</em>'.  But I don't know what values I need to set when I create the Tensor.</p>

<p>I have seen on an answer to another question (<a href=""https://stackoverflow.com/questions/47106331/how-to-use-the-conv1d-transpose-in-tensorflow"">How to use the conv1d_transpose in Tensorflow?</a>) that it may be created as below: </p>

<pre><code>tf.Variable(tf.random_normal([filter_width, output_channels, input_channels])
</code></pre>

<p>I want to know is this the standard way to create the transposed filter?  That is, with <code>tf.random_normal</code>. Specifically, does this match how the conv2d_transpose function creates the transposed filter? (I've looked through the documentation and code on GitHub but can't find where this should be occurring).</p>
",1
51278422,Interpreting the FLOPs profile result of tensorflow,"<p>I want to profile the FLOPs of a very simple neural network model, which is used to classify the MNIST dataset, and the batch size is 128. As I followed the official tutorials, I got the result of the following model, but I cannot understand some parts of the output.</p>

<pre><code>w1 = tf.Variable(tf.random_uniform([784, 15]), name='w1')
w2 = tf.Variable(tf.random_uniform([15, 10]), name='w2')
b1 = tf.Variable(tf.zeros([15, ]), name='b1')
b2 = tf.Variable(tf.zeros([10, ]), name='b2')

hidden_layer = tf.add(tf.matmul(images_iter, w1), b1)
logits = tf.add(tf.matmul(hidden_layer, w2), b2)

loss_op = tf.reduce_sum(\
    tf.nn.softmax_cross_entropy_with_logits(logits=logits, 
                                            labels=labels_iter))
opetimizer = tf.train.AdamOptimizer(learning_rate=0.01)
train_op = opetimizer.minimize(loss_op)
</code></pre>

<p>The <code>images_iter</code> and the <code>labels_iter</code> are the iterators of tf.data, which are similar to the placeholder. </p>

<pre><code>tf.profiler.profile(
    tf.get_default_graph(),
    options=tf.profiler.ProfileOptionBuilder.float_operation())
</code></pre>

<p>I used this code, which equals to <code>scope -min_float_ops 1 -select float_ops -account_displayed_op_only</code> in tfprof comments line tool, to profile the FLOPs and got the below result.</p>

<pre><code>Profile:
node name | # float_ops
_TFProfRoot (--/23.83k flops)
  random_uniform (11.76k/23.52k flops)
    random_uniform/mul (11.76k/11.76k flops)
    random_uniform/sub (1/1 flops)
  random_uniform_1 (150/301 flops)
    random_uniform_1/mul (150/150 flops)
    random_uniform_1/sub (1/1 flops)
  Adam/mul (1/1 flops)
  Adam/mul_1 (1/1 flops)
  softmax_cross_entropy_with_logits_sg/Sub (1/1 flops)
  softmax_cross_entropy_with_logits_sg/Sub_1 (1/1 flops)
  softmax_cross_entropy_with_logits_sg/Sub_2 (1/1 flops)
</code></pre>

<p>My questions are   </p>

<ol>
<li>What do the numbers in the parentheses mean? For example, <code>random_uniform_1 (150/301 flops)</code>, what are 150 and 301?</li>
<li>Why is the first number in the parentheses of _TFProfRoot ""--""?</li>
<li>Why are the flops of Adam/mul and softmax_cross_entropy_with_logits_sg/Sub 1?</li>
</ol>

<p>I know it is discouraging to read a question so long, but a desperate boy who cannot find relating information from the official document needs your guys to help.</p>
",1
51317200,What does scale stand for in regularizers in TensorFlow?,"<p>So my question what does <code>scale</code> stand for in: </p>

<p><code>tf.contrib.layers.l2_regularizer(
    scale,
    scope=None)</code></p>

<p>Here is the <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/layers/l2_regularizer"" rel=""nofollow noreferrer"">link</a> to documentation.</p>
",0
51327224,How to search old/legacy TensorFlow documentations efficiently?,"<p>I need to check the TensorFlow API documentation for an old TF version (0.12.1). The <a href=""https://www.tensorflow.org/versions/"" rel=""noreferrer"">website</a> points me to the GitHub branches. I was able to find my information by checking the code in the sub-packages of the respective branch. However, I am looking for a way to efficiently search old api information, i.e. using the search bar provided on the website and the hierarchical tree search (left hand-side on website) etc.</p>

<p>Is there faster way and if so, what's the best way except searching through .md and code files in this case?</p>
",0
51330841,How to save and restore a tf.estimator.Estimator model with export_savedmodel?,"<p>I started using Tensorflow recently and I try to get use to tf.estimator.Estimator objects. I would like to do something a priori quite natural: after having trained my classifier, i.e. an instance of tf.estimator.Estimator (with the <code>train</code> method), I would like to save it in a file (whatever the extension) and then reload it later to predict the labels for some new data. Since the official documentation recommends to use Estimator APIs, I guess something as important as that should be implemented and documented.</p>

<p>I saw on some other page that the method to do that is <code>export_savedmodel</code> (see <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Estimator#export_savedmodel"" rel=""nofollow noreferrer"">the official documentation</a>) but I simply don't understand the documentation. There is no explanation of how to use this method. What is the argument <code>serving_input_fn</code>? I never encountered it in the <a href=""https://www.tensorflow.org/guide/custom_estimators"" rel=""nofollow noreferrer"">Creating Custom Estimators</a> tutorial or in any of the tutorials that I read. By doing some googling, I discovered that around a year ago the estimators where defined using an other class (<code>tf.contrib.learn.Estimator</code>) and it looks like the tf.estimator.Estimator is reusing some of the previous APIs. But I don't find clear explanations in the documentation about it.</p>

<p>Could someone please give me a toy example? Or explain me how to define/find this <code>serving_input_fn</code>?</p>

<p>And then how would be load the trained classifier again?</p>

<p>Thank you for your help!</p>

<p><strong>Edit:</strong> I discovered that one doesn't necessarily need to use export_savemodel to save the model. It is actually done automatically. Then if we define later a new estimator having the same model_dir argument, it will also automatically restore the previous estimator, as explained <a href=""https://www.tensorflow.org/guide/checkpoints"" rel=""nofollow noreferrer"">here</a>.</p>
",1
51342304,Keras custom layer and eager execution,"<p>I have a simple model with one custom layer which works fine in the normal case.
When I switched to eager execution via <code>tf.enable_eager_execution()</code>, I got stuck on a weird error.</p>

<p>Here is the code so far:</p>

<pre><code>import numpy as np
import tensorflow as tf
import tensorflow.keras.backend as K
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer, Input
from tensorflow.keras.losses import kullback_leibler_divergence

tf.enable_eager_execution()

class ClusteringLayer(Layer):
    def __init__(self, output_dim, input_dim=None, alpha=1.0, **kwargs):
        self.output_dim = output_dim
        self.input_dim = input_dim
        self.alpha = alpha
        super(ClusteringLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name='kernel', shape=(self.output_dim, input_shape[1]), initializer='uniform', trainable=True)
        super(ClusteringLayer, self).build(input_shape)

    def call(self, x, mask=None):
        q = 1.0/(1.0 + K.sqrt(K.sum(K.square(K.expand_dims(x, 1) - self.W), axis=2))**2 /self.alpha)
        q = q**((self.alpha+1.0)/2.0)
        q = K.transpose(K.transpose(q)/K.sum(q, axis=1))
        return q

    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.output_dim)

def clustering_loss(y_true, y_pred): 
    a = K.square(y_pred) / K.sum(y_pred, axis=0) 
    p = K.transpose(K.transpose(a) / K.sum(a, axis=1))
    loss = kullback_leibler_divergence(p, y_pred)
    return loss

input1 = Input(shape=(10,), name=""input"")
out = ClusteringLayer(output_dim = 5, name='clustering')(input1)
model = Model(inputs=input1, outputs=out) 
model.compile(optimizer=tf.train.AdamOptimizer(1e-3), loss={'clustering' : clustering_loss})
X = np.random.random((20, 10)).astype(np.float32)
Y = np.random.random((20, 5)).astype(np.float32)
model.fit(x={'input' : X}, y={'clustering' : Y}, batch_size=1, epochs=10)
</code></pre>

<p>The error message is related to the ""fit"" function:</p>

<pre><code>AssertionError: Could not compute output DeferredTensor('None', shape=(5,), dtype=float32)
</code></pre>

<p>When I tried to check the output of my custom layer, I was surprised to find that this layer is generating two outputs. The first one is ambiguous and undesired.</p>

<p>Code:</p>

<pre><code>input1 = Input(shape=(10,), name=""input"")
layer = ClusteringLayer(output_dim = 5, name='clustering')
out = layer(input1)
print(out)
</code></pre>

<p>Output:</p>

<pre><code>[&lt;DeferredTensor 'None' shape=(?,) dtype=float32&gt;, &lt;DeferredTensor 'None' shape=(5,) dtype=float32&gt;]
</code></pre>

<p>Even when I changed my custom layer with the simplistic <a href=""https://keras.io/layers/writing-your-own-keras-layers/"" rel=""nofollow noreferrer"">custom layer from the Keras documentation</a>, I got the same error:</p>

<pre><code>AssertionError: Could not compute output DeferredTensor('None', shape=(5,), dtype=float32)
</code></pre>
",1
51355729,"Why does the Keras API require the input shape in the first layer, since it actually works well without it?","<p>I am using <code>tf.keras</code> from TensorFlow 1.9.0. It seems that everything works fine without specifying the <code>input_shape</code> in the first layer when building a <code>Sequential</code> model:</p>

<pre><code>import tensorflow as tf
from tensorflow import keras
import numpy as np

X_train = np.random.randn(1000, 10)
y_train = np.random.randn(1000)

model = keras.models.Sequential([
    keras.layers.Dense(32, activation=""relu""),
    keras.layers.Dense(1),
])
optimizer = tf.train.MomentumOptimizer(0.01, 0.9)
model.compile(loss=""mse"", optimizer=optimizer)
model.fit(X_train, y_train, epochs=10)
</code></pre>

<p>This makes sense to me because Keras can easily get the input shape from the inputs (<code>X_train</code>). I also tried using PyPI Keras, and it works fine too (with the TensorFlow 1.9.0 backend).</p>

<p>So I have two questions:</p>

<ol>
<li>why does the official Keras API <a href=""https://keras.io/getting-started/sequential-model-guide/#specifying-the-input-shape"" rel=""nofollow noreferrer"">require</a> the input shape to be specified in the first layer? Is it because it is necessary for backends other than TensorFlow?</li>
<li>is this behavior officially supported by <code>tf.keras</code>, even though it is not officially supported by the Keras API? In other words, if I do not care about other backends, is it legal to do this (not specify the input shape), or is there a risk it will break in future versions?</li>
</ol>

<p>Thanks!</p>
",0
51366455,TensorFlow bidirectional CudnnGRU ambiguous final state format,"<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/cudnn_rnn/CudnnGRU#call"" rel=""nofollow noreferrer"">documentation</a> for <code>tf.contrib.cudnn_rnn.CudnnGRU</code> states that <code>call</code> returns <code>rnn_output, rnn_state</code>. RNN state is a tuple, where in non-LSTM cases like GRU, it has a single element-- a tensor of shape <code>[num_layers * num_dirs, batch_size, num_units]</code> (where <code>num_dirs</code> is 2 in this case, for a bidirectional GRU). However, the documentation does not specify the order of the outer shape.</p>

<p>Succinctly, given an n-layer bidirectional GRU, is this tensor <code>[final_state_fw_1, ..., final_state_fw_n, final_state_bw_1, ..., final_state_bw_n]</code>
OR
<code>[final_state_fw_1, final_state_bw_1, ..., final_state_fw_n, final_state_bw_n]</code> ?</p>
",1
51392594,"`ValueError: Can not squeeze dim[1], expected a dimension of 1, got 10 for 'sparse_softmax_cross_entropy_loss' [Tensorflow]","<p>I am rather new to Tensorflow, and has been trying to pick up the basics by reading through the guides and documentation on tensorflow.org</p>

<p>I have learnt the basics of how to use the tf.data and tf.estimator APIs and is trying to get them to work together on a basic image model for MNIST.</p>

<p>I am currently following these guides: 
<a href=""https://www.tensorflow.org/tutorials/estimators/cnn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/estimators/cnn</a> 
<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py</a></p>

<p>I've changed the original python script to be using <code>Dataset.from_tensor_slices</code> rather than <code>numpy_input_fn</code> but I am facing the error at the evaluation step. (though not at the training step)</p>

<p><code>ValueError: Can not squeeze dim[1], expected a dimension of 1, got 10 for 'sparse_softmax_cross_entropy_loss/remove_squeezable_dimensions/Squeeze' (op: 'Squeeze') with input shapes: [1,10].</code></p>

<p>My code can be found in a python notebook here (only changed the input_fn): <a href=""https://github.com/quanta0801/tf_scripts/blob/master/mnist/mnist_estimator_baseline.ipynb"" rel=""nofollow noreferrer"">https://github.com/quanta0801/tf_scripts/blob/master/mnist/mnist_estimator_baseline.ipynb</a></p>

<p>Thanks!</p>

<p>PS: any additional links to excellent guides to using tf.data &amp; tf.estimators will be great too! Official documentation cycles between these, keras and the low level APIs which is not conducive.</p>
",1
51401935,Arguments to tensorflow session.run() - do you pass operations?,"<p>I'm following this <a href=""https://www.tensorflow.org/versions/r1.3/get_started/mnist/beginners"" rel=""noreferrer"">tutorial for tensorflow</a>:</p>

<p>I'm trying to understand the arguments to <code>tf.session.run()</code>. I understand that you have to run operations in a graph in a session. </p>

<p>Is <code>train_step</code> passed in because it encapsulates all the operations of the network in this particular example? I'm trying to understand why I don't need to pass any other variables to the session like <code>cross_entropy</code>.</p>

<pre class=""lang-python prettyprint-override""><code>sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
</code></pre>

<p>Here is the full code:</p>

<pre class=""lang-python prettyprint-override""><code>from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)

import tensorflow as tf

x = tf.placeholder(tf.float32, [None, 784])

W = tf.Variable(tf.zeros([784, 10]))

b = tf.Variable(tf.zeros([10]))

y = tf.nn.softmax(tf.matmul(x, W) + b)

y_ = tf.placeholder(tf.float32, [None, 10])

cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))

train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)

sess = tf.InteractiveSession()

tf.global_variables_initializer().run()

for _ in range(10):
    batch_xs, batch_ys = mnist.train.next_batch(100)

    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})

    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))

    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
</code></pre>
",0
51407186,Invalid Syntax Error when creating estimator in Tensorflow,"<p>I am attempting to use Tensorflow in a Colab Notebook to train identically structured neural networks with varying sizes of randomly chosen training examples from the MNIST data set. I have previously been successful in training neural networks in a similar fashion several times, but this time I am receiving an Invalid Syntax error that I have not been able to correct. The error appears for the following line of code:</p>

<p><code>mnist_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn)</code></p>

<p>I have used this same line of code successfully in a different notebook, and I have not been able to recognize any problems with the syntax (it's from the Tensorflow documentation found here: <a href=""https://www.tensorflow.org/tutorials/layers"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/layers</a>) </p>

<p>The code I am trying to execute can be found below (but I have omitted the definition of cnn_model_fn for the sake of brevity).</p>

<pre><code>#Initialize list to hold all of the accuracy data
all_accuracies = []

#For each training set group
for group_number in range(len(train_set_sizes)):

  #For each ANN in training set group
  for i in range(num_ANNs):

    #Initialize group_accuracies list
    group_accuracies = []

    #--------------Generate a training set--------------------------------

    #Initialize train_set lists
    train_set_examples = []
    train_set_labels = []

    #randomly select indices to pull train set examples
    example_select_indices = np.random.randint(0,55000,size=train_set_sizes[group_number])

    #For number of images in the appropriate training set group
    for j in range(train_set_sizes[group_number]):

      random_index = example_select_indices[j]
      train_set_examples = np.asarray(train_set_examples.append(train_images[random_index]))
      train_set_labels = np.asarray(train_set_labels.append(train_labels[random_index])

    # Create the MNIST Estimator
    mnist_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn)

    # Train the CNN model
    mnist_classifier.train(input_fn=train_input_fn,steps=train_steps)

    # Evaluate the models and print results
    eval_result = mnist_classifier.evaluate(input_fn=eval_input_fn)
    print(""Group Number "",group_number+1,"", ANN #"",i+1,"" Accuracy: "",eval_result,"" %"")

    #Record ANN accuracy
    group_accuracies.append(eval_result)

#record group accuracies in all_accuracies
all_accuracies.append(group_accuracies)
</code></pre>

<p>Does anybody have any suggestions for why this error might be occurring?</p>

<p>Thank you!</p>
",0
51466554,Correct use of tf.while_loop when variables are created inside body,"<p>I'm using a while_loop in Tensorflow in order to iterate over a tensor and extracting specific slices over a given dimension. For each step, I need to use a decoder RNN to generate a sequence of output symbols. I'm using the code provided in <em>tf.contrib.seq2seq</em>, in particular, <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/dynamic_decode"" rel=""nofollow noreferrer"">tf.contrib.seq2seq.dynamic_decode</a>. The code looks similar to the following:</p>

<pre><code>def decoder_condition(i, data, source_seq_len, ta_outputs):
    return tf.less(i, max_loop_len)

def decode_body(i, data, source_seq_len, ta_outputs):
    curr_data = data[:, i, :]
    curr_source_seq_len = source_seq_len[:, i, :]
    attention_mechanism = tf.contrib.seq2seq.LuongAttention(
        2 * self.opt[""encoder_rnn_h_size""],
        curr_data,
        memory_sequence_length=curr_source_seq_len
    )
    cell = GRUCell(num_units)
    cell = AttentionWrapper(cell, attention_mechanism)
    # ... other code that initialises all the variables required
    # for the RNN decoder
    outputs = tf.contrib.seq2seq.dynamic_decode(
        decoder,
        maximum_iterations=self.opt[""max_sys_seq_len""],
        swap_memory=True
    )
    with tf.control_dependencies([outputs)]:
        ta_outputs = ta_outputs.write(i, outputs)

    return i+1, data, ta_outputs

 loop_index = tf.constant(0)
 gen_outputs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)
 outputs = tf.while_loop(
      decoder_condition,
      decoder_body,
      loop_vars=[
          loop_index,
          data,
          data_source_len,
          ta_outputs
      ],
      swap_memory=True,
      back_prop=True, 
      parallel_iterations=1
)
</code></pre>

<p>So as you can see, I create different objects which depend specifically on the input at the current step <em>i</em>. I'm using <code>tf.AUTO_REUSE</code> in my current variable scope in such a way that the variables are reused even if I'm creating different objects. Unfortunately, my decoder seems that it's not properly training because it keeps generating incorrect values. I've already checked the input data to the decoder RNN and everything is correct. I suspect that there is something that I'm not doing properly in terms of how TensorFlow manages the TensorArray and while_loop.</p>

<p>So my main questions are:</p>

<ol>
<li>Is TensorFlow correctly propagating the gradients for each variable that it's created inside the while loop?</li>
<li>Is it possible to create object inside the while loop that are dependent on specific slices of a Tensor obtained using the loop index?</li>
<li>Does the <em>backprop</em> parameter guarantee that the gradients are propagated during training? Should it be set to False during inference?</li>
<li>In general, are there any sanity check that I can use to spot possible errors in my implementation?</li>
</ol>

<p>Thanks!</p>

<p>UPDATE:
Not sure why but seems that there is an open issue about this which is related to the possibility to invoke custom operations in a while loop as explained here: <a href=""https://github.com/tensorflow/tensorflow/issues/13616"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/13616</a>. Unfortunately, I don't know enough TensorFlow's internals to judge if it's completely related to this.</p>

<p>UPDATE 2: 
I solved using PyTorch :) </p>
",0
51499154,"Mnist with Tensorflow Premade Estimator, input dimension mismatch on evaluate","<p>I am rather new to Tensorflow, and has been trying to pick up the basics by reading through the guides and documentation on tensorflow.org</p>

<p>I have learnt the basics of how to use the <code>tf.data</code> and <code>tf.estimator</code> APIs and is trying to get them to work together on a basic classification model for MNIST.</p>

<p>I am using this script to load MNIST: <a href=""https://github.com/tensorflow/models/blob/master/official/mnist/dataset.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/blob/master/official/mnist/dataset.py</a></p>

<p>I made modifications to the dataset function to return a feature dictionary rather than vector:</p>

<pre><code>def dataset(directory, images_file, labels_file):
  """"""Download and parse MNIST dataset.""""""

  images_file = download(directory, images_file)
  labels_file = download(directory, labels_file)

  check_image_file_header(images_file)
  check_labels_file_header(labels_file)

  def decode_image(image):
    # Normalize from [0, 255] to [0.0, 1.0]
    image = tf.decode_raw(image, tf.uint8)
    image = tf.cast(image, tf.float32)
    image = tf.reshape(image, [784])
    return image / 255.0

  def decode_label(label):
    label = tf.decode_raw(label, tf.uint8)  # tf.string -&gt; [tf.uint8]
    label = tf.reshape(label, [])  # label is a scalar
    return tf.to_int32(label)

  images = tf.data.FixedLengthRecordDataset(
      images_file, 28 * 28, header_bytes=16).map(decode_image)
  labels = tf.data.FixedLengthRecordDataset(
      labels_file, 1, header_bytes=8).map(decode_label)
  return tf.data.Dataset.zip(({""image"":images}, labels))
</code></pre>

<p>My MNIST classifier script using the premade estimator in tf is as follows:</p>

<pre><code>import tensorflow as tf
import dataset

fc = [tf.feature_column.numeric_column(""image"", shape=784)]

mnist_classifier = tf.estimator.DNNClassifier(
    hidden_units=[512,512],
    feature_columns=fc,
    model_dir=""models/mnist/dnn"",
    n_classes=10)

def input_fn(train=False, batch_size=None):
    if train:
        ds = mnist.train(""MNIST-data"")
        ds = ds.shuffle(1000).repeat().batch(batch_size)
    else:
        ds = mnist.test(""MNIST-data"")
    return ds

mnist_classifier.train(
  input_fn=lambda:input_fn(True, 32),
  steps=10000)

eval_results = mnist_classifier.evaluate(input_fn=lambda:input_fn())
</code></pre>

<p>The classifier doesn't crash on training, but on evaluate, I faced the following traceback:</p>

<blockquote>
  <p>ValueError: Cannot reshape a tensor with 784 elements to shape
  [784,784] (614656 elements) for
  'dnn/input_from_feature_columns/input_layer/image/Reshape' (op:
  'Reshape') with input shapes: [784,1], [2] and with input tensors
  computed as partial shapes: input[1] = [784,784].</p>
</blockquote>

<p>What could be causing the issue here?</p>

<p>I have tried printing the output shapes and types of both train and test datasets, and they are exactly the same.</p>

<p>I have also tried viewing the model on tensorboard, and only the projector tab is available, no scalars or graphs tab.</p>

<p>Thanks!</p>

<p>PS: Any links to TF tutorials using the Datasets and Estimators APIs will be great too.</p>
",0
51515569,"In Tensorflow I can't use any MultiRNNCell instance in dynamic decode, but a single RNNCell instance can work on it","<p>I make a seq2seq model using tensorflow and meet a problem that my program throws an error when I use MultiRNNCell in tf.contrib.seq2seq.dynamic_decode.</p>

<p>The problem happens over here:</p>

<pre><code>defw_rnn=tf.nn.rnn_cell.MultiRNNCell([
            tf.nn.rnn_cell.LSTMCell(num_units=self.FLAGS.rnn_units,
            initializer=tf.orthogonal_initializer)
            for _ in range(self.FLAGS.rnn_layer_size)])

        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_inputs,
                                                            sequence_length=self.decoder_targets_length,
                                                            time_major=False)

        training_decoder = \
            tf.contrib.seq2seq.BasicDecoder(
                defw_rnn, training_helper,
                encoder_final_state,
                output_layer)
        training_decoder_output, _, training_decoder_output_length = \
            tf.contrib.seq2seq.dynamic_decode(
                training_decoder,
                  impute_finished=True,
                  maximum_iterations=self.FLAGS.max_len)
</code></pre>

<p>When I run this code,the console shows this Error message:</p>

<blockquote>
  <p><code>C:\Users\TopView\AppData\Local\Programs\Python\Python36\python.exe E:/PycharmProject/cikm_transport/CIKM/CIKM/translate_model/train.py
  WARNING:tensorflow:From C:\Users\TopView\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\rnn.py:417: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.
  Instructions for updating:
  seq_dim is deprecated, use seq_axis instead
  WARNING:tensorflow:From C:\Users\TopView\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\util\deprecation.py:432: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.
  Instructions for updating:
  batch_dim is deprecated, use batch_axis instead
  encoder_final_state shpe
  LSTMStateTuple(c=&lt;tf.Tensor 'encoder/bidirectional_rnn/fw/fw/while/Exit_5:0' shape=(?, 24) dtype=float32&gt;, h=&lt;tf.Tensor 'encoder/bidirectional_rnn/fw/fw/while/Exit_6:0' shape=(?, 24) dtype=float32&gt;)
  decoder_inputs shape before embedded
  (128, 10)
  decoder inputs shape after embedded
  (128, 10, 5)
  Traceback (most recent call last):
    File ""E:/PycharmProject/cikm_transport/CIKM/CIKM/translate_model/train.py"", line 14, in &lt;module&gt;
      len(embedding_matrix['embedding'][0]))
    File ""E:\PycharmProject\cikm_transport\CIKM\CIKM\translate_model\model.py"", line 109, in __init__
      maximum_iterations=self.FLAGS.max_len)
    File ""C:\Users\TopView\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\seq2seq\python\ops\decoder.py"", line 323, in dynamic_decode
      swap_memory=swap_memory)
    File ""C:\Users\TopView\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 3209, in while_loop
      result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
    File ""C:\Users\TopView\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2941, in BuildLoop
      pred, body, original_loop_vars, loop_vars, shape_invariants)
    File ""C:\Users\TopView\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2878, in _BuildLoop
      body_result = body(*packed_vars_for_body)
    File ""C:\Users\TopView\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 3179, in &lt;lambda&gt;
      body = lambda i, lv: (i + 1, orig_body(*lv))
    File ""C:\Users\TopView\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\seq2seq\python\ops\decoder.py"", line 266, in body
      decoder_finished) = decoder.step(time, inputs, state)
    File ""C:\Users\TopView\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\seq2seq\python\ops\basic_decoder.py"", line 137, in step
      cell_outputs, cell_state = self._cell(inputs, state)
    File ""C:\Users\TopView\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py"", line 232, in __call__
      return super(RNNCell, self).__call__(inputs, state)
    File ""C:\Users\TopView\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\layers\base.py"", line 329, in __call__
      outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
    File ""C:\Users\TopView\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 703, in __call__
      outputs = self.call(inputs, *args, **kwargs)
    File ""C:\Users\TopView\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py"", line 1325, in call
      cur_inp, new_state = cell(cur_inp, cur_state)
    File ""C:\Users\TopView\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py"", line 339, in __call__
      *args, **kwargs)
    File ""C:\Users\TopView\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\layers\base.py"", line 329, in __call__
      outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
    File ""C:\Users\TopView\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 703, in __call__
      outputs = self.call(inputs, *args, **kwargs)
    File ""C:\Users\TopView\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\rnn_cell_impl.py"", line 846, in call
      (c_prev, m_prev) = state
    File ""C:\Users\TopView\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 436, in __iter__
      ""Tensor objects are not iterable when eager execution is not ""
  TypeError: Tensor objects are not iterable when eager execution is not enabled. To iterate over this tensor use tf.map_fn.</code></p>
  
  <p><code>Process finished with exit code 1</code></p>
</blockquote>

<p>But when I change the instance of <code>defw_rnn</code>, make it a single RNN instance like LSTMCell, the Error disappears:</p>

<pre><code>defw_rnn=tf.nn.rnn_cell.LSTMCell(num_units=self.FLAGS.rnn_units,
            initializer=tf.orthogonal_initializer)
</code></pre>

<p>And the code works well. However, I've found  that most of the code about seq2seq model on the Internet using MultiRNNCell and they also use tensorflow, so it really confuse me that what is wrong with my program.</p>

<p>Here is the entire code:</p>

<pre><code>import tensorflow as tf
import numpy as np

class Seq2SeqModel(object):
    def bw_fw_rnn(self): 
        with tf.name_scope(""forward_rnn""):
            fw = tf.nn.rnn_cell.MultiRNNCell([
                tf.nn.rnn_cell.LSTMCell(num_units=self.FLAGS.rnn_units,
                                        initializer=tf.orthogonal_initializer) for _ in
                range(self.FLAGS.rnn_layer_size)])
            fw = tf.nn.rnn_cell.DropoutWrapper(fw, output_keep_prob=self.FLAGS.keep_prob)
        with tf.name_scope(""backward_rnn""):
            bw = tf.nn.rnn_cell.MultiRNNCell([
                tf.nn.rnn_cell.LSTMCell(num_units=self.FLAGS.rnn_units,
                                        initializer=tf.orthogonal_initializer) for _ in
                range(self.FLAGS.rnn_layer_size)])
            bw = tf.nn.rnn_cell.DropoutWrapper(bw, output_keep_prob=self.FLAGS.keep_prob)
        return (fw, bw)

    def decode_inputs_preprocess(self, data, id_matrix):
        ending=tf.strided_slice(data,[0,0],[self.batch_size,-1],[1,1])
        decoder_input=tf.concat([tf.fill([self.batch_size,1],id_matrix.index('&lt;go&gt;')),ending],1)
        return decoder_input

    def __init__(self, FLAGS, english_id_matrix, spanish_id_matrix, english_vocab_size,spanish_vocab_size, embedding_size):
        self.FLAGS = FLAGS
        self.english_vocab_size = english_vocab_size
        self.embedding_size = embedding_size
        self.encoder_input = tf.placeholder(shape=[None, self.FLAGS.max_len], dtype=tf.int32, name='encoder_inputs')
        self.decoder_targets = tf.placeholder(shape=[None, self.FLAGS.max_len], dtype=tf.int32, name='decoder_targets')
        self.encoder_input_sequence_length = tf.placeholder(shape=[None], dtype=tf.int32, name='encoder_inputs_length')
        self.decoder_targets_length = tf.placeholder(shape=[None], dtype=tf.int32, name='decoder_targets_length')
        self.batch_size = self.FLAGS.batch_size
        with tf.name_scope('embedding_look_up'):
            spanish_embeddings = tf.Variable(
                tf.random_uniform([english_vocab_size,
                                   embedding_size], -1.0, 1.0),
                dtype=tf.float32)
            english_embeddings = tf.Variable(
                tf.random_uniform([english_vocab_size,
                                   embedding_size], -1.0, 1.0),
                dtype=tf.float32)
            self.spanish_embeddings_inputs = tf.placeholder(
                dtype=tf.float32, shape=[english_vocab_size, embedding_size],
                name='spanish_embeddings_inputs')
            self.english_embeddings_inputs = tf.placeholder(
                dtype=tf.float32, shape=[english_vocab_size, embedding_size],
                name='spanish_embeddings_inputs')
            self.spanish_embeddings_inputs_op = spanish_embeddings.assign(self.spanish_embeddings_inputs)
            self.english_embeddings_inputs_op = english_embeddings.assign(self.english_embeddings_inputs)
            encoder_inputs = tf.nn.embedding_lookup(spanish_embeddings, self.encoder_input)

        with tf.name_scope('encoder'):
            enfw_rnn, enbw_rnn = self.bw_fw_rnn()
            encoder_outputs, encoder_final_state = \
                tf.nn.bidirectional_dynamic_rnn(enfw_rnn, enbw_rnn, encoder_inputs
                                                , sequence_length=self.encoder_input_sequence_length, dtype=tf.float32)
            print(""encoder_final_state shpe"")
            # final_state_c=tf.concat([encoder_final_state[0][-1].c,encoder_final_state[1][-1].c],1)
            # final_state_h=tf.concat([encoder_final_state[0][-1].h,encoder_final_state[1][-1].h],1)
            # encoder_final_state=tf.contrib.rnn.LSTMStateTuple(c=final_state_c,
             #                                        h=final_state_h)
            encoder_final_state=encoder_final_state[0][-1]
            print(encoder_final_state)

        with tf.name_scope('dense_layer'):
            output_layer = tf.layers.Dense(english_vocab_size,
                                           kernel_initializer=tf.truncated_normal_initializer(
                                               mean=0.0, stddev=0.1
                                           ))
        # training decoder
        with tf.name_scope('decoder'), tf.variable_scope('decode'):
            decoder_inputs=self.decode_inputs_preprocess(self.decoder_targets,english_id_matrix)
            print('decoder_inputs shape before embedded')
            print(decoder_inputs.shape)
            decoder_inputs = tf.nn.embedding_lookup(english_embeddings,decoder_inputs)
            print('decoder inputs shape after embedded')
            print(decoder_inputs.shape)
            defw_rnn=tf.nn.rnn_cell.MultiRNNCell([
                tf.nn.rnn_cell.LSTMCell(num_units=self.FLAGS.rnn_units,
                initializer=tf.orthogonal_initializer)
                for _ in range(self.FLAGS.rnn_layer_size)])

            training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_inputs,
                                                                sequence_length=self.decoder_targets_length,
                                                                time_major=False)

            training_decoder = \
                tf.contrib.seq2seq.BasicDecoder(
                    defw_rnn, training_helper,
                    encoder_final_state,
                    output_layer)
            training_decoder_output, _, training_decoder_output_length = \
                tf.contrib.seq2seq.dynamic_decode(
                    training_decoder,
                      impute_finished=True,
                      maximum_iterations=self.FLAGS.max_len)
            training_logits = tf.identity(training_decoder_output.rnn_output, 'logits')
            print(""training logits shape"")
            print(training_logits.shape)
        # predicting decoder
        with tf.variable_scope('decode', reuse=True):
            start_tokens = tf.tile(tf.constant([english_id_matrix.index('&lt;go&gt;')], dtype=tf.int32),
                                   [self.batch_size], name='start_tokens')
            predicting_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(english_embeddings,
                                                                         start_tokens,
                                                                         english_id_matrix.index('&lt;eos&gt;'))
            predicting_decoder = tf.contrib.seq2seq.BasicDecoder(defw_rnn,
                                                                 predicting_helper,
                                                                 encoder_final_state,
                                                                 output_layer)
            predicting_decoder_output, _, predicting_decoder_output_length =\
                tf.contrib.seq2seq.dynamic_decode(
                predicting_decoder,
                impute_finished=True,
                maximum_iterations=self.FLAGS.max_len)

            self.predicting_logits = tf.identity(predicting_decoder_output.sample_id, name='predictions')
            print(""predicting logits shape"")
            print(self.predicting_logits.shape)
        masks = tf.sequence_mask(self.decoder_targets_length, self.FLAGS.max_len, dtype=tf.float32, name='masks')
        with tf.variable_scope('optimization'), tf.name_scope('optimization'):
            # Loss
            self.cost = tf.contrib.seq2seq.sequence_loss(training_logits, self.decoder_targets, masks)
            # Optimizer
            optimizer = tf.train.AdamOptimizer(self.FLAGS.alpha)
            # Gradient Clipping
            gradients = optimizer.compute_gradients(self.cost)
            capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]
            self.train_op = optimizer.apply_gradients(capped_gradients)
</code></pre>
",0
51586693,"Tensor has shape [?, 0] -- how to reshape to [?,]","<p>When <code>src</code> has shape <code>[?]</code>, <code>tf.gather(src, tf.where(src != 0))</code> returns a tensor with shape <code>[?, 0]</code>. I'm not sure how a dimension can have size 0, and I'm especially unsure how to change the tensor back. I didn't find anything in the documentation to explain this, either.</p>

<p>I tried to <code>tf.transpose(tensor)[0]</code>, but the first dimension of the transposed tensor has size 0 and cannot be accessed! What's wrong?</p>
",1
51625529,How to use tf.data's initializable iterator and reinitializable interator and feed data to estimator api?,"<p>All the official google tutorials use the one shot iterator for all the estimator api implementation, i couldnt find any documentation on how to use tf.data's initializable iterator and reinitializable interator instead of one shot iterator.</p>

<p>Can someone kindly show me how to switch between train_data and test_data using tf.data's initializable iterator and reinitializable interator. We need to run a session to use feed dict and switch the dataset in the initializable iterator, its a low level api and its confusing how to use it part of estimator api architecture</p>

<p>PS : I did find that google mentions 
""Note: Currently, one-shot iterators are the only type that is easily usable with an Estimator.""</p>

<p>But is there any work around within the community? or should we just stick with one shot iterator for some good reason</p>
",1
51642951,Tensorflow: predict grow factor in a time series forecast,"<p>I am developing an application to predict future hourly online orders on my e-commerce website (time-series problem) using Canned Estimator <code>tf.estimator.DNNRegressor</code></p>

<pre><code>    estimator = tf.estimator.DNNRegressor(
        feature_columns=my_feature_columns,
        hidden_units=hidden_units, 
        model_dir=model_dir, 
        optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.01,
                                                l1_regularization_strength=0.001))
</code></pre>

<p>The features I am using are pretty much based on the date and time. For example, the <code>csv</code> file from my training data looks like this</p>

<pre><code>year,month,day,weekday,isweekend,hr,weeknum,yearday,orders
2018,7,16,2,0,0,29,197,193
2018,7,16,2,0,1,29,197,131
2018,7,16,2,0,2,29,197,77
2018,7,16,2,0,3,29,197,59
.....
</code></pre>

<p>where orders column is the target for the model.</p>

<p>The model I got so far is working decently but when I run predictions for a high demand day like Black Friday, it is under-predicting. For example, in the graph below we can see that predictions for Black Friday this year 2018 (dashed line) are not as high as we intuitively expect, even though it predicts the shape nicely.</p>

<p><a href=""https://i.stack.imgur.com/8Z1K1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8Z1K1.png"" alt=""enter image description here""></a></p>

<p>With that all being said, I would appreciate any recommendation to add to my model so it can also predict correctly the grow factor and not only the trend.</p>
",0
51687832,Probability Distribution for tf.nn.softmax_cross_entropy_with_logits_v2,"<p>I am trying to understand the Tensorflow documentation better for tf.nn.softmax_cross_entropy_with_logits_v2().</p>

<p>In the documentation, it states:
While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution. If they are not, the computation of the gradient will be incorrect.</p>

<p>Does this mean that, for my labels, I shouldn't be simply using one-hot encoding, but should also account for the number of instances of each label? For example, if I have 2 classes, and there are 90 examples for class ""A"" and only 10 examples for class ""B"", should my label for a class A be [0.9, 0.1], instead of just [1, 0]?</p>

<p>I hope this makes sense. Thanks!</p>
",1
51706848,How does tf.reshape() work internally ?,"<p>I'm trying to understand how tf.reshape works. Let's have an example:</p>

<pre><code>embeddings = tf.placeholder(tf.float32, shape=[N0,N1])
M_2D = tf.placeholder(tf.float32, shape=[N0,None])
M_3D = tf.reshape(M_2D, [-1,N0,1])
weighted_embeddings = tf.multiply(embeddings, M_3D)
</code></pre>

<p>Here I have a 2D tensor M_2D whose columns represent coefficients for the N0 embeddings of dimension N1. I want to create a 3D tensor where each column of M_2D is placed in the first dimension of M_3D, and columns are keep in the same order. My final goal is to create a 3D tensor of 2D embeddings, each weighted by the columns of M_2D. </p>

<p>How can I be sure that reshape actually place each column in the new dimension of M_3D. Is it possible that it places the rows instead ? Is there somewhere in tensorflow documentation a clear explanation on the internal working process of tf.reshape, particularly when -1 is provided ?    </p>
",1
51717817,performance measurement in Tensorflow's eager mode,"<p>In tensorflow's guide about <a href=""https://www.tensorflow.org/guide/eager#performance"" rel=""nofollow noreferrer"">the performance of eager execution</a>, there is a piece of code as follows:</p>

<pre><code>import time
def measure(x, steps):
  # TensorFlow initializes a GPU the first time it's used, exclude from timing.
  tf.matmul(x, x)
  start = time.time()
  for i in range(steps):
      x = tf.matmul(x, x)
      _ = x.numpy()  # Make sure to execute op and not just enqueue it
  end = time.time()
 return end - start
...
with tf.device(""/cpu:0""):
    print(""CPU: {} secs"".format(measure(tf.random_normal(shape), steps)))

with tf.device(""/gpu:0""):
    print(""GPU: {} secs"".format(measure(tf.random_normal(shape), steps)))
</code></pre>

<p>what is the meaning of the code before the second comment: ""_ = x.numpy()""?<br>
If I comment out this line, will tf.matmul(x,x) not be executed on cpu/gpu? </p>
",0
51806852,Can't save custom subclassed model,"<p>Inspired by <a href=""https://www.tensorflow.org/guide/keras#model_subclassing"" rel=""noreferrer"">tf.keras.Model subclassing</a> I created custom model.<br>
I can train it and get successfull results, but <strong>I can't save it</strong>.<br>
I use python3.6 with tensorflow v1.10 (or v1.9)  </p>

<p>Minimal complete code example here:</p>

<pre><code>import tensorflow as tf
from tensorflow.keras.datasets import mnist


class Classifier(tf.keras.Model):
    def __init__(self):
        super().__init__(name=""custom_model"")

        self.batch_norm1 = tf.layers.BatchNormalization()
        self.conv1 = tf.layers.Conv2D(32, (7, 7))
        self.pool1 = tf.layers.MaxPooling2D((2, 2), (2, 2))

        self.batch_norm2 = tf.layers.BatchNormalization()
        self.conv2 = tf.layers.Conv2D(64, (5, 5))
        self.pool2 = tf.layers.MaxPooling2D((2, 2), (2, 2))

    def call(self, inputs, training=None, mask=None):
        x = self.batch_norm1(inputs)
        x = self.conv1(x)
        x = tf.nn.relu(x)
        x = self.pool1(x)

        x = self.batch_norm2(x)
        x = self.conv2(x)
        x = tf.nn.relu(x)
        x = self.pool2(x)

        return x


if __name__ == '__main__':
    (x_train, y_train), (x_test, y_test) = mnist.load_data()

    x_train = x_train.reshape(*x_train.shape, 1)[:1000]
    y_train = y_train.reshape(*y_train.shape, 1)[:1000]

    x_test = x_test.reshape(*x_test.shape, 1)
    y_test = y_test.reshape(*y_test.shape, 1)

    y_train = tf.keras.utils.to_categorical(y_train)
    y_test = tf.keras.utils.to_categorical(y_test)

    model = Classifier()

    inputs = tf.keras.Input((28, 28, 1))

    x = model(inputs)
    x = tf.keras.layers.Flatten()(x)
    x = tf.keras.layers.Dense(10, activation=""sigmoid"")(x)

    model = tf.keras.Model(inputs=inputs, outputs=x)
    model.compile(optimizer=""adam"", loss=""binary_crossentropy"", metrics=[""accuracy""])
    model.fit(x_train, y_train, epochs=1, shuffle=True)

    model.save(""./my_model"")
</code></pre>

<p>Error message:  </p>

<pre><code>1000/1000 [==============================] - 1s 1ms/step - loss: 4.6037 - acc: 0.7025
Traceback (most recent call last):
  File ""/home/user/Data/test/python/mnist/mnist_run.py"", line 62, in &lt;module&gt;
    model.save(""./my_model"")
  File ""/home/user/miniconda3/envs/ml3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1278, in save
    save_model(self, filepath, overwrite, include_optimizer)
  File ""/home/user/miniconda3/envs/ml3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py"", line 101, in save_model
    'config': model.get_config()
  File ""/home/user/miniconda3/envs/ml3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1049, in get_config
    layer_config = layer.get_config()
  File ""/home/user/miniconda3/envs/ml3.6/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1028, in get_config
    raise NotImplementedError
NotImplementedError

Process finished with exit code 1
</code></pre>

<p>I looked into the error line and found out that <strong>get_config</strong> method checks <strong>self._is_graph_network</strong></p>

<p>Do anybody deal with this problem?</p>

<p>Thanks!</p>

<p><strong>Update 1:</strong><br>
On the keras 2.2.2 (not tf.keras)<br>
Found comment (for model saving)<br>
file: keras/engine/network.py<br>
Function: get_config  </p>

<blockquote>
  <p># Subclassed networks are not serializable<br>
  # (unless serialization is implemented by<br>
  # the author of the subclassed network).  </p>
</blockquote>

<p>So, obviously it won't work...<br>
I wonder, why don't they point it out in the <a href=""https://www.tensorflow.org/guide/keras"" rel=""noreferrer"">documentation</a> (Like: ""Use subclassing without ability to save!"")</p>

<p><strong>Update 2:</strong><br>
Found in <a href=""https://keras.io/models/about-keras-models/"" rel=""noreferrer"">keras documentation</a>:  </p>

<blockquote>
  <p>In subclassed models, the model's topology is defined as Python code<br>
  (rather than as a static graph of layers). That means the model's<br>
  topology cannot be inspected or serialized. As a result, the following<br>
  methods and attributes are not available for subclassed models:  </p>
  
  <p>model.inputs        and model.outputs.<br>
  model.to_yaml()     and model.to_json()<br>
  model.get_config()  and model.save().  </p>
</blockquote>

<p><strong>So, there is no way to save model by using subclassing.</strong><br>
It's possible to only use <code>Model.save_weights()</code></p>
",1
51812904,Understanding estimator class of tensorflow,"<p>I'd like to understand why and where do we use <code>tf.estimator.EstimatorSpec()</code>. I read the documentation on the Tensorflow website but I can't get an intuitive idea about it.</p>

<p>Please explain it to me in simple language.</p>
",1
51824310,Difference between Keras and tensorflow implementation of LSTM with dropout,"<p>I was reviewing the documentation for the LSTM cell in tensorflow and Keras. In particular, I want to apply dropout as well. Here is what I have in Keras and would like to apply the same LSTM cell in tensorflow:</p>

<pre><code>cell = LSTM(num_units_2, return_sequences=True, dropout=dropout, recurrent_dropout=dropout)(net)
</code></pre>

<p>Therefore, I know that I need to use <code>tf.nn.rnn_cell.LSTMCell</code> in tensorflow with <code>num_units = num_units_2</code>. Second, I need a <code>DropoutWrapper</code> as:</p>

<pre><code>cell = tf.nn.rnn_cell.DropoutWrapper(cell)
</code></pre>

<p>Now, I want to apply <code>dropout</code> and <code>recurrent_dropout</code> similar to the Keras code. Therefore, I found that tensorflow's implementation of dropout will apply a different dropout mask at every time step unless <code>variational_recurrent</code> is set to True (Yet I'm not sure how variational_recurrent works in details). </p>

<p>Additionally, I'm not sure if the LSTM in Keras apply different Mask at each time step as well. </p>

<p>Second, I was confused about the difference between the <code>output_keep_prob</code> and the <code>state_keep_prob</code> as both mention: </p>

<p><em>output_keep_prob</em>: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added...</p>

<p>Any help is much appreciated!!</p>
",1
51845480,Error when using tf.get_variable as alternativ for tf.Variable in Tensorflow,"<p>Hi I'm new to neural networks and I'm currently working on Tensoflow.
First I did the MNIST tutorial which worked quite well. Now I wanted to deepen the whole by means of an own network for Cifar10 in Google Colab. For this purpose I wrote the following code:</p>

<pre><code>def conv2d(input, size, inputDim, outputCount):
  with tf.variable_scope(""conv2d""): 
    ## -&gt; This area causes problems &lt;- ##
    ##########variant1
    weight = tf.Variable(tf.truncated_normal([size, size, inputDim, outputCount], stddev=0.1),name=""weight"")
    bias = tf.Variable( tf.constant(0.1, shape=[outputCount]),name=""bias"")
    ##########variant2
    weight = tf.get_variable(""weight"", tf.truncated_normal([size, size, inputDim, outputCount], stddev=0.1))
    bias = tf.get_variable(""bias"", tf.constant(0.1, shape=[outputCount]))
    ##################
    conv = tf.nn.relu(tf.nn.conv2d(input, weight, strides=[1, 1, 1, 1], padding='SAME') + bias)    
  return conv

def maxPool(conv2d):....

def fullyConnect(input, inputSize, outputCount, relu):
  with tf.variable_scope(""fullyConnect""):
    ## -&gt; This area causes problems &lt;- ##
    ##########variant1
    weight = tf.Variable( tf.truncated_normal([inputSize, outputCount], stddev=0.1),name=""weight"")
    bias = tf.Variable( tf.constant(0.1, shape=[outputCount]),name=""bias"")
    ##########variant2
    weight = tf.get_variable(""weight"", tf.truncated_normal([inputSize, outputCount], stddev=0.1))
    bias = tf.get_variable(""bias"", tf.constant(0.1, shape=[outputCount]))
    ##################            
    fullyIn = tf.reshape(input, [-1, inputSize])        
    fullyCon = fullyIn
    if relu:
      fullyCon = tf.nn.relu(tf.matmul(fullyIn, weight) + bias)
  return fullyCon

#Model Def.
def getVGG16A(grafic,width,height,dim):
  with tf.name_scope(""VGG16A""):    
    img = tf.reshape(grafic, [-1,width,height,dim])
    with tf.name_scope(""Layer1""): 
      with tf.variable_scope(""Layer1""):   
        with tf.variable_scope(""conv1""):
          l1_c = conv2d(img,3, dim, 64)        
        with tf.variable_scope(""mp1""):
          l1_mp = maxPool(l1_c) #32 &gt; 16

    with tf.name_scope(""Layer2""):
      with tf.variable_scope(""Layer2""):  
        with tf.variable_scope(""conv1""):
          l2_c = conv2d(l1_mp,3, 64, 128)
        with tf.variable_scope(""mp1""):
          l2_mp = maxPool(l2_c) #16 &gt; 8

    with tf.name_scope(""Layer6""):
      with tf.variable_scope(""Layer6""):  
        with tf.variable_scope(""fully1""):
          L6_fc1 = fullyConnect(l2_mp, 8*8*128 , 1024, True)
        with tf.variable_scope(""fully2""):
          L6_fc2 = fullyConnect(L6_fc1, 1024, 1024, True)         

        keep_prob = tf.placeholder(tf.float32)
        drop = tf.nn.dropout(L6_fc2, keep_prob)

        with tf.variable_scope(""fully3""):
          L6_fc3 = fullyConnect(drop,1024, 3, False)
  return L6_fc3, keep_prob

x = tf.placeholder(tf.float32, [None, 3072]) #input
y_ = tf.placeholder(tf.float32, [None, 3])   #output

# Build the graph for the deep net
y_conv, keep_prob = getVGG16A(x,32,32,3) #create Model

cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))
train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())   

  for batch in getBatchData(prep_filter_dataBatch1,2): #a self-written method for custom batch return

    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.8})

  print('test accuracy %g' % accuracy.eval(feed_dict={
      x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))
</code></pre>

<p>For the definition of the tensorflow variables I first used variant1 (tf.variable). 
This caused an overflow of the graphics memory after repeated execution. 
Then I used variant2 (tf.get_variable). If I have understood the documentation correctly, this should use already existing variables if they exist. </p>

<p>But as soon as I do this I get the following error message:</p>

<pre><code>TypeError: Tensor objects are not iterable when eager execution is not enabled. To iterate over this tensor use tf.map_fn.
</code></pre>

<p>I've been looking the hole day, but I haven't found an explanation for this.</p>

<p>Now I hope that there is someone here who can explain to me why this is not possible, or where I can find further information. The error message is getting me nowhere. I don't want a solution because I want to and have to understand this, because I want to write my bachelor thesis in the field of CNN.</p>

<p>Why can I use tf.variable but not tf.get_variable which should do the same?</p>

<p>Thanks for the help,
best regards, Pascal :)</p>
",1
51858891,Loading a NumPy array into a Tensor,"<p>According to the TensorFlow webpage at (<a href=""https://www.tensorflow.org/versions/r1.3/programmers_guide/datasets"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r1.3/programmers_guide/datasets</a>), the <code>tf.read_file</code> can be used to load an image file from a given filename, and convert it to a Tensor:</p>

<pre><code>image_string = tf.read_file(filename)
image_decoded = tf.image.decode_image(image_string)
</code></pre>

<p>In my case however, I want to load a NumPy array rather than an image. So the <code>filename</code> above points to a NumPy array on my machine.</p>

<p>If I were to use <code>tf.read_file(filename)</code> on this filename, then according to the documentation, this function returns a string Tensor (a byte array). How can I convert this into a Tensor representing the data in the NumPy array? Is there an equivalent function to <code>tf.image.decode_image()</code> for decoding a NumPy array?</p>
",0
51858970,"tf.gradients() sums over ys, does it?","<p><a href=""https://www.tensorflow.org/versions/r1.6/api_docs/python/tf/gradients"" rel=""noreferrer"">https://www.tensorflow.org/versions/r1.6/api_docs/python/tf/gradients</a></p>

<p>In the documentation for tf.gradients(ys, xs) it states that </p>

<blockquote>
  <p>Constructs symbolic derivatives of sum of ys w.r.t. x in xs</p>
</blockquote>

<p>I am confused about the summing part, I have read elsewhere that this sums the derivatives dy/dx across the batch for every x in the batch. However, whenever I use this I fail to see this happening. Take the following simple example:</p>

<pre><code>x_dims = 3
batch_size = 4

x = tf.placeholder(tf.float32, (None, x_dims))

y = 2*(x**2)

grads = tf.gradients(y,x)

sess = tf.Session()

x_val = np.random.randint(0, 10, (batch_size, x_dims))
y_val, grads_val = sess.run([y, grads], {x:x_val})

print('x = \n', x_val)
print('y = \n', y_val)
print('dy/dx = \n', grads_val[0])
</code></pre>

<p>This gives the following output:</p>

<pre><code>x = 
 [[5 3 7]
 [2 2 5]
 [7 5 0]
 [3 7 6]]
y = 
 [[50. 18. 98.]
 [ 8.  8. 50.]
 [98. 50.  0.]
 [18. 98. 72.]]
dy/dx = 
 [[20. 12. 28.]
 [ 8.  8. 20.]
 [28. 20.  0.]
 [12. 28. 24.]]
</code></pre>

<p>This is the output I would expect, simply the derivative dy/dx for every element in the batch. I don't see any summing happening. I have seen in other examples that this operation is followed by dividing by the batch size to account for tf.gradients() summing the gradients over the batch (see here: <a href=""https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html"" rel=""noreferrer"">https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html</a>). Why is this necessary?</p>

<p>I am using Tensorflow 1.6 and Python 3.</p>
",1
51861790,How to preprocess features before training a TensorFlow model end-to-end in TF graph,"<p>By this piece of code from the documentation, we can create multiple features to feed batches of data into a DNN model:</p>

<pre><code>my_feature_columns = []
for key in train_x.keys():
    my_feature_columns.append(tf.feature_column.numeric_column(key=key))
</code></pre>

<p>But the problem is what is the proper way to transform the original features before they are fed to the input layer? Typical transformations that I can think of include normalization and clipping. </p>

<p><code>tf.feature_column.numeric_column</code> does have a parameter specifying the normalization function. But the example in the doc only demonstrate a scenario where the normalization factors are pre-defined and fixed, like <code>lambda x: (x-3.2)/1.5</code>. How can I perform normalization (e.g. <code>MinMaxScaler</code> in sklearn) across all those features without knowing its maximum and minimum beforehand.</p>

<p>Also, is there any pipeline implementation where it's possible to do all sorts of feature transformations before they go into the input layer? Is creating a custom estimator <code>tf.estimator.Estimator</code> the answer to this problem? or anything else I'm not aware of.</p>
",1
51882846,tensorflow: how to use tf.nn.leaky_relu with alpha as activation_fn for tf.contrib.layers.fully_connected?,"<p>The default <code>activation_fn</code> for <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected"" rel=""nofollow noreferrer"">tf.contrib.layers.fully_connected</a> is <code>tf.nn.relu</code>.</p>

<p>If I want to change the <code>activation_fn</code> for a certain fully connected layer to <code>tf.nn.tanh</code> then I can call it as: <code>tf.contrib.layers.fully_connected(inputs, num_outputs, activation_fn=tf.nn.tanh)</code></p>

<p>Now if I want to use <code>tf.nn.leaky_relu</code> with <code>alpha=0.01</code>, I can't do that. I can only use <code>tf.nn.leaky_relu</code> with the default value of <code>alpha</code>.</p>

<p>Is there any elegant way to do that ? Or should I make <code>activation_fn=None</code> and then manually call <code>tf.nn.leaky_relu</code> after that ?</p>
",0
51962001,Finding roots of 4th degree polynomial using tensorflow by Halley's method,"<p>I've just started learning tensorflow, and basically I'm learning to do various numerical computations using tensorflow rather than directly jumping into its use in ML. I'm doing this on the Google Cloud Platform, where I came across this problem and got stuck.</p>

<p><a href=""https://i.stack.imgur.com/T9L6e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/T9L6e.png"" alt=""Roots of 4th degree polynomial""></a></p>

<p>I'm using lazy evaluation and I can make instances of a0, a1, a2...,a4 in the tensorflow graph using placeholders, also I can write out the function. But how do I make the initial guess using tensorflow? And moreover, even if I get value of x0 how do I apply loop using <a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""nofollow noreferrer"">tf.while_loop</a>
I went through its documentation and this <a href=""https://stackoverflow.com/questions/43792961/understanding-the-while-loop-in-tensorflow"">post</a>, but I'm still clueless as to how do I proceed. I tried to find a post with similar question or content but couldn't find a one in which tensorflow is used.
It would be great if I could get insights or way to use intrinsic tensorflow functions and commands :) Thanks in advance! </p>
",1
51997426,TensorFlow: alternate between datasets of different output shapes,"<p>I'm trying to use <code>tf.Dataset</code> for a 3D image CNN where the shape of the 3D image fed into it from the training set and the validation set are different (training: (64, 64, 64), validation: (176, 176, 160)). I didn't even know this was possible, but I'm recreating this network based on a paper, and using the classic <code>feed_dict</code> method the network indeed works. For performance reasons (and just to learn) I'm trying to switch the network to use <code>tf.Dataset</code> instead.</p>

<p>I have two datasets and iterators built like the following:</p>

<pre class=""lang-py prettyprint-override""><code>def _data_parser(dataset, shape):
        features = {""input"": tf.FixedLenFeature((), tf.string),
                    ""label"": tf.FixedLenFeature((), tf.string)}
        parsed_features = tf.parse_single_example(dataset, features)

        image = tf.decode_raw(parsed_features[""input""], tf.float32)
        image = tf.reshape(image, shape + (1,))

        label = tf.decode_raw(parsed_features[""label""], tf.float32)
        label = tf.reshape(label, shape + (1,))
        return image, label

train_datasets = [""train.tfrecord""]
train_dataset = tf.data.TFRecordDataset(train_datasets)
train_dataset = train_dataset.map(lambda x: _data_parser(x, (64, 64, 64)))
train_dataset = train_dataset.batch(batch_size) # batch_size = 16
train_iterator = train_dataset.make_initializable_iterator()

val_datasets = [""validation.tfrecord""]
val_dataset = tf.data.TFRecordDataset(val_datasets)
val_dataset = val_dataset.map(lambda x: _data_parser(x, (176, 176, 160)))
val_dataset = val_dataset.batch(1)
val_iterator = val_dataset.make_initializable_iterator()
</code></pre>

<p><a href=""https://www.tensorflow.org/guide/datasets#creating_an_iterator"" rel=""nofollow noreferrer"">TensorFlow documentation</a> has examples regarding switching between datasets using <code>reinitializable_iterator</code> or <code>feedable_iterator</code>, but they all switch between iterators of <strong>same</strong> output shape, which is not the case here.</p>

<p>How should I switch between training set and validation set using <code>tf.Dataset</code> and <code>tf.data.Iterator</code> in my case then?</p>
",1
52017367,Manual initialization of conv1d in TensorFlow,"<p>How can I set custom coefficients to <code>tf.layers.conv1d</code>.
I found out how to read current coefficients, but how can I write them?</p>

<pre><code>import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

sess = tf.Session()
order = 5
x = np.zeros(30)
x[10] = 1
y = tf.layers.conv1d(inputs=tf.reshape(x,[1, len(x), 1]),
                     filters=1,
                     kernel_size=order,
                     padding='same')
sess.run(tf.global_variables_initializer())
y_out = sess.run(y)

# get coef
coef = sess.run(tf.all_variables()[-2].value())
print(coef.reshape(order))
</code></pre>

<p>Here is a link to notebook with code at google colab:
<a href=""https://colab.research.google.com/drive/1YNSzKmtC88b__LqYcfD-tFHFG3jOZIAz"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1YNSzKmtC88b__LqYcfD-tFHFG3jOZIAz</a></p>

<p>In general, I'm interested in how to make a FIR-filter in TensorFlow.</p>
",0
52035692,Tensorflow v1.10: store images as byte strings or per channel?,"<h2>Context</h2>
<p>It is known that, at the moment, TF's Record documentation leaves something to be desired.</p>
<p>My question is in regards to what is optimal for storing:</p>
<ul>
<li>a sequence,</li>
<li>its per-element class probabilities, and</li>
<li>some (context?) information (e.g. name of the sequence)</li>
</ul>
<p>as a TF Record.</p>
<p>Namely, this questions considers storing the sequence and class probabilities as channels vs as a byte string and whether or not the meta information should go in as features of a <code>tf.train.Example</code> or as the context of a <code>tf.train.SequenceExample</code>. (see questions at the bottom).</p>
<h2>M.W.E.</h2>
<p>For example, lets assume my looks sequence like this</p>
<pre><code>seq = [ 
        # el1, el2 
        [ 0,   1   ], # channel 1
        [ 0,   1   ]  # channel 2
      ]
</code></pre>
<p>i.e. it is a 2 channel sequence of <em>fixed</em> length (in this example, 2) where the values can only be integer value.</p>
<p>and that we have three classes for which we are trying to segment the sequence into</p>
<pre><code>cls_probs = [ 
        #cls1, cls2, cls3
        [0   , 0.9 , 0.1 ], # class probabilities element 1
        [0   , 0.1 , 0.9 ]  # class probabilities element 2
      ]
</code></pre>
<p>where in effect both <code>seq</code> and <code>cls_probs</code> are <code>numpy.array</code>s.</p>
<p>The network only <em>requires</em> this information. However, I also have some meta data which I would like to keep with the sequence.</p>
<p>e.g.</p>
<pre><code>meta = {
           'name': 'my_seq',  # safer to keep this with the data rather than as file name
           'meta_val_1': 100, # not used by network, but may be useful when evaluating network's predictions for this particular sequence
           'meta_val_2': 10
       }
</code></pre>
<h1>Making TF Record</h1>
<h2>tf.train.Example</h2>
<p>Then I have several ways I could construct my <code>tf.train.Example</code>:</p>
<h3>as channels</h3>
<pre><code>example = tf.train.Example(
    features = tf.train.Features(
        feature = {
            'channel_1': tf.train.Feature(int64_list=tf.train.Int64List(value=seq[:,0])),
            'channel_2': tf.train.Feature(int64_list=tf.train.Int64List(value=seq[:,1])),
            'class_1'  : tf.train.Feature(float_list=tf.train.FloatList(value=cls_probs[:,0])),
            'class_2'  : tf.train.Feature(float_list=tf.train.FloatList(value=cls_probs[:,1])),
            'class_3'  : tf.train.Feature(float_list=tf.train.FloatList(value=cls_probs[:,2])),
            'name'     : tf.train.Feature(bytes_list=tf.train.BytesList(value=[f'{meta[&quot;name&quot;]}'.encode('utf-8')])), 
            # should these be FloatList even though it is just a single value?
            # should these be included here if they are not used by the network?
            'val_1'    : tf.train.Feature(float_list=tf.train.FloatList(value=[f'{meta[&quot;meta_val_1&quot;]}'])),
            'val_2'    : tf.train.Feature(float_list=tf.train.FloatList(value=[f'{meta[&quot;meta_val_2&quot;]}'])),
    })
)
</code></pre>
<p>where <code>f'{variable}'.encode('utf-8')</code> is the currently not suggested <a href=""https://www.python.org/dev/peps/pep-0498/"" rel=""nofollow noreferrer""><code>fb'&lt;string&gt;'</code></a> (note: <code>f-strings</code> only work with python3.6+).</p>
<p>This format is somewhat nice as each sequence channel is explicit. However it is also verbose and requires preprocessing when loaded to be feed into the network.</p>
<h3>as string</h3>
<p>or, I could dump my array to an string</p>
<pre><code>example = tf.train.Example(
    features = tf.train.Features(
        feature = {
            'sequence' : tf.train.Feature(bytes_list=tf.train.BytesList(value=seq.tostring())),
            'cls_probs': tf.train.Feature(bytes_list=tf.train.BytesList(value=cls_probs.tostring())),
            # ... see encoding of meta values from above
    })
)
</code></pre>
<h2>tf.train.SequenceExample</h2>
<p>TF Records also accept another form: <code>tf.train.SequenceExample</code>. <code>SequenceExample</code> expects context features and an ordered list of unnamed features.</p>
<h3>as channels</h3>
<p>So restructuring above's as <em>channels</em> example:</p>
<pre><code>example = tf.train.SequenceExample(
    context = tf.train.Features(
        feature = {
            'Name' : tf.train.Feature(bytes_list=tf.train.BytesList(value=[f'{meta[&quot;name&quot;]}'.encode('utf-8')])), 
            'Val_1': tf.train.Feature(float_list=tf.train.FloatList(value=[f'{meta[&quot;meta_val_1&quot;]}'])),
            'Val_2': tf.train.Feature(float_list=tf.train.FloatList(value=[f'{meta[&quot;meta_val_2&quot;]}'])),
        }
    ),
    feature_lists = tf.train.FeatureLists(
        feature_list = {
            'sequence': tf.train.FeatureList(
                feature = [
                    tf.train.Feature(int64_list=tf.train.Int64List(value=seq[:,0])),
                    tf.train.Feature(int64_list=tf.train.Int64List(value=seq[:,1])),
                ]
            ),
            'class_probabilities': tf.train.FeatureList(
                feature = [
                    tf.train.Feature(float_list=tf.train.FloatList(value=cls_probs[:,0])),
                    tf.train.Feature(float_list=tf.train.FloatList(value=cls_probs[:,1])),
                    tf.train.Feature(float_list=tf.train.FloatList(value=cls_probs[:,2]))
                ]
            )
        }
    )
)
</code></pre>
<h3>as string</h3>
<p>likewise we can create the as <em>string</em> example:</p>
<pre><code>example = tf.train.SequenceExample(
    context = tf.train.Features(
        # see above
    ),
    feature_lists = tf.train.FeatureLists(
        feature_list = {
            'sequence': tf.train.FeatureList(
                feature = [
                    tf.train.Feature(bytes_list=tf.train.BytesList(value=seq.tostring()))
                ]
            ),
            'class_probabilities': tf.train.FeatureList(
                feature = [
                    tf.train.Feature(bytes_list=tf.train.BytesList(value=cls_probs.tostring()))
                ]
            )
        }
    )
)
</code></pre>
<h1>Questions</h1>
<p>Here I gave a M.W.E. for how one could construct an example (ready to be exported to a TF Record) as both <code>tf.train.Example</code> and <code>tf.train.SequenceExample</code>. Further, I demonstrated both how to do this per channel or by dumping as a byte string. Both of these methods (as channels / as strings) include the meta information within the example.</p>
<p>Thus my questions are:</p>
<ol>
<li><p>which way (as channels / as string) of storage is more optimal (for read, write, re-use, etc) ?</p>
</li>
<li><p>given the meta information which should be kept with the example, is better to use <code>tf.train.Example</code> and store the meta information as features there? or use <code>tf.train.SequenceExample</code> and store the meta information in the context argument?</p>
</li>
</ol>
<p>Does anyone know if there are any notable advantages / disadvantages for any of four these strategies?</p>
<p>For those who would like to test this on larger less dummy like data, some functions for producing this code can be found <a href=""https://stackoverflow.com/a/52041027/5623899""><strong>below</strong></a></p>
<p>Lastly, I would like to point out this <a href=""https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564"" rel=""nofollow noreferrer"">medium post</a> which greatly elaborates on TF's docs.</p>
",1
52047963,Tensorflow remove layers from pretrained model,"<p>Is there a way to load a pretrained model in Tensorflow and remove the top layers in the network? I am looking at Tensorflow release r1.10</p>

<p>The only documentation I could find is with <code>tf.keras.Sequential.pop</code>
<a href=""https://www.tensorflow.org/versions/r1.10/api_docs/python/tf/keras/Sequential#pop"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r1.10/api_docs/python/tf/keras/Sequential#pop</a></p>

<p>I want to manually prune a pretrained network by removing bunch of top convolution layers and add a custom fully convoluted layer.</p>

<p>EDIT:</p>

<p>The model is ssd_mobilenet_v1_coco downloaded from <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"" rel=""nofollow noreferrer"">Tensorflow Model Zoo</a>. I have access to both the frozen_inference_graph.pb model file and checkpoint file.</p>

<p>I donot have access to the python code which is used to construct the model.</p>

<p>Thanks.</p>
",1
52073782,Computations (such as tf.greater and tf.cond) on random value tensors not working as expected,"<p>I am a tensorflow beginner. According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/greater"" rel=""nofollow noreferrer"">documentation</a>, <strong>tf.greater returns the truth value of (x>y) element-wise</strong></p>

<p>My code is as below:</p>

<pre><code>x = tf.random_uniform([])  # Empty array as shape creates a scalar.
y = tf.random_uniform([])
print('x: '+str(x.eval()))
print('y: ' +str(y.eval()))
out = tf.cond(tf.greater(x, y), lambda: x + y, lambda: x - y)
print(sess.run(tf.greater(x, y)))
print(sess.run(out))
</code></pre>

<p>The output I got is:</p>

<pre><code>x: 0.79379404
y: 0.30891895
False
0.3438499
</code></pre>

<p>x is bigger than y so it should return True and x+y should be 1.10271299
why is my expected output different than the actual output?</p>
",1
52129909,tensorflow equivalent of torch.gather,"<p>I have a tensor of shape <code>(16, 4096, 3)</code>. I have another tensor of indices of shape <code>(16, 32768, 3)</code>. I am trying to collect the values along <code>dim=1</code>. This was initially done in pytorch using <a href=""https://pytorch.org/docs/stable/torch.html#torch.gather"" rel=""nofollow noreferrer"">gather function</a> as shown below-</p>

<pre><code># a.shape (16L, 4096L, 3L)
# idx.shape (16L, 32768L, 3L)
b = a.gather(1, idx)
# b.shape (16L, 32768L, 3L)
</code></pre>

<p>Please note that the size of output <code>b</code> is the same as that of <code>idx</code>. However, when I apply <code>gather</code> function of tensorflow, I get a completely different output. The output dimension was found mismatching as shown below-</p>

<pre><code>b = tf.gather(a, idx, axis=1)
# b.shape (16, 16, 32768, 3, 3)
</code></pre>

<p>I also tried using <code>tf.gather_nd</code> but got in vain. See below-</p>

<pre><code>b = tf.gather_nd(a, idx)
# b.shape (16, 32768)
</code></pre>

<p>Why am I getting different shapes of tensors? <em>I want to get the tensor of the same shape as calculated by pytorch.</em> </p>

<p><strong>In other words, I want to know the tensorflow equivalent of torch.gather.</strong></p>
",0
52180502,Multi-dimensional tensors as input to rnn in tensorflow (tf.contrib.rnn.RNNCell),"<p>From tensorflow documentation about tf.contrib.rnn.RNNCell: ""This definition of cell differs from the definition used in the literature. In the literature, 'cell' refers to an object with a single scalar output. This definition refers to a horizontal array of such units.""</p>

<p>It seems, that rnn cell only accepts vectors as inputs. However I would like to feed images/videos to an rnn (e.g. [batch size, steps, height, width, channels]). Is there a way to do this using rnn cell and dynamic rnn, or do I have to manually construct an rnn?</p>
",0
52246415,Dense vector on sparse matrix multiplication in tensorflow,"<p>What is proper way of dense vector on sparse matrix multiplication in tensorflow?</p>

<p>According to documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/matmul"" rel=""nofollow noreferrer"">tf.matmul</a> support sparse matrix multiplication, so do I need to use <a href=""https://www.tensorflow.org/api_docs/python/tf/sparse_matmul"" rel=""nofollow noreferrer"">tf.sparse_matmul</a>? (And also <a href=""https://www.tensorflow.org/api_docs/python/tf/sparse_tensor_dense_matmul"" rel=""nofollow noreferrer"">tf.sparse_tensor_dense_matmul</a> exist, so in which cases each of them should be used?)</p>

<p>Also do I need to convert my sparse matrix to <a href=""https://www.tensorflow.org/api_docs/python/tf/SparseTensor"" rel=""nofollow noreferrer"">tf.SparseTensor</a> ? Also it not obvious what <a href=""https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor_or_sparse_tensor"" rel=""nofollow noreferrer"">tf.convert_to_tensor_or_sparse_tensor</a> is doing and how to convert dense numpy matrix or scipy sparse matrix for tensorflow suitable input.</p>

<p>Here what I have tried(timing are for CPU):</p>

<pre><code>import numpy as np
import tensorflow as tf

np.random.seed(2018)

# Parameters
n = 10*1000
m = 4*1000
p = 0.1

%%time

# Data preparation
dense_vector = np.random.rand(1,n).astype(np.float32)
print('dense_vector.shape', dense_vector.shape)
#print('dense_vector:')
#print(dense_vector)

dense_matrix = np.random.rand(n*m)
idx = np.random.choice(range(n*m), int((1.0-p)*n*m), replace=False)
dense_matrix[idx] = 0.0
dense_matrix = dense_matrix.reshape(n,m).astype(np.float32)
print('dense_matrix.shape', dense_matrix.shape)
#print('dense_matrix:')
#print(dense_matrix)

dense_vector.shape (1, 10000)
dense_matrix.shape (10000, 4000)
CPU times: user 9.8 s, sys: 2.38 s, total: 12.2 s
Wall time: 12.2 s

%%time

# Dense vector on dense matrix multiplication using numpy

res = dense_vector @ dense_matrix
print('res.shape', res.shape)
#print('res:')
#print(res)

%%time

# Dense vector on dense matrix multiplication using tensorflow tf.matmul V1

dense_vector_tf = tf.convert_to_tensor(dense_vector, np.float32)
dense_matrix_tf = tf.convert_to_tensor(dense_matrix, np.float32)
res_tf = tf.matmul(dense_vector_tf, dense_matrix_tf)

with tf.Session() as sess:
    res = sess.run(res_tf)
    print('res.shape', res.shape)
    #print('res:')
    #print(res)

res.shape (1, 4000)
CPU times: user 1.88 s, sys: 1.82 s, total: 3.7 s
Wall time: 3.54 s

%%time

# Dense vector on dense matrix multiplication using tensorflow tf.matmul V2

dense_vector_tf = tf.convert_to_tensor(dense_vector, np.float32)
dense_matrix_tf = tf.convert_to_tensor(dense_matrix, np.float32)
res_tf = tf.matmul(dense_vector_tf, dense_matrix_tf,
                   a_is_sparse=False,
                   b_is_sparse=True)

with tf.Session() as sess:
    res = sess.run(res_tf)
    print('res.shape', res.shape)
    #print('res:')
    #print(res)

res.shape (1, 4000)
CPU times: user 4.91 s, sys: 4.28 s, total: 9.19 s
Wall time: 9.07 s

%%time

# Dense vector on sparse matrix multiplication using tensorflow tf.sparse_matmul V1

dense_vector_tf = tf.convert_to_tensor(dense_vector, np.float32)
dense_matrix_tf = tf.convert_to_tensor(dense_matrix, np.float32)
res_tf = tf.sparse_matmul(dense_vector_tf, dense_matrix_tf,
                         a_is_sparse=False,
                         b_is_sparse=True)

with tf.Session() as sess:
    res = sess.run(res_tf)
    print('res.shape', res.shape)
    #print('res:')
    #print(res)

res.shape (1, 4000)
CPU times: user 4.82 s, sys: 4.18 s, total: 8.99 s
Wall time: 9 s

%%time

# Dense vector on sparse matrix multiplication using tensorflow tf.sparse_matmul V2

dense_vector_tf = tf.convert_to_tensor(dense_vector, np.float32)
dense_matrix_tf = tf.convert_to_tensor_or_sparse_tensor(dense_matrix, np.float32)
res_tf = tf.sparse_matmul(dense_vector_tf, dense_matrix_tf,
                         a_is_sparse=False,
                         b_is_sparse=True)

with tf.Session() as sess:
    res = sess.run(res_tf)
    print('res.shape', res.shape)
    #print('res:')
    #print(res)

res.shape (1, 4000)
CPU times: user 5.07 s, sys: 4.53 s, total: 9.6 s
Wall time: 9.61 s
</code></pre>

<p>Also I can't see any improvement using sparse matrices, what I'm doing wrong?</p>
",1
52254253,How does tf.layers.dense() interact with inputs of higher dim?,"<p>In tensorflow layers.dense(inputs, units, activation) implements a Multi-Layer Perceptron layer with arbitrary activation function. </p>

<p>Output = activation(matmul(input, weights) + bias)</p>

<p>Typically input has shape=[batch_size, input_size] and might look like this: (units = 128 and activation = tf.nn.relu are chosen arbitrarily)</p>

<pre><code>inputx = tf.placeholder(float, shape=[batch_size, input_size])
dense_layer = tf.layers.dense(inputx, 128, tf.nn.relu)
</code></pre>

<p>I have not found any documentation on what would happen, if i fed higher dimensional input, e.g. because one might have time_steps resulting in a tensor of shape=[time_step, batch_size, input_size]. What one would want here is that the layer is applied to each single input_vector for each timestep for each element of the batch. To put it a bit differently, the internal matmul of layers.dense() should simply use broadcasting in numpy style. Is the behaviour i expect here what actually happens? I.e. is: </p>

<pre><code>inputx = tf.placeholder(float, shape=[time_step, batch_size, input_size])
dense_layer = tf.layers.dense(inputx, 128, tf.nn.relu)
</code></pre>

<p>applying the dense layer to each input of size input_size for each time_step for each element in batch_size? This should then result in a tensor(in dense_layer above) of shape=[time_step, batch_size, 128]
I'm asking, as e.g. tf.matmul does not support broadcasting in the numpy style, so i'm not sure, how tensorflow handles these cases.</p>

<p>Edit: <a href=""https://stackoverflow.com/questions/46697389/reshape-3d-tensor-before-dense-layer"">This post is related, but does not finally answer my question</a></p>
",1
52306358,tf.unstack did not work with tf 1.8 CudnnGRU tensors,"<h2>Describe the problem</h2>

<p><code>tf.unstack</code> did not work as expected. It did not reduce <code>R</code> rank tensor to <code>R-1</code> rank tensor</p>

<p>the corresponding issue in tensorflow github issue list: <a href=""https://github.com/tensorflow/tensorflow/issues/22223"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/22223</a></p>

<h2>Source code / logs</h2>

<p>code:</p>

<pre><code>#! /usr/bin/env python
# -*- coding: utf-8 -*-

import sys
import tensorflow as tf
rnn_model = tf.contrib.cudnn_rnn.CudnnGRU(
        num_layers=1,
        num_units=64,
        direction='unidirectional')
rnn_model.build([3, 1, 3])
inputs=[[[1,1,1],[1,1,1],[1,1,1]]]
inputs_tensor= tf.convert_to_tensor(inputs, dtype=tf.float32)
print(tf.shape(inputs_tensor))
rnn_out, rnn_state = rnn_model(inputs_tensor)
print(""rnn_state: "", rnn_state)
rnn_layers = tf.unstack(rnn_state)
print(""rnn_layers"", rnn_layers)
</code></pre>

<p>paste the code to file <code>demo.py</code>, then run from linux command line:</p>

<pre><code>$ python3.6 demo.py
</code></pre>

<p>output:</p>

<pre><code>Tensor(""Shape:0"", shape=(3,), dtype=int32)
rnn_state:  (&lt;tf.Tensor 'cudnn_gru/CudnnRNN:1' shape=(1, ?, 64) dtype=float32&gt;,)
rnn_layers [&lt;tf.Tensor 'unstack:0' shape=(1, ?, 64) dtype=float32&gt;]
</code></pre>

<p>the <code>rnn_layers</code> should be <code>rnn_layers [&lt;tf.Tensor 'unstack:0' shape=(?, 64) dtype=float32&gt;]</code></p>

<h2>Env Details</h2>

<h3>Have I written custom code (as opposed to using a stock example script provided in TensorFlow):</h3>

<p>Yes</p>

<h3>OS Platform and Distribution (e.g., Linux Ubuntu 16.04):</h3>

<pre><code>$uname -r
3.10.0-327.el7.x86_64
</code></pre>

<h3>Mobile device:</h3>

<p>Not mobile</p>

<h3>TensorFlow installed from (source or binary):</h3>

<p>anaconda tf 1.8</p>

<h3>TensorFlow version (use command below):</h3>

<pre><code>$conda list|grep tensor
tensorboard               1.8.0            py36hf484d3e_0
tensorflow                1.8.0                hb381393_0
tensorflow-base           1.8.0            py36h4df133c_0
tensorflow-gpu            1.8.0                h7b35bdc_0
</code></pre>

<h3>Python version:</h3>

<pre><code>$python3.6 -V
Python 3.6.2 :: Continuum Analytics, Inc.
</code></pre>

<h3>Bazel version (if compiling from source):</h3>

<pre><code>$bazel version
Build label: 0.4.5
Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Thu Mar 16 12:19:38 2017 (1489666778)
Build timestamp: 1489666778
Build timestamp as int: 1489666778
</code></pre>

<h3>CUDA/cuDNN version:</h3>

<pre><code>$conda list|grep -i cuda
cudatoolkit               8.0                           3    https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free
cudnn                     7.0.5                 cuda8.0_0
</code></pre>

<h3>GPU model and memory:</h3>

<pre><code>== cat /etc/issue ===============================================
Linux rvab01298.sqa.ztt 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7.2 (Paladin)""
VERSION_ID=""7.2""
qihoo360_BUGZILLA_PRODUCT_VERSION=7.2
qihoo360_SUPPORT_PRODUCT_VERSION=7.2

== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 4.9.2
Copyright (C) 2014 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux rvab01298.sqa.ztt 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.5.1)
tensorflow (1.8.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH :/usr/local/mpc-0.8.1/lib:/usr/local/gmp-4.3.2/lib:/usr/local/mpfr-2.4.2/lib:/gruntdata/qihoo360/cuda/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Sep 12 13:34:30 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K40m          On   | 0000:02:00.0     Off |                    0 |
| N/A   36C    P0    67W / 235W |   1161MiB / 11439MiB |     39%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla K40m          On   | 0000:03:00.0     Off |                    0 |
| N/A   35C    P0    60W / 235W |     73MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|    0     13950    C   bin/arks                                       868MiB |
|    0     27880    C   python3.6                                      288MiB |
|    1     27880    C   python3.6                                       71MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-7.5/doc/man/man7/libcudart.7
/usr/local/cuda-7.5/doc/man/man7/libcudart.so.7
/usr/local/cuda-7.5/lib64/libcudart.so.7.5.18
/usr/local/cuda-7.5/lib64/libcudart_static.a
/usr/local/cuda-7.5/lib/libcudart.so.7.5.18
/usr/local/cuda-7.5/lib/libcudart_static.a
</code></pre>
",0
52319765,Swap a TensorFlow Dataset input pipeline with a placeholder after training,"<p>I'm working with the new <code>tf.data.Dataset</code> API and I can't seem to figure out how to perform inference. Ultimately, I want to convert my model to a TensorRT graph and run it on the TX2, and all of the <a href=""https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/python_api/workflows/tf_to_tensorrt.html#Importing-the-UFF-Model-into-TensorRT-and-Building-an-Engine"" rel=""noreferrer"">examples I have found</a> assume you have a <code>tf.placeholder</code> for the input. Here is pseudocode for how I am training. The [...] is just meant to be a placeholder since I didn't actually run the code. Let's not debate the model, as it is just suppose to give an example:</p>

<pre><code>import tensorflow as tf

# Setup iterator
datain = tf.data.FixedLengthRecordDataset(datafiles, record_bytes1)
labels = tf.data.FixedLengthRecordDataset(labelfiles, record_bytes2)
dataset = tf.data.Dataset.zip((datain, labels))
dataset = dataset.prefetch(batch_size)
dataset = dataset.repeat(n_epoch)
iterator = dataset.make_initializable_iterator()

sess = tf.Session()
sess.run(iterator.initializer)
[batch_x, batch_y] = iterator.get_next()

# Define model function (let's not debate model except as relevant to question)
def model_fn(xin):
    x0 = tf.transpose(tf.reshape(xin, [...], name='input'))
    w = tf.Variable(tf.truncated_normal([...], stddev=0.1))
    x1 = tf.nn.conv2d(x0, w, strides=[...], padding='VALID')
    b = tf.Variable(tf.constant(0.0, shape=[...]))
    x2 = tf.nn.bias_add(x1, b)
    x3 = tf.nn.relu(x2, name='output')
    return x3

# Setup training environment
model = model_fn(batch_x)
loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=model, labels=batch_y))
optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)

# Train Model
while True:
    try:
        sess.run(optimizer)
    except tf.errors.OutOfRangeError:
        break

# Save model
saver = tf.train.Saver(name='saver')
saver.save(sess, 'temp/path')
</code></pre>

<p>My question is how do I get this into TensorRT without having the input be a <code>tf.placeholder</code>? All of the example I can find use a <code>tf.placeholder</code> as the input. <a href=""https://stackoverflow.com/questions/50061416/freezing-graph-in-tensorflow-when-using-tf-image-dataset/"">This example</a> suggests that I can replace the iterator with a placeholder using the <code>SavedModel</code> class, but I cannot seem to find any documentation on how to accomplish that.</p>

<p>Thanks!</p>

<p><strong>EDIT: Here is my solution thanks to the help below</strong></p>

<pre><code>from tensorflow.python.tools import optimize_for_inference_lib
import uff

# You can feed data to the IteratorGetNext node using feed_dict
input_node_name = 'iterator_scope_name/IteratorGetNext'
output_node_name = 'model_scope_name/output'

# Run inference on the trained model:
graph = tf.get_default_graph()
batch_x = graph.get_tensor_by_name(input_node_name + ':0')
networkout = graph.get_tensor_by_name(output_node_name + ':0')
testdata, testlabel = custom_data_reader_fn(data_folder)
# This will evaluate the model
label = sess.run(networkout, feed_dict={batch_x: testdata})

# Freeze model and create a UFF file:
graph_def = graph.as_graph_def() # Convert the graph to a serialized pb
frozen_graph_def = tf.graph_util.convert_variables_to_constants(sess,
    graph_def, [output_node_name])
opt_graph_def = optimize_for_inference_lib.optimize_for_inference(
    frozen_graph_def, [input_node_name], [output_node_name],
    tf.float32.as_datatype_enum)
uff.from_tensorflow(opt_graph_def, [output_node_name], quiet=False,
    output_filename='opt_model.uff')
</code></pre>

<p>that will write out a UFF file that TensorRT can utilize. The biggest issues that I encountered was:</p>

<ol>
<li>I didn't realize that the <code>optimize_for_inference_lib.optimize_for_inference</code> operation replaced the <code>iterator</code> with a <code>tf.placeholder</code></li>
<li>I did not know what node to feed data to for evaluation: you can feed data to the <code>IteratorGetNext</code> node</li>
</ol>
",1
52483259,Map values to a PNG-encoded TensorFlow tensor,"<p>I'm working on creating a data loader, similar to: <a href=""https://github.com/tensorflow/models/blob/master/research/deeplab/datasets/build_cityscapes_data.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/blob/master/research/deeplab/datasets/build_cityscapes_data.py</a></p>

<p>In this script a semantic label map containing uint8 labels, encoded as a grayscale PNG image, is read and later serialized to a tfrecord file:</p>

<pre><code>seg_data = tf.gfile.FastGFile(label_file, 'rb').read()
</code></pre>

<p>Now I want to map a new labeling scheme according to a dictionary:</p>

<pre><code>old_to_new = {""1"": 30, ""2"": 50, ""255"": 15}
</code></pre>

<p>If I had a numpy array, I could do something like:</p>

<pre><code>seg_data_converted = seg_data.copy()
for old_label in old_to_new:
    seg_data_converted[seg_data==old_label] = old_to_new[old_label]
</code></pre>

<p>or use more efficient functions for mapping the values.
Unfortunately, the output of <code>tf.gfile.FastGFile().read(n=-1)</code> returns</p>

<blockquote>
  <p>'n' bytes of the file (or whole file) requested as a string</p>
</blockquote>

<p>as documented in: <a href=""https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/gfile/FastGFile"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/gfile/FastGFile</a></p>

<p>How can I map the new values to the tensor and re-encode the tensor, such that I am back to a representation similar to the one given by <code>tf.gfile.FastGFile().read()</code>?</p>

<p>First approach (incomplete!):</p>

<ol>
<li>Decode using: <code>tf.image.decode_png()</code></li>
<li>Map values using: <code>tf.map_fn()</code></li>
<li>Re-encode using: <code>tf.image.encode_png()</code></li>
</ol>
",0
52484694,TF Eager mode: Load a complete model from disk if possible,"<p><em>Update: ""AttributeError: 'AdamOptimizer' object has no attribute 'name'"" might be an alternative title for this question. If this can be solved then the whole thing might work right.</em></p>

<p>Have a new jupyter notebook that has NO model saving code.  The trained model was already saved using another notebook like this. The iris dataset was used here and the model is all trained up:</p>

<pre><code>#This might be the way:
#https://www.tensorflow.org/guide/eager#object_based_saving
#https://stackoverflow.com/questions/47852516/tensorflow-eager-mode-how-to-restore-a-model-from-a-checkpoint
savePrefix = ""/tmp/iris""
root = tfe.Checkpoint(optimizer=optimizer, model=model, optimizer_step=global_step)
restorePrefix = root.save(savePrefix)
# '/tmp/iris-1'
print(restorePrefix)
print(root)
print(""The model was saved to {}\nRestore the model using {}"".format(savePrefix, restorePrefix))
# Try loading this in a different notebook to prove it worked.

&gt;/tmp/iris-1
&gt;&lt;tensorflow.contrib.eager.python.checkpointable_utils.Checkpoint object at 0x7fb308799e80&gt;
&gt;The model was saved to /tmp/iris
&gt;Restore the model using /tmp/iris-1
</code></pre>

<p>I grabbed the output path from the above and then tried to load the model in a new notebook using tf.contrib.eager code but it fails:</p>

<pre><code>s = tfe.Saver([optimizer, model, global_step])
s.restore(file_prefix=""/tmp/iris-1"")

&gt;NameError: name 'optimizer' is not defined
</code></pre>

<p>So what is an actually WORKING use case code to load a previously developed model with the tf.contrib.eager api (not the session api) WHEN THE CODE IN THE MODEL-LOADING NOTEBOOK DOES NOT SAVE THE MODEL AND DOES NOT HAVE THE MODEL'S PARTS IN MEMORY ALREADY like optimizer, graph definition, and global_state?</p>

<p>The TF docs always choose to demo a pointless example of loading a model we already have in memory.  I can't tell if ""With the TF.contrib.eager API you have to have explicit model creation code and optimizer creation code and global step code right in your notebook because TFE cannot load this stuff from disk"" or ""its a new API and some features are missing"", or ""you have to also use session and graph coding api along with tf.contrib.eager api"" or ""just use Microsoft cntk which actually works with imperative code and they didnt forget critical parts of the API"". </p>

<p>Thanks if you know something. If speculating, please state.</p>

<p>I suspect it's some superset and subset combination of the following if it's going to work. (Scraped from SO posts on the subject.)</p>

<pre><code>tfe.restore_variables_on_create 
tf.train.import_meta_graph() 
new_saver.restore()
tf.train.latest_checkpoint()
tfe.save_network_checkpoint()
tfe.restore_network_checkpoint()
</code></pre>

<p>Update - I added code to first manually recreate the model, optimizer, etc into memory, and then ran the restore() code. Error - optimizer does not have a name. But, oddly, it actually does have a name attribute according to the documentation:</p>

<pre><code>#OK I'll just try to use code to create the model artifacts first. 
# I'd rather load it all from disk but maybe at least it might work.

optimizer = tf.train.AdamOptimizer() #ga
global_step = tf.train.get_or_create_global_step()  # what is a global_step?
model = tf.keras.Sequential([
  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required
  tf.keras.layers.Dense(10, activation=tf.nn.relu, kernel_initializer='glorot_uniform'),
  tf.keras.layers.Dense(3)
])
s = tfe.Saver([optimizer, model, global_step])
s.restore(file_prefix=""/tmp/iris-1"")
</code></pre>

<p>INFO:tensorflow:Restoring parameters from /tmp/iris-1</p>

<hr>

<p>AttributeError: 'AdamOptimizer' object has no attribute 'name'</p>

<p>The documentation says:</p>

<pre><code>__init__(
    learning_rate=0.001,
    beta1=0.9,
    beta2=0.999,
    epsilon=1e-08,
    use_locking=False,
    name='Adam'
)
</code></pre>

<p>It sure looks like Adam optimizer has a name!  That's funny. What's the problem then?</p>

<p>Maybe the TF error is really saying that an optimizer and a model and a global_state variable cannot be restored.  So then what would be restored from a checkpoint -- specifically what variables would go in the save and corresponding restore?  Thanks if you know anything.</p>
",1
52507352,Difference between './path' and 'path' in python,"<p>Generally they work both okay for me and I rarely met the situations where they are different.</p>

<p>Today, when I try to load a customized defined module:</p>

<pre><code>&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; encoding = tf.load_op_library('./ops/encoding.so')
&gt;&gt;&gt; dir(encoding)
['LIB_HANDLE', 'OP_LIST', '_AggregateGradOutput', '_InitOpDefLibrary', '_ScaleL2GradOutput', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_aggregate_grad_outputs', '_collections', '_common_shapes', '_context', '_core', '_dtypes', '_errors', '_execute', '_op_def_lib', '_op_def_library', '_op_def_pb2', '_op_def_registry', '_ops', '_pywrap_tensorflow', '_scale_l2_grad_outputs', '_six', '_tensor_shape', 'aggregate', 'aggregate_eager_fallback', 'aggregate_grad', 'aggregate_grad_eager_fallback', 'scale_l2', 'scale_l2_eager_fallback', 'scale_l2_grad', 'scale_l2_grad_eager_fallback', 'tf_export']
</code></pre>

<p>They gives out different results.</p>

<pre><code>&gt;&gt;&gt; encoding = tf.load_op_library('ops/encoding.so')
&gt;&gt;&gt; dir(encoding)
['LIB_HANDLE', 'OP_LIST', '_InitOpDefLibrary', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_collections', '_common_shapes', '_context', '_core', '_dtypes', '_errors', '_execute', '_op_def_lib', '_op_def_library', '_op_def_pb2', '_op_def_registry', '_ops', '_pywrap_tensorflow', '_six', '_tensor_shape', 'tf_export']
</code></pre>

<p>The <code>tf.load_op_library</code>:</p>

<pre><code>Args:

    library_filename: Path to the plugin. Relative or absolute filesystem path to a dynamic library file.

Returns:

    A python module containing the Python wrappers for Ops defined in the plugin.
</code></pre>

<p>Since it is hard to search related questions by using keywords <code>./</code> and <code>python</code>, I am wondering whether you could give me some advice.</p>

<p>Thank you!</p>
",0
52533156,Weight Initialization Tensorflow tf.estimator,"<p>Is there a way to adjust the weight initialization in the pre-built tf.estimator?
I would like to use the method after Xavier (<code>tf.contrib.layers.xavier_initializer</code>) or from He. Which method is used by default? I couldn't figure it out from the documentation. </p>

<p>I use the DNNRegressor.</p>
",1
52555024,Tensorflow Eager Execution GPU count_nonzero NotFoundError,"<p>According to an answer in this thread (<a href=""https://stackoverflow.com/questions/49550749/notfounderror-on-opkernel-when-using-tf-nn-embedding-lookup-in-tensorflow-eager"">NotFoundError on OpKernel when using tf.nn.embedding_lookup in tensorflow eager mode</a>) some ops are not implemented on GPU yet.</p>

<p>I have a problem with an op, where I also get an <strong>NotFoundError</strong>, but the error-message confuses me. Here my sample-code with Tensorflow 1.10. I know that I can ommit the device-forcing and tensorflow will run the operation on CPU, but I would like to do as much on GPU as possible.</p>

<pre><code>import tensorflow as tf

tf.enable_eager_execution()
print(""Eager execution: {}"".format(tf.executing_eagerly()))

device = 'gpu:0'
with tf.device(device):

    x = tf.constant([195330., 195075., 173910., 167535., 167535., 170340., 206040., 175185., 206040.,
                     118575., 214710., 171870., 204765., 202215.,      0.,      0.,      0.,      0.,
                     0.,      0.], dtype=tf.float32)

    print(tf.count_nonzero(x))
</code></pre>

<p>I get the following error:</p>

<pre><code>python3 test.py 
Eager execution: True
2018-09-28 14:41:51.186066: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: FMA
2018-09-28 14:41:51.370081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: 
name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:01:00.0
totalMemory: 5.93GiB freeMemory: 5.38GiB
2018-09-28 14:41:51.467475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 1 with properties: 
name: GeForce GT 730 major: 3 minor: 5 memoryClockRate(GHz): 0.9015
pciBusID: 0000:02:00.0
totalMemory: 1.96GiB freeMemory: 1.93GiB
2018-09-28 14:41:51.467534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1469] Ignoring visible gpu device (device: 1, name: GeForce GT 730, pci bus id: 0000:02:00.0, compute capability: 3.5) with Cuda multiprocessor count: 2. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
2018-09-28 14:41:51.467543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
2018-09-28 14:41:51.848119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-28 14:41:51.848172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 1 
2018-09-28 14:41:51.848195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N N 
2018-09-28 14:41:51.848206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 1:   N N 
2018-09-28 14:41:51.848446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5143 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""test.py"", line 13, in &lt;module&gt;
    print(tf.count_nonzero(x))
  File ""/home/joe/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/home/joe/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 1384, in count_nonzero
    reduction_indices=reduction_indices),
  File ""/home/joe/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
    return func(*args, **kwargs)
  File ""/home/joe/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 1307, in reduce_sum
    name=name))
  File ""/home/joe/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 8283, in _sum
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""&lt;string&gt;"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'Sum' OpKernel for GPU devices compatible with node Sum = Sum[T=DT_INT64, Tidx=DT_INT32, keep_dims=false](dummy_input, dummy_input)
     (OpKernel was found, but attributes didn't match)
    .  Registered:  device='CPU'; T in [DT_COMPLEX128]; Tidx in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX128]; Tidx in [DT_INT32]
  device='CPU'; T in [DT_COMPLEX64]; Tidx in [DT_INT64]
  device='CPU'; T in [DT_COMPLEX64]; Tidx in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; Tidx in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; Tidx in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; Tidx in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; Tidx in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; Tidx in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; Tidx in [DT_INT32]
  device='CPU'; T in [DT_HALF]; Tidx in [DT_INT64]
  device='CPU'; T in [DT_HALF]; Tidx in [DT_INT32]
  device='CPU'; T in [DT_INT8]; Tidx in [DT_INT64]
  device='CPU'; T in [DT_INT8]; Tidx in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; Tidx in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; Tidx in [DT_INT32]
  device='CPU'; T in [DT_INT16]; Tidx in [DT_INT64]
  device='CPU'; T in [DT_INT16]; Tidx in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; Tidx in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; Tidx in [DT_INT32]
  device='CPU'; T in [DT_INT32]; Tidx in [DT_INT64]
  device='CPU'; T in [DT_INT32]; Tidx in [DT_INT32]
  device='CPU'; T in [DT_INT64]; Tidx in [DT_INT64]
  device='CPU'; T in [DT_INT64]; Tidx in [DT_INT32]
  device='GPU'; T in [DT_INT32]; Tidx in [DT_INT64]
  device='GPU'; T in [DT_INT32]; Tidx in [DT_INT32]
  device='GPU'; T in [DT_COMPLEX128]; Tidx in [DT_INT64]
  device='GPU'; T in [DT_COMPLEX128]; Tidx in [DT_INT32]
  device='GPU'; T in [DT_COMPLEX64]; Tidx in [DT_INT64]
  device='GPU'; T in [DT_COMPLEX64]; Tidx in [DT_INT32]
  device='GPU'; T in [DT_DOUBLE]; Tidx in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]; Tidx in [DT_INT32]
  device='GPU'; T in [DT_FLOAT]; Tidx in [DT_INT64]
  device='GPU'; T in [DT_FLOAT]; Tidx in [DT_INT32]
  device='GPU'; T in [DT_HALF]; Tidx in [DT_INT64]
  device='GPU'; T in [DT_HALF]; Tidx in [DT_INT32]
 [Op:Sum]
</code></pre>

<p>As far as I understand the error </p>

<pre><code>No registered 'Sum' OpKernel for GPU devices compatible with node Sum = Sum[T=DT_INT64, Tidx=DT_INT32, keep_dims=false](dummy_input, dummy_input)
</code></pre>

<p>it looks for an implementation for <strong>T=DT_INT64, Tidx=DT_INT32</strong>, but the Tensor is from type <strong>float32</strong>. Do I miss something?</p>
",0
52561224,Tensorflow Dataset API memory error when trying to input large dataframe,"<p>I have pandas dataframe of 350K rows and 200 columns. When trying to construct input pipeline using dataset api I get memory error. When I'm inputting only, say, 10K rows everything works fine, but with all rows it does not. Also when using <code>tf.estimator.inputs.pandas_input_fn</code> everything is ok. </p>

<p>Here's the code</p>

<pre><code>x_train, x_test, y_train, y_test = train_test_split(train, labels, test_size=0.25)

feature_columns = [tf.feature_column.numeric_column(c) for c in train.columns
                if train[c].dtype != 'object']

def train_input_fn():
    dataset = tf.data.Dataset.from_tensor_slices((dict(x_train), y_train))
    dataset = dataset.shuffle(1000)
    dataset = dataset.batch(100)
    iterator = dataset.make_one_shot_iterator()
    return iterator.get_next()

model = tf.estimator.DNNClassifier(feature_columns=feature_columns, hidden_units=[20, 2]
model.train(input_fn=train_input_fn, steps=1000)
</code></pre>

<p>And the error message</p>

<pre><code>INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
2018-09-28 14:42:03.736495: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2018-09-28 14:42:04.070692: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.797
pciBusID: 0000:01:00.0
totalMemory: 8.00GiB freeMemory: 6.63GiB
2018-09-28 14:42:04.072060: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1435] Adding visible gpu devices: 0
2018-09-28 14:42:05.139979: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-09-28 14:42:05.140271: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:929]      0 
2018-09-28 14:42:05.140461: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:942] 0:   N 
2018-09-28 14:42:05.141143: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6401 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
Traceback (most recent call last):
  File ""C:/Users/.../test.py"", line 150, in &lt;module&gt;
    nn.train(input_fn=train_input_fn, steps=10)
  ...
  File ""C:\Users\...\google\protobuf\text_format.py"", line 118, in getvalue
    return self._writer.getvalue()
MemoryError
</code></pre>

<p>I tried to set different batch size, net architecture but the error persists. </p>
",0
52572275,tensorflow: how to interleave columns of two tensors (e.g. using tf.scatter_nd)?,"<p>I've read the <a href=""https://www.tensorflow.org/api_docs/python/tf/manip/scatter_nd"" rel=""nofollow noreferrer"">tf.scatter_nd documentation</a> and run the example code for 1D and 3D tensors... and now I'm trying to do it for a 2D tensor.  I want to 'interleave' the columns of two tensors.  For 1D tensors, one can do this via</p>

<pre><code>'''
We want to interleave elements of 1D tensors arr1 and arr2, where
arr1 = [10, 11, 12]
arr2 = [1, 2, 3, 4, 5, 6]
such that
desired result = [1, 2, 10, 3, 4, 11, 5, 6, 12]
'''

import tensorflow as tf

with tf.Session() as sess:

    updates1 = tf.constant([1,2,3,4,5,6])
    indices1 = tf.constant([[0], [1], [3], [4], [6], [7]])
    shape = tf.constant([9])
    scatter1 = tf.scatter_nd(indices1, updates1, shape)

    updates2 = tf.constant([10,11,12])
    indices2 = tf.constant([[2], [5], [8]])
    scatter2 = tf.scatter_nd(indices2, updates2, shape)

    result = scatter1 + scatter2

    print(sess.run(result))
</code></pre>

<p>(aside: is there a <em>better</em> way to do this?  I'm all ears.)</p>

<p>This gives the output</p>

<p><code>[ 1  2 10  3  4 11  5  6 12]</code></p>

<p>Yay! that worked!</p>

<p>Now lets' try to extend this to 2D.</p>

<pre><code>    '''
    We want to interleave the *columns* (not rows; rows would be easy!) of

    arr1 = [[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]]
    arr2 = [[10 11 12], [10 11 12], [10 11 12]]
    such that
    desired result = [[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12],[1,2,10,3,4,11,5,6,12]]
    '''

    updates1 = tf.constant([[1,2,3,4,5,6],[1,2,3,4,5,6],[1,2,3,4,5,6]])
    indices1 = tf.constant([[0], [1], [3], [4], [6], [7]])
    shape = tf.constant([3, 9])
    scatter1 = tf.scatter_nd(indices1, updates1, shape)
</code></pre>

<p>This gives the error
<code>ValueError: The outer 1 dimensions of indices.shape=[6,1] must match the outer 1
dimensions of updates.shape=[3,6]: Dimension 0 in both shapes must be equal, but
are 6 and 3. Shapes are [6] and [3]. for 'ScatterNd_2' (op: 'ScatterNd') with
input shapes: [6,1], [3,6], [2].</code></p>

<p>Seems like my <code>indices</code> is specifying row indices instead of column indices, and given the way that arrays are ""connected"" in numpy and tensorflow (i.e. row-major order), does that mean
I need to <em>explicitly</em> specify every single pair of indices for every element in <code>updates1</code>?
Or is there some kind of 'wildcard' specification I can use for the rows? (Note <code>indices1 = tf.constant([[:,0], [:,1], [:,3], [:,4], [:,6], [:,7]])</code> gives syntax errors, as it probably should.)</p>

<p>Would it be easier to just do a transpose, interleave the rows, then transpose back?
Because I tried that...</p>

<pre><code>scatter1 = tf.scatter_nd(indices1, tf.transpose(updates1), tf.transpose(shape))
print(sess.run(tf.transpose(scatter1)))
</code></pre>

<p>...and got a <em>much</em> longer error message, that I don't feel like posting unless someone requests it. </p>

<p>PS- I searched to make sure this isn't a duplicate -- I find it hard to imagine that someone else hasn't asked this before -- but turned up nothing. </p>
",1
52671481,Why are variables defined with 'self' automatically given a ListWrapper() while inheriting from tf.keras.Model?,"<p>I am not familiar with <code>ListWrapper()</code>, but it is being applied to all list variables created with <code>self</code> when my class inherits from <code>tf.keras.Model</code>. 
<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/models/Model"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/models/Model</a></p>

<p>This is bad because it is causing an <code>IndexError</code> when I use it in certain functions, or even by just passing it through my Tensorflow model. (I am using eager execution)</p>

<p>A small reproduction of the problem can be seen with this code:</p>

<pre><code>import tensorflow as tf

class my_class(tf.keras.Model):

    def __init__(self):
        super(my_class, self).__init__()

        self.x = [0]
        print(self.x)

model = my_class()
</code></pre>

<p>Output:</p>

<pre><code>ListWrapper([0])
</code></pre>

<p>Switching the inheritance to be from <code>object</code> solves the issue, which is how I know its <code>tf.keras.Model</code> that is causing this.</p>

<p>I tried looking it up but can't find anything on this. Any tips? Thanks!</p>
",0
52695286,Visualize the edge thickness in tensorboard,"<p>I want the graph visualizer to label edges with tensor dimensions, and edge thickness to reflect total tensor size. Basically exactly the same as written in <a href=""https://www.tensorflow.org/guide/graph_viz#tensor_shape_information"" rel=""nofollow noreferrer"">this doc</a>:</p>

<blockquote>
  <p>When the serialized GraphDef includes tensor shapes, the graph
  visualizer labels edges with tensor dimensions, and edge thickness
  reflects total tensor size. To include tensor shapes in the GraphDef
  pass the actual graph object (as in sess.graph) to the FileWriter when
  serializing the graph. The images below show the CIFAR-10 model with
  tensor shape information:</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/KdEeo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KdEeo.png"" alt=""enter image description here""></a></p>

<p>I pass the graph object to my summary.FileWriter:</p>

<p><code>writer = tf.summary.FileWriter(_dir_tensorboard, graph=sess.graph, flush_secs=300)</code></p>

<p>But I do not get any information about the <em>thickness</em> (all the lines are of the same size). I have just information about the shape of a tensor and information about the number of tensors.</p>

<p>How can I achive the same visual effect as the tutorial claims?</p>
",0
52706923,Error Running Tensorflow Model while Using tf.data api and customized optimizer,"<p>In the following code, I wanted to train a model in tensorflow. the model is a ResNet model, a deep one, hence the batch should be small for data/all activations to fit in memory. For this reason, I have implemented a custom optimizer that accumulates the gradients over the different fed mini-batchs, and finally apply the gradient descent once. In addition, I have used <code>tf.data</code> api to fetch data from <code>tfrecords</code> which I created. Please note that my input data are video frames; and the detected variable used indicated whether a face is detected in a certain frame or not. Hence, detected is used only for <code>MSE</code> (Just for clarification).</p>

<pre><code>import tensorflow as tf
import numpy as np
import csv
import os

num_epoch = 100
latent_dim = 100
cell_size = 100

# for each input frame, I have 3 outputs.
num_classes = 3

common = ""C:/Users/user/Documents/SEWA_db/tfrecords_db/""

filenames_train = []
filenames_dev = []

for i in range(1, 35):
    filenames_train.append(common + ""Train_DE_{num:02d}.tfrecords"".format(num=i))

for i in range(1, 15):
    filenames_dev.append(common + ""Devel_DE_{num:02d}.tfrecords"".format(num=i))

phase_train = tf.placeholder_with_default(True, shape=(), name='phase')

train_batch_size = 5
test_batch_size = 5

tf.set_random_seed(123)
mseed = 123

# this method is used within the model()...
def create_variables(name, shape, initializer=tf.contrib.layers.xavier_initializer(), weight_decay=0.0001):
    '''
    :param name: A string. The name of the new variable
    :param shape: A list of dimensions
    :param initializer: User Xavier as default.
    :param is_fc_layer: Want to create fc layer variable? May use different weight_decay for fc
    layers.
    :return: The created variable
    '''

    ## TODO: to allow different weight decay to fully connected layer and conv layer
    regularizer = tf.contrib.layers.l2_regularizer(scale=weight_decay)

    new_variables = tf.get_variable(name, shape=shape, initializer=initializer,
                                    regularizer=regularizer)
    return new_variables

def model(inputs, n):
    ....
    # predictions shape: (batch_size, 3)
    return predictions

# loss function:
summaries_while_testing = []
summaries_while_training = []

def loss(predictions, labels, detected, name_scope, train_test):
    # MSE
    with tf.name_scope(name_scope):

        MSE = tf.square(tf.subtract(predictions, labels))
        MSE = tf.boolean_mask(MSE, detected)
        MSE = tf.reduce_mean(MSE)

        if train_test == 'Train':
            reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
            MSE += tf.reduce_sum(reg_losses)
            loss_s = tf.summary.scalar('MSE', MSE)
            summaries_while_training.append(loss_s)
        else:
            loss_s = tf.summary.scalar('MSE', MSE)
            summaries_while_testing.append(loss_s)

    return MSE

# optimizer:
def optimize(mse):
     with tf.name_scope('Optimizer'):
        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)

        with tf.control_dependencies(update_ops):
            optimizer = tf.train.AdamOptimizer(learning_rate=0.001)

            trainable_variables = tf.trainable_variables()

            accum_vars = [tf.Variable(tf.zeros_like(single_tr_variable.value()), trainable=False)
                          for single_tr_variable in trainable_variables]

            # This is used as a rest mode between different training iterations...
            zero_ops = [tv.assign(tf.zeros_like(tv)) for tv in accum_vars]

            grads_vars = optimizer.compute_gradients(mse, trainable_variables)

            accum_ops = [accum_vars[i].assign_add(gv[0]) for i, gv in enumerate(grads_vars) if gv[0] is not None]
            train_step = optimizer.apply_gradients([(accum_vars[i], gv[1]) for i, gv in enumerate(grads_vars)])

            return train_step, accum_ops, zero_ops

# retrieve data section
def _parse_function(example_proto):

    # The annotation contains the following features: timestamp; arousal; valence; liking
    features = {
        'height': tf.FixedLenFeature([], tf.int64),
        'width': tf.FixedLenFeature([], tf.int64),
        'image_raw': tf.FixedLenFeature([], tf.string),
        'frame_number': tf.FixedLenFeature([1], tf.int64),
        'detected': tf.FixedLenFeature([1], tf.int64),
        'arousal': tf.FixedLenFeature([1], tf.float32),
        'valence': tf.FixedLenFeature([1], tf.float32),
        'liking': tf.FixedLenFeature([1], tf.float32)
    }

    parsed_features = tf.parse_single_example(example_proto, features)

    # This is how we create one example, that is, extract one example from the database.
    image = tf.decode_raw(parsed_features['image_raw'], tf.uint8)
    # The height and the weights are used to
    height = tf.cast(parsed_features['height'], tf.int32)
    width = tf.cast(parsed_features['width'], tf.int32)

    # The image is reshaped since when stored as a binary format, it is flattened. Therefore, we need the
    # height and the weight to restore the original image back.
    # Tensor(""Reshape:0"", shape=(112, 112, 3), dtype=uint8)
    image = tf.reshape(image, [112, 112, 3])

    detected = parsed_features['detected']
    arousal = parsed_features['arousal']
    valence = parsed_features['valence']
    liking = parsed_features['liking']

    return detected, arousal, valence, liking, image

###############################      TRAINING      ###################################

datasets_train_iterators = []

for file_name in filenames_train:
    dataset_train = tf.data.TFRecordDataset(file_name).map(_parse_function).batch(train_batch_size)
    datasets_train_iterators.append(dataset_train)

dataset_train_all = tf.data.Dataset.zip(tuple(datasets_train_iterators))
iterator_train_all = dataset_train_all.make_initializable_iterator()

def retrieve_inputs_train():

    next_batch = iterator_train_all.get_next()

    detected = []
    arousal = []
    valence = []
    liking = []
    images = []

    for n in next_batch:
        detected.append(n[0])
        arousal.append(n[1])
        valence.append(n[2])
        liking.append(n[3])
        images.append(n[4])

    detected = tf.concat(detected, axis=0)
    arousal = tf.concat(arousal, axis=0)
    valence = tf.concat(valence, axis=0)
    liking = tf.concat(liking, axis=0)
    images = tf.concat(images, axis=0)

    return detected, arousal, valence, liking, images

###############################      TESTING      ###################################
datasets_dev_iterators = []

for file_name in filenames_dev:
    dataset_dev = tf.data.TFRecordDataset(file_name).map(_parse_function).batch(test_batch_size)
    datasets_dev_iterators.append(dataset_dev)

dataset_dev_all = tf.data.Dataset.zip(tuple(datasets_dev_iterators))
iterator_dev_all = dataset_dev_all.make_initializable_iterator()

def retrieve_inputs_dev():

    next_batch = iterator_dev_all.get_next()

    detected = []
    arousal = []
    valence = []
    liking = []
    images = []

    for n in next_batch:
        detected.append(n[0])
        arousal.append(n[1])
        valence.append(n[2])
        liking.append(n[3])
        images.append(n[4])

    detected = tf.concat(detected, axis=0)
    arousal = tf.concat(arousal, axis=0)
    valence = tf.concat(valence, axis=0)
    liking = tf.concat(liking, axis=0)
    images = tf.concat(images, axis=0)

    return detected, arousal, valence, liking, images

# preparing model before training
detected, arousal, valence, liking, images = tf.cond(phase_train,
                                                     lambda: retrieve_inputs_train(),
                                                     lambda: retrieve_inputs_dev())

images_casted = tf.cast(images, tf.float32)
with tf.name_scope('image_normal'):
    images_casted_normalized = tf.map_fn(lambda img: tf.image.per_image_standardization(img), images_casted)

# shape of predictions: (680, 3) -&gt; 3 since we are outputing arousal, valence and liking
# the n parameter is for Resnet configuration... Not important for now
predictions = model(images_casted_normalized, n=[3, 4, 6, 3])

predicted_arousal = tf.slice(predictions, begin=[0, 0], size=[-1, 1], name='predicted_arousal')
predicted_valence = tf.slice(predictions, begin=[0, 1], size=[-1, 1], name='predicted_valence')
predicted_liking = tf.slice(predictions, begin=[0, 2], size=[-1, 1], name='predicted_liking')

MSE_a = tf.cond(phase_train, 
                lambda: loss(predicted_arousal, arousal, detected, 'MSE_arousal_Train', 'Train'),
                lambda: loss(predicted_arousal, arousal, detected, 'MSE_arousal_Devel', 'Devel'))

MSE_v = tf.cond(phase_train, 
                lambda: loss(predicted_valence, valence, detected, 'MSE_valence_Train', 'Train'),
                lambda: loss(predicted_valence, valence, detected, 'MSE_valence_Devel', 'Devel'))

MSE_l = tf.cond(phase_train,
                lambda: loss(predicted_liking, liking, detected, 'MSE_liking_Train', 'Train'),
                lambda: loss(predicted_liking, liking, detected, 'MSE_liking_Devel', 'Devel'))

MSE = MSE_a + MSE_v + MSE_l

train_step, accum_ops, zero_ops = optimize(MSE)

init_op = tf.global_variables_initializer()

model_path = ""C:/Users/user/Documents/f24/model""
events_path = ""C:/Users/user/Documents/f24/event_files/34_layers""
with tf.Session() as sess:

    sess.run(init_op)

    train_writer = tf.summary.FileWriter(events_path, sess.graph)

    merged_train = tf.summary.merge(summaries_while_training)
    merged_val = tf.summary.merge(summaries_while_testing)

    sess.run(iterator_train_all.initializer)
    sess.run(iterator_dev_all.initializer)
</code></pre>

<p>Finally, I am getting the following error:</p>

<pre><code>FailedPreconditionError: Attempting to use uninitialized value conv3_1/conv2_in_block/conv
     [[Node: conv3_1/conv2_in_block/conv/read = Identity[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](conv3_1/conv2_in_block/conv)]]

During handling of the above exception, another exception occurred:

FailedPreconditionError                   Traceback (most recent call last)
&lt;ipython-input-11-dbe6d12c67ce&gt; in &lt;module&gt;()
      7 
      8     for v in accum_vars:
----&gt; 9         sess.run(v.initializer)
     10 
     11     sess.run(init_op)
</code></pre>

<p>...</p>

<pre><code>File ""&lt;ipython-input-10-8d7d7b4aa814&gt;"", line 10, in &lt;module&gt;
    predictions = model(images_casted_normalized, n=[3, 4, 6, 3])
  File ""&lt;ipython-input-5-fae307f9536f&gt;"", line 25, in model
    conv3 = residual_block(layers[-1], 256, is_training=phase_train)
  File ""&lt;ipython-input-4-d8a2d1403f18&gt;"", line 97, in residual_block
    conv2 = bn_relu_conv_layer(conv1, [3, 3, output_channel, output_channel], 1, is_training=is_training)
  File ""&lt;ipython-input-4-d8a2d1403f18&gt;"", line 61, in bn_relu_conv_layer
    filter = create_variables(name='conv', shape=filter_shape)
  File ""&lt;ipython-input-4-d8a2d1403f18&gt;"", line 15, in create_variables
    regularizer=regularizer)
  File ""C:\Users\user\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 1317, in get_variable
    constraint=constraint)
</code></pre>

<p>Now when I remove these 2 lines in the <code>optimize()</code>, my code works fine, but I know that this is wrong.</p>

<pre><code>update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)

    with tf.control_dependencies(update_ops):
</code></pre>

<p>Or, if I use the following code for the optimizer, my code runs fine.</p>

<pre><code>def optimize(mse):
     with tf.name_scope('Optimizer'):
        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)

        with tf.control_dependencies(update_ops):
            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
            train_step = optimizer.minimize(mse)
            return train_step
</code></pre>

<p>To me, this is weird and strange. I would love to know the reason for the error I am getting.</p>

<p>Any help is much appreciated!!</p>
",0
52711895,How to run define Tensorflow graph were all variables are in float16 instead instead of float32,"<p>By default, the variables Tensorflow is in float32. To save memory, I'm trying to run in float16. In my graph, every place where I could define the datatype as float16, I did. However, I get an error when I run the code </p>

<p>Here's my code below. </p>

<pre><code>import math
import numpy as np
import tensorflow as tf

vocabulary_size = 10
batch_size = 64 
embedding_size = 100 
num_inputs =4
num_sampled = 128 

graph = tf.Graph()

with graph.as_default(): #took out "" , tf.device('/cpu:0')""


    train_dataset = tf.placeholder(tf.int32, shape=[batch_size, num_inputs ])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])

    embeddings = tf.get_variable( 'embeddings', dtype=tf.float16,
        initializer= tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0, dtype=tf.float16) )

    softmax_weights = tf.get_variable( 'softmax_weights', dtype=tf.float16,
        initializer= tf.truncated_normal([vocabulary_size, embedding_size],
                             stddev=1.0 / math.sqrt(embedding_size), dtype=tf.float16 ) )

    softmax_biases = tf.get_variable('softmax_biases', dtype=tf.float16,
        initializer= tf.zeros([vocabulary_size], dtype=tf.float16),  trainable=False )

    embed = tf.nn.embedding_lookup(embeddings, train_dataset) #train data set is

    embed_reshaped = tf.reshape( embed, [batch_size*num_inputs, embedding_size] )

    segments= np.arange(batch_size).repeat(num_inputs)

    averaged_embeds = tf.segment_mean(embed_reshaped, segments, name=None)

    sam_sof_los = tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds,
                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size)

    loss = tf.reduce_mean( sam_sof_los )

    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss) 

    saver = tf.train.Saver()
</code></pre>

<p>And this is this is the error message</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    509                 as_ref=input_arg.is_ref,
--&gt; 510                 preferred_dtype=default_dtype)
    511           except TypeError as err:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)
   1143     if ret is None:
-&gt; 1144       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1145 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)
    980         ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r"" %
--&gt; 981         (dtype.name, t.dtype.name, str(t)))
    982   return t

ValueError: Tensor conversion requested dtype float16 for Tensor with dtype float32: 'Tensor(""sampled_softmax_loss/Log:0"", shape=(64, 1), dtype=float32)'

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
&lt;ipython-input-2-12d508b9e5d7&gt; in &lt;module&gt;()
     46 
     47     sam_sof_los = tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds,
---&gt; 48                                    labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size)
     49 
     50     loss = tf.reduce_mean( sam_sof_los )

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py in sampled_softmax_loss(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, remove_accidental_hits, partition_strategy, name, seed)
   1347       partition_strategy=partition_strategy,
   1348       name=name,
-&gt; 1349       seed=seed)
   1350   labels = array_ops.stop_gradient(labels, name=""labels_stop_gradient"")
   1351   sampled_losses = nn_ops.softmax_cross_entropy_with_logits_v2(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py in _compute_sampled_logits(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, subtract_log_q, remove_accidental_hits, partition_strategy, name, seed)
   1126     if subtract_log_q:
   1127       # Subtract log of Q(l), prior probability that l appears in sampled.
-&gt; 1128       true_logits -= math_ops.log(true_expected_count)
   1129       sampled_logits -= math_ops.log(sampled_expected_count)
   1130 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)
    860     with ops.name_scope(None, op_name, [x, y]) as name:
    861       if isinstance(x, ops.Tensor) and isinstance(y, ops.Tensor):
--&gt; 862         return func(x, y, name=name)
    863       elif not isinstance(y, sparse_tensor.SparseTensor):
    864         try:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in sub(x, y, name)
   8316   if _ctx is None or not _ctx._eager_context.is_eager:
   8317     _, _, _op = _op_def_lib._apply_op_helper(
-&gt; 8318         ""Sub"", x=x, y=y, name=name)
   8319     _result = _op.outputs[:]
   8320     _inputs_flat = _op.inputs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    544                   ""%s type %s of argument '%s'."" %
    545                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,
--&gt; 546                    inferred_from[input_arg.type_attr]))
    547 
    548           types = [values.dtype]

TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type float16 of argument 'x'.
</code></pre>

<p>The error comes from line <code>tf.nn.sampled_softmax_loss</code>.</p>

<p>At first I thought perhaps tf.segment_mean may cast the output as a float32, so I tried casting averaged_embeds to float16 but I still get the same error. </p>

<p>From the documentation, there doesn't seem to be a way to define any data types in sampled_softmax_loss</p>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss</a></p>
",1
52731151,Tesnorflow: How to provide your own `sampled_values` for tf.nn.sampled_softmax_loss?,"<p>In tf.nn.sampled_softmax_loss, one of the optional inputs is to put your own samples values. I would like to provide my own samples values so that I can use float16 (half precision) variables. If <code>sampled_values</code> is left blank, Tensorflow will use <code>log_uniform_candidate_sampler</code> to get values, which can only return float32. </p>

<p>Here are all the inputs. </p>

<pre><code>tf.nn.sampled_softmax_loss(
    weights,
    biases,
    labels,
    inputs,
    num_sampled,
    num_classes,
    num_true=1,
    sampled_values=None,
    remove_accidental_hits=True,
    partition_strategy='mod',
    name='sampled_softmax_loss',
    seed=None
)
</code></pre>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss</a></p>

<p>This is the information they give for the sampled_values arg :</p>

<blockquote>
  <p>sampled_values: a tuple of (sampled_candidates, true_expected_count,
  sampled_expected_count) returned by a *_candidate_sampler function.
  (if None, we default to log_uniform_candidate_sampler)</p>
</blockquote>

<p>I'm trying to figure out how to provide this tuple. What exactly are the <code>sampled_candidates</code>, <code>true_expected_count</code>, <code>sampled_expected_count</code> ? </p>

<p>I know that it's sampling the weights and corresponding biases, so do I put them together in it's own tuple for <code>sampled_candidates</code> ? Also, am I putting the int for the place of the weight in the matrix, or am I putting the whole embedding itself?</p>

<p>I've also looked at Tensorflow's math supplimental on negative sampling but I couldn't find any information for my issue <a href=""https://www.tensorflow.org/extras/candidate_sampling.pdf"" rel=""noreferrer"">https://www.tensorflow.org/extras/candidate_sampling.pdf</a></p>

<p>In my search, I found this very similar question on a google forum</p>

<p><a href=""https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/6IDJ-XAIb9M"" rel=""noreferrer"">https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/6IDJ-XAIb9M</a></p>

<p>The Answer given is </p>

<blockquote>
  <p><code>sampled_values</code> is the tuple returned by our *candidate_sampler
  classes. These classes implement methods that sample contrastive
  labels (not observed, but used during training) according to some
  distribution Q for use in approximate training methods like
  noise-contrastive estimation (NCE) and Sampled Softmax. An example is
  the log_uniform_candidate_sampler, which samples labels according to
  the log-uniform distribution. </p>
  
  <p>You almost never need to provide these yourself. You would simply pass
  in the result of a call to a *candidate_sampler function in the tf.nn
  module (where * can be ""uniform"", ""log_uniform"", ""zipfian_binned"",
  etc), e.g.</p>
  
  <p>sampled_values = tf.nn.zipfian_binned_candidate_sampler(...)</p>
  
  <p>If you just want to get it to work, just leave it to None, and it
  would default to the log_uniform_candidate_sampler (often a good
  choice).</p>
  
  <p>If you are interested in the math behind this, see this document:
  <a href=""https://www.tensorflow.org/versions/r0.8/extras/candidate_sampling.pdf"" rel=""noreferrer"">https://www.tensorflow.org/versions/r0.8/extras/candidate_sampling.pdf</a>.</p>
  
  <p>But to answer your question: For each batch of observed labels L, and
  a candidate sampling distribution Q, the tuple consists of:</p>
  
  <ul>
  <li>the tensor with the actual sampled contrastive labels N, </li>
  <li>the tensor with the log-expected-values of the observed labels L under Q, i.e. log Q(L), and </li>
  <li>the tensor with the log-expected values of the contrastive labels under Q, i.e. log Q(N).</li>
  </ul>
  
  <p>The latter are required for the math to go through (see above
  document). So sampled_values contains(with a hopefully clear abuse of
  notation): </p>
  
  <p>sampled_values = (N, log Q(L), log Q(N)).</p>
</blockquote>

<p>However, I still don't know how to input a value. I'm not sure what the datatypes should be, and if N is the int place in the embedding matrix, or the embedding itself. Also, I'm guessing N should be a list of values itself, the size of the number of negative labels we have to sample. </p>

<p>I was wondering if I could get a example with some values. For example, for a negative sampling of 3, do I do something like this? </p>

<p>sampled_values = ([4,29, 12], [1, 1, 1], [0, 0, 0])</p>

<p>Also, the documentation says that the tuple should be "" returned by a *_candidate_sampler function""</p>

<p>Does that mean I need to provide a function that returns the tuple, instead of the tuple itself?</p>
",0
52814880,Neuron freezing in Tensorflow,"<p>I need to implement <strong>neurons freezing</strong> in CNN for a deep learning research,
I tried to find any function in the Tensorflow docs, but I didn't find anything.
How can I freeze specific neuron when I implemented the layers with tf.nn.conv2d?</p>
",1
52820076,Tensorflow device only acknowledges CPU even when explicitly specifying tf.device('gpu:0'),"<p>After many attempts to verify that my nvidia quadro M4000M is capable of tensor computations,  Cuda installed correctly,  cuda toolkit 8.0 installed correctly,  deviceQuery and nvidia-smi confirms that the device is functioning properly.</p>

<p><a href=""https://i.stack.imgur.com/SU2BP.jpg"" rel=""nofollow noreferrer"">deviceQuery</a></p>

<p><a href=""https://i.stack.imgur.com/li8Ki.jpg"" rel=""nofollow noreferrer"">nvidia-smi</a></p>

<p>When running tensorflow's example:</p>

<pre><code>import tensorflow as tf

with tf.device(""/gpu:0""):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a', dtype=tf.float32)
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b', dtype=tf.float32)
    c = tf.matmul(a, b)

sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=False,log_device_placement=True)) 
print(sess.run(c))
</code></pre>

<p>I don't get the device mapping information and the only way I can get the  session to complete is to change the allow_soft_placement=True, which doesn't appear to use the GPU.</p>

<p>When allow_soft_placement = False for tf.device(""/gpu:0"") or /cpu:0 the error is:</p>

<blockquote>
  <blockquote>
    <p>InvalidArgumentError                      Traceback (most recent call last)
    ~\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tensorflow\python\client\session.py
    in _do_call(self, fn, *args)    1291     try:
    -> 1292       return fn(*args)    1293     except errors.OpError as e:</p>
    
    <p>~\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tensorflow\python\client\session.py
    in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata) 
    1274       # Ensure any changes to the graph are reflected in the
    runtime.
    -> 1275       self._extend_graph()    1276       return self._call_tf_sessionrun(</p>
    
    <p>~\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tensorflow\python\client\session.py
    in _extend_graph(self)    1311     with
    self._graph._session_run_lock():  # pylint: disable=protected-access
    -> 1312       tf_session.ExtendSession(self._session)    1313 </p>
    
    <p>InvalidArgumentError: Cannot assign a device for operation 'MatMul': Operation was explicitly assigned to /device:GPU:0 but available
    devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make
    sure the device specification refers to a valid device.    [[{{node
    MatMul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false,
    _device=""/device:GPU:0""](a, b)]]</p>
    
    <p>During handling of the above exception, another exception occurred:</p>
    
    <p>InvalidArgumentError                      Traceback (most recent call last)  in 
          7 sess =  tf.Session(config=tf.ConfigProto(allow_soft_placement=False,log_device_placement=True))
          8 
    ----> 9 print(sess.run(c))
         10 
         11 </p>
    
    <p>~\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tensorflow\python\client\session.py
    in run(self, fetches, feed_dict, options, run_metadata)
        885     try:
        886       result = self._run(None, fetches, feed_dict, options_ptr,
    --> 887                          run_metadata_ptr)
        888       if run_metadata:
        889         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)</p>
    
    <p>~\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tensorflow\python\client\session.py
    in _run(self, handle, fetches, feed_dict, options, run_metadata)<br>
    1108     if final_fetches or final_targets or (handle and
    feed_dict_tensor):    1109       results = self._do_run(handle,
    final_targets, final_fetches,
    -> 1110                              feed_dict_tensor, options, run_metadata)    1111     else:    1112       results = []</p>
    
    <p>~\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tensorflow\python\client\session.py
    in _do_run(self, handle, target_list, fetch_list, feed_dict, options,
    run_metadata)    1284     if handle is None:    1285       return
    self._do_call(_run_fn, feeds, fetches, targets, options,
    -> 1286                            run_metadata)    1287     else:    1288       return self._do_call(_prun_fn, handle, feeds, fetches)</p>
    
    <p>~\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tensorflow\python\client\session.py
    in _do_call(self, fn, *args)    1306<br>
    self._config.experimental.client_handles_error_formatting):    1307<br>
    message = error_interpolation.interpolate(message, self._graph)
    -> 1308       raise type(e)(node_def, op, message)    1309     1310   def _extend_graph(self):</p>
    
    <p>InvalidArgumentError: Cannot assign a device for operation 'MatMul': Operation was explicitly assigned to /device:GPU:0 but available
    devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make
    sure the device specification refers to a valid device.    [[{{node
    MatMul}} = MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false,
    _device=""/device:GPU:0""](a, b)]]</p>
    
    <p>Caused by op 'MatMul', defined at:   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\runpy.py"",
    line 193, in _run_module_as_main
        ""<strong>main</strong>"", mod_spec)   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\runpy.py"",
    line 85, in _run_code
        exec(code, run_globals)   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\ipykernel_launcher.py"",
    line 16, in 
        app.launch_new_instance()   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\traitlets\config\application.py"",
    line 658, in launch_instance
        app.start()   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\ipykernel\kernelapp.py"",
    line 499, in start
        self.io_loop.start()   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tornado\platform\asyncio.py"",
    line 132, in start
        self.asyncio_loop.run_forever()   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\asyncio\base_events.py"",
    line 422, in run_forever
        self._run_once()   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\asyncio\base_events.py"",
    line 1434, in _run_once
        handle._run()   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\asyncio\events.py"",
    line 145, in _run
        self._callback(*self._args)   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tornado\ioloop.py"",
    line 758, in _run_callback
        ret = callback()   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tornado\stack_context.py"",
    line 300, in null_wrapper
        return fn(*args, **kwargs)   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tornado\gen.py"",
    line 1233, in inner
        self.run()   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tornado\gen.py"",
    line 1147, in run
        yielded = self.gen.send(value)   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\ipykernel\kernelbase.py"",
    line 346, in process_one
        yield gen.maybe_future(dispatch(*args))   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tornado\gen.py"",
    line 326, in wrapper
        yielded = next(result)   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\ipykernel\kernelbase.py"",
    line 259, in dispatch_shell
        yield gen.maybe_future(handler(stream, idents, msg))   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tornado\gen.py"",
    line 326, in wrapper
        yielded = next(result)   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\ipykernel\kernelbase.py"",
    line 513, in execute_request
        user_expressions, allow_stdin,   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tornado\gen.py"",
    line 326, in wrapper
        yielded = next(result)   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\ipykernel\ipkernel.py"",
    line 294, in do_execute
        res = shell.run_cell(code, store_history=store_history, silent=silent)   File
    ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\ipykernel\zmqshell.py"",
    line 536, in run_cell
        return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)   File
    ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\IPython\core\interactiveshell.py"",
    line 2817, in run_cell
        raw_cell, store_history, silent, shell_futures)   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\IPython\core\interactiveshell.py"",
    line 2843, in _run_cell
        return runner(coro)   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\IPython\core\async_helpers.py"",
    line 67, in _pseudo_sync_runner
        coro.send(None)   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\IPython\core\interactiveshell.py"",
    line 3018, in run_cell_async
        interactivity=interactivity, compiler=compiler, result=result)   File
    ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\IPython\core\interactiveshell.py"",
    line 3183, in run_ast_nodes
        if (yield from self.run_code(code, result)):   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\IPython\core\interactiveshell.py"",
    line 3265, in run_code
        exec(code_obj, self.user_global_ns, self.user_ns)   File """", line 5, in 
        c = tf.matmul(a, b)   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tensorflow\python\ops\math_ops.py"",
    line 2053, in matmul
        a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)   File
    ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"",
    line 4856, in mat_mul
        name=name)   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tensorflow\python\framework\op_def_library.py"",
    line 787, in _apply_op_helper
        op_def=op_def)   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tensorflow\python\util\deprecation.py"",
    line 488, in new_func
        return func(*args, **kwargs)   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tensorflow\python\framework\ops.py"",
    line 3272, in create_op
        op_def=op_def)   File ""C:\Users\sellersp\AppData\Local\conda\conda\envs\tensor\lib\site-packages\tensorflow\python\framework\ops.py"",
    line 1768, in <strong>init</strong>
        self._traceback = tf_stack.extract_stack()</p>
    
    <p>InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'MatMul': Operation was explicitly assigned to
    /device:GPU:0 but available devices are [
    /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device
    specification refers to a valid device.    [[{{node MatMul}} =
    MatMul[T=DT_FLOAT, transpose_a=false, transpose_b=false,
    _device=""/device:GPU:0""](a, b)]]</p>
  </blockquote>
</blockquote>

<p>After searching many locations I can't seem to find where my error is.   I've run it in a Jupyter Notebook and as a script at the interpreter.</p>

<p>Any help would be greatly appreciated.  Sorry for the lengthy error.</p>
",0
52864435,Measuring GPU memory usage with TensorFlow profiler,"<p>Is there a way to properly measure GPU usage of tf.Estimator model using TensorFlow profiler? I've followed the documentation:</p>

<pre><code>g = tf.Graph()
sess = tf.Session(graph=g)
run_meta = tf.RunMetadata()

time_and_memory_args = tf.profiler.ProfileOptionBuilder.time_and_memory()

with g.as_default():
    data_shape = [BATCH_SIZE] + [3, 224, 224]
    in_plh = tf.placeholder(tf.float32, data_shape)

    model = some_model(in_plh, args=model_args, training=True)
    images = np.random.rand(BATCH_SIZE, 3, 224, 224)

    sess.run(tf.global_variables_initializer())
    sess.run(model, feed_dict={in_plh: images})

    time_and_memory = tf.profiler.profile(g, run_meta=run_meta, cmd='op',
                                          options=time_and_memory_args) 

    if time_and_memory is not None:
        print('Total requested bytes:', time_and_memory.total_requested_bytes)
</code></pre>

<p>But the printed result is always 0.</p>
",1
52874647,Tensorflow v1.10+ why is an input serving receiver function needed when checkpoints are made without it?,"<p>I'm in the process of adapting my model to TensorFlow's estimator API. </p>

<p>I recently asked a question regarding <a href=""https://stackoverflow.com/questions/52641737/tensorflow-1-10-custom-estimator-early-stopping-with-train-and-evaluate/52642619#52642619"">early stopping based on validation data</a> where in addition to early stopping, the best model at this point should be exported. </p>

<p>It seems that my understanding of what a model export is and what a checkpoint is is not complete. </p>

<p>Checkpoints are made automatically. From my understanding, the checkpoints are sufficient for the estimator to start ""warm"" - either using so per-trained weights or weights prior to an error (e.g. if you experienced a power outage). </p>

<p>What is nice about checkpoints is that I do not have to write any code besides what is necessary for a custom estimator (namely, <code>input_fn</code> and <code>model_fn</code>). </p>

<p>While, given an initialized estimator, one can just call its <code>train</code> method to train the model, in practice this method is rather lackluster. Often one would like to do several things:</p>

<ol>
<li>compare the network periodically to a validation dataset to ensure you are not over-fitting</li>
<li>stop the training early if over-fitting occurs </li>
<li>save the best model whenever the network finishes (either by hitting the specified number of training steps or by the early stopping criteria). </li>
</ol>

<p>To someone new to the ""high level"" estimator API, a lot of low level expertise seems to be required (e.g. for the <code>input_fn</code>) as how one could get the estimator to do this is not straight forward.</p>

<p>By some light code reworking <strong>#1</strong> can be achieved by using <code>tf.estimator.TrainSpec</code> and <code>tf.estimator.EvalSpec</code> with <code>tf.estimator.train_and_evaluate</code>.</p>

<p>In the <a href=""https://stackoverflow.com/questions/52641737/tensorflow-1-10-custom-estimator-early-stopping-with-train-and-evaluate/52642619#52642619"">previous question</a> user <strong>@GPhilo</strong> clarifies how <strong>#2</strong> can be achieved by using a semi-unintuitive function from the <code>tf.contrib</code>:</p>

<pre><code>tf.contrib.estimator.stop_if_no_decrease_hook(my_estimator,'my_metric_to_monitor', 10000)
</code></pre>

<p>(unintuitive as ""the early stopping is not triggered according to the number of non-improving evaluations, but to the number of non-improving evals in a certain step range""). </p>

<p><strong>@GPhilo</strong> - noting that it is unrelated to <strong>#2</strong> - also answered how to do <strong>#3</strong> (as requested in the original post). Yet, I do not understand what an <code>input_serving_fn</code> is, why it is needed, or how to make it.</p>

<p>This is further confusing to me as no such function is needed to make checkpoints, or for the estimator to start ""warm"" from the checkpoint.</p>

<p>So my questions are:</p>

<ul>
<li>what is the difference between a checkpoint and an exported best model?</li>
<li>what exactly is a serving input receiver function and how to write one? (I have spent a bit of time reading over the tensorflow docs and do not find it sufficient to understand how I should write one, and why I even have to). </li>
<li>how can I train my estimator, save the best model, and then later load it. </li>
</ul>

<p>To aid in answering my question I am providing this <a href=""https://colab.research.google.com/drive/16G9jEwSCoLJ1MvkLzZGRY8YAfalV5qOT"" rel=""noreferrer"">Colab</a> document.</p>

<p>This self contained notebook produces some dummy data, saves it in TF Records, has a very simple custom estimator via <code>model_fn</code> and trains this model with an <code>input_fn</code> that uses the TF Record files. Thus it should be sufficient for someone to explain to me what placeholders I need to make for the input serving receiver function and and how I can accomplish <strong>#3</strong>.</p>

<h1>Update</h1>

<p><strong>@GPhilo</strong> foremost I can not understate my appreciation for you thoughtful consideration and care in aiding me (and hopefully others) understand this matter.</p>

<p>My “goal” (motivating me to ask this question) is to try and build a reusable framework for training networks so I can just pass a different <code>build_fn</code> and go (plus have the quality of life features of exported model, early stopping, etc).  </p>

<p>An updated (based off your answers) <a href=""https://colab.research.google.com/drive/10VKcE1sZh8F2-HYASxx7OgV8Mf4DQKFy"" rel=""noreferrer"">Colab</a>  can be found <a href=""https://colab.research.google.com/drive/10VKcE1sZh8F2-HYASxx7OgV8Mf4DQKFy"" rel=""noreferrer"">here</a>.</p>

<p>After several readings of your answer, I have found now some more confusion:</p>

<p>1.</p>

<blockquote>
  <p>the way you provide input to the inference model is different than the one you use for the training</p>
</blockquote>

<p>Why? To my understanding the data input pipeline is not:</p>

<pre><code>load raw —&gt; process —&gt; feed to model
</code></pre>

<p>But rather:</p>

<pre><code>Load raw —&gt; pre process —&gt; store (perhaps as tf records)
# data processing has nothing to do with feeding data to the model?
Load processed —&gt; feed to model
</code></pre>

<p>In other words, it is my understanding (perhaps wrongly) that the point of a tf <code>Example</code> / <code>SequenceExample</code> is to store a complete singular datum entity ready to go - no other processing needed other than reading from the <code>TFRecord</code> file.</p>

<p> Thus there can be a difference between the training / evaluation <code>input_fn</code> and the inference one (e.g. reading from file vs eager / interactive evaluation of in memory), but the data format is the same (except for inference you might want to feed only 1 example rather than a batch…)</p>

<p>I agree that the “<em>input pipeline is not part of the model itself</em>”. However, in my mind, and I am apparently wrong in thinking so, with the estimator I should be able to feed it a batch for training and a single example (or batch) for inference. </p>

<p>An aside: “<em>When evaluating, you don't need the gradients and you need a different input function.</em> “, the only difference (at least in my case) is the files from which you reading?</p>

<ol start=""2"">
<li>I am familiar with that TF Guide, but I have not found it useful because it is unclear to me what placeholders I need to add and what additional ops needed to be added to convert the data.</li>
</ol>

<p>What if I train my model with records and want to inference with just the dense tensors?</p>

<p>Tangentially, I find the <em>example</em> in the linked guide subpar, given the tf record interface requires the user to define multiple times how to write to / extract features from a tf record file in different contexts.  Further, given that the TF team has explicitly stated they have little interest in documenting tf records, any documentation built on top of it, to me, is therefore equally unenlightening.</p>

<ol start=""3"">
<li><p>Regarding <code>tf.estimator.export.build_raw_serving_input_receiver_fn</code>.
What is the placeholder called? Input?  Could you perhaps show the analog of <code>tf.estimator.export.build_raw_serving_input_receiver_fn</code> by writing the equivalent <code>serving_input_receiver_fn</code></p></li>
<li><p>Regarding your example <code>serving_input_receiver_fn</code> with the input images. How do you know to call features ‘images’ and the receiver tensor ‘input_data’ ? Is that (the latter) standard?</p></li>
<li><p>How to name an export with <code>signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY</code>.</p></li>
</ol>
",1
52878311,How to extract rows and columns from a 3D array in Tensorflow,"<p>I wanted to do the following indexing operation on a TensorFlow tensor.
What should be the equivalent operations in TensorFlow to get <code>b</code> and <code>c</code> as output? Although <code>tf.gather_nd</code> documentation has several examples but I could not generate equivalent <code>indices</code> tensor to get these results.</p>
<pre><code>import tensorflow as tf
import numpy as np

a=np.arange(18).reshape((2,3,3))

idx=[2,0,1] #it can be any validing re-ordering index list

#These are the two numpy operations that I want to do in Tensorflow
b=a[:,idx,:]
c=a[:,:,idx] 

# TensorFlow operations

aT=tf.constant(a)
idxT=tf.constant(idx)

# what should be these two indices  
idx1T=tf.reshape(idxT, (3,1)) 
idx2T=tf.reshape(idxT, (1,1,3))

bT=tf.gather_nd(aT, idx1T ) #does not work
cT=tf.gather_nd(aT, idx2T)  #does not work

with tf.Session() as sess:
    b1,c1=sess.run([bT,cT])

print(np.allclose(b,b1))
print(np.allclose(c,c1))
</code></pre>
<p>I am not restricted to <code>tf.gather_nd</code> Any other suggestion to achieve the same operations on GPU will be helpful.</p>
<h1>Edit: I have updated the question for a typo:</h1>
<p>old statement: <code>c=a[:,idx]</code>,</p>
<p>New statement: <code>c=a[:,:,idx]</code>
What I wanted to achieve was re-ordering of columns as well.</p>
",1
52888929,Stopping criteria for pre-made estimators in TensorFlow,"<p>I have a question about TensorFlow's estimators in <code>tf.estimator</code>, in particular <code>DNNClassifier</code>. It says in the <a href=""https://www.tensorflow.org/versions/r1.7/api_docs/python/tf/estimator/DNNClassifier"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p><code>max_steps</code>: Number of total steps for which to train model. If <code>None</code>, train forever or train until <code>input_fn</code> generates the <code>OutOfRange</code> error or <code>StopIteration</code> exception</p>
</blockquote>

<p>In the doc on <a href=""https://www.tensorflow.org/guide/datasets_for_estimators"" rel=""nofollow noreferrer"">datasets for estimators</a> it mentions that for training you need to use the <code>shuffle()</code>, <code>repeat()</code>, and <code>batch_size</code> methods, so that the iterator on the data set does stop after it's gone through the data once.</p>

<p>Does this mean that the pre-made estimators such as <code>DNNClassifier</code> have no stopping criterion based on the learning rate or changes in the loss? Is it really the case you can only have these models stop training based on how you specify the input function or by giving a maximum number of steps?</p>
",0
52914302,How to process Estimator categorical_column when request tensorflow-model-server?,"<p>As you known, request Tensorflow-serving server need organize request data format like this structure: </p>

<pre><code>Example(
    features=Features(
        feature={
            'SepalLength': Feature(float_list=FloatList(value=[5.1])),
            'SepalWidth': Feature(float_list=FloatList(value=[3.3])),
            'PetalLength': Feature(float_list=FloatList(value=[1.7])),
            'PetalWidth': Feature(float_list=FloatList(value=[0.5])),
        }
    )
)
</code></pre>

<p>if my data only <code>tf.feature_column.numeric_column</code>, that's easy, i can process like this:</p>

<pre><code>tf.train.Feature(float_list=tf.train.FloatList(value=[value]))
</code></pre>

<p>but my training data like is : </p>

<pre><code>categorical_column = tf.feature_column.categorical_column_with_identity(
            key = name, num_buckets = 3 )
indicator_column = tf.feature_column.indicator_column( categorical_column )
</code></pre>

<p>it's <code>categorical_column</code>, like some kind of one hot structure, in this situation, i don't know how to handle this, and i can't find any advanced tutorials to handle this,</p>

<p>How should i do if i want to process my data to adapt <code>tensorflow-model-server
</code> request requirement ? thanks.</p>

<p>PS: 
i use components below:
training framework:
<code>tensorflow estimator</code>
client:
<code>tensorflow-seving-api</code>
server:
<code>tensorflow-model-server</code></p>
",0
52914542,Tensorflow 1.11.0 throws an error when trying to reuse cudnnGRU variables with scope.reusevariables(),"<p>I'm implementing an encoder-decoder rnn by using <code>tf.contrib.cudnn_rnn.CudnnGRU()</code> as the encoder and I've found a problem:</p>

<p>I want to reuse the variables so I can create the same model but use it with other data and to put it simple this would be the code to reproduce my problem:</p>

<pre><code>tf.reset_default_graph()

def create_model():
    return tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=100,
                         direction='unidirectional')

# (time, batch_size, num_inputs)
x = tf.random_normal((100, 16, 100))

with tf.variable_scope('model') as scope:
    model_1 = create_model()
    rnn_out_1, rnn_state_1 = model_1(x)
    scope.reuse_variables()
    model_2 = create_model()
    rnn_out_2, rnn_state_2 = model_2(x)
</code></pre>

<p>This throws the following error: </p>

<blockquote>
  <p>Variable model/cudnn_gru_1/opaque_kernel does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?</p>
</blockquote>

<p>So the second model is trying to find the <code>model/cudnn_gru_1/opaque_kernel</code> variable but can not find it because it should be looking for <code>model/cudnn_gru/opaque_kernel:0</code>.</p>

<p>The thing is I don't know why this is happening since by following the Variables tensorflow reference it seems to be ok. On the other hand, I also tried to write it differently as tensorflow doc states that my above implementation and the one I'm showing next do actually the same:</p>

<pre><code>tf.reset_default_graph()

def create_model():
    return tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=100,
                         direction='unidirectional')

# (time, batch_size, num_inputs)
x = tf.random_normal((100, 16, 100))

with tf.variable_scope('model'):
    model_1 = create_model()
    rnn_out_1, rnn_state_1 = model_1(x)
with tf.variable_scope('model', reuse=True):
    model_2 = create_model()
    rnn_out_2, rnn_state_2 = model_2(x)
</code></pre>

<p>This second way is actually working (or at least I think it is). So I don't really know what I'm doing wrong in the first implementation, I'm also not sure on wether both implementations should be doing the same (which I think they should).
So does anyone please can help me figure out what I'm doing wrong or the things that I'm not able to understand properly?</p>

<p>Thanks in advance</p>
",0
52966243,Tensorflow: NotImplementedError: The reduce() transformation does not currently support nested datasets as inputs,"<p>In Tensorflow 1.12 the <code>tf.data.Dataset.reduce()</code> and <code>tf.data.Dataset.window()</code> methods are introduced.</p>

<p>From the release notes:</p>

<ul>
<li><p>""New <code>tf.data.Dataset.reduce()</code> API allows users to reduce a finite dataset to a single element using a user-provided reduce function.""</p></li>
<li><p>""New <code>tf.data.Dataset.window()</code> API allows users to create finite windows of input dataset; when combined with the <code>tf.data.Dataset.reduce()</code> API, this allows users to implement customized batching.""</p></li>
</ul>

<p>However how to use these functions?</p>

<pre><code>def reduce_func(old_state, input_element):
    pdb.set_trace()
    return new_state

dataset = tf.data.Dataset.from_generator(frame_generator, (tf.string, tf.string))
dataset = dataset.window(2).reduce(np.int64(0), reduce_func)
</code></pre>

<p>This gives a NotImplementedError:</p>

<blockquote>
  <p>NotImplementedError: The reduce() transformation does not currently<br>
  support nested datasets as inputs. </p>
</blockquote>

<p>I use tensorflow version '1.12.0-rc1'</p>

<p>EDIT:
From <a href=""https://www.tensorflow.org/versions/r1.12/api_docs/python/tf/contrib/data/sliding_window_batch"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r1.12/api_docs/python/tf/contrib/data/sliding_window_batch</a></p>

<p><em>THIS FUNCTION IS DEPRECATED.</em> It will be removed in a future version. Instructions for updating: Use <code>tf.data.Dataset.window(size=window_size, shift=window_shift, stride=window_stride).flat_map(lambda x: x.batch(window.size))</code></p>

<p>But how to use this if the dataset is generated with</p>

<pre><code>dataset = tf.data.Dataset.from_generator(frame_generator, (tf.string, tf.string))
</code></pre>

<p>So each item in the dataset contains two elements. Then there is a TypeError:</p>

<blockquote>
  <p>TypeError: () takes 1 positional argument but 2 were given</p>
</blockquote>

<p>EDIT: 
Solved by using zip</p>

<pre><code>dataset = tf.data.Dataset.from_generator(frame_generator, (tf.string, tf.string))
window_size = 2
dataset = dataset.window(window_size).flat_map(lambda x,y: tf.data.Dataset.zip((x,y)).batch(window_size))
dataset = dataset.map(self.parse_function)
</code></pre>
",0
52969867,make tensorflow dataset from huge number of images(*.jpg) and labels(*.mat),"<p>I have a huge number of images with their labels (.mat) file (cannot use <code>tf.data.Dataset.from_tensor_slices()</code>) and I want to use <code>tf.data</code> API to make a tensorflow dataset out of it.</p>

<p>As I read in the documentation, I can use <code>tf.data.TextLineDataset</code> for large number of data(I have to have a txt file with the address of all the images and send the path of the txt file as <code>tf.data.TextLineDataset</code> argument).
Then, I can use <code>map</code> method to read txt file (<code>tf.read_file</code>) decode jpg image (<code>tf.image.decode_jpeg</code>) and do some basic transformation on the image.</p>

<p>However, I cannot use <code>scipy.io.loadmat</code> in any part of <code>map</code> method because I have no string indicating the path to the mat file. All I have is <code>tf.Tensor</code>.</p>

<p>I don't think that reading all images and making a TFRecord out of it is that much efficient in this case because then I am basically doing every thing two times. Once, reading the whole images and making TFRecord, and once again, reading TFRecord to make tensorflow dataset.</p>

<p>Any idea how I can resolve this issue?</p>

<p>This is my code:</p>

<pre><code>dataset = tf.data.TextLineDataset(txt_file).map(read_img_and_mat)
</code></pre>

<p>and then:</p>

<pre><code>def read_img_and_mat(path):
    image_string = tf.read_file(path)
    image_decoded = tf.image.decode_jpeg(image_string, channels=3)
    label = ... # get label from mat file
    return image_decoded, label
</code></pre>
",1
52973241,Consuming big data by Tensorflow,"<p>Suppose, I have a huge list of objects, each of which could be a list of numpy arrays, for example. </p>

<p>What’s the best way to pass this dataset to tensorflow? </p>

<p>I want to be able to randomly shuffle the data and form batches. May be it’s worth to shuffle the dataset and form batches using standard python(numpy) procedures and after that use something like <code>tf.data.Dataset.from_generator()</code>? </p>

<p>Straightforward approach of transforming full dataset to <code>tf.Tensor</code> seems to be useless due to size limit for the <code>tf.GraphDef</code> protocol buffer(according to the Tensorflow documentation).</p>
",0
52976606,Global step not incrementing with batch norm and custom estimator,"<p>I have a customer estimator that has several layers that look like the following in the model function:</p>

<pre><code>natural_layer = tf.layers.dense(inputs = natural_layer, 
                                units = units, 
                                activation = None,
                                use_bias = False,
                                kernel_regularizer = params['regularizer'],
                                name = 'pre_batch_norm_layer_' + str(i + 1))

natural_layer = tf.layers.batch_normalization(natural_layer,
                                              axis = 1,
                                              center = True,
                                              scale = True,
                                              training = (mode == tf.estimator.ModeKeys.TRAIN),
                                              name = 'batch_norm_layer_' + str(i + 1))

natural_layer = params['natural_layer_activation'](natural_layer, name = 'activation_layer_' + str(i + 1))
</code></pre>

<p>Because I'm using batch norm, the training op is set up like this:</p>

<pre><code>update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    optimizer = tf.contrib.opt.MultitaskOptimizerWrapper(params['optimization_algorithm'](params['training_rate']))
    train_op = optimizer.minimize(loss, global_step = tf.train.get_global_step())
</code></pre>

<p>Where the optimizer is usually tf.train.AdamOptimizer.</p>

<p>However, when I go to train the estimator the global step never increments (so training will run forever), and I get this:</p>

<p>WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.</p>

<p>I am passing tf.train.get_global_step() to minimize, so I'm not sure why it never gets updated. My hunch is that it has something to do with the batch normalization because when I remove that or replace it with dropout, everything works fine (even when keeping the update ops lines that are required for batch normalization per the documentation).</p>

<p>Anyone know what is going on? Happy to post more code if helpful. </p>
",1
53000921,Tensorflow Sequence to Sequence CustomHelper,"<p>There are limited amount of documentation on Sequence to Sequence CustomHelper </p>

<p><code>helper = tf.contrib.seq2seq.CustomHelper(initialize_fn = initialize_fn,sample_fn = sample_fn, next_inputs_fn = next_inputs_fn)</code></p>

<p>in Tensorflow.</p>

<p>Would anyone explain the inputs of the Custom-helper, in terms of the input data</p>

<p><code>X = tf.placeholder(tf.float32, [batch_size x time_steps x features])</code></p>

<p>and encoder, </p>

<p><code>encoder_cell = tf.contrib.rnn.BasicLSTMCell(hidden_size)</code></p>

<p><code>initial_state = encoder_cell.zero_state(batch_size, dtype=tf.float32)</code></p>

<p>and/or possibly </p>

<p><code>rnn_output, rnn_states = tf.nn.dynamic_rnn(encoder_cell, X, dtype=tf.float32)</code>
?</p>
",1
53032922,TensorFlow while loop with condition dependent on body,"<p>I want to have a while loop with the condition dependent on a tensor computed in the loop body, but I don't know how to accomplish this with <a href=""https://www.tensorflow.org/api_docs/python/tf/while_loop"" rel=""nofollow noreferrer""><code>tf.while_loop()</code></a>.</p>

<p>My input processing includes random cropping, but some crops can lead to low-quality examples and I want to discard those and try a new random crop until an example of sufficient quality is obtained. The inputs are cropped by</p>

<pre><code>import numpy as np
import tensorflow as tf
IMAGE_SHAPE = [960, 720]
CROP_SHAPE = [320, 240]
max_begin_index = np.array(IMAGE_SHAPE) - np.array(CROP_SHAPE)
crop_begin_index = tf.round(tf.random_uniform([2]) * max_begin_index)
img_crop = tf.slice(img, crop_begin_index, crop_shape + [-1])
</code></pre>

<p>and the condition is</p>

<pre><code>cond = tf.count_nonzero(img_crop &gt; 0) &gt; 0.5 * tf.size(img_crop)
</code></pre>

<p>Going over the documentation and examples of <code>tf.while_loop(cond, body, loop_vars, ...)</code>, what I understand is that both <code>cond</code> and <code>body</code> should take the same arguments given in <code>loop_vars</code>.
I don't see how I can have <code>cond</code> depend on <code>img_crop</code> which would be calculated inside <code>body</code>, and isn't provided in <code>loop_vars</code>.</p>

<p>I could equivalently compute <code>cond</code> using <code>crop_begin_index</code> without actually cropping, but it depends on the random values computed inside the loop, so I have the same problem.</p>

<p>Is this indeed a limitation of TF looping? If not, how can I rewrite my code to use <code>tf.while_loop()</code>?</p>
",1
53033767,How to print a tensor's value inside tf.while_loop without returning it?,"<p>What I want is to print a tensor's value inside tf.while_loop body without returning the tensor but still using a computational graph. Below I have some simple examples to explain what I want to succeed and what I have done so far.</p>

<p>Method 1 (works): </p>

<p>TF supports an option to print tensors while evaluating a model by introducing tf.Print operation to the graph, but that requires for the tensor to be returned from the body : </p>

<pre><code>import tensorflow as tf
import numpy as np

x = tf.placeholder(tf.float32, [1])

def body(x):
    a = tf.constant( np.array([2]) , dtype=tf.float32)
    x = a + x
    x = tf.Print(x,[x], summarize=100) &lt;= print here (works)
    return x

def condition(x):
    return tf.reduce_sum(x) &lt; 10

with tf.Session() as sess:
    tf.global_variables_initializer().run()
    result = tf.while_loop(cond=condition, body=body, loop_vars=[x])
    result_out = sess.run([result], feed_dict={ x : np.zeros(1)})
    print(result_out)
</code></pre>

<p>Out :</p>

<pre><code>[2]
[4]
[6]
[8] 
[10]
[array([10.], dtype=float32)] 
</code></pre>

<p>Method 2 (works):  </p>

<p>TF supports an option to print tensors without creating a computational graph using Eager mode. Same example below : </p>

<pre><code>import tensorflow as tf
import numpy as np

tf.enable_eager_execution()

def body(x):
    a = tf.constant(np.array([2]), dtype=tf.int32)
    x = a + x
    print(x) &lt;= print here (works)
    return x

def cond(x):
    return tf.reduce_sum(x) &lt; 10 # sum over an axis

x = tf.constant(0, shape=[1])

#result = tf.while_loop(cond=condition, body=body, loop_vars=(x,0))
result=tf.while_loop(cond, body, [x])
print(result)
</code></pre>

<p>Out :</p>

<pre><code>tf.Tensor([2], shape=(1,), dtype=int32)
tf.Tensor([4], shape=(1,), dtype=int32)
tf.Tensor([6], shape=(1,), dtype=int32)
tf.Tensor([8], shape=(1,), dtype=int32)

tf.Tensor([10], shape=(1,), dtype=int32)
tf.Tensor([10], shape=(1,), dtype=int32) 
</code></pre>

<p>Method 3 (failed):</p>

<p>What I want is to print the tensor using eager execution in a graph environment (like described : <a href=""https://www.tensorflow.org/guide/eager"" rel=""nofollow noreferrer"">here</a>).  </p>

<pre><code>import tensorflow as tf
import numpy as np

tfe = tf.contrib.eager

x = tf.placeholder(tf.int32, [1])

def my_py_func(x):
  print(x)  # It's eager!

def body(x):
    a = tf.constant( np.array([2]) , dtype=tf.int32)
    x = a + x
    tfe.py_func(my_py_func, x, tf.int32) &lt;= print here (does not work)
    return x

def condition(x):
    return tf.reduce_sum(x) &lt; 10

with tf.Session() as sess:
    tf.global_variables_initializer().run()
    result = tf.while_loop(condition, body, [x])
    result_out = sess.run([result], feed_dict={ x : np.zeros(1)} )
    print(result_out)
</code></pre>

<p>Out :</p>

<pre><code>TypeError: Expected list for 'input' argument to 'EagerPyFunc' Op, not Tensor(""while/add:0"", shape=(1,), dtype=int32).
</code></pre>

<p>Of course in this example I return tensor <code>x</code> from the body, but I want to print inside the loop !</p>
",0
53079436,tensorflow Tf.cond giving unexpected output,"<p>I seem to be having a misunderstanding on how <code>tf.cond</code> works. In the tensorflow <a href=""https://www.tensorflow.org/api_docs/python/tf/cond"" rel=""nofollow noreferrer"">documentation</a>, it gives the following example:</p>

<pre><code>z = tf.multiply(a, b)
result = tf.cond(x &lt; y, lambda: tf.add(x, z), lambda: tf.square(y))
</code></pre>

<p>The result of the example, if <code>x&lt;y</code> is <code>True</code> is <code>tf.add(x,z)</code> else <code>tf.square(y)</code></p>

<p>Following this example, I am trying to build a small example with tf.cond and the result doesnt go along the lines mentioned in the documentation.</p>

<p>in my example, <code>deterministic_action = 4</code>, <code>random_action = 11</code>, <code>chose_random=False</code>. The <code>stochastic_action</code> should be <code>4</code>, instead it is <code>1</code>.
Where did the value 1 come from?</p>

<pre><code>#!/usr/bin/env python3

import tensorflow as tf
import numpy as np

with tf.Graph().as_default():
    with tf.device('/cpu:0'):
        stochastic_ph = tf.placeholder(tf.bool, (), name=""stochastic"")
        eps = tf.get_variable(""eps"", (), initializer=tf.constant_initializer(0))
        with tf.variable_scope('test_cond') as sc:
            deterministic_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=0) # 4
            random_action = tf.random_uniform([], minval=0, maxval=15, dtype=tf.int64, seed=1) # 11
            chose_random = tf.random_uniform([], minval=0, maxval=1, dtype=tf.float32) &lt; eps # False because eps = 0
            stochastic_action = tf.cond(chose_random, lambda: random_action, lambda: deterministic_action) # S_action should be 4 but it is 1
            #output_action = tf.cond(stochastic_ph, lambda: stochastic_action, lambda: deterministic_action)


    init = tf.global_variables_initializer()
    sess = tf.Session()
    sess.run(init, feed_dict={stochastic_ph: True})
    print (""s_ph = "", stochastic_ph)
    d_action = sess.run(deterministic_action)
    print (""det_action= "", d_action)
    r_action = sess.run(random_action)
    print (""rand_action= "", r_action)
    e = sess.run(eps)
    c_action = sess.run(chose_random)
    print (""chose_rand= "", c_action)
    s_action = sess.run(stochastic_action)
    print (""s_action= "", s_action)
    #output = sess.run(output_action)
</code></pre>

<p>here is the output:</p>

<pre><code>python random_vec.py
2018-10-31 09:46:15.028376: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
s_ph =  Tensor(""stochastic:0"", shape=(), dtype=bool, device=/device:CPU:0)
det_action=  4
rand_action=  11
chose_rand=  False
s_action=  1
</code></pre>
",1
53107594,How do I import and use quantized_matmul and quantized_biasadd operations in tensorflow,"<p>I am trying to use the quantized matmul operation in tensorflow. However I am not sure if I am doing it right. I could not find an example as to how to call the function and use it.</p>

<p>I know that quantized conv2D can be called as tf.nn.conv2D(), just wanted to know a similar way of calling quantized_matmul operation as well.</p>

<p>The class and the functions are present in tensorflow library and are documented below:</p>

<p>Quantized Bias Add:
<a href=""https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/quantized-bias-add"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/quantized-bias-add</a></p>

<p>Quantized Matmul:
<a href=""https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/quantized-mat-mul"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/quantized-mat-mul</a></p>

<p>Thanks and Regards,
Abhinav George</p>
",0
53122412,AOT compilation of an Estimator,"<p>I find it very challenging to find in the documentation any help on this important issue.. Indeed, after creating an estimator (canned or custom), one wants to <code>tf.compile</code> the resulting predictor, produce the <code>.so</code> and link it to one's project..</p>

<p>So I have my calib class in which I define a simple linear estimator</p>

<pre><code>self.model = tf.estimator.LinearRegressor(
        feature_columns=self.feature_columns,
        model_dir = self.model_dir)
</code></pre>

<p>After training, I want to 
1- get the trained model with optimal parameters (load it in my variable self.model)</p>

<p>2- extract the graph and freeze it</p>

<p>3- tf.compile that graph</p>

<p>I could not find any way to do parts 1- and 2-. Once I have them, part 3 is solved by using tf.compile</p>

<p>Can you please point me to a good way to it?</p>
",1
53149059,Why do these two fc api act differently?,"<p>I was implementing the resnet and training on cifar-10 dataset. After ""global average pooling"" layer, I add 10-way fully connected layer. I used two tensorflow API to implement fc layer. However, the method 2 works to me, but the method 1 doesn't work. The method 1 is according to tensorflow official api: <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected"" rel=""nofollow noreferrer"">tf.contrib.layers.fully_connected</a>. The second method is according to <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/dense"" rel=""nofollow noreferrer"">tf.layers.dense</a>. I'm so confused why the first method doesn't work here. FYI, the output of global average pooling layer is [batch_size, 1, 1, 64]. Looking for some help! Thanks in advance!</p>

<p>Edit: Doesn't work means if I use the first method, all output logit of fc become zero, which doesn't make any sense to me. </p>

<pre><code># Method 1: 
def fc(x, n_outputs, scope):
  with tf.variable_scope(scope):
    # x = tf.layers.flatten(x)
    x = tf.contrib.layers.fully_connected(x, num_outputs=n_outputs, weights_initializer=weight_init, weights_regularizer=weight_reg, scope=scope)
    return x

# Method 2:
def fc(x, n_outputs, scope):
  with tf.variable_scope(scope):
     x = tf.layers.flatten(x)
     x = tf.layers.dense(x, units=n_outputs, kernel_initializer=weight_init, kernel_regularizer=weight_reg)
     return x
</code></pre>
",1
53164055,training tensorflow data API with procedural on-the-fly data generation,"<p>My question is similar to <a href=""https://stackoverflow.com/questions/47318734/on-the-fly-generation-with-dataset-api-tensorflow"">this</a> one, in that I would like to generate batches of training data on the fly. I have a function <code>get_random_batch(batch_size, input_path, target_path, **other_kwargs)</code> which returns <code>inputs</code> and <code>targets</code>, but it is a pure vanilla python / numpy function, not tensorflow. So it returns numpy arrays, not tensors. (The function is quite a large complex one with some 3rd party libs. To port it to tensorflow is not very feasible. To preprocess everything is also not feasible as the data is huge, and I don't have the space to store it all preprocessed! In fact I don't even train a single epoch, I just take random selections for hundreds of thousands of iterations).</p>

<p>For a few years I've been using tensorflow's low level training API: generate a batch of input-target pairs, run forward pass with feed (into placeholders) and fetch, calculate loss, apply gradients, repeat etc.</p>

<p>Now I finally would like to try out the newer data API (so that I can use the Keras API for building the model and 'fitting' etc), but I can't figure out how to migrate. All of the documentation I've seen assumes that the data loading and preprocessing is part of the graph, and the output of the dataset are already tensors. </p>

<p>--</p>

<p><strong>Update</strong>: Ok this seems to work with <strong>tf.data.Dataset.from_generator</strong></p>

<pre><code>compile_kwargs = dict(
    optimizer = tf.train.AdamOptimizer(3e-4),
    loss = 'mse',
    metrics = ['accuracy', 'mse']
)

def prepare_data():
    x,t = get_random_training_pair() # this returns one training pair (each np.float32 ndarrays)
    yield (x, t)

dataset = tf.data.Dataset.from_generator(prepare_data, (tf.float32, tf.float32))
dataset = dataset.batch(128).repeat()
model = build_keras_model()
model.compile(**compile_kwargs)
model.summary()
model.fit(dataset, epochs=10, steps_per_epoch=100, shuffle=False)
</code></pre>
",0
53206900,Sound way of managing multiple sessions and graphs,"<p>I'd like to manage multiple Keras models in multiple sessions. My application is constructed such that models can be live at the same time, in addition to creating, saving and loading them.</p>

<p><strong>What is the proper way of managing this situation?</strong></p>

<p>Currently one model is represented by an instance of a wrapper class. This is used in the training, saving, loading and prediction. One <code>tf.Graph</code> and <code>tf.Session</code> is created per instance, and they are used in every function requiring the actual model.</p>

<pre><code>class NN:
    def __init__(self):
        self.graph = tf.Graph()
        self.session = tf.Session(graph=self.graph)

    def predict(self, x):
        with self.graph.as_default():
            with self.session.as_default():
                return self.model.predict(x)
</code></pre>

<p>Similar functions using the <code>with</code> statements are created for compiling the network, fitting, saving (weights to .h5 and model to JSON) and loading. So whenever the model is needed, the graph and session are brought to context.</p>

<p>This resulted in a strange error (<a href=""https://stackoverflow.com/questions/53002518/poor-exit-code-when-managing-multiple-sessions"">Q</a> for further context), and I was left wondering, what is the standard way of dealing with this. I tried to release all possible resources before creating or loading a model, but it hasn't helped. This function is just a compilation of all possible routines scraped off the internet, and is purely guesswork.</p>

<pre><code>def _new_session(self):
    if self.session is not None:
        self.session.close()
    k.clear_session()
    gc.collect()
    self.graph = tf.Graph()
    self.session = tf.Session(graph=self.graph)
</code></pre>

<p>I've not found good documentation of a similar situation. So I'd very much appreciate any real insight into this.</p>

<hr>

<p>I might need to delete the old question, as it's quite all over the place. At the time of asking I had no idea what was going on. But it's there for now.</p>

<hr>

<p>Some specific questions have arisen.</p>

<ul>
<li>Loading and making predictions on a model works, compiling and fitting doesn't, although just compiling does. Do the two contexts differ in any way? Is the loaded model exactly the same?</li>
<li>At which points should a new context be created when manipulating the models? (e.g. at load, compilation, fitting, probably not with every prediction)</li>
<li>Which actions are necessary to take when releasing the resources of previous contexts? Either when a network is disposed of or when creating a new context.</li>
<li>Why exactly is the context switch needed for multiple models?</li>
<li>What are the roles of graph vs. session, given that different things are executed on the graph and session?</li>
</ul>

<h3>Updates</h3>

<ul>
<li>Compiling, fitting and saving one network works without any context trickery. Doing the same for another model in the same context works too (or at least does not produce an error).</li>
<li>In addition to above, <strong>loading the saved model and predicting works too</strong>, right after the training and for both models! Now I'm not sure whether the prediction is made correctly, but again, no error. This only begs the question I posed above even more: <em>why are the different contexts needed?</em></li>
</ul>

<hr>

<p>The underlying issue with the error has been finally (and somewhat embarassingly) <a href=""https://stackoverflow.com/questions/53002518/poor-exit-code-when-managing-multiple-sessions"">resolved</a> by updating all packages.</p>
",1
53242817,Which sorting algorithm does TensorFlow use?,"<p>I'm trying to figure out how TensorFlow's <code>sort</code> function works.</p>

<p>I've looked in the docs and tried to find in the source code without luck.</p>

<p>The function is <code>tf.contrib.framework.sort(values)</code>?
<a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/framework/sort"" rel=""nofollow noreferrer"">1</a></p>
",0
53255873,tf.metrics.accuracy and hand-written accuracy function give different results,"<p>I am trying to see how <code>tf.metrics.accuracy</code> works. I want to compare batch accuracy results of the function given below</p>

<pre><code>with tf.name_scope('Accuracy1'):
        correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))
        accuracy1 = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""accuracy"")
</code></pre>

<p>with </p>

<pre><code>with tf.name_scope('Accuracy2'):
        accuracy2, accuracy_op = tf.metrics.accuracy(labels=tf.argmax(y, 1), predictions=tf.argmax(predictions, 1))
</code></pre>

<p>Minimal working example is provided below:</p>

<pre><code>import numpy as np 
import pandas as pd 
import tensorflow as tf
import math

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)

num_steps=28
num_inputs = 28
num_classes = 10
num_neurons = 128
num_layers = 3
batch_size = 500

graph = tf.Graph()
with graph.as_default():
    with tf.name_scope(""graph_inputs""):
        X = tf.placeholder(tf.float32, [None, num_steps, num_inputs], name='input_placeholder')
        y = tf.placeholder(tf.float32, [None, num_classes], name='labels_placeholder')
       output_keep_prob = tf.placeholder_with_default(1.0, shape=(), name =""output_dropout"")

def build_lstm_cell(num_neurons, output_keep_prob):
    """"""Returns a dropout-wrapped LSTM-cell.
    See https://stackoverflow.com/a/44882273/2628369 for why this local function is necessary.
    Returns:
    tf.contrib.rnn.DropoutWrapper: The dropout-wrapped LSTM cell.
    """"""
    initializer = tf.contrib.layers.xavier_initializer()
    lstm_cell = tf.contrib.rnn.LSTMCell(num_units=num_neurons, initializer=initializer, forget_bias=1.0, state_is_tuple=True, name='LSTM_cell')
    lstm_cell_drop = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=output_keep_prob)
    return lstm_cell_drop

with tf.name_scope(""LSTM""):
    with tf.name_scope(""Cell""):
        multi_layer_cell = tf.contrib.rnn.MultiRNNCell([build_lstm_cell(num_neurons, output_keep_prob) for _ in range(num_layers)], state_is_tuple=True)
    with tf.name_scope(""Model""):
        outputs, states = tf.nn.dynamic_rnn(cell=multi_layer_cell, inputs=X, swap_memory=False, time_major = False, dtype=tf.float32)#[Batch_size, time_steps, num_neurons]
    with tf.name_scope(""Graph_Outputs""):
        outputs = tf.transpose(outputs, [1, 0, 2]) # [num_timesteps, batch_size, num_neurons]
        outputs = tf.gather(outputs, int(outputs.get_shape()[0]) - 1) # [batch_size, num_neurons]
    with tf.variable_scope('Softmax'):
        logits =  tf.layers.dense(inputs = outputs, units = num_classes, name=""logits"") #[Batch_size, num_classes]
    with tf.name_scope('Predictions'):
        predictions = tf.nn.softmax(logits, name=""predictions"")  #[Batch_size, num_classes]
    with tf.name_scope('Accuracy1'):
        correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))
        accuracy1 = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=""accuracy"")
    with tf.name_scope('Accuracy2'):
        accuracy2, accuracy_op = tf.metrics.accuracy(labels=tf.argmax(y, 1), predictions=tf.argmax(predictions, 1))
    with tf.name_scope('Loss'):
        xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y)
        loss = tf.reduce_mean(xentropy, name=""loss"")
    with tf.name_scope('Train'):
        optimizer= tf.train.AdamOptimizer(learning_rate=0.0001)
        trainer=optimizer.minimize(loss, name=""training_op"")

with tf.Session(graph = graph) as sess:
    tf.global_variables_initializer().run()
    total_batch = mnist.train.num_examples // batch_size
    for batch in range(total_batch):
        tf.local_variables_initializer().run()
        xBatch, yBatch = mnist.train.next_batch(batch_size)
        xBatch = xBatch.reshape((batch_size, num_steps, num_inputs))
        sess.run(trainer, feed_dict={X: xBatch, y: yBatch, output_keep_prob: 0.5})
        miniBatchAccuracy1 = sess.run(accuracy1, feed_dict={X: xBatch, y: yBatch, output_keep_prob: 0.5})
        print('[hand-written] Batch {} accuracy: {}'.format(batch, miniBatchAccuracy1))
        accuracy_op_val = sess.run(accuracy_op, feed_dict={X: xBatch, y: yBatch, output_keep_prob: 0.5})
        miniBatchAccuracy2 = sess.run(accuracy2)
        print(""[tf.metrics.accuracy] Batch {} accuracy: {}"".format(batch, miniBatchAccuracy2))
    sess.close()
</code></pre>

<p>I print the accuracy values of each batches using these two approaches and they are different. Should not the results be the same? </p>

<pre><code>[hand-written] Batch 0 accuracy: 0.09600000083446503
[tf.metrics.accuracy] Batch 0 accuracy: 0.09399999678134918

[hand-written] Batch 1 accuracy: 0.1120000034570694
[tf.metrics.accuracy] Batch 1 accuracy: 0.07800000160932541

[hand-written] Batch 2 accuracy: 0.10199999809265137
[tf.metrics.accuracy] Batch 2 accuracy: 0.09600000083446503

[hand-written] Batch 3 accuracy: 0.12999999523162842
[tf.metrics.accuracy] Batch 3 accuracy: 0.12800000607967377

[hand-written] Batch 4 accuracy: 0.1379999965429306
[tf.metrics.accuracy] Batch 4 accuracy: 0.10199999809265137

[hand-written] Batch 5 accuracy: 0.16200000047683716
[tf.metrics.accuracy] Batch 5 accuracy: 0.1340000033378601

[hand-written] Batch 6 accuracy: 0.1340000033378601
[tf.metrics.accuracy] Batch 6 accuracy: 0.12600000202655792

[hand-written] Batch 7 accuracy: 0.12999999523162842
[tf.metrics.accuracy] Batch 7 accuracy: 0.16200000047683716
...
...
...
...
</code></pre>
",0
53272508,inception v3 using tf.data?,"<p>I'm using a bit of code that is derived from inception v3 as distributed by the Google folks, but it's now complaining that the queue runners used to read the data are deprecated (tf.train.string_input_producer in image_processing.py, and similar).  Apparently I'm supposed to switch to tf.data for this kind of stuff.</p>

<p>Unfortunately, the documentation on tf.data isn't doing much to relieve my concern that I've got too much data to fit in memory, especially given that I want to batch it in a reusable way, etc. I'm confident that the tf.data stuff <em>can</em> do this; I just don't know <em>how</em> to do it. Can anyone point me to a full example of code that uses tf.data to deal with batches of data that won't all fit in memory?  Ideally, it would simply be an updated version of the inception-v3 code, but I'd be happy to try and work with anything.  Thanks!</p>
",1
53300337,variable_scope does not get reused when using default scope name,"<p>I have a question regarding sub-scopes when reusing variables. This</p>

<pre><code>import tensorflow as tf

def make_bar():
  with tf.variable_scope('bar'):
    tf.get_variable('baz', ()) 

with tf.variable_scope('foo') as scope:
  make_bar()
  scope.reuse_variables()
  make_bar()
</code></pre>

<p>works perfectly fine, only a single variable <code>foo/bar/baz</code> is created.</p>

<p>However, if I change <code>make_bar</code> to</p>

<pre><code>def make_bar(scope=None):
  with tf.variable_scope(scope, 'bar'):
    tf.get_variable('baz', ()) 
</code></pre>

<p>the code now fails with a </p>

<pre><code>ValueError: Variable foo/bar_1/baz does not exist
</code></pre>

<p>Question: why does variable scope reuse fail when using <code>default name</code>s? If it is on purpose, what is the rationale behind this choice?</p>

<p><strong>EDIT</strong></p>

<p>Some precisions on the <code>default_name</code> argument of <code>tf.variable_scope</code>. <a href=""https://www.tensorflow.org/api_docs/python/tf/variable_scope#__init__"" rel=""nofollow noreferrer"">From the documentation,</a></p>

<blockquote>
  <ul>
  <li><code>default_name</code>: The default name to use if the <code>name_or_scope</code> argument is <code>None</code>, this name will be uniquified. If <code>name_or_scope</code> is provided it won't be used and therefore it is not required and can be <code>None</code>.</li>
  </ul>
</blockquote>

<p>So as it name underlies, it is a way to provide a default scope name.</p>

<p>In the first version of <code>make_bar</code>, the scope name is forced to be <code>bar</code> -- the function has no parameter to change it.</p>

<p>In the second version of <code>make_bar</code>, I enhance this function to make it parameterizable. So <code>bar</code> is still the default scope name (provided this time as the <code>default_name</code> argument of <code>tf.variable_scope</code>), but this time the caller has the possibility to change it by setting the default argument <code>scope</code> of <code>make_bar</code> to anything else than <code>None</code>.`</p>

<p>When this second version of <code>make_bar</code> is used without an argument, it should, I think, fall back to the behavior of the first version -- which it does not.</p>

<p>Note that in my example, <code>bar</code> is intended to be a subscope of <code>foo</code>. The variable to be reused is <em>meant</em> to be <code>foo/bar/baz</code>.</p>
",0
53301015,Tensorflow - How to Convert int32 to string (using Python API for Tensorflow),"<p>Simple question really but cannot seem to find a function in the TensorFlow docs or by googling.</p>

<p>How can I convert a tensor of type <code>tf.int32</code> to one of type <code>tf.string</code>?</p>

<p>I tried simply casting it with something like this:</p>

<pre><code>x = tf.constant([1,2,3], dtype=tf.int32)
x_as_string = tf.cast(x, dtype=tf.string) # hoping for this output: [ '1', '2', '3' ]

with tf.Session() as sess:
  res = sess.run(x_as_string)
</code></pre>

<p>but hit the error message:</p>

<blockquote>
  <p>Cast int32 to string is not supported</p>
</blockquote>

<p>Is there a simple function somewhere in the documentation that I am missing?</p>

<hr>

<p>UPDATE:</p>

<p>To clarify: I realise I could 'work around' this issue using a python function with <code>tf.py_func</code> but asking if there is a solution in TensorFlow itself</p>
",0
53317523,Part 3: Switching between multiple contexts - no error and a bad exit code,"<p>I've been struggling with managing multiple Keras models with <code>tf.Graphs</code> and <code>tf.Sessions</code> for several weeks now. In short, I'd like to have multiple models open and switch between them as needed. This includes training new models, opening from file and making predictions.</p>

<p>The bottom line is: (almost) everything works fine until the program crashes with exit code <code>0xC0000005</code>. No error messages are given. Let me explain.</p>

<ul>
<li>I can load a model and make predictions on it. Results are recieved, and after printing them, the program crashes.</li>
<li>I can load multiple models and make predictions on them. Then the program crashes.</li>
<li>I can create a new model, and make predictions on it. Finally, the program crashes.</li>
<li>I cannot create two models, even the same model twice with different instances of the class below. The program crashes.</li>
</ul>

<p>You get the point. This is how I currently manage the graphs and sessions. I use a context manager to set the created graph and session as defaults and later switch to the previous state.</p>

<pre><code>class NeuralNetwork:
    def __init__(self):
        self.graph = tf.Graph()
        self.session = tf.Session(graph=self.graph)
        self.model = None

    def close(self):
        self.session.close()
        del self.graph
        self.graph = None
        gc.collect()

    @contextmanager
    def _context(self):
        prev = k.get_session()
        k.set_session(self.session)
        with self.graph.as_default(), self.session.as_default():
            yield
        k.set_session(prev)

    def predict(self, x):
        with self._context():
            return self.model.predict(x)

    def fit(self, x_train, y_train, n=20, batch=256):
        with self._context():
            self.model.fit(x_train, y_train, epochs=n, batch_size=batch, verbose=0)

    def create(self, shape):
        with self._context():
            self.model = Sequential()
            self.model.add(Dense(shape[1], input_dim=shape[0], activation='relu'))
            self.model.add(Dropout(drop))
            self.model.add(Dense(shape[2], activation='sigmoid'))
            self.model.compile(loss='binary_crossentropy', optimizer='rmsprop')

    def load(self, path, sfx=''):
        with open(path / ('architecture' + sfx + '.json'), 'r') as f:
            js = f.read()

        with self._context():
            self.model = model_from_json(js)
            self.model.load_weights(path / ('weights' + sfx + '.h5'))
            self.model.compile(loss='binary_crossentropy', optimizer='rmsprop')

    def save(self, path, sfx=''):
        path.mkdir(exist_ok=True)
        with self._context():
            js = self.model.to_json()
            with open(path / ('architecture' + sfx + '.json'), 'w') as f:
                f.write(js)
            self.model.save_weights(path / ('weights' + sfx + '.h5'))
</code></pre>

<p>And with the above class, here's how a network is used elsewhere:</p>

<pre><code>def create(self):
    x, y = [], []
    shape = (15, 30, 1)

    self.predictor = NeuralNetwork()
    self.predictor.create(shape)
    self.predictor.fit(x, y)
    self.predictor.save(path=self.path)
    self.predictor.close()

def load(self):
    self.predictor.load(path=self.path)

def predict(x):
    # Executed only on loaded networks, never on created networks
    # due to program structure
    return self.predictor.predict(x)
</code></pre>

<p>Here are my previous efforts at articulating the problem.</p>

<ul>
<li><a href=""https://stackoverflow.com/questions/53002518/poor-exit-code-when-managing-multiple-sessions"">Part 1</a>, the one where I had no clue</li>
<li><a href=""https://stackoverflow.com/questions/53206900/sound-way-of-managing-multiple-sessions-and-graphs"">Part 2</a>, the one where I started to figure things out</li>
</ul>

<p>To the best of my abilities and with the help of some people, I've tried to come up with a way to manage these resources (context manager and ""closing"" the network after training). But I have not come across documentation or a tutorial describing the process of Tensorflow or Keras resource management in detail.</p>

<hr>

<p>My goals are two-fold.</p>

<ul>
<li>First and foremost, get rid of this error</li>
<li>Hopefully learn the absolutely correct way of dealing with this scenario</li>
</ul>

<p>If you can help me achieve or even step a tiny amount towards the direction of either one, I'd greatly appreciate it! I have the experience, that my struggles are neither unique nor something that others haven't already thought of. So I must just be lacking the proper approach.</p>
",1
53344172,Tensorflow: tf.contrib deprecation,"<p>I've been doing some NLP using tensorflow and have been using <code>tf.contrib.lookup.index_table_from_tensor</code> to create a look-up table for tokens.</p>

<p>I know tensorflow is coming out with 2.0 in Spring '19 and they've said:</p>

<p>""tf.contrib</p>

<p>TensorFlow’s contrib module has grown beyond what can be maintained and supported in a single repository. Larger projects are better maintained separately, while we will incubate smaller extensions along with the main TensorFlow code. Consequently, as part of releasing TensorFlow 2.0, we will stop distributing tf.contrib. We will work with the respective owners on detailed migration plans in the coming months, including how to publicise your TensorFlow extension in our community pages and documentation. For each of the contrib modules we will either a) integrate the project into TensorFlow; b) move it to a separate repository or c) remove it entirely. This does mean that all of tf.contrib will be deprecated, and we will stop adding new tf.contrib projects today. We are looking for owners/maintainers for a number of projects currently in tf.contrib, please contact us (reply to this email) if you are interested.""</p>

<p>Two questions then:</p>

<p>1) Is there another way to build a lookup-table easily in Tensorflow?</p>

<p>2) I couldn't determine how likely it is that this functionality will be included in 2.0.  Is there any indication?</p>

<p>Thanks!</p>
",0
53360012,How do I resolve an InvalidArgumentError in Classifier model?,"<p>New to TensorFlow, so apologies for newbie question.</p>

<p>Following this <a href=""https://www.tensorflow.org/tutorials/eager/custom_training_walkthrough"" rel=""nofollow noreferrer"">tutorial</a> but instead of using image data I am using numerical data.</p>

<p><strong>Load the dataset:</strong></p>

<pre><code>train_dataset_url = ""xxx.csv""
train_dataset_fp = tf.keras.utils.get_file(
  fname=os.path.basename(train_dataset_url),
  origin=train_dataset_url)
</code></pre>

<p><strong>Make training dataset:</strong></p>

<pre><code>batch_size = 32

train_dataset = tf.contrib.data.make_csv_dataset(
    train_dataset_fp,
    batch_size, 
    column_names=column_names,
    label_name=label_name,
    num_epochs=1)
</code></pre>

<p><strong>Train classified model using:</strong></p>

<p><code>model = tf.keras.Sequential([
  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(1,)),
  tf.keras.layers.Dense(10, activation=tf.nn.relu),
  tf.keras.layers.Dense(4)
])</code></p>

<p>But when I ""test"" the model with the same inputs:</p>

<p><code>predictions = model(features)</code></p>

<p>I receive the error:</p>

<p><code>InvalidArgumentError: cannot compute MatMul as input #0(zero-based) was expected to be a float tensor but is a int32 tensor [Op:MatMul]</code></p>

<p>It's possible I have missed something fundamental. I feel like I need to specify a type somewhere.</p>
",0
53366491,Can the TensorFlow's map_and_batch be used with padded batches?,"<p>From TensorFlow's article on <a href=""https://www.tensorflow.org/guide/performance/datasets"" rel=""nofollow noreferrer"">Data Input Pipeline Performance</a> I can see that in some cases it makes sense to use the <code>map_and_batch</code> function. However, I use padded shapes (<code>tf.data.padded_batch()</code>) and would like to know if there's still a way to use the suggested <code>map_and_batch</code>. The documentation doesn't explain this case.</p>

<p>Thank you</p>
",1
53367063,"tensorflow python expected dense_input to have 2 dimensions, but got array with shape (5, 28, 5)","<p>I am a complete newbie to tensorflow, trying to learn about it and solve a problem.  I tried a lot of tutorials but they all talked about the same classify image or mnist stuff, so I followed the documentation and tried to figure something out.  </p>

<p>The goal is to find a pattern to predict the result when the input is [[1000,10, 5, 3, 1744...etc.  There are only 5 cases when the value is 300 400, 500, 600, 700, with shape 28,5 and the result for each is 28,2 list.  The data is loaded from file and assigned to tf.tensor.  </p>

<p>Here's my code:</p>

<pre><code>model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(28, activation=tf.nn.relu, input_shape=(5,)))
model.add(tf.keras.layers.Dense(28, activation=tf.nn.relu, input_shape=(5,)))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(28))

model.compile(optimizer='adam',
              loss='mean_squared_error',
              metrics=['accuracy'])

model.fit(newData, newResults, epochs=3, steps_per_epoch=5)
</code></pre>

<p>newData:</p>

<pre><code>[[[300, 10, 5, 3, 1744], [300, 10, 5, 5, 2848], [300, 10, 5, 4, 2418], [300, 10, 5, 2, 1152], [300, 10, 5, 3, 1126], [300, 10, 5, 3, 1897], [300, 10, 5, 3, 1089], [300, 10, 5, 2, 1581], [300, 10, 5, 4, 1793], [300, 10, 5, 3, 1525], [300, 10, 5, 2, 1529], [300, 10, 5, 3, 1052], [300, 10, 5, 2, 1556], [300, 10, 5, 3, 1569], [300, 10, 5, 5, 2873], [300, 10, 5, 4, 2269], [300, 10, 5, 3, 3003], [300, 10, 5, 3, 1310], [300, 10, 5, 3, 1464], [300, 10, 5, 3, 2807], [300, 10, 5, 2, 1262], [300, 10, 5, 3, 1734], [300, 10, 5, 2, 2709], [300, 10, 5, 3, 2234], [300, 10, 5, 3, 1961], [300, 10, 5, 2, 1594], [300, 10, 5, 2, 1836], [300, 10, 5, 2, 1345]], 
[[400, 10, 5, 3, 1744], [400, 10, 5, 5, 2848], [400, 10, 5, 4, 2418], [400, 10, 5, 2, 1152], [400, 10, 5, 3, 1126], [400, 10, 5, 3, 1897], [400, 10, 5, 3, 1089], [400, 10, 5, 2, 1581], [400, 10, 5, 4, 1793], [400, 10, 5, 3, 1525], [400, 10, 5, 2, 1529], [400, 10, 5, 3, 1052], [400, 10, 5, 2, 1556], [400, 10, 5, 3, 1569], [400, 10, 5, 5, 2873], [400, 10, 5, 4, 2269], [400, 10, 5, 3, 3003], [400, 10, 5, 3, 1310], [400, 10, 5, 3, 1464], [400, 10, 5, 3, 2807], [400, 10, 5, 2, 1262], [400, 10, 5, 3, 1734], [400, 10, 5, 2, 2709], [400, 10, 5, 3, 2234], [400, 10, 5, 3, 1961], [400, 10, 5, 2, 1594], [400, 10, 5, 2, 1836], [400, 10, 5, 2, 1345]], 
[[500, 10, 5, 3, 1744], [500, 10, 5, 5, 2848], [500, 10, 5, 4, 2418], [500, 10, 5, 2, 1152], [500, 10, 5, 3, 1126], [500, 10, 5, 3, 1897], [500, 10, 5, 3, 1089], [500, 10, 5, 2, 1581], [500, 10, 5, 4, 1793], [500, 10, 5, 3, 1525], [500, 10, 5, 2, 1529], [500, 10, 5, 3, 1052], [500, 10, 5, 2, 1556], [500, 10, 5, 3, 1569], [500, 10, 5, 5, 2873], [500, 10, 5, 4, 2269], [500, 10, 5, 3, 3003], [500, 10, 5, 3, 1310], [500, 10, 5, 3, 1464], [500, 10, 5, 3, 2807], [500, 10, 5, 2, 1262], [500, 10, 5, 3, 1734], [500, 10, 5, 2, 2709], [500, 10, 5, 3, 2234], [500, 10, 5, 3, 1961], [500, 10, 5, 2, 1594], [500, 10, 5, 2, 1836], [500, 10, 5, 2, 1345]], 
[[600, 10, 5, 3, 1744], [600, 10, 5, 5, 2848], [600, 10, 5, 4, 2418], [600, 10, 5, 2, 1152], [600, 10, 5, 3, 1126], [600, 10, 5, 3, 1897], [600, 10, 5, 3, 1089], [600, 10, 5, 2, 1581], [600, 10, 5, 4, 1793], [600, 10, 5, 3, 1525], [600, 10, 5, 2, 1529], [600, 10, 5, 3, 1052], [600, 10, 5, 2, 1556], [600, 10, 5, 3, 1569], [600, 10, 5, 5, 2873], [600, 10, 5, 4, 2269], [600, 10, 5, 3, 3003], [600, 10, 5, 3, 1310], [600, 10, 5, 3, 1464], [600, 10, 5, 3, 2807], [600, 10, 5, 2, 1262], [600, 10, 5, 3, 1734], [600, 10, 5, 2, 2709], [600, 10, 5, 3, 2234], [600, 10, 5, 3, 1961], [600, 10, 5, 2, 1594], [600, 10, 5, 2, 1836], [600, 10, 5, 2, 1345]], 
[[700, 10, 5, 3, 1744], [700, 10, 5, 5, 2848], [700, 10, 5, 4, 2418], [700, 10, 5, 2, 1152], [700, 10, 5, 3, 1126], [700, 10, 5, 3, 1897], [700, 10, 5, 3, 1089], [700, 10, 5, 2, 1581], [700, 10, 5, 4, 1793], [700, 10, 5, 3, 1525], [700, 10, 5, 2, 1529], [700, 10, 5, 3, 1052], [700, 10, 5, 2, 1556], [700, 10, 5, 3, 1569], [700, 10, 5, 5, 2873], [700, 10, 5, 4, 2269], [700, 10, 5, 3, 3003], [700, 10, 5, 3, 1310], [700, 10, 5, 3, 1464], [700, 10, 5, 3, 2807], [700, 10, 5, 2, 1262], [700, 10, 5, 3, 1734], [700, 10, 5, 2, 2709], [700, 10, 5, 3, 2234], [700, 10, 5, 3, 1961], [700, 10, 5, 2, 1594], [700, 10, 5, 2, 1836], [700, 10, 5, 2, 1345]]]
</code></pre>

<p>newResult:</p>

<pre><code>[[[29.0, 8.92], [52.0, 21.67], [41.0, 14.38], [7.0, 1.49], [26.0, 8.25], [18.0, 4.53], [24.0, 6.61], [21.0, 9.54], [17.0, 5.53], [27.0, 9.61], [11.0, 0.35], [22.0, 8.11], [7.0, 1.22], [36.0, 15.49], [57.0, 31.44], [43.0, 16.52], [34.0, 11.46], [15.0, 2.49], [20.0, 2.34], [16.0, 4.86], [10.0, 0.8], [8.0, 0.4], [1.0, 0.0], [30.0, 7.57], [24.0, 7.21], [5.0, 0.58], [14.0, 0.73], [4.0, 0.15]], 
[[45.0, 8.17], [100.0, 43.28], [54.0, 16.05], [10.0, 2.77], [37.0, 8.86], [27.0, 6.12], [33.0, 9.13], [34.0, 14.03], [20.0, 5.06], [45.0, 15.42], [21.0, 0.69], [26.0, 8.83], [11.0, 2.14], [44.0, 17.74], [73.0, 43.39], [43.0, 18.8], [46.0, 21.56], [29.0, 9.16], [21.0, 3.76], [20.0, 7.39], [16.0, 2.54], [1.0, 1.63], [1.0, 0.02], [28.0, 12.14], [30.0, 12.35], [7.0, 1.18], [19.0, 3.29], [4.0, 0.16]], 
[[59.0, 18.74], [100.0, 75.18], [69.0, 32.13], [11.0, 3.04], [49.0, 15.76], [30.0, 10.33], [45.0, 14.51], [43.0, 20.82], [37.0, 8.2], [69.0, 24.53], [1.0, 0.3], [38.0, 12.57], [1.0, 3.67], [65.0, 24.77], [91.0, 57.39], [53.0, 18.22], [47.0, 27.07], [34.0, 16.31], [25.0, 5.39], [31.0, 11.5], [23.0, 5.73], [19.0, 4.11], [2.0, 0.11], [35.0, 15.52], [41.0, 18.15], [7.0, 1.48], [25.0, 7.53], [3.0, 0.14]], 
[[80.0, 30.29], [100.0, 85.22], [94.0, 52.73], [11.0, 2.45], [72.0, 30.7], [46.0, 14.75], [70.0, 22.81], [50.0, 28.26], [40.0, 14.19], [60.0, 26.82], [14.0, 0.28], [45.0, 19.1], [16.0, 4.72], [82.0, 40.98], [100.0, 78.96], [66.0, 27.05], [67.0, 31.09], [34.0, 16.92], [23.0, 7.03], [48.0, 21.28], [27.0, 8.19], [21.0, 3.95], [2.0, 0.17], [43.0, 19.96], [55.0, 23.54], [8.0, 1.47], [28.0, 12.04], [4.0, 0.13]], 
[[95.0, 38.09], [100.0, 92.88], [99.0, 58.96], [13.0, 3.54], [96.0, 45.78], [33.0, 12.05], [87.0, 38.11], [62.0, 34.97], [48.0, 15.49], [84.0, 33.13], [10.0, 0.09], [63.0, 25.52], [16.0, 4.87], [100.0, 55.9], [100.0, 91.32], [90.0, 34.24], [96.0, 45.36], [37.0, 15.13], [27.0, 9.28], [49.0, 26.3], [30.0, 10.92], [22.0, 3.72], [3.0, 0.14], [67.0, 24.82], [73.0, 31.32], [8.0, 1.36], [31.0, 15.03], [4.0, 0.2]]]
</code></pre>

<p>Getting this error when I run it:</p>

<pre><code>  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1536, in fit
    validation_split=validation_split)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 992, in _standardize_user_data
    class_weight, batch_size)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1117, in _standardize_weights
    exception_prefix='input')
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\keras\engine\training_utils.py"", line 323, in standardize_input_data
    'with shape ' + str(data_shape))
ValueError: Error when checking input: expected dense_input to have 2 dimensions, but got array with shape (5, 28, 5)
</code></pre>

<p>I know my model definitely has something wrong with it, but I can't quite figure out what.  I have trouble finding information other than the afore mentioned examples.</p>
",0
53367724,Theoretical underpinning behind Hardmax operator,"<p>In the tensor flow Github repository, in the file <a href=""https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L1025"" rel=""nofollow noreferrer"">attentionwrapper.py</a>, hardmax operator has been defined. On the docs, it has been mentioned <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/hardmax"" rel=""nofollow noreferrer"">tf.contrib.seq2seq.hardmax</a></p>

<p>I want to know what's the theoretical underpinning behind providing this functionality for hardmax operator. Prima facie google searches for past few weeks haven't led me to concrete understanding of the concept. </p>

<ol>
<li><p>If softmax is differentiable (soft), why would hardmax be ever used? If it can't be used in back propagation (due to non-differentiability required in gradient calculation), where else can it be used?</p></li>
<li><p>Reinforcement learning literature talks about Soft vs Hard attention. However I couldn't see concrete examples nor explanations of where the <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/hardmax"" rel=""nofollow noreferrer"">tf.contrib.seq2seq.hardmax</a> can be actually used in some RL model.</p></li>
<li><p>By the looks of it, since it is mentioned in seq2seq, it should be obviously having some application in Natural Language Processing. But exactly where? There are tonnes of NLP tasks. Couldn't find any direct task SOTA algorithm which uses hardmax.</p></li>
</ol>
",0
53470309,Using tf.batch_scatter_add in Keras,"<p>I'm looking for a way to use <a href=""https://www.tensorflow.org/api_docs/python/tf/scatter_add"" rel=""nofollow noreferrer"">tf.scatter_add</a> with Keras batches. 
Shape of outputs: <code>(?, 1000)</code> and shapes of indices and updates are <code>(?, 100)</code> each. </p>

<p>Try1: Using Keras tensors </p>

<pre><code>vals = tf.scatter_add(outputs, indices, updates)
</code></pre>

<p>This throws an error: </p>

<blockquote>
  <p>'Tensor' object has no attribute '_lazy_read'</p>
</blockquote>

<p>Try2: Tried using k.variable which should be updatable </p>

<pre><code>vals = K.variable(outputs)
vals = tf.scatter_add(vals, inputs[1], inputs[2]) 
</code></pre>

<blockquote>
  <p>ValueError: initial_value must have a shape specified: <br>
      Tensor(""scatter_add_43/zeros_like:0"", shape=(?, 1000), dtype=float32))</p>
</blockquote>

<p>Any clues? <code>Scatter_add</code> and <code>batch_scatter_add</code> result in the same errors. Will I need to write a custom layer for this? Seems like even that will run into one of the above errors. </p>
",0
53486698,tensorflow LinearClassifier predict function,"<p>I've been playing around with tensorflow and because of lack of documentation I'm having a lot of troubles to understand some of the function outputs. </p>

<p>In this case, for the module <strong>tf.estimator.LinearClassifier</strong> I don't understand what the parameters in the dictionary returned by the function <strong>predict()</strong> mean. Below there's an example:</p>

<blockquote>
  <p>{'probabilities': array([0.92966837, 0.07033166], dtype=float32),
  'logits': array([-2.581606], dtype=float32), 'classes': array(['0'],
  dtype=object), 'class_ids': array([0]), 'logistic':
  array([0.07033166], dtype=float32)}</p>
</blockquote>

<p>I assume that the array ""probabilities"" contains = [1-p, p], where ""p"" is the probability and I understand the ""classes"" and ""class_ids"" meaning. </p>

<p>However I get lost with the other dict keys. Can someone point me to some good documentation where the outputs of functions are explained?</p>

<p>Thanks!</p>
",1
53527368,Translating tensorflows conv2d to numpy/scipy operations?,"<p>This is the documentation for tf.nn.conv2d: Given an input tensor of shape [batch, in_height, in_width, in_channels] and a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels], this op performs the following</p>

<ol>
<li>Flattens the filter to a 2-D matrix with shape [filter_height *
filter_width * in_channels, </li>
<li>Extracts image patches from the input tensor to form a virtual tensor of shape [batch, out_height, out_width, filter_height *
filter_width * in_channels].</li>
<li>For each patch, right-multiplies the filter matrix and the image patch vector.</li>
</ol>

<p>In other words, it takes in a tensor of n images and does convolution with out_channel filters. </p>

<p>I am trying to translate to code that uses only numpy operations and the code is the following:</p>

<pre><code>def my_conv2d(x, kernel):
   nf = kernel.shape[-1]  # number of filters
   rf = kernel.shape[0]  # filter size
   w = kernel
   s = 1 # stride

   h_range = int((x.shape[2] - rf) / s) + 1  # (W - F + 2P) / S
   w_range = int((x.shape[1] - rf) / s) + 1  # (W - F + 2P) / S
   np_o = np.zeros((1, h_range, w_range, nf))
   for i in range(x.shape[0]):
     for z in range(nf):
       for _h in range(h_range):
         for _w in range(w_range):
           np_o[0, _h, _w, z] = np.sum(x[i, _h * s:_h * s + rf, _w * s:_w * s 
                                + rf, * w[:, :, :, z])                     
    return np_o
</code></pre>

<p>The problem is that code is extremely slow. Are there any numpy or scipy functions that can replicate what tensorflows' conv2d is doing that is of similar efficiency? I have looked at <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html"" rel=""nofollow noreferrer"">https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html</a> and it does convolution ONCE, meaning I have to pass a 2d tensor alongside a 2d kernel (it does not do multiple filters).
None of the previous stackoverflow questions helped much with this.</p>

<p>Thanks</p>

<p>Edit: did some testing and my code is about 44000% slower than doing tf.nn.conv2d!</p>
",0
53568337,Print accuracy when training tf.estimator.DNNClassifier,"<p>I am new to tensorflow, using <a href=""https://www.tensorflow.org/guide/custom_estimators"" rel=""nofollow noreferrer"">official tutorial</a> tf.estimator.DNNClassifier and custom estimator to build simple NN to solve classification problem.</p>

<p>While training :</p>

<pre><code>dnn_model = tf.estimator.DNNClassifier(hidden_units=[10,10,10],
                                       feature_columns = my_features_column,
                                       n_classes=5,
                                       optimizer = tf.train.AdamOptimizer()
                                      )

dnn_model.train(input_fn=train_input_func)
</code></pre>

<p>It will report loss at specific time as following:</p>

<pre><code>INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from /tmp/tmphwkvj5le/model.ckpt-150
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 150 into /tmp/tmphwkvj5le/model.ckpt.
INFO:tensorflow:loss = 133.04277, step = 150
INFO:tensorflow:global_step/sec: 115.114
INFO:tensorflow:loss = 128.15938, step = 250 (0.872 sec)
INFO:tensorflow:global_step/sec: 134.317
INFO:tensorflow:loss = 123.093094, step = 350 (0.743 sec)
INFO:tensorflow:global_step/sec: 133.573
INFO:tensorflow:loss = 117.80729, step = 450 (0.748 sec)
INFO:tensorflow:global_step/sec: 135.081
INFO:tensorflow:loss = 114.07168, step = 550 (0.741 sec)
INFO:tensorflow:Saving checkpoints for 650 into /tmp/tmphwkvj5le/model.ckpt.
INFO:tensorflow:Loss for final step: 118.19583.
</code></pre>

<p>I want to print classification accuracy every batch or epoch, likes the log Info in keras:</p>

<pre><code>Epoch 1/20
5000/5000 [==============================] - 1s 157us/step - loss: 1.4885 - acc: 0.3276 - val_loss: 1.4397 - val_acc: 0.3620
Epoch 2/20
5000/5000 [==============================] - 0s 66us/step - loss: 1.3792 - acc: 0.3922 - val_loss: 1.4001 - val_acc: 0.3768
.
.
</code></pre>

<p>How can I find the tutorial on this problem ? All I find were talking about more lower API (tensor, session, etc.).</p>
",1
53569622,Difference between tf.train.Checkpoint and tf.train.Saver,"<p>I found there are different ways to save/restore models and variables in <code>Tensorflow</code>. These ways including:</p>

<ul>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/saved_model/simple_save"" rel=""nofollow noreferrer"">tf.saved_model.simple_save</a></li>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint"" rel=""nofollow noreferrer"">tf.train.Checkpoint</a></li>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/train/Saver"" rel=""nofollow noreferrer"">tf.train.Saver</a></li>
</ul>

<p>In tensorflow's documentations, I found some differences between them:</p>

<ol>
<li><code>tf.saved_model</code> is a thin wrapper around <code>tf.train.Saver</code></li>
<li><code>tf.train.Checkpoint</code> support eager execution but <code>tf.train.Saver</code> <strong>not</strong>.</li>
<li><code>tf.train.Checkpoint</code> not creating <code>.meta</code> file but still can load graph structure (here is a big question! how it can do that?)</li>
</ol>

<p>How <code>tf.train.Checkpoint</code> can load graph without <code>.meta</code> file? or more generally What is the difference between <code>tf.train.Saver</code> and <code>tf.train.Checkpoint</code>?</p>
",1
53572533,What is the second argument of TensorFlow's tf.data.filter() that I find no documentation of?,"<p>I recently had a <code>TypeError</code> when using</p>

<pre><code>def lie_filter(line):
    return tf.equal(line['lie_id'], 2)
</code></pre>

<p>in</p>

<pre><code>dataset = (
    tf.data
    .TextLineDataset('shots.csv')
    .skip(1)
    .map(decode_line)
    .filter(lie_filter)
    .cache())
</code></pre>

<p>The exact error was <code>TypeError: lie_filter() takes 1 positional argument but 2 were given</code>.</p>

<p>Simply changing the function signature to <code>lie_filter(line, x)</code> made the error go away and the filtering appears to work as intended. However, it left me wondering what is this mysterious second argument.</p>

<p><a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#filter"" rel=""nofollow noreferrer"">TensorFlow manual for tf.data.filter()</a> only specifies one argument. There are also numerous examples by TensorFlow where filtering is done as per my attempt above. Take a look at, e.g., <a href=""https://github.com/tensorflow/tensorflow/blob/6d14dcba7225d205f2e7834551f42385802aa2cf/tensorflow/examples/get_started/regression/imports85.py#L116"" rel=""nofollow noreferrer"">imports85.py</a>.</p>

<p>Printing the <code>x</code> inside <code>lie_filter</code> yields <code>Tensor(""arg12:0"", shape=(), dtype=float32)</code>.</p>

<p>What is the second argument and where can I find documentation about it?</p>

<p>Thank you!</p>
",1
53578484,tf.gather with indices of higher dimention than input data?,"<p>Reading <a href=""https://github.com/WangYueFt/dgcnn"" rel=""nofollow noreferrer"">Dynamic Graph CNN for Learning on Point Clouds</a> code, I came across this snippet:</p>

<pre><code>  idx_ = tf.range(batch_size) * num_points
  idx_ = tf.reshape(idx_, [batch_size, 1, 1]) 

  point_cloud_flat = tf.reshape(point_cloud, [-1, num_dims])
  point_cloud_neighbors = tf.gather(point_cloud_flat, nn_idx+idx_)  &lt;--- what happens here?
  point_cloud_central = tf.expand_dims(point_cloud_central, axis=-2)
</code></pre>

<p>debugging the line I made sure that the dims are</p>

<pre><code>point_cloud_flat:(32768,3) nn_idx:(32,1024,20), idx_:(32,1,1) 
// indices are (32,1024,20) after broadcasting
</code></pre>

<p>Reading the <a href=""https://www.tensorflow.org/api_docs/python/tf/gather"" rel=""nofollow noreferrer"">tf.gather doc</a> I couldn't understand what the function does with dimensions higher that the input dimensions</p>
",1
53580335,Group by Regression in TensorFlow,"<p>I am very new to TensorFlow - so please bear with me if this is a trivial question.</p>

<p>I'm coding in Python+TensorFlow. I have a dataframe with the following structure - </p>

<p>Y | X_1 | X_2 | ... | X_p | Grp </p>

<p>where Y is the continuous response, X_1 through X_p are features, and Grp is a categorical value indicating group. I want to fit a separate linear regression of Y on (X_1,...,X_p)for each Grp and save the weights/coefficients. I do not want to use the out of the shelf tf.estimator.LinearRegressor. Instead I want to go the loss function-optimizer-session.run() route. </p>

<p>The relevant tutorial pages on internet talk about linear regression but not per group. I would appreciate any suggestions. I am thinking to do this - </p>

<p>For each g in Grps : 
 1. Call the optimizer by passing the data for Group g as the placeholders.
 2. Get the estimated weights (for Group g) and save them in a dataframe : Grp | weights</p>

<p>Another approach that sounds reasonable is to have separate graphs for each group and kick them all together using various ""sessions"".</p>

<p>Are these reasonable and feasible in TF? Which one is easier or are there better approaches? </p>

<p>Thank you,
Sai</p>
",0
53583456,What problem does a reinitializable iterator solve?,"<p>From the <a href=""https://www.tensorflow.org/guide/datasets#creating_an_iterator"" rel=""nofollow noreferrer"">tf.data documentation</a>:</p>

<blockquote>
  <p>A reinitializable iterator can be initialized from multiple different
  Dataset objects. For example, you might have a training input pipeline
  that uses random perturbations to the input images to improve
  generalization, and a validation input pipeline that evaluates
  predictions on unmodified data. These pipelines will typically use
  different Dataset objects that have the same structure (i.e. the same
  types and compatible shapes for each component).</p>
</blockquote>

<p>the following example was given:</p>

<pre><code># Define training and validation datasets with the same structure.
training_dataset = tf.data.Dataset.range(100).map(
    lambda x: x + tf.random_uniform([], -10, 10, tf.int64))
validation_dataset = tf.data.Dataset.range(50)

# A reinitializable iterator is defined by its structure. We could use the
# `output_types` and `output_shapes` properties of either `training_dataset`
# or `validation_dataset` here, because they are compatible.
iterator = tf.data.Iterator.from_structure(training_dataset.output_types,
                                           training_dataset.output_shapes)
next_element = iterator.get_next()

training_init_op = iterator.make_initializer(training_dataset)
validation_init_op = iterator.make_initializer(validation_dataset)

# Run 20 epochs in which the training dataset is traversed, followed by the
# validation dataset.
for _ in range(20):
  # Initialize an iterator over the training dataset.
  sess.run(training_init_op)
  for _ in range(100):
    sess.run(next_element)

  # Initialize an iterator over the validation dataset.
  sess.run(validation_init_op)
  for _ in range(50):
    sess.run(next_element)
</code></pre>

<p>It is unclear what the benefit of this complexity is.<br>
Why not simply create 2 different iterators?</p>
",1
53611227,Changing the shape of a new assigned variable in Tensorflow,"<p>if you change a tf.Variable using <a href=""https://www.tensorflow.org/api_docs/python/tf/assign"" rel=""nofollow noreferrer"">tf.assign</a> with <code>validate_shape=False</code> <a href=""https://github.com/tensorflow/tensorflow/issues/10091"" rel=""nofollow noreferrer"">the shape is not updated</a>.</p>

<p>But if I use <a href=""https://www.tensorflow.org/api_docs/python/tf/Variable#set_shape"" rel=""nofollow noreferrer"">set_shape</a> to set the new (correct) shape I get a ValueError.</p>

<p>Here a quick example:</p>

<pre><code>import tensorflow as tf 

a = tf.Variable([3,3,3])

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    # [3 3 3]
    print(sess.run(a))

    sess.run(tf.assign(a, [4,4,4,4], validate_shape=False))
    # [4 4 4 4]
    print(sess.run(a))

# (3,)
print(a.get_shape())

# ValueError: Dimension 0 in both shapes must be equal, but are 3 and 4. Shapes are [3] and [4].
a.set_shape([4])
</code></pre>

<p>How do I change the shape of the Variable?</p>

<p>Note: I am aware that the code works if I use <code>a = tf.Variable([3,3,3], validate_shape=False)</code> but in my context I will not be able to initialize the variable myself.</p>
",0
53612973,TensorFlow Sigmoid Cross Entropy with Logits for 1D data,"<h1>Context</h1>

<p>Suppose we have some 1D data (e.g. time series), where all series have fixed length <em>l</em>:</p>

<pre><code>        # [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11] index
example = [ 0,  1,  1,  0, 23, 22, 20, 14,  9,  2,  0,  0] # l = 12
</code></pre>

<p>and we want to perform semantic segmentation, with <em>n</em> classes:</p>

<pre><code>          # [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]    index            
labeled = [
            [ 0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0], # class 1
            [ 0,  0,  0,  0,  1,  1,  1,  1,  0,  0,  0,  0], # class 2
            [ 0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  0,  0], # class 3
           #[                     ...                      ],
            [ 1,  1,  1,  0,  0,  0,  0,  0,  1,  1,  1,  1], # class n
 ]
</code></pre>

<p>then the output for a single example has shape <code>[n, l]</code> (i.e. the <code>data_format</code> is not <code>""channels_last""</code>) and the batched output has shape <code>[b, n, l]</code>, where <code>b</code> is the number of examples in the batch.</p>

<p>These classes are independent, so it is my understanding that the use <em>sigmoid</em> cross entropy is applicable here as the loss rather than softmax cross entropy.</p>

<hr>

<h1>Question</h1>

<p>I have a few small related questions in regards to the expected format for and use of <code>tf.nn.sigmoid_cross_entropy_with_logits</code>:</p>

<ol>
<li><p>since the network outputs a tensor in the same shape as the batched labels, should I train the network under the assumption that it outputs logits, or take the keras approach (see keras's <code>binary_crossentropy</code>) and assume it outputs probabilities?</p></li>
<li><p>given the 1d segmentation problem, should I call <code>tf.nn.sigmoid_cross_entropy_with_logits</code> on:</p>

<ul>
<li><code>data_format='channels_first'</code> (as shown above), or</li>
<li><code>data_format='channels_last'</code>  (example.T)</li>
</ul>

<p>if I want the labels to be assigned individually per channel?</p></li>
<li><p>should the loss operation passed to the optimizer be:</p>

<ul>
<li><code>tf.nn.sigmoid_cross_entropy_with_logits(labels, logits)</code>, </li>
<li><code>tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels, logits))</code>, or</li>
<li><code>tf.losses.sigmoid_cross_entropy</code>?</li>
</ul></li>
</ol>

<hr>

<h1>Code</h1>

<p>This <a href=""https://colab.research.google.com/drive/12EiVRy8Tdt4UyvSuuBr3wcXfCmXPVFku"" rel=""nofollow noreferrer"">Colab</a>, highlights my confusion and demonstrates that the <code>data_format</code> does in fact matter..., but the documentation does not explicitly state which is expected.</p>

<h2>Dummy data</h2>

<pre><code>c = 5  # number of channels (label classes)
p = 10 # number of positions ('pixels')


# data_format = 'channels_first', shape = [classes, pixels]
# 'logits' for 2 examples
pred_1 = np.array([[random.random() for v in range(p)]for n in range(c)]).astype(float)
pred_2 = np.array([[random.random() for v in range(p)]for n in range(c)]).astype(float)

# 'ground truth' for the above 2 examples
targ_1 = np.array([[0 if random.random() &lt; 0.8 else 1 for v in range(p)]for n in range(c)]).astype(float)
targ_2 = np.array([[0 if random.random() &lt; 0.8 else 1 for v in range(p)]for n in range(c)]).astype(float)

# batched form of the above examples
preds = np.array([pred_1, pred_2])
targs = np.array([targ_1, targ_2])


# data_format = 'channels_last', shape = [pixels, classes]
t_pred_1 = pred_1.T
t_pred_2 = pred_2.T
t_targ_1 = targ_1.T
t_targ_2 = targ_2.T

t_preds = np.array([t_pred_1, t_pred_2])
t_targs = np.array([t_targ_1, t_targ_2])
</code></pre>

<h2>losses</h2>

<h3>tf.nn</h3>

<pre><code># calculate individual losses for 'channels_first'
loss_1 = tf.nn.sigmoid_cross_entropy_with_logits(labels=targ_1, logits=pred_1)
loss_2 = tf.nn.sigmoid_cross_entropy_with_logits(labels=targ_2, logits=pred_2)
# calculate batch loss for 'channels_first'
b_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targs, logits=preds)

# calculate individual losses for 'channels_last'
t_loss_1 = tf.nn.sigmoid_cross_entropy_with_logits(labels=t_targ_1, logits=t_pred_1)
t_loss_2 = tf.nn.sigmoid_cross_entropy_with_logits(labels=t_targ_2, logits=t_pred_2)
# calculate batch loss for 'channels_last'
t_b_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=t_targs, logits=t_preds)
# get actual tensors
with tf.Session() as sess:
  # loss for 'channels_first'
  l1   = sess.run(loss_1)
  l2   = sess.run(loss_2)
  # batch loss for 'channels_first'
  bl   = sess.run(b_loss)

  # loss for 'channels_last'
  t_l1 = sess.run(t_loss_1)
  t_l2 = sess.run(t_loss_2)

  # batch loss for 'channels_last'
  t_bl = sess.run(t_b_loss)
</code></pre>

<h3>tf.reduced_mean(tf.nn)</h3>

<pre><code># calculate individual losses for 'channels_first'
rm_loss_1 = tf.reduce_mean(loss_1)
rm_loss_2 = tf.reduce_mean(loss_2)
# calculate batch loss for 'channels_first'
rm_b_loss = tf.reduce_mean(b_loss)

# calculate individual losses for 'channels_last'
rm_t_loss_1 = tf.reduce_mean(t_loss_1)
rm_t_loss_2 = tf.reduce_mean(t_loss_2)
# calculate batch loss for 'channels_last'
rm_t_b_loss = tf.reduce_mean(t_b_loss)
# get actual tensors
with tf.Session() as sess:
  # loss for 'channels_first'
  rm_l1   = sess.run(rm_loss_1)
  rm_l2   = sess.run(rm_loss_2)
  # batch loss for 'channels_first'
  rm_bl   = sess.run(rm_b_loss)

  # loss for 'channels_last'
  rm_t_l1 = sess.run(rm_t_loss_1)
  rm_t_l2 = sess.run(rm_t_loss_2)

  # batch loss for 'channels_last'
  rm_t_bl = sess.run(rm_t_b_loss)
</code></pre>

<h3>tf.losses</h3>

<pre><code># calculate individual losses for 'channels_first'
tf_loss_1 = tf.losses.sigmoid_cross_entropy(multi_class_labels=targ_1, logits=pred_1)
tf_loss_2 = tf.losses.sigmoid_cross_entropy(multi_class_labels=targ_2, logits=pred_2)
# calculate batch loss for 'channels_first'
tf_b_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=targs, logits=preds)

# calculate individual losses for 'channels_last'
tf_t_loss_1 = tf.losses.sigmoid_cross_entropy(multi_class_labels=t_targ_1, logits=t_pred_1)
tf_t_loss_2 = tf.losses.sigmoid_cross_entropy(multi_class_labels=t_targ_2, logits=t_pred_2)
# calculate batch loss for 'channels_last'
tf_t_b_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=t_targs, logits=t_preds)
# get actual tensors
with tf.Session() as sess:
  # loss for 'channels_first'
  tf_l1   = sess.run(tf_loss_1)
  tf_l2   = sess.run(tf_loss_2)
  # batch loss for 'channels_first'
  tf_bl   = sess.run(tf_b_loss)

  # loss for 'channels_last'
  tf_t_l1 = sess.run(tf_t_loss_1)
  tf_t_l2 = sess.run(tf_t_loss_2)

  # batch loss for 'channels_last'
  tf_t_bl = sess.run(tf_t_b_loss)
</code></pre>

<h2>Test equivalency</h2>

<h3>data_format equivalency</h3>

<pre><code># loss _should_(?) be the same for 'channels_first' and 'channels_last' data_format
# test example_1
e1 = (l1 == t_l1.T).all()
# test example 2
e2 = (l2 == t_l2.T).all()

# loss calculated for each example and then batched together should be the same 
# as the loss calculated on the batched examples
ea = (np.array([l1, l2]) == bl).all()
t_ea = (np.array([t_l1, t_l2]) == t_bl).all()

# loss calculated on the batched examples for 'channels_first' should be the same
# as loss calculated on the batched examples for 'channels_last'
eb = (bl == np.transpose(t_bl, (0, 2, 1))).all()


e1, e2, ea, t_ea, eb
# (True, False, False, False, True) &lt;- changes every time, so True is happenstance
</code></pre>

<h3>equivalency between tf.reduce_mean and tf.losses</h3>

<pre><code>l_e1 = tf_l1 == rm_l1
l_e2 = tf_l2 == rm_l2
l_eb = tf_bl == rm_bl

l_t_e1 = tf_t_l1 == rm_t_l1
l_t_e2 = tf_t_l2 == rm_t_l2
l_t_eb = tf_t_bl == rm_t_bl

l_e1, l_e2, l_eb, l_t_e1, l_t_e2, l_t_eb
# (False, False, False, False, False, False)
</code></pre>
",1
53677345,Passing >2GB data to tf.estimator,"<p>I have <code>x_train</code> and <code>y_train</code> numpy arrays, each of >2GB. I want to train model using the tf.estimator API, but I am getting the errors:</p>

<pre><code>ValueError: Cannot create a tensor proto whose content is larger than 2GB
</code></pre>

<p>I am passing the data using:</p>

<pre><code>def input_fn(features, labels=None, batch_size=None,
             shuffle=False, repeats=False):
    if labels is not None:
        inputs = (features, labels)
    else:
        inputs = features
    dataset = tf.data.Dataset.from_tensor_slices(inputs)
    if shuffle:
        dataset = dataset.shuffle(shuffle)
    if batch_size:
        dataset = dataset.batch(batch_size)
    if repeats:
        # if False, evaluate after each epoch
        dataset = dataset.repeat(repeats)
    return dataset

train_spec = tf.estimator.TrainSpec(
    lambda : input_fn(x_train, y_train,
                      batch_size=BATCH_SIZE, shuffle=50),
    max_steps=EPOCHS
)

eval_spec = tf.estimator.EvalSpec(lambda : input_fn(x_dev, y_dev))

tf.estimator.train_and_evaluate(model, train_spec, eval_spec)
</code></pre>

<p>The tf.data documentation <a href=""https://www.tensorflow.org/guide/datasets"" rel=""nofollow noreferrer"">mentions this error</a> and provides solution using traditional TenforFlow API with placeholders. Unfortunately, I don't know how this could be translated into tf.estimator API?</p>
",1
53699201,How to print training progress in Tensorflow DNNClassifier estimator?,"<p>I want to train a model with <code>tf.estimator.DNNClassifier</code> in a Kaggle notebook environment where the internet is blocked. Therefore, I cannot use Tensorboard to monitor the progress. So instead, I want to log the progress in the standard output (similar to when we call <code>fit</code> method on a Keras model) but I couldn't make it work.</p>

<p>What I have tried so far is set logging levels to <code>INFO</code> and passing a <code>tf.estimator.RunConfig</code> instance to the estimator. <code>RunConfig</code> has a <code>log_step_count_steps</code> attribute with default value = 100 which seems to be related to what I am looking for, but It does not work. Here is part of the code:</p>

<pre><code>import logging;
logging.getLogger().setLevel(logging.INFO)
tf.logging.set_verbosity(tf.logging.INFO)

config = tf.estimator.RunConfig()
classifier = tf.estimator.DNNClassifier(
    feature_columns = feature_columns,
    hidden_units = [128, 64],
    n_classes = 2,
    config = config
)

classifier.train(input_fn=train_input_fn)
</code></pre>

<p>I use Tensorflow version <code>1.11.0-rc1</code>.</p>
",0
53758374,AttributeError: 'Sequential' object has no attribute '_feed_input_names',"<p>I'm following the <a href=""https://www.tensorflow.org/guide/keras"" rel=""nofollow noreferrer""><code>tf.keras</code> tutorial</a> with python 2.7.5 but <code>model.fit(data, labels, epochs=10, batch_size=32)</code> gives me the error message: </p>

<pre><code>&gt;&gt;&gt; model.fit(data, labels, epochs=10, batch_size=32,validation_data=    (val_data, val_labels))

Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/software/TensorFlow/1.8-GPU-py2/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py"", line 1143, in fit
    batch_size=batch_size)
  File ""/software/TensorFlow/1.8-GPU-py2/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/engine/training.py"", line 751, in _standardize_user_data
    feed_input_names = self._feed_input_names
AttributeError: 'Sequential' object has no attribute '_feed_input_names'
</code></pre>

<p>Can anyone point out what is wrong?</p>
",0
53828895,Does distributed TensorFlow requires a distributed file system to save checkpoints?,"<p>I am trying to run some sample codes based on <a href=""https://www.tensorflow.org/deploy/distributed"" rel=""nofollow noreferrer"">this tf official tutorial</a>.<br>
And I watched <a href=""https://www.youtube.com/watch?v=la_M6bCV91M&amp;index=11&amp;list=PLOU2XLYxmsIKGc_NBoIhTn2Qhraji53cv"" rel=""nofollow noreferrer"">this video</a> which is really nice.<br>
As mentioned in the video above, the chief worker is responsible for saving checkpoints, and it's implemented by tf.train.MonitoredTrainingSession.<br>
Then I thought that only the chief worker needs a directory to save checkpoints.<br>
When I run the codes with ps0 on machine1, worker0 on machine2, everything seems ok.<br>
But when I run with ps0, worker0 on machine1, ps1 and worker1 on machine2, errors occurs, and the error in worker0's log is like:  </p>

<pre><code>Traceback (most recent call last):
File ""distributed_train.py"", line 136, in &lt;module&gt;
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""distributed_train.py"", line 97, in main
    hooks=hooks) as mon_sess:
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 415, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 826, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 549, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1012, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1017, in _create_session
    return self._sess_creator.create_session()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 712, in create_session
    hook.after_create_session(self.tf_sess, self.coord)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 450, in after_create_session
    self._save(session, global_step)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 481, in _save
    self._get_saver().save(session, self._save_path, global_step=step)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1669, in save
    raise exc
tensorflow.python.framework.errors_impl.NotFoundError: ./train_dir/dist_worker_0/model.ckpt-0_temp_cf2b45f059b74507a65cae9b7a9ea5b4; No such file or directory
     [[Node: save/SaveV2_1 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:ps/replica:0/task:1/device:CPU:0""](save/ShardedFilename_1, save/SaveV2_1/tensor_names, save/SaveV2_1/shape_and_slices, conv1/biases, conv1/biases/Adagrad, conv2/biases, conv2/biases/Adagrad, local3/biases, local3/biases/Adagrad, local4/biases, local4/biases/Adagrad, softmax_linear/biases, softmax_linear/biases/Adagrad)]]

Caused by op u'save/SaveV2_1', defined at:
  File ""distributed_train.py"", line 136, in &lt;module&gt;
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""distributed_train.py"", line 97, in main
    hooks=hooks) as mon_sess:
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 415, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 826, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 549, in __init__
    self._sess = _RecoverableSession(self._coordinated_creator)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1012, in __init__
    _WrappedSession.__init__(self, self._create_session())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1017, in _create_session
    return self._sess_creator.create_session()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 706, in create_session
    self.tf_sess = self._session_creator.create_session()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 468, in create_session
    self._scaffold.finalize()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 212, in finalize
    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 856, in _get_saver_or_default
    saver = Saver(sharded=True, allow_empty=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1284, in __init__
    self.build()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1296, in build
    self._build(self._filename, build_save=True, build_restore=True)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1333, in _build
    build_save=build_save, build_restore=build_restore)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 772, in _build_internal
    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 363, in _AddShardedSaveOps
    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 337, in _AddShardedSaveOpsForV2
    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/s`enter code here`aver.py"", line 278, in _AddSaveOps
   save = self.save_op(filename_tensor, saveables)

File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 194, in save_op
    tensors)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1687, in save_v2
    shape_and_slices=shape_and_slices, tensors=tensors, name=name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): ./train_dir/dist_worker_0/model.ckpt-0_temp_cf2b45f059b74507a65cae9b7a9ea5b4; No such file or directory
     [[Node: save/SaveV2_1 = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:ps/replica:0/task:1/device:CPU:0""](save/ShardedFilename_1, save/SaveV2_1/tensor_names, save/SaveV2_1/shape_and_slices, conv1/biases, conv1/biases/Adagrad, conv2/biases, conv2/biases/Adagrad, local3/biases, local3/biases/Adagrad, local4/biases, local4/biases/Adagrad, softmax_linear/biases, softmax_linear/biases/Adagrad)]]
</code></pre>

<p>But the directory ./train_dir/dist_worker_0/model.ckpt-0_temp_cf2b45f059b74507a65cae9b7a9ea5b4  really exists (on machine1).</p>

<p>part of the code (actually from the official tutorial):  </p>

<pre><code>        # The MonitoredTrainingSession takes care of session initialization,
        # restoring from a checkpoint, saving to a checkpoint, and closing when done
        # or an error occurs.
        with tf.train.MonitoredTrainingSession(
                master=server.target,
                config=config,
                is_chief=(FLAGS.task_index == 0),
                checkpoint_dir=""./train_dir/dist_{0}_{1}"".format(FLAGS.job_name,
                                                             FLAGS.task_index),
                hooks=hooks) as mon_sess:
            while not mon_sess.should_stop():
                # Run a training step asynchronously.
                # See &lt;a href=""./../api_docs/python/tf/train/SyncReplicasOptimizer""&gt;&lt;code&gt;tf.train.SyncReplicasOptimizer&lt;/code&gt;&lt;/a&gt; for additional details on how to
                # perform *synchronous* training.
                # mon_sess.run handles AbortedError in case of preempted PS.
                mon_sess.run(train_op)
</code></pre>

<p>I searched some questions on stackoverflow and issues on github, answers to similar questions suggest to use HDFS.<br>
Doesn't ""Chief worker is responsible for saving chechpoints"" mean that I only need a local directory on the machine where the chief worker is located? Am I misunderstanding something? Do I really need to use HDFS or such?  </p>
",0
53915078,"What are b, y, x and c which get flattened and returned along with the max-pooled features in tf.nn.max_pool_with_argmax?","<p>I went through the documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/max_pool_with_argmax"" rel=""nofollow noreferrer"">tf.nn.max_pool_with_argmax</a> where it is written</p>

<blockquote>
  <p>Performs max pooling on the input and outputs both max values and indices.</p>
  
  <p>The indices in argmax are flattened, so that a maximum value at
  position [b, y, x, c] becomes flattened index ((b * height + y) *
  width + x) * channels + c.</p>
  
  <p>The indices returned are always in [0, height) x [0, width) before
  flattening, even if padding is involved and the mathematically correct
  answer is outside (either negative or too large). This is a bug, but
  fixing it is difficult to do in a safe backwards compatible way,
  especially due to flattening.</p>
</blockquote>

<p>The variables b, y, x and c haven't been explicitly defined hence I was having issues implementing this method. Can someone please provide the same.</p>
",1
53919290,tensorflow sparse categorical cross entropy with logits,"<p>I am a novice programmer trying to follow <a href=""https://www.tensorflow.org/tutorials/sequences/text_generation"" rel=""nofollow noreferrer"">this</a> guide.
However, I ran across an issue. The guide says to define the loss function as:</p>

<pre><code>def loss(labels, logits):
    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)
</code></pre>

<p>This gives me the following error:</p>

<blockquote>
  <p>sparse_categorical_crossentropy() got an unexpected keyword argument
  'from_logits'</p>
</blockquote>

<p>which I take to mean that <code>from_logits</code> is an argument not specified in the function, which is supported by the documentation, which that <code>tf.keras.losses.sparse_categorical_crossentropy()</code> has only two possible inputs. </p>

<p>Is there a way to specify that logits are being used or is that even necesarry?</p>
",1
53922040,How does tf.keras.layers.Conv2DTranspose behave with stride and padding?,"<p>While a convolution layer in TensorFlow has a complete description <a href=""https://www.tensorflow.org/api_guides/python/nn#Convolution"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_guides/python/nn#Convolution</a>, transposed convolution does not have one.</p>

<p>Although tf.keras.layers.Conv2DTranspose has a reference to <a href=""https://arxiv.org/pdf/1603.07285.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1603.07285.pdf</a>, it is not complete.</p>

<p>Is there any documentation that describes how tf.keras.layers.Conv2DTranspose behaves?</p>
",1
53951941,How can I load a saved model from object detection for inference?,"<p>I'm pretty new to Tensorflow and have been running experiments with SSDs with the Tensorflow Object Detection API.  I can successfully train a model, but by default, it only save the last n checkpoints.  I'd like to instead save the last n checkpoints with the lowest loss (I'm assuming that's the best metric to use).</p>

<p>I found tf.estimator.BestExporter and it exports a saved_model.pb along with variables.  However, I have yet to figure out how to load that saved model and run inference on it.  After running models/research/object_detection/export_inference_graph.py on the checkpoiont, I can easily load a checkpoint and run inference on it using the object detection jupyter notebook: <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb</a></p>

<p>I've found documentation on loading saved models, and can load a graph like this:</p>

<pre><code>with tf.Session(graph=tf.Graph()) as sess:
        tags = [tag_constants.SERVING]
        meta_graph = tf.saved_model.loader.load(sess, tags, PATH_TO_SAVED_MODEL)
        detection_graph = tf.get_default_graph()
</code></pre>

<p>However, when I use that graph with the above jupyter notebook, I get errors:</p>

<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-17-9e48f0d04df2&gt; in &lt;module&gt;
      7   image_np_expanded = np.expand_dims(image_np, axis=0)
      8   # Actual detection.
----&gt; 9   output_dict = run_inference_for_single_image(image_np, detection_graph)
     10   # Visualization of the results of a detection.
     11   vis_util.visualize_boxes_and_labels_on_image_array(

&lt;ipython-input-16-0df86999596e&gt; in run_inference_for_single_image(image, graph)
     31             detection_masks_reframed, 0)
     32 
---&gt; 33       image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')
     34       # image_tensor = tf.get_default_graph().get_tensor_by_name('serialized_example')
     35 

~/anaconda3/envs/sb/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in get_tensor_by_name(self, name)
   3664       raise TypeError(""Tensor names are strings (or similar), not %s."" %
   3665                       type(name).__name__)
-&gt; 3666     return self.as_graph_element(name, allow_tensor=True, allow_operation=False)
   3667 
   3668   def _get_tensor_by_tf_output(self, tf_output):

~/anaconda3/envs/sb/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in as_graph_element(self, obj, allow_tensor, allow_operation)
   3488 
   3489     with self._lock:
-&gt; 3490       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
   3491 
   3492   def _as_graph_element_locked(self, obj, allow_tensor, allow_operation):

~/anaconda3/envs/sb/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _as_graph_element_locked(self, obj, allow_tensor, allow_operation)
   3530           raise KeyError(""The name %s refers to a Tensor which does not ""
   3531                          ""exist. The operation, %s, does not exist in the ""
-&gt; 3532                          ""graph."" % (repr(name), repr(op_name)))
   3533         try:
   3534           return op.outputs[out_n]

KeyError: ""The name 'image_tensor:0' refers to a Tensor which does not exist. The operation, 'image_tensor', does not exist in the graph.""
</code></pre>

<p>Is there a better way to load the saved model or convert it to an inference graph?</p>

<p>Thanks!</p>
",0
54007669,Reset local variables of metrics after each epoch,"<p>I use the built-in method <a href=""https://www.tensorflow.org/api_docs/python/tf/metrics/precision"" rel=""nofollow noreferrer"">tf.metrics.precision</a> to evaluate my model. I was looking through its <a href=""https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/python/ops/metrics_impl.py"" rel=""nofollow noreferrer"">definition</a>, but the local variables are never reset.</p>

<p>Shouldn't they be reset after each epoch in order to remove the counts from the last epochs? Is this done automatically and I was just overlooking it in the source code, or am I supposed to do it? If the latter is true, how do I reset the local variables? I didn't read anything about it in the documentation.</p>
",1
54055707,Keras model.fit() with tf.dataset fails while using tf.train works fine,"<p>Summary: according to the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit"" rel=""nofollow noreferrer"">documentation</a>, Keras <code>model.fit()</code> should accept tf.dataset as input (I am using TF version 1.12.0). I can train my model if I manually do the training steps but using <code>model.fit()</code> on the same model, I get an error I cannot resolve.</p>

<p>Here is a sketch of what I did: my dataset, which is too big to fit in the memory, consists of many files each with different number of rows of (100 features, label). I'd like to use <code>tf.data</code> to build my data pipeline:</p>

<pre class=""lang-py prettyprint-override""><code>def data_loader(filename):
    '''load a single data file with many rows'''
    features, labels = load_hdf5(filename)
    ...
    return features, labels

def make_dataset(filenames, batch_size):
    '''read files one by one, pick individual rows, batch them and repeat'''
    dataset = tf.data.Dataset.from_tensor_slices(filenames)
    dataset = dataset.map(      # Problem here! See edit for solution
        lambda filename: tuple(tf.py_func(data_loader, [filename], [float32, tf.float32])))
    dataset = dataset.flat_map(
        lambda features, labels: tf.data.Dataset.from_tensor_slices((features, labels)))
    dataset = dataset.batch(batch_size)
    dataset = dataset.repeat()
    dataset = dataset.prefetch(1000)
    return dataset

_BATCH_SIZE = 128
training_set = make_dataset(training_files, batch_size=_BATCH_SIZE)
</code></pre>

<p>I'd like to try a very basic logistic regression model:</p>

<pre class=""lang-py prettyprint-override""><code>inputs = tf.keras.layers.Input(shape=(100,))
outputs = tf.keras.layers.Dense(1, activation='softmax')(inputs)
model = tf.keras.Model(inputs, outputs)
</code></pre>

<p>If I train it <em>manually</em> everything works fine, e.g.:</p>

<pre class=""lang-py prettyprint-override""><code>labels = tf.placeholder(tf.float32)
loss = tf.reduce_mean(tf.keras.backend.categorical_crossentropy(labels, outputs))
train_step = tf.train.GradientDescentOptimizer(.05).minimize(loss)

iterator = training_set.make_one_shot_iterator()
next_element = iterator.get_next()
init_op = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init_op)
    for i in range(training_size // _BATCH_SIZE):
        x, y = sess.run(next_element)
        train_step.run(feed_dict={inputs: x, labels: y})
</code></pre>

<p>However, if I instead try to use <code>model.fit</code> like this:</p>

<pre class=""lang-py prettyprint-override""><code>model.compile('adam', 'categorical_crossentropy', metrics=['acc'])
model.fit(training_set.make_one_shot_iterator(),
          steps_per_epoch=training_size // _BATCH_SIZE,
          epochs=1,
          verbose=1)
</code></pre>

<p>I get an error message <code>ValueError: Cannot take the length of Shape with unknown rank.</code> inside the keras'es <code>_standardize_user_data</code> function.</p>

<p>I have tried quite a few things but could not resolve the issue. Any ideas?</p>

<p><strong>Edit:</strong> based on @kvish's answer, the solution was to change the map from a lambda to a function that would specify the correct tensor dimensions, e.g.:</p>

<pre class=""lang-py prettyprint-override""><code>def data_loader(filename):
    def loader_impl(filename):
        features, labels, _ = load_hdf5(filename)
        ...
        return features, labels

    features, labels = tf.py_func(loader_impl, [filename], [tf.float32, tf.float32])
    features.set_shape((None, 100))
    labels.set_shape((None, 1))
    return features, labels
</code></pre>

<p>and now, all needed to do is to call this function from <code>map</code>:</p>

<pre class=""lang-py prettyprint-override""><code>dataset = dataset.map(data_loader)
</code></pre>
",0
54059524,Tensorflow freezes when trying to create a session in docker container,"<p>I'm trying to deploy a web site for the demo using CNN.
To serve this purpose, I built a docker image with dependencies (in my case tensorflow, keras and any other miscellanies). </p>

<p>I managed to built the docker image. However It fails when I tested on some sample images. I found out that the problem is tensorflow can not create a session. I still can import tensorflow and other packages. Even I can run a code for building computation graph with tensorflow code</p>

<pre><code>(Ex,  
a=tf.constant(7)
b=tf.constant(10)
c = tf.add(a,b)
).
</code></pre>

<p>But it freezes whenever I try to create a tensorflow session <code>(Ex, sess = tf.Session()).</code>
Below is the code for building docker image.</p>

<pre><code>def CNN_forward_example(self,image):

    a=tf.constant(7)
    b=tf.constant(10)
    c = tf.add(a,b)
    sess = tf.Session()
    print(""I hope to see this message------------------------"",c)
</code></pre>

<p>Expected result is to see the message ""printed by print(""I hope to see this message------------------------"",c)"".</p>
",0
54060667,Continue training of a custom tf.Estimator with AdamOptimizer,"<p>I created a custom tf.Estimator whose weights I'm training using the tf.train.AdamOptimizer. When I continue training of an existing model, I observe a steep change in the metrics at the start of the continued training in Tensorboard. After a few steps, the metrics stabilise. The behaviour looks similar to the initial transients when training a model. The behaviour is the same if I continue training on the same Estimator instance, or if I recreate the estimator from a checkpoint. I suspect that the moving averages and/or the bias correction factor are reset when restarting the training. The model weights themselves seem to be properly restored, as the metrics do continue from where they settled before, only the effective learning rate seems to be too high.</p>

<p>Previous Stack-Overflow answers seem to suggest that these auxiliary learning parameters should be stored with the checkpoints together with the model weights. So what am I doing wrong here? How can I control restoring of these auxiliary variables? I would like to be able to continue training as if it had never been stopped. However, other people sometimes seem look for the opposite control, to completely reset the optimizer without resetting the model weights. An answer that shows how both effects can be achieved would probably most helpful.</p>

<p>Here is a sketch of my <code>model_fn</code>:</p>

<pre><code>def model_fn(features, labels, mode, params):
    inputs = features['inputs']
    logits = create_model(inputs, training=mode == tf.estimator.ModeKeys.TRAIN)

    if mode == tf.estimator.ModeKeys.PREDICT:
        ...

    if mode == tf.estimator.ModeKeys.TRAIN:
        outputs = labels['outputs']

        loss = tf.losses.softmax_cross_entropy(
            tf.one_hot(outputs,tf.shape(inputs)[-1]),
            logits,
#            reduction=tf.losses.Reduction.MEAN,
        )
        optimizer = tf.train.AdamOptimizer(learning_rate=params.learning_rate)

        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)

        with tf.control_dependencies(update_ops):
            train_op = optimizer.minimize(loss, tf.train.get_or_create_global_step())

        accuracy = tf.metrics.accuracy(
            labels = outputs,
            predictions = tf.argmax(logits, axis=-1),
        )

        tf.summary.histogram('logits',logits)
        tf.summary.scalar('accuracy', accuracy[1])
        tf.summary.scalar('loss', loss)

        return tf.estimator.EstimatorSpec(
            mode=tf.estimator.ModeKeys.TRAIN,
            loss=loss,
            train_op=train_op)

    if mode == tf.estimator.ModeKeys.EVAL:
        ...

    raise ValueError(mode)
</code></pre>

<p>The training step is called as follows:</p>

<pre><code>cfg = tf.estimator.RunConfig(
    save_checkpoints_secs = 5*60,  # Save checkpoints every 1 minutes.
    keep_checkpoint_max = 10,       # Retain the 10 most recent checkpoints.
    save_summary_steps = 10,
    log_step_count_steps = 100,
)
estimator = tf.estimator.Estimator(
    model_fn = model_fn,
    params = dict(
        learning_rate = 1e-3,
    ),
    model_dir = model_dir,
    config=cfg,
)
# train for the first time
estimator.train(
    input_fn=train_input_fn,
)
# ... at some later time, train again
estimator.train(
    input_fn=train_input_fn,
)
</code></pre>

<p>EDIT:</p>

<p>The documentation of the <code>warm_start_from</code> argument of <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator"" rel=""nofollow noreferrer""><code>tf.estimator.Estimator</code></a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/WarmStartSettings"" rel=""nofollow noreferrer""><code>tf.estimator.WarmStartSettings</code></a> are not entirely clear what exactly will happen in the default case, as I am using in the example above. However, the documentation of [<code>tf.train.warm_start</code>] (<a href=""https://www.tensorflow.org/api_docs/python/tf/train/warm_start"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/train/warm_start</a>) seems to suggest that in the default case, all <code>TRAINABLE_VARIABLES</code> will be warm-started, which</p>

<blockquote>
  <p>excludes variables such as accumulators and moving statistics from batch norm</p>
</blockquote>

<p>Indeed, I find Adam's accumulator variables in <code>VARIABLES</code>, but not in <code>TRAINABLE_VARIABLES</code>. These documentation pages also state how to change the list of warm-started variables, to either a list of <code>tf.Variable</code> instances, or a list of their names. However, one question remains: How do I create one of those lists in advance, given that with <code>tf.Estimator</code>, I have no graph to collect those variables/their names from?</p>

<p>EDIT2:</p>

<p>The source-code of <code>warm_start</code> highlights an undocumented feature: The list of variable names is in fact a list of regexes, to be matched against GLOBAL_VARIABLES. Thus, one may use </p>

<pre><code>    warm_start_from=tf.estimator.WarmStartSettings(
        ckpt_to_initialize_from=str(model_dir),
    #    vars_to_warm_start="".*"", # everything in TRAINABLE_VARIABLES - excluding optimiser params 
        vars_to_warm_start=["".*""], # everything in GLOBAL_VARIABLES - including optimiser params 
    ),
</code></pre>

<p>to load all variables. However, even with that, the spikes in the summary stats remain. With that, I'm completely at a loss now what is going on.</p>
",1
54069395,"Input contains NaN, infinity or a value too large for dtype('float64') in Tensorflow","<p>I am trying to train a LSTM and in my model I have an exponential learning rate decay and a dropout layer. In order to deactivate the dropout layer when testing and validating, I have put a placeholder for the dropout rate and given it a default value of 1.0 and when training i am setting it to 0.5. The dropou_rate placeholder value is passed to the tf.layers.dropout(). When I run this during the validation I get the following error.</p>

<blockquote>
  <p>ValueError: Input contains NaN, infinity or a value too large for
  dtype('float64').</p>
</blockquote>

<p>shown below is the stack trace:</p>

<blockquote>
  <p>Traceback (most recent call last):   File
  ""/home/suleka/Documents/sales_prediction/SalesPrediction_LSTM_mv.py"",
  line 329, in 
      train_test()   File ""/home/suleka/Documents/sales_prediction/SalesPrediction_LSTM_mv.py"",
  line 270, in train_test
      meanSquaredError = mean_squared_error(nonescaled_y, pred_vals)   File
  ""/home/suleka/anaconda3/lib/python3.6/site-packages/sklearn/metrics/regression.py"",
  line 238, in mean_squared_error
      y_true, y_pred, multioutput)   File ""/home/suleka/anaconda3/lib/python3.6/site-packages/sklearn/metrics/regression.py"",
  line 77, in _check_reg_targets
      y_pred = check_array(y_pred, ensure_2d=False)   File ""/home/suleka/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py"",
  line 453, in check_array
      _assert_all_finite(array)   File ""/home/suleka/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py"",
  line 44, in _assert_all_finite
      "" or a value too large for %r."" % X.dtype) ValueError: Input contains NaN, infinity or a value too large for dtype('float64').</p>
</blockquote>

<p>When I put the learning rate as a value in  tf.layers.dropout like:</p>

<blockquote>
  <p>dropout = tf.layers.dropout(last, rate=0.5, training=True)</p>
</blockquote>

<p>The code works fine. I am not sure what is happening in the code.</p>

<p>Shown below is my complete code:</p>

<pre><code>import tensorflow as tf
import matplotlib as mplt
mplt.use('agg')  # Must be before importing matplotlib.pyplot or pylab!
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from math import sqrt
import csv
np.random.seed(1)
tf.set_random_seed(1)


class RNNConfig():
    input_size = 1
    num_steps = 7#5
    lstm_size = 64 #16
    num_layers = 1
    keep_prob = 0.8
    batch_size = 16 #64
    init_epoch = 15  # 5
    max_epoch = 20 # 100 or 50
    # test_ratio = 0.2
    fileName = 'store2_1.csv'
    graph = tf.Graph()
    column_min_max = [[0,11000], [1,7]]
    columns = ['Sales', 'DayOfWeek','SchoolHoliday', 'Promo']
    features = len(columns)
    hidden1_nodes = 64
    hidden2_nodes = 8



config = RNNConfig()

def segmentation(data):

    seq = [price for tup in data[config.columns].values for price in tup]

    seq = np.array(seq)

    # split into items of features
    seq = [np.array(seq[i * config.features: (i + 1) * config.features])
           for i in range(len(seq) // config.features)]

    # split into groups of num_steps
    X = np.array([seq[i: i + config.num_steps] for i in range(len(seq) -  config.num_steps)])

    y = np.array([seq[i +  config.num_steps] for i in range(len(seq) -  config.num_steps)])

    # get only sales value
    y = [[y[i][0]] for i in range(len(y))]

    y = np.asarray(y)

    return X, y

def scale(data):

    for i in range (len(config.column_min_max)):
        data[config.columns[i]] = (data[config.columns[i]] - config.column_min_max[i][0]) / ((config.column_min_max[i][1]) - (config.column_min_max[i][0]))

    return data

def rescle(test_pred):

    prediction = [(pred * (config.column_min_max[0][1] - config.column_min_max[0][0])) + config.column_min_max[0][0] for pred in test_pred]

    return prediction


def pre_process():
    store_data = pd.read_csv(config.fileName)

    store_data = store_data.drop(store_data[(store_data.Open == 0) &amp; (store_data.Sales == 0)].index)
    #
    # store_data = store_data.drop(store_data[(store_data.Open != 0) &amp; (store_data.Sales == 0)].index)

    # ---for segmenting original data --------------------------------
    # original_data = store_data.copy()

    ## train_size = int(len(store_data) * (1.0 - test_ratio))

    validation_len = len(store_data[(store_data.Month == 6) &amp; (store_data.Year == 2015)].index)
    test_len = len(store_data[(store_data.Month == 7) &amp; (store_data.Year == 2015)].index)
    train_size = int(len(store_data) - (validation_len + test_len))

    train_data = store_data[:train_size]
    validation_data = store_data[(train_size - config.num_steps): validation_len + train_size]
    test_data = store_data[((validation_len + train_size) - config.num_steps):]
    original_val_data = validation_data.copy()
    original_test_data = test_data.copy()

    # -------------- processing train data---------------------------------------
    scaled_train_data = scale(train_data)
    train_X, train_y = segmentation(scaled_train_data)

    # -------------- processing validation data---------------------------------------
    scaled_validation_data = scale(validation_data)
    val_X, val_y = segmentation(scaled_validation_data)

    # -------------- processing test data---------------------------------------
    scaled_test_data = scale(test_data)
    test_X, test_y = segmentation(scaled_test_data)

    # ----segmenting original validation data-----------------------------------------------
    nonescaled_val_X, nonescaled_val_y = segmentation(original_val_data)

    # ----segmenting original test data---------------------------------------------
    nonescaled_test_X, nonescaled_test_y = segmentation(original_test_data)

    return train_X, train_y, test_X, test_y, val_X, val_y, nonescaled_test_y, nonescaled_val_y


def generate_batches(train_X, train_y, batch_size):
    num_batches = int(len(train_X)) // batch_size
    if batch_size * num_batches &lt; len(train_X):
        num_batches += 1

    batch_indices = range(num_batches)
    for j in batch_indices:
        batch_X = train_X[j * batch_size: (j + 1) * batch_size]
        batch_y = train_y[j * batch_size: (j + 1) * batch_size]
        assert set(map(len, batch_X)) == {config.num_steps}
        yield batch_X, batch_y

def mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    itemindex = np.where(y_true == 0)
    y_true = np.delete(y_true, itemindex)
    y_pred = np.delete(y_pred, itemindex)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

def RMSPE(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.sqrt(np.mean(np.square(((y_true - y_pred) / y_pred)), axis=0))

def plot(true_vals,pred_vals,name):
    fig = plt.figure()
    fig = plt.figure(dpi=100, figsize=(20, 7))
    days = range(len(true_vals))
    plt.plot(days, pred_vals, label='pred sales')
    plt.plot(days, true_vals, label='truth sales')
    plt.legend(loc='upper left', frameon=False)
    plt.xlabel(""day"")
    plt.ylabel(""sales"")
    plt.grid(ls='--')
    plt.savefig(name, format='png', bbox_inches='tight', transparent=False)
    plt.close()

def write_results(true_vals,pred_vals,name):

    with open(name, ""w"") as f:
        writer = csv.writer(f)
        writer.writerows(zip(true_vals, pred_vals))


def train_test():
    train_X, train_y, test_X, test_y, val_X, val_y, nonescaled_test_y, nonescaled_val_y = pre_process()


    # Add nodes to the graph
    with config.graph.as_default():

        tf.set_random_seed(1)

        learning_rate = tf.placeholder(tf.float32, None, name=""learning_rate"")
        inputs = tf.placeholder(tf.float32, [None, config.num_steps, config.features], name=""inputs"")
        targets = tf.placeholder(tf.float32, [None, config.input_size], name=""targets"")
        global_step = tf.Variable(0, trainable=False)
        dropout_rate = tf.placeholder_with_default(1.0, shape=())

        learning_rate = tf.train.exponential_decay(learning_rate=learning_rate, global_step=global_step, decay_rate=0.96,  decay_steps=5, staircase=False)

        cell = tf.contrib.rnn.LSTMCell(config.lstm_size, state_is_tuple=True, activation=tf.nn.relu)

        val1, _ = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)

        val = tf.transpose(val1, [1, 0, 2])

        last = tf.gather(val, int(val.get_shape()[0]) - 1, name=""last_lstm_output"")


        # hidden layer
        last = tf.layers.dense(last, units=config.hidden1_nodes, activation=tf.nn.relu)
        last = tf.layers.dense(last, units=config.hidden2_nodes, activation=tf.nn.relu)



        weight = tf.Variable(tf.truncated_normal([config.hidden2_nodes, config.input_size]))
        bias = tf.Variable(tf.constant(0.1, shape=[config.input_size]))

        dropout = tf.layers.dropout(last, rate=dropout_rate, training=True)

        prediction = tf.matmul(dropout, weight) + bias

        loss = tf.losses.mean_squared_error(targets,prediction)
        optimizer = tf.train.AdamOptimizer(learning_rate)
        minimize = optimizer.minimize(loss, global_step=global_step)

        # correct_prediction = tf.sqrt(tf.losses.mean_squared_error(prediction, targets))


    # --------------------training------------------------------------------------------

    with tf.Session(graph=config.graph) as sess:
        tf.set_random_seed(1)

        tf.global_variables_initializer().run()

        iteration = 1


        for epoch_step in range(config.max_epoch):


            for batch_X, batch_y in generate_batches(train_X, train_y, config.batch_size):
                train_data_feed = {
                    inputs: batch_X,
                    targets: batch_y,
                    learning_rate: 0.01,
                    dropout_rate: 0.5
                }

                train_loss, _, value,gs = sess.run([loss, minimize, val1,global_step], train_data_feed)

                if iteration % 5 == 0:
                    print(""Epoch: {}/{}"".format(epoch_step, config.max_epoch),
                          ""Iteration: {}"".format(iteration),
                          ""Train loss: {:.6f}"".format(train_loss))
                iteration += 1

        saver = tf.train.Saver()
        saver.save(sess, ""checkpoints_sales/sales_pred.ckpt"")

        # --------------------validation------------------------------------------------------

        with tf.Session(graph=config.graph) as sess:
            tf.set_random_seed(1)

            saver.restore(sess, tf.train.latest_checkpoint('checkpoints_sales'))

            test_data_feed = {
                inputs: val_X,
                dropout_rate: 1.0
            }

            test_pred = sess.run(prediction, test_data_feed)

            # rmsse = sess.run(correct_prediction, test_data_feed)

            pred_vals = rescle(test_pred)

            pred_vals = np.array(pred_vals)

            pred_vals = pred_vals.flatten()

            pred_vals = pred_vals.tolist()

            nonescaled_y = nonescaled_val_y.flatten()

            nonescaled_y = nonescaled_y.tolist()

            plot(nonescaled_y, pred_vals, ""Sales Prediction VS Truth mv testSet.png"")
            write_results(nonescaled_y, pred_vals, ""Sales Prediction batch mv results_all validationSet.csv"")

            meanSquaredError = mean_squared_error(nonescaled_y, pred_vals)
            rootMeanSquaredError = sqrt(meanSquaredError)
            print(""RMSE:"", rootMeanSquaredError)
            mae = mean_absolute_error(nonescaled_y, pred_vals)
            print(""MAE:"", mae)
            mape = mean_absolute_percentage_error(nonescaled_y, pred_vals)
            print(""MAPE:"", mape)
            rmse_val = RMSPE(nonescaled_y, pred_vals)
            print(""RMSPE:"", rmse_val)

    # --------------------testing------------------------------------------------------

    with tf.Session(graph=config.graph) as sess:
        tf.set_random_seed(1)

        saver.restore(sess, tf.train.latest_checkpoint('checkpoints_sales'))

        test_data_feed = {
            inputs: test_X,
            dropout_rate: 1.0
        }

        test_pred = sess.run(prediction, test_data_feed)

        # rmsse = sess.run(correct_prediction, test_data_feed)


        pred_vals = rescle(test_pred)

        pred_vals = np.array(pred_vals)

        pred_vals = (np.round(pred_vals, 0)).astype(np.int32)

        pred_vals = pred_vals.flatten()


        pred_vals = pred_vals.tolist()

        nonescaled_y = nonescaled_test_y.flatten()

        nonescaled_y = nonescaled_y.tolist()

        plot(nonescaled_y, pred_vals, ""Sales Prediction VS Truth mv testSet.png"")
        write_results(nonescaled_y, pred_vals, ""Sales Prediction batch mv results_all validationSet.csv"")

        meanSquaredError = mean_squared_error(nonescaled_y, pred_vals)
        rootMeanSquaredError = sqrt(meanSquaredError)
        print(""RMSE:"", rootMeanSquaredError)
        mae = mean_absolute_error(nonescaled_y, pred_vals)
        print(""MAE:"", mae)
        mape = mean_absolute_percentage_error(nonescaled_y, pred_vals)
        print(""MAPE:"", mape)
        rmse_val = RMSPE(nonescaled_y, pred_vals)
        print(""RMSPE:"", rmse_val)




if __name__ == '__main__':
    train_test()
</code></pre>
",0
54081739,Apply TensorFlow Transform to transform/scale features in production,"<h1>Overview</h1>

<p>I followed the following guide to write TF Records, where I used <code>tf.Transform</code> to preprocess my features. Now, I would like to deploy my model, for which I need apply this preprocessing function on real live data. </p>

<h1>My Approach</h1>

<p>First, suppose I have 2 features:</p>

<pre><code>features = ['amount', 'age']
</code></pre>

<p>I have the <code>transform_fn</code> from the Apache Beam, residing in <code>working_dir=gs://path-to-transform-fn/</code></p>

<p>Then I load the transform function using:</p>

<p><code>tf_transform_output = tft.TFTransformOutput(working_dir)</code></p>

<p>I thought that the easiest way to serve in in production was to get a numpy array of processed data, and call <code>model.predict()</code> (I am using Keras model).</p>

<p>To do this, I thought <a href=""http://transform_raw_features"" rel=""nofollow noreferrer""><code>transform_raw_features()</code></a> method is exactly what I need.</p>

<p>However, it seems that after building the schema:</p>

<pre><code>raw_features = {}
for k in features:
    raw_features.update({k: tf.constant(1)})

print(tf_transform_output.transform_raw_features(raw_features))
</code></pre>

<p>I get:</p>

<pre><code>AttributeError: 'Tensor' object has no attribute 'indices'
</code></pre>

<p>Now, I am assuming this happens because I used <code>tf.VarLenFeature()</code> when I defined schema in my <code>preprocessing_fn</code>.</p>

<pre><code>def preprocessing_fn(inputs):
    outputs = inputs.copy()

    for _ in features:
        outputs[_] = tft.scale_to_z_score(outputs[_])
</code></pre>

<p>And I build the metadata using:</p>

<pre><code>RAW_DATA_FEATURE_SPEC = {}
for _ in features:
    RAW_DATA_FEATURE_SPEC[_] = tf.VarLenFeature(dtype=tf.float32)
    RAW_DATA_METADATA = dataset_metadata.DatasetMetadata(
    dataset_schema.from_feature_spec(RAW_DATA_FEATURE_SPEC))
</code></pre>

<p>So in short, given a dictionary:</p>

<p><code>d = {'amount': [50], 'age': [32]}</code>, I would like to apply this <code>transform_fn</code>, and scale these values appropriately to input into my model for prediction. This dictionary is exactly the format of my <code>PCollection</code> before the data is processed by the <code>pre_processing()</code> function.</p>

<h1>Pipeline Structure:</h1>

<pre><code>class BeamProccess():

def __init__(self):

    # init 

    self.run()


def run(self):

    def preprocessing_fn(inputs):

         # outputs = { 'id' : [list], 'amount': [list], 'age': [list] }
         return outputs

    with beam.Pipeline(options=self.pipe_opt) as p:
        with beam_impl.Context(temp_dir=self.google_cloud_options.temp_location):
            data = p | ""read_table"" &gt;&gt; beam.io.Read(table_bq) \
            | ""create_data"" &gt;&gt; beam.ParDo(ProcessFn())

            transformed_dataset, transform_fn = (
                        (train, RAW_DATA_METADATA) | beam_impl.AnalyzeAndTransformDataset(
                    preprocessing_fn))

            transformed_data, transformed_metadata = transformed_dataset

            transformed_data | ""WriteTrainTFRecords"" &gt;&gt; tfrecordio.WriteToTFRecord(
                    file_path_prefix=self.JOB_DIR + '/train/data',
                    file_name_suffix='.tfrecord',
                    coder=example_proto_coder.ExampleProtoCoder(transformed_metadata.schema))

            _ = (
                        transform_fn
                        | 'WriteTransformFn' &gt;&gt;
                        transform_fn_io.WriteTransformFn(path=self.JOB_DIR + '/transform/'))
</code></pre>

<p>And finally the <code>ParDo()</code> is:</p>

<pre><code>class ProcessFn(beam.DoFn):

    def process(self, element):

        yield { 'id' : [list], 'amount': [list], 'age': [list] }
</code></pre>
",0
54155481,TensorFlow per_image_standardization vs mean standardization across full dataset,"<p>I am curious about the difference between standardizing each image individually vs standardizing across the full data set.</p>

<p>I am using <a href=""https://github.com/tensorflow/models/tree/master/official/resnet"" rel=""nofollow noreferrer"">tensorflow/models/official/resnet</a> which is built using tf.estimator.  The tf estimator supports an input pipeline function that produces a tf Dataset.  The Dataset object applies the <a href=""https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization"" rel=""nofollow noreferrer"">tf.image.per_image_standardization</a> op that standardizes by subtracting the mean of the image itself from each pixel and enforces unit variance.  </p>

<p>This is different from other ML preprocessing that standardizes the image based on the mean across the whole dataset, such as with <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"" rel=""nofollow noreferrer"">sklearn.preprocessing.StandardScaler</a>.</p>

<p>I'm confused as to whether any aspect of this input pipeline is persisted in the tf SavedModel exported from the tf.estimator.Estimator.</p>

<p>So I'm wondering if I need to still apply feature standardization when serving the model, either via tf.contrib.predictor or when deploying the model in any other dnn format.</p>

<p>Should I be applying standardization across the dataset even though I'm using the per_image_standardization?  If so, should I just export the mean value from the whole image set somehow so that when serving the model the server can just pick up the mean value from the whole dataset and apply standardization that way?</p>
",0
54171478,TensorFlow accuracy not being updated,"<p>Training accuracy is not being updated.</p>

<p>What could be going wrong?</p>

<p>Followed <a href=""https://www.youtube.com/watch?v=yX8KuPZCAMo&amp;t=2381s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=yX8KuPZCAMo&amp;t=2381s</a> step by step. I've even verified code matches 100% multiple times.</p>

<p>Tried using both ""tf.nn.softmax_cross_entropy_with_logits_v2"" and ""tf.nn.softmax_cross_entropy_with_logits"" for cross entropy.</p>

<p>The dataset I am using is the pandas Rock/Mine dataset. 
I tried changing the label column from 1/0 to ""R""/""M"" as well. (<a href=""https://github.com/selva86/datasets/blob/master/Sonar.csv"" rel=""nofollow noreferrer"">https://github.com/selva86/datasets/blob/master/Sonar.csv</a>)</p>

<pre><code>import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split


def read_dataset():
    # Read CSV file.
    df = pd.read_csv(""c:\\users\\developer\\documents\\sonar-fixed.csv"")
    # print(len(df.columns))

    X = df[df.columns[0:60]].values

    # ""R"" = Rock, ""M"" = Mine
    y = df[df.columns[60]] 

    # Label encoding.
    encoder = LabelEncoder()
    encoder.fit(y)

    y = encoder.transform(y)
    Y = one_hot_encode(y)
    return (X, Y)

def one_hot_encode(labels):
    n_labels = len(labels)
    n_unique_labels = len(np.unique(labels))
    one_hot_encode = np.zeros((n_labels, n_unique_labels))
    one_hot_encode[np.arange(n_labels), labels] = 1
    return one_hot_encode


X, Y = read_dataset()

X, Y = shuffle(X, Y, random_state=1)

train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.20, random_state=415)

print(train_x.shape)
print(train_y.shape)
print(test_x.shape)
print(test_y.shape)

learning_rate = 0.3
training_epochs = 1000
cost_history = np.empty(shape=[1], dtype=float)
n_dim = X.shape[1]
print(""n_dim"", n_dim)
n_class = 2
model_path = ""c:\\users\\developer\\documents""

n_hidden_1 = 60
n_hidden_2 = 60
n_hidden_3 = 60
n_hidden_4 = 60

x = tf.placeholder(tf.float32, [None, n_dim])
W = tf.Variable(tf.zeros([n_dim, n_class]))
b = tf.Variable(tf.zeros([n_class]))
y_ = tf.placeholder(tf.float32, [None, n_class])

# Define the model.
def multilayer_perceptron(x, weights, biases):

    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])
    layer_1 = tf.nn.sigmoid(layer_1)

    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])
    layer_2 = tf.nn.sigmoid(layer_2)

    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])
    layer_3 = tf.nn.sigmoid(layer_3)

    layer_4 = tf.add(tf.matmul(layer_3, weights['h4']), biases['b4'])
    layer_4 = tf.nn.relu(layer_4)

    out_layer = tf.matmul(layer_4, weights['out'] + biases['out'])

    return out_layer

# Weights and biases for each layer

weights = {
    'h1': tf.Variable(tf.truncated_normal([n_dim, n_hidden_1])),
    'h2': tf.Variable(tf.truncated_normal([n_hidden_1, n_hidden_2])),
    'h3': tf.Variable(tf.truncated_normal([n_hidden_2, n_hidden_3])),
    'h4': tf.Variable(tf.truncated_normal([n_hidden_3, n_hidden_4])),
    'out': tf.Variable(tf.truncated_normal([n_hidden_4, n_class])),
}

biases = {
    'b1': tf.Variable(tf.truncated_normal([n_hidden_1])),          
    'b2': tf.Variable(tf.truncated_normal([n_hidden_2])),          
    'b3': tf.Variable(tf.truncated_normal([n_hidden_3])),          
    'b4': tf.Variable(tf.truncated_normal([n_hidden_4])),          
    'out': tf.Variable(tf.truncated_normal([n_class]))          
}

# Initialize variables
init = tf.global_variables_initializer()

saver = tf.train.Saver()

y = multilayer_perceptron(x, weights, biases)

# Cost and optimizer
cost_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))

training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)

sess = tf.Session()
sess.run(init)

mse_history = []
accuracy_history = []

for epoch in range(training_epochs):
    sess.run(training_step, feed_dict={x: train_x, y_: train_y})
    cost = sess.run(cost_function, feed_dict={x: train_x, y_: train_y})
    cost_history = np.append(cost_history, cost)
    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    #print(""accuracy"", sess.run(accuracy, feed_dict={x: test_x, y_: test_y}))
    pred_y = sess.run(y, feed_dict={x: test_x})
    mse = tf.reduce_mean(tf.square(pred_y - test_y))
    mse_ = sess.run(mse)
    mse_history.append(mse_)
    accuracy = (sess.run(accuracy, feed_dict={x: train_x, y_: train_y}))
    accuracy_history.append(accuracy)

    print(""epoch: "", epoch, "" - "", ""cost: "", cost, "" - MSE: "", mse_, "" - accuracy: "", accuracy)
</code></pre>

<p>Aside from the first two or so executions, the accuracy in the for loop is always ""0.5481928"".</p>
",0
54175038,Creating a serving graph separately from training in tensorflow for Google CloudML deployment?,"<p>I am trying to deploy a <code>tf.keras</code> image classification model to Google CloudML Engine. Do I have to include code to create serving graph separately from training to get it to serve my models in a web app? I already have my model in SavedModel format (<code>saved_model.pb</code> &amp; variable files), so I'm not sure if I need to do this extra step to get it to work. </p>

<p>e.g. this is code directly from GCP Tensorflow Deploying models <a href=""https://cloud.google.com/ml-engine/docs/tensorflow/deploying-models"" rel=""nofollow noreferrer"">documentation</a></p>

<pre><code>def json_serving_input_fn():
  """"""Build the serving inputs.""""""
  inputs = {}
  for feat in INPUT_COLUMNS:
    inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)

  return tf.estimator.export.ServingInputReceiver(inputs, inputs)
</code></pre>
",0
54183967,Using tf.map_fn with multiple GPUs,"<p>I'm trying to extend my single-GPU TensorFlow code to multi-GPU. I have to work on 3 degrees of freedom and unfortunately I need to use tf.map_fn to parallelize over the 3rd one. I tried to use device placement as shown in the official documentation, but it looks like it is impossible to do it with <code>tf.map_fn</code>. Is there a way to run <code>tf.map_fn</code> on multiple GPUs?</p>

<p>Here the error output:</p>

<pre><code>InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'map_1/TensorArray_1': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/device:GPU:1'
Colocation Debug Info:
Colocation group had the following types and devices: 
TensorArrayGatherV3: GPU CPU 
Range: GPU CPU 
TensorArrayWriteV3: GPU CPU 
TensorArraySizeV3: GPU CPU 
MatMul: GPU CPU 
Enter: GPU CPU 
TensorArrayV3: GPU CPU 
Const: GPU CPU 

Colocation members and user-requested devices:
  map_1/TensorArrayStack/range/delta (Const) 
  map_1/TensorArrayStack/range/start (Const) 
  map_1/TensorArray_1 (TensorArrayV3) 
  map_1/while/TensorArrayWrite/TensorArrayWriteV3/Enter (Enter) /device:GPU:1
  map_1/TensorArrayStack/TensorArraySizeV3 (TensorArraySizeV3) 
  map_1/TensorArrayStack/range (Range) 
  map_1/TensorArrayStack/TensorArrayGatherV3 (TensorArrayGatherV3) 
  map_1/while/MatMul (MatMul) /device:GPU:1
  map_1/while/TensorArrayWrite/TensorArrayWriteV3 (TensorArrayWriteV3) /device:GPU:1

         [[Node: map_1/TensorArray_1 = TensorArrayV3[clear_after_read=true, dtype=DT_FLOAT, dynamic_size=false, element_shape=&lt;unknown&gt;, identical_element_shapes=true, tensor_array_name=""""](map_1/TensorArray_1/size)]]
</code></pre>

<p>Here a simple code example to reproduce it:</p>

<pre><code>import tensorflow as tf
import numpy

rc = 1000

sess = tf.Session()

for deviceName in ['/cpu:0', '/device:GPU:0', '/device:GPU:1']:
        with tf.device(deviceName):
                matrices = tf.random_uniform([rc,rc,4],minval = 0, maxval = 1, dtype = tf.float32)

                def mult(i):
                        product = tf.matmul(matrices[:,:,i],matrices[:,:,i+1])
                        return product

                mul = tf.zeros([rc,rc,3], dtype = tf.float32)
                mul = tf.map_fn(mult, numpy.array([0,1,2]), dtype = tf.float32, parallel_iterations = 10)

m = sess.run(mul)


</code></pre>
",1
54228014,Changing label_img.py in inception image classifier to accept numpy arrays (opencv images) instead of jpg files,"<p>I am building an image classifier using the tensorflow for poets inception pretrained model. My model is trained and it works well. However, when integrating it into my application, I have run into a problem. The existing code reads a jpg (or other image format) file and creates a tensor which it applies to the model. I want to input a numpy array (opencv image) instead of reading the image from disk.</p>

<p>I have tried convert_to_tensor (<a href=""https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor</a>) but i am not able to use the result in the rest of the code.  Other methods refer to an older example from a deprecated repo that use something like </p>

<p>tfImage = tf.gfile.FastGFile(imageFileWithPath, 'rb').read()
run the network to get the predictions
predictions = sess.run(finalTensor, {'DecodeJpeg/contents:0': tfImage})</p>

<p>I have screwed up the indentation while pasting but you should be able to get the idea</p>

<pre><code>def read_tensor_from_image_file(file_name, input_height=299, 
input_width=299,
            input_mean=0, input_std=255):
input_name = ""file_reader""
output_name = ""normalized""

if type(file_name) is str:
 file_reader = tf.read_file(file_name, input_name)
if file_name.endswith("".png""):
  image_reader = tf.image.decode_png(file_reader, channels = 3,
                                     name='png_reader')
elif file_name.endswith("".gif""):
  image_reader = tf.squeeze(tf.image.decode_gif(file_reader,
                                                name='gif_reader'))
elif file_name.endswith("".bmp""):
  image_reader = tf.image.decode_bmp(file_reader, name='bmp_reader')
else:
  image_reader = tf.image.decode_jpeg(file_reader, channels = 3,
                                      name='jpeg_reader')
float_caster = tf.cast(image_reader, tf.float32)
dims_expander = tf.expand_dims(float_caster, 0);
resized = tf.image.resize_bilinear(dims_expander, [input_height, 
input_width])

elif type(file_name) is np.ndarray:
 tf_img = tf.convert_to_tensor(file_name)
 resized = tf.image.resize_bilinear(tf_img, [input_height, input_width])
 normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])
sess = tf.Session()
result = sess.run(normalized)

return result

def classify(file_name,model_file = 
 ""tf_files/retrained_graph.pb"",label_file = 
 ""tf_files/retrained_labels.txt"",output_layer = ""final_result""):



 input_height = 299
 input_width = 299
 input_mean = 128
 input_std = 128
 input_layer = ""Mul""

 graph = load_graph(model_file)

 t = read_tensor_from_image_file(file_name,
                              input_height=input_height,
                              input_width=input_width,
                              input_mean=input_mean,
                              input_std=input_std)


 input_name = ""import/"" + input_layer
 output_name = ""import/"" + output_layer
 input_operation = graph.get_operation_by_name(input_name);
 output_operation = graph.get_operation_by_name(output_name);



 with tf.Session(graph=graph) as sess:
  start = time.time()
  results = sess.run(output_operation.outputs[0],
                  {input_operation.outputs[0]: t})
  end=time.time()
 results = np.squeeze(results)

top_k = results.argsort()[-5:][::-1]
labels = load_labels(label_file)

print('\nEvaluation time (1-image): {:.3f}s\n'.format(end-start))
template = ""{} (score={:0.5f})""
for i in top_k:
 print(template.format(labels[i], results[i]))
</code></pre>

<p>The above code works when given a path to a file like it was intended, but my modifications to read_tensor_from_image_file use a numpy array give errors. I suspect i need to replace 't' to change it to a tensor operator for output_operations. Any help is appreciated.</p>
",0
54272541,Data type error for tf.data.Dataset.from_tensor_slices... Cannot convert a TensorShape to dtype: <dtype: 'float32'>,"<p>I'm trying to take data from a csv with a list of files and a list of labels, and convert it to being one-hot labeled for a categorical classification using tf.keras. I am using eager mode for the code. </p>

<p>I'm trying to follow the tf.data example from CS230 building a data pipeline.</p>

<p><a href=""https://cs230-stanford.github.io/tensorflow-input-data.html"" rel=""nofollow noreferrer"">https://cs230-stanford.github.io/tensorflow-input-data.html</a></p>

<p>my code is below under the code section. </p>

<p>the csv file that lists the location of all the pictures is located on dropbox here: 
<a href=""https://www.dropbox.com/s/5uo8o1p30g2aeta/Clock.csv?dl=0"" rel=""nofollow noreferrer"">https://www.dropbox.com/s/5uo8o1p30g2aeta/Clock.csv?dl=0</a></p>

<p>When I run the code as shown below I get a </p>

<pre><code>TypeError: Cannot convert a TensorShape to dtype: &lt;dtype: 'float32'&gt;
error.
</code></pre>

<p>When I add to line 55 and make line 56 :</p>

<pre><code>one_hot_Hr = tf.one_hot(file.Hr,classes)
one_hot_Hr = tf.to_int32(one_hot_Hr)
</code></pre>

<p>I get this  error:</p>

<pre><code>InvalidArgumentError: cannot compute Mul as input #0 was expected to be 
a float tensor but is a int32 tensor [Op:Mul] 
name: loss/activation_2_loss/mul/
</code></pre>

<p>when I run</p>

<pre><code>iterator.get_next()
</code></pre>

<p>the pictures are formated as </p>

<pre><code>&lt;tf.Tensor: id=12462, shape=(32, 300, 300, 3), dtype=float32, numpy=
</code></pre>

<p>the labels are formated as:</p>

<pre><code> &lt;tf.Tensor: id=12463, shape=(32, 13), dtype=float32, numpy=
</code></pre>

<p>based on the errors, it seems like it should be a simple formatting issue with the labels, but I'm stumped and neither error brings up much useful information on stack overflow. </p>

<p>Code:</p>

<pre><code>import pandas as pd
import tensorflow as tf
import tensorflow.keras as k
#import cv2
#tf.enable_eager_execution()
#import argparse
#from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense



def parse_function(filename, label):
    image_string = tf.read_file(filename)

    # Don't use tf.image.decode_image, or the output shape will be undefined
    image = tf.image.decode_jpeg(image_string, channels=3)

    # This will convert to float values in [0, 1]
    image = tf.image.convert_image_dtype(image, tf.float32)


    image = tf.image.resize_images(image, [300, 300])
    return image, label



def train_preprocess(image, label):
    image = tf.image.random_flip_left_right(image)

    image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)
    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)

    # Make sure the image is still in [0, 1]
    image = tf.clip_by_value(image, 0.0, 1.0)

    return image, label

batch_size = 32
classes = 13

fileLoc = ""C:/Users/USAgData/TF/Clock.csv""
file = pd.read_csv(fileLoc)
file['Loc']=''
file.Loc = str(str(file.Location)[9:23] + str(file.Location)[28:46])


one_hot_Hr = tf.one_hot(file.Hr,classes)
#one_hot_Hr = tf.to_int32(one_hot_Hr)



dataset = tf.data.Dataset.from_tensor_slices((file.Loc, one_hot_Hr))
dataset = dataset.shuffle(len(file.Location))
dataset = dataset.map(parse_function, num_parallel_calls=4)
dataset = dataset.map(train_preprocess, num_parallel_calls=4)
dataset = dataset.batch(batch_size)
dataset = dataset.prefetch(1)

#print(dataset.shape) # ==&gt; ""(tf.float32, tf.float32)""

iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()

#print(next_element)

tf.keras.backend.clear_session()

model_name=""Documentation""
model = k.Sequential()
model.add(Conv2D(64, (3, 3), input_shape=(300,300,3))) #Changed shape to include batch
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

#model.add(Conv2D(32, (3, 3)))
#model.add(Activation('relu'))
#model.add(MaxPooling2D(pool_size=(2, 2)))

#model.add(Conv2D(64, (3, 3)))
#model.add(Activation('relu'))
#model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Flatten())
model.add(Dense(32))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(classes))
model.add(Activation('softmax')) #Changed from sigmoid




#changed from categorical cross entropy
model.compile(loss='categorical_crossentropy',
              optimizer=tf.train.RMSPropOptimizer(.0001),
              metrics=['accuracy'])

model.summary()



fitting = model.fit_generator(iterator,epochs =1 ,shuffle=False, steps_per_epoch=14400//batch_size)


#model.evaluate(dataset,steps=30)


import sys
print(sys.version)
tf.__version__
</code></pre>

<p>I'm running:
tf: 1.10.0
Python: 3.6.7 |Anaconda custom (64-bit)| (default, Dec 10 2018, 20:35:02) [MSC v.1915 64 bit (AMD64)]</p>

<p>I don't know if this should truly be the solution, but when I switch: </p>

<pre><code>fitting = model.fit_generator(iterator,epochs =1 ,shuffle=False, steps_per_epoch=14400//batch_size)
</code></pre>

<p>to </p>

<pre><code>fitting = model.fit(iterator,epochs = 1 , shuffle = False, steps_per_epoch = 14400//batch_size)
</code></pre>

<p>The model does start to train. But, then them model runs out of data points because the iterator will not start over again. </p>
",0
54516938,Weights decay on evaluation step - Tensorflow,"<p>My weights are defined as </p>

<pre><code>weights = {
        'W_conv1': tf.get_variable('W_conv1', shape=[...], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.01)),
        'W_conv2': tf.get_variable('W_conv2', shape=[...], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.01)),
        'W_conv3': tf.get_variable('W_conv3', shape=[...], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.01)),
        ...
}

# conv2d network
...
</code></pre>

<p>I want to use the weights decay so I add, for example, the argument</p>

<pre><code>regularizer=tf.contrib.layers.l1_regularizer(0.0005)
</code></pre>

<p>to the <code>tf.get_variable</code>. Now I'm wondering if during the evaluation phase this is still correct or maybe I have to set the regularizer factor to 0.</p>

<p>There is also another argument <code>trainable</code>. The documentation says <code>If True also add the variable to the graph collection GraphKeys.TRAINABLE_VARIABLES.</code> which is not clear to me. Should I use it?</p>

<p>Can someone explain to me if the weights decay effects in a sort of wrong way the evaluation step? How can I solve in that case?</p>
",1
54524992,Tensorflow serving trained model saved with saved_model,"<p>I find tf.saved_model documentation not clear, is there any valuable resources how to read trained model within other session?</p>
",1
54539396,Distributing a Keras Model Across Multiple GPUs,"<p>I'm trying to create a very large Keras model and distribute it across multiple GPUs. To be clear I'm not trying to put multiple copies of the same model on multiple GPUs; I'm trying to put one large model across multiple GPUs. I've been using the multi_gpu_model function in Keras but based off a lot of the out of memory errors I've gotten while doing this it seems like it's just replicating the model rather than distributing it like I'd like.</p>

<p>I looked into Horovod but because I have a lot of windows specific logging tools running I'm hesitant to use it.</p>

<p>This seems to leave only tf.estimators for me to use. It's not clear from documentation though how I would use these estimators to do what I'm trying to do. For example which distribution strategy in 
tf.contrib.distribute would allow me to effectively batch out the model in the way I'm looking to do?</p>

<p>Is what I'm seeking to do with estimators possible and if so which strategy should I use?</p>
",1
54615708,Exporting a Keras model as a TF Estimator: trained model cannot be found,"<p>I encountered the following issue when trying to export a Keras model as a TensorFlow Estimator with the purpose of serving the model. Since the same problem also popped up <a href=""https://stackoverflow.com/questions/54175038/creating-a-serving-graph-separately-from-training-in-tensorflow-for-google-cloud/54178060#54178060"">in an answer to this question</a>, I will illustrate what happens on a toy example and provide my workaround solution for documentation purposes. This behaviour occurs with Tensorflow 1.12.0 and Keras 2.2.4. This happens with actual Keras as well as with <code>tf.keras</code>.</p>

<p>The problem occurs when trying to export an Estimator that was created from a Keras model with <code>tf.keras.estimator.model_to_estimator</code>. Upon calling <code>estimator.export_savedmodel</code>, either a <code>NotFoundError</code> or a <code>ValueError</code> is thrown.</p>

<p>The below code reproduces this for a toy example.</p>

<p>Create a Keras model and save it:</p>

<pre><code>import keras
model = keras.Sequential()
model.add(keras.layers.Dense(units=1,
                                activation='sigmoid',
                                input_shape=(10, )))
model.compile(loss='binary_crossentropy', optimizer='sgd')
model.save('./model.h5')
</code></pre>

<p>Next, convert the model to an estimator with <code>tf.keras.estimator.model_to_estimator</code>, add an input receiver function and export it in the <code>Savedmodel</code> format with <code>estimator.export_savedmodel</code>:</p>

<pre><code># Convert keras model to TF estimator
tf_files_path = './tf'
estimator =\
    tf.keras.estimator.model_to_estimator(keras_model=model,
                                          model_dir=tf_files_path)
def serving_input_receiver_fn():
    return tf.estimator.export.build_raw_serving_input_receiver_fn(
        {model.input_names[0]: tf.placeholder(tf.float32, shape=[None, 10])})

# Export the estimator
export_path = './export'
estimator.export_savedmodel(
    export_path,
    serving_input_receiver_fn=serving_input_receiver_fn())
</code></pre>

<p>This will throw:</p>

<pre><code>ValueError: Couldn't find trained model at ./tf.
</code></pre>
",0
54686895,Tensorflow dilation behave differently than morphological dilation,"<p>As the following piece of code shows, the <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dilation2d"" rel=""noreferrer"">tensorflow <code>tf.nn.dilation2D</code> function</a> doesn't behave as a <a href=""https://homepages.inf.ed.ac.uk/rbf/HIPR2/dilate.htm"" rel=""noreferrer"">conventional dilation operator</a>. </p>

<pre><code>import tensorflow as tf
tf.InteractiveSession()
A = [[0, 0, 0, 0, 0, 0, 0],
     [0, 0, 0, 0, 1, 0, 0],
     [0, 0, 0, 1, 1, 1, 0],
     [0, 0, 0, 0, 1, 0, 0],
     [0, 0, 0, 0, 0, 0, 0],
     [0, 0, 0, 0, 0, 0, 0]]
kernel = tf.ones((3,3,1))
input4D = tf.cast(tf.expand_dims(tf.expand_dims(A, -1), 0), tf.float32)
output4D = tf.nn.dilation2d(input4D, filter=kernel, strides=(1,1,1,1), rates=(1,1,1,1), padding=""SAME"")
print(tf.cast(output4D[0,:,:,0], tf.int32).eval())
</code></pre>

<p>Returns the following tensor:</p>

<pre><code>array([[1, 1, 1, 2, 2, 2, 1],
       [1, 1, 2, 2, 2, 2, 2],
       [1, 1, 2, 2, 2, 2, 2],
       [1, 1, 2, 2, 2, 2, 2],
       [1, 1, 1, 2, 2, 2, 1],
       [1, 1, 1, 1, 1, 1, 1]], dtype=int32)
</code></pre>

<p>I don't understand neither <strong>why</strong> it behaves like that, neither <strong>how</strong> I should use <code>tf.nn.dilation2d</code> to retrieve the expected output:</p>

<pre><code>array([[0, 0, 0, 1, 1, 1, 0],
       [0, 0, 1, 1, 1, 1, 1],
       [0, 0, 1, 1, 1, 1, 1],
       [0, 0, 1, 1, 1, 1, 1],
       [0, 0, 0, 1, 1, 1, 0],
       [0, 0, 0, 0, 0, 0, 0]], dtype=int32)
</code></pre>

<p>Can someone enlighten the succinct documentation of tensorflow and give an explanation of what the the <code>tf.nn.dilation2D</code> function does ?</p>
",1
54720027,Tensorflow=1.12.0 AttributeError: module 'tensorflow' has no attribute 'feature_column',"<p>I am trying to go through a <code>Tensorflow</code> tutorial that uses <code>tf.feature_column</code>, however when running it I am encountered with this error. </p>

<p>I have <code>tensorflow=1.12.0</code> installed. I am running it on <code>Python 3.6.8</code>.</p>

<p>This looks to be the most recent stable package of <code>tensorflow</code> and the docs say that Python 3.6 is supported. I have also checked the tensorflow package files and found that <code>feature_column</code> is included.</p>

<p>Any idea why this error is still persisting?</p>

<p>Full error:</p>

<pre><code>Traceback (most recent call last):
  File ""tensorflow.py"", line 1, in &lt;module&gt;
    import tensorflow as tf
  File ""/Users/blakecarroll/SFInsuretech/virtEnv1/tensorflow.py"", line 50, in &lt;module&gt;
    categorical_object_feat_cols = [tf.feature_column.embedding_column(tf.feature_column.categorical_column_with_hash_bucket(key=col,hash_bucket_size=1000), dimension = len(df[col].unique())) for col in categorical_columns if df[col].dtype=='O']
  File ""/Users/blakecarroll/SFInsuretech/virtEnv1/tensorflow.py"", line 50, in &lt;listcomp&gt;
    categorical_object_feat_cols = [tf.feature_column.embedding_column(tf.feature_column.categorical_column_with_hash_bucket(key=col,hash_bucket_size=1000), dimension = len(df[col].unique())) for col in categorical_columns if df[col].dtype=='O']
AttributeError: module 'tensorflow' has no attribute 'feature_column'
</code></pre>
",0
54843448,"How to ""zip"" Tensorflow Dataset and train in Keras correctly?","<p>I have a <code>train_x.csv</code> and a <code>train_y.csv</code>, and I'd like to train a model using Dataset API and Keras interface. This what I'm trying to do:</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
import pandas as pd
import tensorflow as tf

tf.enable_eager_execution()

N_FEATURES = 10
N_SAMPLES = 100
N_OUTPUTS = 2
BATCH_SIZE = 8
EPOCHS = 5

# prepare fake data
train_x = pd.DataFrame(np.random.rand(N_SAMPLES, N_FEATURES))
train_x.to_csv('train_x.csv', index=False)
train_y = pd.DataFrame(np.random.rand(N_SAMPLES, N_OUTPUTS))
train_y.to_csv('train_y.csv', index=False)

train_x = tf.data.experimental.CsvDataset('train_x.csv', [tf.float32] * N_FEATURES, header=True)
train_y = tf.data.experimental.CsvDataset('train_y.csv', [tf.float32] * N_OUTPUTS, header=True)
dataset = ...  # What to do here?

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(N_OUTPUTS, input_shape=(N_FEATURES,)),
    tf.keras.layers.Activation('linear'),
])
model.compile('sgd', 'mse')
model.fit(dataset, steps_per_epoch=N_SAMPLES/BATCH_SIZE, epochs=EPOCHS)
</code></pre>

<p>What's the right way to implement this <code>dataset</code>?</p>

<p>I tried <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip"" rel=""nofollow noreferrer""><code>Dataset.zip</code></a> API like <code>dataset = tf.data.Dataset.zip((train_x, train_y))</code> but it seems not working(code <a href=""https://pastebin.com/7nX8bxus"" rel=""nofollow noreferrer"">here</a> and error <a href=""https://pastebin.com/ikfZZ88E"" rel=""nofollow noreferrer"">here</a>). I also read <a href=""https://stackoverflow.com/a/46140332/2666624"">this</a> answer, it's working but I'd like a non-functional model declaration way.</p>
",0
54863255,How to create only one copy of graph in tensorboard events file with custom tf.Estimator?,"<p>I'm using a custom <code>tf. Estimator</code> object to train a neural network. The problem is in the size of the events file after training - it is unreasonably large.
I've already solved the problem with saving part of a dataset as constant by using <code>tf.Dataset.from_generator()</code>.
However, the size is still quite large and while starting <code>tensorboard</code> I'm getting the message</p>
<p><code>W0225 10:38:07.443567 140693578311424 tf_logging.py:120] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.</code></p>
<p>So, I suppose, that I'm creating and saving many different graphs in this event file. Is it possible to turn off this saving or how to save an only first copy?</p>
<p>For know, I found only the way to delete all the default logs by deleting the events filts with</p>
<pre class=""lang-py prettyprint-override""><code>list(map(os.remove, glob.glob(os.path.join(runtime_params['model_dir'], 'events.out.tfevents*'))))
</code></pre>
<p>However, it is a bad solution for me, as I would prefer to keep the summaries and, ideally, <strong>one</strong> copy of the graph.</p>
<p>From the documentation, I can see that</p>
<p>Estimators <strong>automatically</strong> write the following to disk:</p>
<ul>
<li>checkpoints, which are versions of the model created during training.</li>
<li><strong>event files</strong>, which contain information that TensorBoard uses to create visualizations.
Is it a way to turn off writing of the <strong>events file</strong>?</li>
</ul>
",0
54897832,Feeding large numpy arrays into TensorFlow estimators via tf.data.Dataset,"<p>TensorFlow's <a href=""https://www.tensorflow.org/guide/datasets#consuming_numpy_arrays"" rel=""nofollow noreferrer""><code>tf.data.Dataset</code> documentation on consuming numpy arrays</a> states that in order to use numpy arrays in combination with the <code>Dataset</code> API, the arrays have to be small enough (&lt;2 GB in total) to be used as tensors, or they can be fed into the dataset via placeholders.</p>

<p>However, if you use <code>Dataset</code> in conjunction with estimators (where placeholders are not available), the documentation does not provide a solution on working with large arrays without placeholders. </p>

<p>Are there other options for passing placeholder values into estimators that can be used or is the solution to provide the data in <code>tfrecord</code> or <code>csv</code> format?</p>
",1
54933296,TensorFlow dataset from list of images in keras model,"<p>I'm trying to understand how to read local images, use them as TensorFlow <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">Dataset</a> and train Keras model with TF Dataset. I'm following TF Keras MNIST TPU <a href=""https://github.com/tensorflow/tpu/blob/master/tools/colab/keras_mnist_tpu.ipynb"" rel=""nofollow noreferrer"">tutorial</a>. The only difference that I want to read my set of images and train on them.</p>

<p>Let's say I have list of images (file names) and corresponding list of labels.</p>

<pre><code>files = [...] # list of file names
labels = [...] # list of labels (integers)
images = tf.constant(files) # or tf.convert_to_tensor(files)
labels = tf.constant(labels) # or tf.convert_to_tensor(labels)
dataset = tf.data.Dataset.from_tensor_slices((images, labels))
dataset = dataset.shuffle(len(files))
dataset = dataset.repeat()
dataset = dataset.map(parse_function).batch(batch_size)
</code></pre>

<p>The <code>parse_function</code> is a simple function which reads the input file name and yields the image data and corresponding label, e.g.</p>

<pre><code>def parse_function(filename, label):
    image_string = tf.read_file(filename)
    image_decoded = tf.image.decode_image(image_string)
    image = tf.cast(image_decoded, tf.float32)
    return image, label
</code></pre>

<p>At this point I have a <code>dataset</code> which is a tf.data.Dataset type (more precisely tf.data.BatchDataset) and I pass it along to keras model <code>trained_model</code> from <a href=""https://github.com/tensorflow/tpu/blob/master/tools/colab/keras_mnist_tpu.ipynb"" rel=""nofollow noreferrer"">tutorial</a>, e.g.</p>

<pre><code>history = trained_model.fit(dataset, ...)
</code></pre>

<p>But at this point code breaks with the following error:</p>

<pre><code>AttributeError: 'BatchDataset' object has no attribute 'ndim'
</code></pre>

<p>The error comes from keras which performs the check on given input like that</p>

<pre><code>from keras import backend as K
K.is_tensor(dataset) # which returns false
</code></pre>

<p>Keras tries to determine type of the input and since it is not a tensor it assumes it is numpy array and tries to get its dimension. That's why the error occurs.</p>

<p>My questions here are the following:</p>

<ul>
<li>am I reading TF dataset correctly? I looked up plenty of examples on internet and it seems I'm reading it as people suggest</li>
<li>why my dataset is not a tensor? may be I need to perform additional conversion, but it is not the case of TF <a href=""https://github.com/tensorflow/tpu/blob/master/tools/colab/keras_mnist_tpu.ipynb"" rel=""nofollow noreferrer"">tutorial</a></li>
<li>why in TF <a href=""https://github.com/tensorflow/tpu/blob/master/tools/colab/keras_mnist_tpu.ipynb"" rel=""nofollow noreferrer"">tutorial</a> everything works with tf datasets, I really don't see any difference from they way how they read MNIST data (which is in different data-format, but eventually they get images) and what I'm doing here.</li>
</ul>

<p>Any suggestion would be greatly appreciated.</p>

<p>Please note, even TF <a href=""https://github.com/tensorflow/tpu/blob/master/tools/colab/keras_mnist_tpu.ipynb"" rel=""nofollow noreferrer"">tutorial</a> is about TPUs it is structured such that it works on both TPUs and CPU/GPUs.</p>
",0
54945641,"keras, model still setting expected input shape from training despite input_shape(None, ...)","<p>I have a simple CNN model written in the tf.keras framework, which I wish to use with variable input size.</p>

<p>According to <a href=""https://github.com/keras-team/keras/issues/1920#issuecomment-193883690"" rel=""nofollow noreferrer"">this</a> ""documentation"" I can use variable input size by setting <code>input_shape=(None, None, n_channels)</code>, and I have used a <code>GlobalMaxPooling2D</code> layer before my dense layer to standardize the input to the dense layer.</p>

<p>Yet when I train the model with one size of image and try to predict on a different size I get the error:</p>

<pre><code>  File ""multilabel_384.py"", line 180, in main
probabilities = model.predict(test_data)
File ""/usr/local/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py"", line 1471, in predict
x, check_steps=True, steps_name='steps', steps=steps)
File ""/usr/local/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/keras/engine/training.py"", line 868, in _standardize_user_data
exception_prefix='input')
File ""/usr/local/miniconda3/envs/deepchem/lib/python3.5/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 191, in standardize_input_data
' but got array with shape ' + str(data_shape))
ValueError: Error when checking input: expected sequential_input to have shape (16, 24, 1) but got array with shape (32, 48, 1)
</code></pre>

<p>This is the code used to define my model:</p>

<pre><code>from tensorflow.keras import layers
import tensorflow as tf

def make_model(num_classes=8):
    # type (int) -&gt; tf.keras.model
    """"""implementation of SimpleNet in keras""""""
    model = tf.keras.Sequential()
    # conv layers
    model.add(layers.ZeroPadding2D(2))
    model.add(layers.Conv2D(input_shape=(None, None, 1),
                            filters=32, kernel_size=5, activation=""relu""))
    model.add(layers.BatchNormalization())
    model.add(layers.ZeroPadding2D(2))
    model.add(layers.Conv2D(filters=64,  kernel_size=5, activation=""relu""))
    model.add(layers.Conv2D(filters=128, kernel_size=3, activation=""relu""))
    model.add(layers.Conv2D(filters=256, kernel_size=3, activation=""relu""))
    model.add(layers.Conv2D(filters=128, kernel_size=3, activation=""relu""))
    model.add(layers.GlobalMaxPooling2D())
    # dense layers
    model.add(layers.Flatten())
    model.add(layers.Dense(128, activation=""relu""))
    model.add(layers.Dropout(0.25))
    model.add(layers.Dense(256, activation=""relu""))
    model.add(layers.Dropout(0.25))
    # use sigmoid for multiclass problems
    model.add(layers.Dense(num_classes, activation=""sigmoid""))
    return model
</code></pre>

<p>So in essence my question is why is keras <em>still</em> defining an expected input shape, and is there any way to disable this implicit <code>standardize_input_data</code> that's going on?</p>
",1
54966581,tf.boolean_mask not accepting the axis argument,"<p>Here is my code:  </p>

<pre><code> 44     scores = tf.boolean_mask(box_class_scores,filtering_mask,axis=-1)
 45     boxes = tf.boolean_mask(boxes,filtering_mask,axis=-1)
 46     classes = tf.boolean_mask(box_classes,filtering_mask,axis=-1)
</code></pre>

<p>Error, I'm getting:</p>

<blockquote>
  <p>TypeError: boolean_mask() got an unexpected keyword argument 'axis'</p>
</blockquote>

<p>The <code>tf.boolean_mask()</code> is not accepting the axis argument but is a valid argument as can be seen in the documentation: <a href=""https://www.tensorflow.org/api_docs/python/tf/boolean_mask"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/boolean_mask</a></p>

<p><a href=""https://i.stack.imgur.com/bne6B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bne6B.png"" alt=""enter image description here""></a></p>
",1
54990510,How to allocate multiple TPUs and adjust code to run on all of them,"<p>as a part of my R&amp;D I was given access to multiple TPUs, but I can't find documentation how to allocate them together for my training purposes, both node-wise and code-wise. The documentation said <code>ctpu up -zone MY_ZONE_CHOICE</code> but this command allocates only single TPU. And, similar what changes should I add to my code if I want to use multiple TPUs? So far I've used this call <code>tf.contrib.cluster_resolver.TPUClusterResolver()</code> to check for TPU, what should be changed (if any) to check if I can access multiple TPUs?</p>
",1
55005915,How to resize image to put into tf.train.Example,"<p>I have an image (JPEG or PNG) as a byte buffer (read from the internet), and this is the way I was putting it in a <code>tf.train.Example</code> before:</p>

<pre><code>record = tf.train.Example(features=tf.train.Features(feature={
    'image/encoded': dataset_util.bytes_feature(image_bytes)
    # there are more features but they're not relevant
}))
</code></pre>

<p>However, for my usecase, the images are too big, so I'd like to resize them either before I put them in the <code>tf.train.Example</code> or just after (whichever is easiest).</p>

<p>Here's what I'm trying:</p>

<pre><code># predeclared
# - image_bytes
# - image_format
# - height
# - width

# resize image
if image_format == b'jpeg':
    image = tf.image.decode_jpeg(image_bytes, None, tf.float32)
elif image_format == b'png':
    image = tf.image.decode_png(image_bytes, None, tf.float32)

image = tf.image.resize_images(image, (int(height), int(width)))

image = tf.image.convert_image_dtype(image, tf.uint8)
record = tf.train.Example(features=tf.train.Features(feature={
    'image/encoded': dataset_util.bytes_feature(tf.image.encode_jpeg(image))
    # there are more features but they're not relevant
}))
</code></pre>

<p>I suspect this is valid right up until I actually try to put it in the <code>tf.train.Example</code>, at which point it tells me <code>TypeError: &lt;tf.Tensor 'EncodeJpeg:0' shape=() dtype=string&gt; has type Tensor, but expected one of: bytes</code>. I've tried figuring out how to get the <code>Tensor</code> into a <code>BytesList</code> or something like it, but I haven't been able to find any documentation for this. I suspect there may be a better way to approach the entire process however.</p>

<p>How can I do this the right way?</p>
",1
55044905,"Tensorflow low level api, batch normalization problem","<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization"" rel=""nofollow noreferrer"">tf.layers.batch_normalization</a> documentation says it will be removed in a future version, and should be replaced by <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization"" rel=""nofollow noreferrer"">tf.keras.layers.BatchNormalization</a>, but i cannot find a way to replace the functionality using tensorflow low level api.</p>

<pre><code>import tensorflow as tf
bn = tf.layers.batch_normalization(tf.constant([0.0]), training=True)
print(tf.get_collection(tf.GraphKeys.UPDATE_OPS))
</code></pre>

<p>which outputs:</p>

<pre><code>[&lt;tf.Operation 'batch_normalization/AssignMovingAvg' type=AssignSub&gt;,
&lt;tf.Operation 'batch_normalization/AssignMovingAvg_1' type=AssignSub&gt;]
</code></pre>

<p>If we instead use keras as suggested in the documentation</p>

<pre><code>bn = tf.keras.layers.BatchNormalization(axis=-1)(tf.constant([0.0]), training=True)
</code></pre>

<p>we get an empty output:</p>

<pre><code>[]
</code></pre>

<p>Since UPDATE_OPS is empty, the model is unable to update the batch normalization moving_avg_mean and moving_avg_variance during training using keras (resulting in a much higer test error). Any suggestion how to solve this is greatly appreciated! </p>

<p>The example above is taken from an older post of how to use <a href=""https://stackoverflow.com/questions/48874558/tensorflow-tf-layers-batch-normalization-doesnt-add-update-ops-to-tf-graphke"">tf.layers.batch_normalization</a></p>
",1
55094952,Understanding Tensorflow control dependencies,"<p>I am trying to gain a stronger grasp of TensorFlow. I came across the concept of control dependencies. I understand that the order of ops as specified by us is not really relevant to Tensorflow during execution. In order to optimise the speed of execution TensorFlow decides its own order of calculating nodes. 
But we can customise order of execution by using tf.control_dependencies.
I am not able to understand the use cases of the function. Can anyone direct me to some resource(other than the documentation) or explain the working of this function?
An example:</p>

<pre><code>tf.reset_default_graph()
x = tf.Variable(5)
y=tf.Variable(3)
assign = tf.assign(x,x+y)
z = x+assign
with tf.Session() as sess:
   sess.run(tf.global_variables_initializer())
   with tf.control_dependencies([assign]):
        z_out = sess.run(z)

print(z_out)
</code></pre>

<p>The output of the code is 8. So I infer that since z=x+y,the assign node has not been evaluated(right?). But doesn't this mean that the result of tensorflow may be erroneous? This means we need to create new nodes during every operation to force TensorFlow to calculate all the nodes leading up to the result. But in say training a neural network with 10000 steps if each step creates a new set of 1000 weights/parameters won't the space complexity explode?</p>
",1
55109696,TensorFlow - Difference between tf.keras.layers.Layer vs tf.keras.Model,"<p>Reading through the <a href=""https://www.tensorflow.org/tutorials/eager/custom_layers"" rel=""noreferrer"">documentation of implementing custom layers</a> with <code>tf.keras</code>, they specify two options to inherit from, <code>tf.keras.Layer</code> and <code>tf.keras.Model</code>.</p>

<p>Under the context of <em>creating custom layers</em>, I'm asking myself what is the difference between these two? Technically what is different?</p>

<p>If I were to implement the transformer encoder for example, which one would be more suitable? (assuming the transformer is a only a ""layer"" in my full model)</p>
",1
55116078,Sample weighting for CNN in TensorFlow,"<p>I have a CNN implemented in Tensorflow adapted from the tutorial:
<a href=""https://www.tensorflow.org/tutorials/estimators/cnn#load_training_and_test_data"" rel=""nofollow noreferrer"">CNN with Estimators</a>.</p>

<p>Excerpt from data_input_fn:</p>

<pre><code>dataset = dataset.batch(batch_size)  
iterator = dataset.make_one_shot_iterator()
features, labels = iterator.get_next()
return features, labels
</code></pre>

<p>Excerpt from model_fn:</p>

<pre><code>loss = tf.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)
train_op = optimizer.minimize(  loss=loss)
</code></pre>

<p>I’m using <code>tf.estimator.Estimator</code> and providing steps = 10000 while calling the train function. How can I provide weights to all the samples (around 3M samples) in the dataset during training? My understanding is all samples have equal weight (1/N) during training of the first model and are used to calculate a weighted loss. Then, based on calculated loss values, the weights should be modified, saved and reloaded during training of the next model. Should the weights be provided from a csv file because the original TF Record files don’t contain weights or is there a better way to do this? Also, is there an accepted function to modify the weights based on the loss values?</p>
",0
55122902,from_tensor_slices() with big numpy array while using tf.keras,"<p>I have some training data in a numpy array - it fits in the memory but it is bigger than 2GB. I'm using tf.keras and the dataset API. To give you a simplified, self-contained example:</p>

<pre><code>import numpy as np
import tensorflow as tf
from tensorflow.keras import layers

model = tf.keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(32,)),
    layers.Dense(64, activation='relu'),
    layers.Dense(1)
])

model.compile(optimizer=tf.train.AdamOptimizer(0.001),
          loss='mse',
          metrics=['mae'])

# generate some big input datasets, bigger than 2GB
data = np.random.random((1024*1024*8, 32))
labels = np.random.random((1024*1024*8, 1))
val_data = np.random.random((100, 32))
val_labels = np.random.random((100, 1))

train_dataset = tf.data.Dataset.from_tensor_slices((data, labels))
train_dataset = train_dataset.batch(32).repeat()

val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels))
val_dataset = val_dataset.batch(32).repeat()

model.fit(train_dataset, epochs=10, steps_per_epoch=30,
      validation_data=val_dataset, validation_steps=3)
</code></pre>

<p>So, executing this results in an error ""Cannot create a tensor proto whose content is larger than 2GB"". The documentation lists a solution to this problem: <a href=""https://www.tensorflow.org/guide/datasets#consuming_numpy_arrays"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/datasets#consuming_numpy_arrays</a> - just use tf.placeholders and then feed_dict in session run.</p>

<p>Now the main question is: how to do this with tf.keras? I cannot feed anything for the placeholders when I call model.fit() and in fact when I introduced the placeholders I got errors saying ""You must feed a value for placeholder tensor"".</p>
",0
55140060,Tensorflow cannot open MNIST anymore,"<p>I am starting to work again on tensorflow. I was relaunching some codes I did a few years ago, it's not working though.</p>

<p><strong>Old version</strong></p>

<pre><code>from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets(""MNIST_data"", one_hot=True)

import tensorflow as tf

# Parameters
learning_rate = 0.001
training_epochs = 20
batch_size = 128  # Decrease batch size if you don't have enough memory
display_step = 1

n_input = 784  # MNIST data input (img shape: 28*28)
n_classes = 10  # MNIST total classes (0-9 digits)

n_hidden_layer = 256 # layer number of features

# Store layers weight &amp; bias
weights = {
    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),
    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))
}
biases = {
    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),
    'out': tf.Variable(tf.random_normal([n_classes]))
}

# tf Graph input
x = tf.placeholder(""float"", [None, 28, 28, 1])
y = tf.placeholder(""float"", [None, n_classes])

x_flat = tf.reshape(x, [-1, n_input])

# Hidden layer with RELU activation
layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']),\
    biases['hidden_layer'])
layer_1 = tf.nn.relu(layer_1)
# Output layer with linear activation
logits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])

# Define loss and optimizer
cost = tf.reduce_mean(\
    tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\
    .minimize(cost)

# Initializing the variables
init = tf.global_variables_initializer()

# Launch the graph
with tf.Session() as sess:
    sess.run(init)
    # Training cycle
    for epoch in range(training_epochs):
        total_batch = int(mnist.train.num_examples/batch_size)
        # Loop over all batches
        for i in range(total_batch):
            batch_x, batch_y = mnist.train.next_batch(batch_size)
            # Run optimization op (backprop) and cost op (to get loss value)
            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})
</code></pre>

<p>From what I have understood the cause comes from the ""read_data_sets"" and I should use ""tf.data"". The problem with ""tf.data"" is that I cannot use that anymore:</p>

<pre><code>mnist.train.num_examples
mnist.train.next_batch
</code></pre>

<p>And the data is not one encoded.</p>

<p>I have tried something like that:</p>

<pre><code>import tensorflow_datasets as tfds
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

# Mandatory: to launch 
#tf.enable_eager_execution() 

mnist_data, info = tfds.load(""mnist"", with_info=True, as_supervised=True)
mnist_train, mnist_test = mnist_data[""train""], mnist_data[""test""]
</code></pre>

<p>And, mnist_train.batch instead of mnist.train.next_batch</p>

<pre><code># Launch the graph
with tf.Session() as sess:
    sess.run(init)
    # Training cycle
    for epoch in range(training_epochs):
        total_batch = int(info.splits[""train""].num_examples/batch_size)
        print(total_batch)
        # Loop over all batches
        for i in range(total_batch):
            batch_x, batch_y = mnist_train.batch(batch_size)
            # Run optimization op (backprop) and cost op (to get loss value)
            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})
</code></pre>

<p>With the error:</p>

<pre><code>RuntimeError: dataset.__iter__() is only supported when eager execution is enabled.
</code></pre>

<p>And if I do :</p>

<pre><code>tf.enable_eager_execution() 
</code></pre>

<p>I cannot use </p>

<pre><code>tf.placeholder()
</code></pre>

<p><strong>New version</strong></p>

<pre><code>import tensorflow_datasets as tfds
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

# Mandatory: to launch 
#tf.enable_eager_execution() 

mnist_data, info = tfds.load(""mnist"", with_info=True, as_supervised=True)
mnist_train, mnist_test = mnist_data[""train""], mnist_data[""test""]

    import tensorflow as tf

    # Parameters
    learning_rate = 0.001
    training_epochs = 20
    batch_size = 128  # Decrease batch size if you don't have enough memory
    display_step = 1

    n_input = 784  # MNIST data input (img shape: 28*28)
    n_classes = 10  # MNIST total classes (0-9 digits)

    n_hidden_layer = 256 # layer number of features

    # Store layers weight &amp; bias
    weights = {
        'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),
        'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))
    }
    biases = {
        'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),
        'out': tf.Variable(tf.random_normal([n_classes]))
    }

    # tf Graph input
    x = tf.placeholder(""float"", [None, 28, 28, 1])
    y = tf.placeholder(""float"", [None, n_classes])

    x_flat = tf.reshape(x, [-1, n_input])

    # Hidden layer with RELU activation
    layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']),\
        biases['hidden_layer'])
    layer_1 = tf.nn.relu(layer_1)
    # Output layer with linear activation
    logits = tf.add(tf.matmul(layer_1, weights['out']), biases['out'])

    # Define loss and optimizer
    cost = tf.reduce_mean(\
        tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))
    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\
        .minimize(cost)

    # Initializing the variables
    init = tf.global_variables_initializer()

# Launch the graph
with tf.Session() as sess:
    sess.run(init)
    # Training cycle
    for epoch in range(training_epochs):
        total_batch = int(info.splits[""train""].num_examples/batch_size)
        print(total_batch)
        # Loop over all batches
        for i in range(total_batch):
            batch_x, batch_y = mnist_train.batch(batch_size)
            # Run optimization op (backprop) and cost op (to get loss value)
            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})
</code></pre>
",0
55141486,Unable to see keras model graph in Tensorboard when using TensorFlow 2.0 Alpha,"<p>I am trying custom training on TensorFlow 2.0 alpha and at the same time I am trying to add some metrics and my training graph to TensorBoard. Consider the following contrived example</p>

<pre><code>import tensorflow as tf
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.models import Model


def create_model():
    inp = Input((32, ))
    net = Dense(16, activation=""relu"")(inp)
    net = Dense(8, activation=""relu"")(net)
    net = Dense(2, activation=None)(net)
    return Model(inp, net)


@tf.function
def grad(model, loss, x, y):
    with tf.GradientTape() as tape:
        y_ = model(x)
        loss_value = loss(y_true=y, y_pred=y_)
    return loss_value, tape.gradient(loss_value, model.trainable_variables)


@tf.function
def train_step(model, loss, optimizer, features, labels):
    loss_value, grads = grad(model, loss, features, labels)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
    return loss_value


def train():
    tf.summary.trace_on(graph=True, profiler=True)

    with tf.summary.create_file_writer(""model"").as_default():
        model = create_model()

        loss = tf.keras.losses.MeanSquaredError()
        optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)

        for i in range(10):
            tf.summary.experimental.set_step(i)

            features = tf.random.normal((16, 32))
            labels = tf.random.normal((16, 2))
            loss_value = train_step(model, loss, optimizer, features, labels)
            print(loss_value)

        tf.summary.trace_export(""model"", profiler_outdir=""model"")


if __name__ == ""__main__"":
    train()
</code></pre>

<p>This, does not show the model graph properly, on doing</p>

<pre><code>tensorboard --logdir model
</code></pre>

<p>In the graphs tab I am seeing <a href=""https://i.stack.imgur.com/TRbqc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/TRbqc.png"" alt=""tensorboard""></a></p>

<p>I am getting the graph when I am training through model.fit or estimator. For example, here is the graphs section when I use <code>model_to_estimator</code> to convert a model</p>

<p><a href=""https://i.stack.imgur.com/h0rBQ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/h0rBQ.png"" alt=""model_to_estiamtor""></a></p>

<p><a href=""https://www.tensorflow.org/alpha/tutorials/eager/custom_training_walkthrough"" rel=""noreferrer"">The guide article</a> does not track metrics through tensorboard, and I did not find any sections on the new workflow for custom adding and tracking of metrics in TensorBoard on alpha (<a href=""https://www.tensorflow.org/alpha"" rel=""noreferrer"">https://www.tensorflow.org/alpha</a>). My contrived implementation is based on the API documentation of tf.summary (<a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/summary"" rel=""noreferrer"">https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/summary</a>) </p>
",1
55163302,Use tensorflow learning-rate decay in a Keras-to-TPU model,"<p>I'm following the ""How to train Keras model x20 times faster with TPU for free"" guide (<a href=""https://www.dlology.com/blog/how-to-train-keras-model-x20-times-faster-with-tpu-for-free/"" rel=""nofollow noreferrer"">click here</a>) to run a keras model on google's colab TPU. It works perfectly. But...I like to use cosine restart learning rate decay when I fit my models. I've coded up my own as a keras callback, but it won't work within this framework because the tensorflow <code>TFOptimizer</code> class doesn't have a learning-rate variable that can be reset. I see that tensorflow itself has a bunch of decay function in <code>tf.train</code>, like <code>tf.train.cosine_decay</code> but I can't figure out how to embed it within my model. </p>

<p>Here's the basic code from that blog post. Anyone have a fix?</p>

<pre><code>import tensorflow as tf
import os
from tensorflow.python.keras.layers import Input, LSTM, Bidirectional, Dense, Embedding

def make_model(batch_size=None):
    source = Input(shape=(maxlen,), batch_size=batch_size,
                   dtype=tf.int32, name='Input')
    embedding = Embedding(input_dim=max_features,
                          output_dim=128, name='Embedding')(source)
    lstm = LSTM(32, name='LSTM')(embedding)
    predicted_var = Dense(1, activation='sigmoid', name='Output')(lstm)
    model = tf.keras.Model(inputs=[source], outputs=[predicted_var])
    model.compile(
        optimizer=tf.train.RMSPropOptimizer(learning_rate=0.01),
        loss='binary_crossentropy',
        metrics=['acc'])
    return model

training_model = make_model(batch_size=128)

# This address identifies the TPU we'll use when configuring TensorFlow.
TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']
tf.logging.set_verbosity(tf.logging.INFO)

tpu_model = tf.contrib.tpu.keras_to_tpu_model(
    training_model,
    strategy=tf.contrib.tpu.TPUDistributionStrategy(
        tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))

history = tpu_model.fit(x_train, y_train,
                    epochs=20,
                    batch_size=128 * 8,
                    validation_split=0.2)
</code></pre>
",0
55163846,Adding tf.identity ops to existing graph,"<p>I have modified existing graph</p>

<p><a href=""https://github.com/TropComplique/FaceBoxes-tensorflow/blob/master/src/detector.py#L70"" rel=""nofollow noreferrer"">https://github.com/TropComplique/FaceBoxes-tensorflow/blob/master/src/detector.py#L70</a></p>

<p>Adding <code>tf.identity</code> ops to get readable names, to find out them in graph after.</p>

<pre><code>with tf.name_scope('postprocessing'):
    boxes = batch_decode(self.box_encodings, self.anchors)
    # if the images were padded we need to rescale predicted boxes:
    boxes = boxes / self.box_scaler
    boxes = tf.clip_by_value(boxes, 0.0, 1.0)
    # it has shape [batch_size, num_anchors, 4]

    scores = tf.nn.softmax(self.class_predictions_with_background, axis=2)[:, :, 1]
    # it has shape [batch_size, num_anchors]

    boxes = tf.identity(input=boxes, name=""my_boxes"")
    scores = tf.identity(input=scores, name=""my_scores"")
</code></pre>

<p>Then having existing checkpoint I converted checkpoint to .pb using this modified graph like decribed here:
<a href=""https://github.com/TropComplique/FaceBoxes-tensorflow/issues/6"" rel=""nofollow noreferrer"">https://github.com/TropComplique/FaceBoxes-tensorflow/issues/6</a></p>

<p>But then when I try to convert .pb to tensorboard via <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/import_pb_to_tensorboard.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/import_pb_to_tensorboard.py</a> and then try to view it via tensorboard</p>

<pre><code>python import_pb_to_tensorboard.py --model_dir model_v3.pb --log_dir model_v3_log_dir

2019-03-14 16:28:43.572017: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 418, in import_graph_def
    graph._c_graph, serialized, options)  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Node 'nms/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3' expects to be colocated with unknown node 'postprocessing/my_boxes'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""import_pb_to_tensorboard.py"", line 86, in &lt;module&gt;
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""import_pb_to_tensorboard.py"", line 68, in main
    import_to_tensorboard(FLAGS.model_dir, FLAGS.log_dir)
  File ""import_pb_to_tensorboard.py"", line 59, in import_to_tensorboard
    importer.import_graph_def(graph_def)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 422, in import_graph_def
    raise ValueError(str(e))
ValueError: Node 'nms/map/TensorArrayUnstack/TensorArrayScatter/TensorArrayScatterV3' expects to be colocated with unknown node 'postprocessing/my_boxes'
</code></pre>

<p>So is it possible to set readable names to some ops of existing graph?</p>
",0
55168906,Tensorflow - tf.nn.weighted_cross_entropy_with_logits - logits and targets must have the same shape,"<p>I've just started using tensorflow for a project I'm working on. The program aims to be a binary classifier with input being 12 features. The output is either normal patient or patient with a disease. The prevalence of the disease is quite low and so my dataset is very imbalanced, with 502 examples of normal controls and only 38 diseased patients. For this reason, I'm trying to use <code>tf.nn.weighted_cross_entropy_with_logits</code> as my cost function.</p>

<p>The code is based on the iris custom estimator from the official tensorflow documentation, and works with <code>tf.losses.sparse_softmax_cross_entropy</code> as the cost function. However, when I change to <code>weighted_cross_entropy_with_logits</code>, I get a shape error and I'm not sure how to fix this.</p>

<pre><code>ValueError: logits and targets must have the same shape ((?, 2) vs (?,))
</code></pre>

<p>I have searched and similar problems have been solved by just reshaping the labels - I have tried to do this unsuccessfully (and don't understand why <code>tf.losses.sparse_softmax_cross_entropy</code> works fine and the weighted version does not). </p>

<p>My full code is here
<a href=""https://gist.github.com/revacious/83142573700c17b8d26a4a1b84b0dff7"" rel=""nofollow noreferrer"">https://gist.github.com/revacious/83142573700c17b8d26a4a1b84b0dff7</a></p>

<p>Thanks!</p>
",1
55286115,Tensorflow Keras: Can `Conv2d` layers now accept multispectral images meaning bands greater than 3,"<p>I have some 8-band satellite images and wanted to do some image segmentation with <code>Tensorflow</code> and <code>Keras</code>. I tried to do this a couple of years ago, but saw that <code>TF</code> and <code>Keras</code> could not handle images with bands greater than 3. However, I am seeing more blog posts about deep learning with multiband images. </p>

<p>In looking at the <code>Keras</code> documentation, it does not specifically list any problems with accepting multiband images. And I found this <a href=""https://github.com/reachsumit/deep-unet-for-satellite-image-segmentation/blob/master/unet_model_deeper.py"" rel=""nofollow noreferrer"">code</a> which seems to make it work: </p>

<pre><code>def unet_model(n_classes=5, im_sz=320, n_channels=8, n_filters_start=32, growth_factor=2, upconv=True,
               class_weights=[0.2, 0.3, 0.1, 0.1, 0.3]):
    droprate=0.25
    n_filters = n_filters_start
    inputs = Input((im_sz, im_sz, n_channels))
    #inputs = BatchNormalization()(inputs)
    conv1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(inputs)
    conv1 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv1)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
pool1 = Dropout(droprate)(pool1)
</code></pre>

<p>So just wanted to clarify, can <code>tf.keras.Conv2d</code> layers and other layers accept 8 or more band images now? Are there any pitfalls of using multiband images--like needing some transformations on the data before processing. Are there any limitations on using multispectral images?</p>
",0
55296509,strange problem reading uint16 images in tensorflow 1.12,"<p>I am currently working on an optical flow project and I come across a strange error. </p>

<p>I have uint16 images stored in bytes in my TFrecords. When I read the TFrecords from my local machine it is giving me uint16 values, but when I deploy the same code and read it from the docker I am getting uint8 values eventhough my dtype is uint16. I mean the uint16 values are getting reduced to uint8 like 32768 --> 128.</p>

<p>What is causing this error?</p>

<p>My local machine has: Tensorflow 1.10.1 and python 3.6
My Docker Image has: Tensorflow 1.12.0 and python 3.5</p>

<p>I am working on tensorflow object detection API
While creating the TF records I use:</p>

<pre><code>with tf.gfile.GFile(flows, 'rb') as fid:
    flow_images = fid.read()
</code></pre>

<p>While reading it back I am using: tf.image.decoderaw</p>

<p>Dataset: KITTI FLOW 2015</p>
",0
55298323,TensorFlow 2.0 returns unexpected output on dtype=int32 with GradientTape,"<p>The following code should output the gradient of y=x*x for x=2, i.e. the value of 4. However the code prints a value of None when using TensorFlow 2.0.0-alpha0. When the definition of x changes to use <code>tf.float32</code> instead of <code>tf.int32</code> as shown in the next snippet, the output changes to the correct value of 4. Is there any documentation that clarifies the requirement for the data type to be a floating point number for GradientTape to work correctly in this scenario?</p>

<pre class=""lang-py prettyprint-override""><code>print(tf.__version__)

x = tf.constant(2, dtype=tf.int32)

with tf.GradientTape() as tape:
  tape.watch(x)
  y = x ** 2
  print(tape.gradient(y, x))
</code></pre>

<p>outputs:</p>

<pre><code>2.0.0-alpha0
None
</code></pre>

<p>Notice the change to <code>tf.float32</code> in the next snippet:</p>

<pre class=""lang-py prettyprint-override""><code>print(tf.__version__)

x = tf.constant(2, dtype=tf.float32)

with tf.GradientTape() as tape:
  tape.watch(x)
  y = x ** 2
  print(tape.gradient(y, x))
</code></pre>

<p>outputs:</p>

<pre><code>2.0.0-alpha0
tf.Tensor(4.0, shape=(), dtype=float32)
</code></pre>
",1
55306443,Normalizing an image in R using mean and standard deviation,"<p>Noob machine learner here!</p>

<p>I am attempting to normalize images in <code>R</code>, as I plan to submit them to be classified in the <em>Machine learning for wildlife image classification</em> (<code>MLWIC</code>) package in R.</p>

<p>The authors of the package mention that before images are classified, they should be resized to 256 x 256 pixels (easy enough) and then normalized. They cite using methods available in this appendix (<a href=""https://www.pnas.org/content/pnas/suppl/2018/06/04/1719367115.DCSupplemental/pnas.1719367115.sapp.pdf"" rel=""nofollow noreferrer"">Norouzzadeh et al 2018</a>, <a href=""https://github.com/Evolving-AI-Lab/deep_learning_for_camera_trap_images/"" rel=""nofollow noreferrer"">code here</a>)).</p>

<p>In brief, the normalization procedure is carried out using a python command that, for each color channel in the image, ""Subtracts off the mean and divides by the variance of the pixels"".</p>

<p>The documentation for the python function they used,  <code>tf.image.per_image_standardization()</code> states:</p>

<blockquote>
  <p>Linearly scales image to have zero mean and unit variance.
  This op computes (x - mean) / adjusted_stddev, where mean is the average of all values in image, and adjusted_stddev = max(stddev, 1.0/sqrt(image.NumElements())).</p>
</blockquote>

<p>If I am trying to replicate this process in <code>R</code>. However, when I attempt to re-write the <code>tf.image.per_image_standardization()</code> function in R, it changes the image from having a range of [0,1] in each color channel to a range that is, for example, -1.85 to 1.85. The resulting image does not look like the saturated image displayed in the appendix, linked to above.</p>

<p>Here's the original image:</p>

<p><a href=""https://i.stack.imgur.com/K9rcn.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/K9rcn.jpg"" alt=""animal_original""></a></p>

<p>Here's the processed image as it is supposed to look:</p>

<p><a href=""https://i.stack.imgur.com/0bF2H.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0bF2H.jpg"" alt=""enter image description here""></a></p>

<p>The code I've tried:</p>

<pre><code>library(imager)
library(tidyverse)
ante &lt;- load.image(""original.jpg"")
plot(ante)
normalize &lt;- function(x) (x - mean(x))/sd(x)
ante %&gt;% 
  imsplit(""c"") %&gt;% 
  modify_at(1, normalize) %&gt;% 
  modify_at(2, normalize) %&gt;% 
  modify_at(3, normalize) %&gt;% 
  imappend(""c"") %&gt;% 
  plot()
</code></pre>

<p>I also tried the following, the resulting image looks more saturated than the original, but it does not use the mean or sd...</p>

<pre><code>library(magick)
process_image &lt;- function(image.path, new.image.path){
  tryCatch(
    {
      image_write(image_equalize(image_scale(image_read(image.path),""256x256!"")), path = new.image.path)
      return(substr(image.path, nchar(image.path) - 2, nchar(image.path)))
    },
    error = function(e)  
    {
      return(NA)
    }
  )
}
process_image(""original.jpg"", ""processed.jpg"")
</code></pre>

<p><strong>What am I doing wrong?</strong> </p>
",0
55308630,update instruction of the deprecated tensorflow dataset sliding window with tf.data.experimental.CsvDataset,"<p>I am trying without success to update my code with the instruction given in the tensorflow documentation (api r1.13). I am using the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset"" rel=""nofollow noreferrer"">tf.data.experimental.CsvDataset</a> and the deprecated <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/data/sliding_window_batch"" rel=""nofollow noreferrer"">tf.contrib.data.sliding_window_batch</a> for a RNN and all works fine (except the deprecated sliding_window warning message). </p>

<p>For the update I have simply replaced </p>

<pre class=""lang-py prettyprint-override""><code>dataset = dataset.apply(tf.contrib.data.sliding_window_batch(batch_size, 1))
</code></pre>

<p>with</p>

<pre class=""lang-py prettyprint-override""><code>dataset = dataset.window(size=batch_size, stride=1).flat_map(lambda x: x.batch(batch_size))
</code></pre>

<p>and I got the following error for a csv file with 50 columns:</p>

<pre class=""lang-sh prettyprint-override""><code>TypeError: &lt;lambda&gt;() takes 1 positional argument but 50 were given
</code></pre>

<p>How can I solve this problem for any csv file (with any number of columns)?</p>
",0
55310900,TensorFlow: How to find minimum/maximum coordinates of segmentations in a tensor excluding zeros?,"<p>For the computation of Intersection over Union (IoU) I want to find coordinates of minimum and maximum values (the border pixels) in a segmentation image <code>image_pred</code> that is represented by a float32 3D tensor. In particular, I aim at finding top left and bottom right corner coordinates of objects in an image. The image is entirely comprised of black pixels (value 0.0) except where the object is located, I have color pixels (0.0 &lt; values &lt; 1.0). Here's an example for such a bounding box (in my case, the object is the traffic sign and the environment is blacked out):</p>

<p><a href=""https://i.stack.imgur.com/QU04v.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QU04v.jpg"" alt=""enter image description here""></a></p>

<p>My approach so far is to <code>tf.boolean_mask</code> for setting every pixel to False except for the color pixels:</p>

<pre><code>zeros = tf.zeros_like(image_pred)
mask = tf.greater(image_pred, zeros)
boolean_mask_pred = tf.boolean_mask(image_pred, mask)
</code></pre>

<p>and then use <code>tf.where</code> to find the coordinates of the masked image. To determine the <strong>horizontal and vertical coordinate values</strong> of the top left and bottom right corners of the rectangle, I thought about using <code>tf.recude_max</code> and <code>tf.reduce_min</code>, but since these do not return a single value if I provide an <code>axis</code>, I am unsure if this is the correct function to use. According to the docs, if I do not specify <code>axis</code>, the function will reduce all dimensions which is not what I want either. Which is the correct function to do this? The IoU in the end is a single 1D float value.</p>

<pre><code>coordinates_pred = tf.where(boolean_mask_pred)
x21 = tf.reduce_min(coordinates_pred, axis=1)
y21 = tf.reduce_min(coordinates_pred, axis=0)
x22 = tf.reduce_max(coordinates_pred, axis=1)
y22 = tf.reduce_max(coordinates_pred, axis=0)
</code></pre>
",0
55347304,Error when applying sample/class weights to fit generator,"<p>I am using a tf.keras.Model fit_generator (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit_generator"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit_generator</a>)  to feed batches of data to a model. According to TensorFlow Documentation, the fit generator should be able to accept size 2 (inputs, targets) or 3 (inputs, targets, sample_weights) tuple. We have the size 2 working, but we have unbalanced classes, so I have determined sample weights. When the fit generator returns a size 3 tuple, I get the error:
”tensorflow.python.framework.errors_impl.InvalidArgumentError: Can not squeeze dim[0], expected a dimension of 1, got [batch_size]""</p>

<p>I am using tensorflow 1.12</p>

<p>Loss Function is tf.losses.softmax_cross_entropy</p>
",1
55363728,How to feed .h5 files in tf.data pipeline in tensorflow model,"<p>I'm trying to optimize the input pipeline for .h5 data with tf.data. But I encountered a <code>TypeError: expected str, bytes or os.PathLike object, not Tensor</code>. I did a research but can't find anything about converting a tensor of string to string.</p>

<p>This simplified code is executable and return the same error:</p>

<pre><code>batch_size = 1000
conv_size = 3
nb_conv = 32
learning_rate = 0.0001

# define parser function
def parse_function(fname):
    with h5py.File(fname, 'r') as f: #Error comes from here
        X = f['X'].reshape(batch_size, patch_size, patch_size, 1)
        y = f['y'].reshape(batch_size, patch_size, patch_size, 1)
        return X, y

# create a list of files path
flist = []
for dirpath, _, fnames in os.walk('./proc/'):
    for fname in fnames:
        if fname.startswith('{}_{}'.format(patch_size, batch_size)) and fname.endswith('h5'):
            flist.append(fname)

# prefetch data
dataset = tf.data.Dataset.from_tensor_slices((flist))
dataset = dataset.shuffle(len(flist))
dataset = dataset.map(parse_function, num_parallel_calls=4)
dataset = dataset.batch(1)
dataset = dataset.prefetch(3)

# simplest model that I think of
X_ph = tf.placeholder(tf.float32, shape=None)
y_ph = tf.placeholder(tf.float32, shape=None)
W = tf.get_variable('w', shape=[conv_size, conv_size, 1, 1], initializer=tf.contrib.layers.xavier_initializer())
loss = tf.reduce_mean(tf.losses.mean_squared_error(tf.nn.softmax(labels=y_ph, predictions=tf.matmul(X_ph, W))))
train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)

# start session
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(train_op, feed_dict={X_ph: dataset[0], y_ph: dataset[1]}))
</code></pre>

<p>Apparently the <code>fname</code> is a tensor of string but the positional argument waits for only a string. I can't find any documentation on this. And the answer of <a href=""https://stackoverflow.com/questions/45246764/how-to-convert-a-string-tensor-to-a-python-string-in-tensorflow"">another post</a> doesn't solve this problem. In my case, I work only with h5 where one h5 store one batch.</p>

<hr>

<p><strong>Update Solution:</strong>
Thanks to the comment of @kvish, the part of loading .h5 file is solved.
The code is upgraded with a simple conv layer, the placeholders have been taken. <strong>Each .h5 is one batch.</strong> I want to prefetch in parallele multiple batches(h5py doesn't support multithread reading so I write batches into multiple files). One can <strong>copy-paste-and-launch</strong>:</p>

<pre><code>import h5py
import threading
import numpy as np
import tensorflow as tf

# generate some img data
for i in range(5):
    with h5py.File('./test_{}.h5'.format(i), 'w') as f:
        f.create_dataset('X', shape=(1000, 100, 100), dtype='float32', data=np.random.rand(10**7).reshape(1000, 100, 100))
        f.create_dataset('y', shape=(1000, 100, 100), dtype='float32', data=np.random.rand(10**7).reshape(1000, 100, 100))
        print(threading.get_ident())

# params
num_cores = 3
shuffle_size = 1
batch_size = 1

# read .h5 file
def parse_file(f):
    print(f.decode('utf-8'))
    with h5py.File(f.decode(""utf-8""), 'r') as fi:
        X = fi['X'][:].reshape(1000, 100, 100, 1)
        y = fi['y'][:].reshape(1000, 100, 100, 1)
        print(threading.get_ident())  # to see the thread id
        return X, y

# py_func wrapper
def parse_file_tf(filename):
    return tf.py_func(parse_file, [filename], [tf.float32, tf.float32])

# tf.data input pipeline
files = tf.data.Dataset.list_files('./test_*.h5')
dataset = files.map(parse_file_tf, num_parallel_calls=num_core)
dataset = dataset.batch(batch_size).shuffle(shuffle_size).prefetch(3)
it = dataset.make_initializable_iterator()
iter_init_op = it.initializer
X_it, y_it = it.get_next()

# simplest model that I can think of 
with tf.name_scope(""Conv1""):
    W = tf.get_variable(""W"", shape=[3, 3, 1, 1],
                         initializer=tf.contrib.layers.xavier_initializer())
    b = tf.get_variable(""b"", shape=[1], initializer=tf.contrib.layers.xavier_initializer())
    layer1 = tf.nn.conv2d(X_it, W, strides=[1, 1, 1, 1], padding='SAME') + b
    out = tf.nn.relu(layer1)

loss = tf.reduce_mean(tf.losses.mean_squared_error(labels=y_it, predictions=out))
train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)

# session
sess = tf.Session()
sess.run(tf.global_variables_initializer())
sess.run(iter_init_op)
sess.run([train_op])
sess.close()
</code></pre>

<hr>

<p>Somehow there will be another cudnn issue which isn't related to this post.</p>

<p>tensorflow-cpu v1.12: work fine</p>

<p>tensorflow-gpu v1.12: <strong>runtime</strong> issue happens</p>

<blockquote>
  <p>Traceback (most recent call last):   File
  ""/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"",
  line 1334, in _do_call
      return fn(*args)   File ""/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"",
  line 1319, in _run_fn
      options, feed_dict, fetch_list, target_list, run_metadata)   File ""/envs/tf/lib/python3.6/site-packages/tensorflow/python/client/session.py"",
  line 1407, in _call_tf_sessionrun
      run_metadata) tensorflow.python.framework.errors_impl.NotFoundError: No algorithm
  worked!    [[{{node Conv1/Conv2D}} = Conv2D[T=DT_FLOAT,
  data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"",
  strides=[1, 1, 1, 1], use_cudnn_on_gpu=true,
  _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients/Conv1/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer,
  W/read)]]      [[{{node
  mean_squared_error/num_present/broadcast_weights/assert_broadcastable/AssertGuard/Assert/Switch_2/_37}}
  = _Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"",
  send_device=""/job:localhost/replica:0/task:0/device:GPU:0"",
  send_device_incarnation=1, tensor_name=""edge_63_me...t/Switch_2"",
  tensor_type=DT_INT32,
  _device=""/job:localhost/replica:0/task:0/device:CPU:0""]]
  tensorflow-cpu v1.12: works fine!</p>
</blockquote>
",1
55418440,Creating softmax from a tf.distributions.Categorical output layer,"<p>I'm training an agent to act in a discrete environment, and I'm using a <code>tf.distributions.Categorical</code> output layer which I then sample to create a softmax output to determine what action to take. I create my policy network like this:</p>

<pre><code>pi_eval, _ = self._build_anet(self.state, 'pi', reuse=True)

def _build_anet(self, state_in, name, reuse=False):
    w_reg = tf.contrib.layers.l2_regularizer(L2_REG)
    with tf.variable_scope(name, reuse=reuse):
        layer_1 = tf.layers.dense(state_in, HIDDEN_LAYER_NEURONS, tf.nn.relu, kernel_regularizer=w_reg, name=""pi_l1"")
        layer_2 = tf.layers.dense(layer_1, HIDDEN_LAYER_NEURONS, tf.nn.relu, kernel_regularizer=w_reg, name=""pi_l2"")
        a_logits = tf.layers.dense(layer_2, self.a_dim, kernel_regularizer=w_reg, name=""pi_logits"")
        dist = tf.distributions.Categorical(logits=a_logits)
    params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)
    return dist, params
</code></pre>

<p>I then sample the network and build up a class distribution output to act as a softmax output, using the example from the <a href=""https://www.tensorflow.org/api_docs/python/tf/distributions/Categorical"" rel=""nofollow noreferrer"">tf.distributions.Categorical webpage</a>:</p>

<pre><code>n = 1e4
self.logits_action = tf.cast(tf.histogram_fixed_width(values=pi_eval.sample(int(n)), value_range=[0, 1], nbins=self.a_dim), dtype=tf.float32) / n
</code></pre>

<p>Run like this:</p>

<pre><code>softmax = self.sess.run([self.logits_action], {self.state: state[np.newaxis, :]})
</code></pre>

<p>But the outputs only ever have two non-zero entries:</p>

<pre><code>[0.44329998 0.         0.         0.5567    ]
[0.92139995 0.         0.         0.0786    ]
[0.95699996 0.         0.         0.043     ]
[0.7051 0.     0.     0.2949]
</code></pre>

<p>My hunch is something to do with <code>value_range</code> which the <a href=""https://www.tensorflow.org/api_docs/python/tf/histogram_fixed_width"" rel=""nofollow noreferrer"">documentation</a> says:</p>

<blockquote>
  <p>value_range: Shape <a href=""https://www.tensorflow.org/api_docs/python/tf/histogram_fixed_width"" rel=""nofollow noreferrer"">2</a> Tensor of same dtype as values. values &lt;=
  value_range[0] will be mapped to hist[0], values >= value_range<a href=""https://www.tensorflow.org/api_docs/python/tf/distributions/Categorical"" rel=""nofollow noreferrer"">1</a>
  will be mapped to hist[-1].</p>
</blockquote>

<p>But I'm not sure what value range I should use?  I wonder if anyone had any ideas?</p>
",1
55421386,"Tensorflow/Keras, How to convert tf.feature_column into input tensors?","<p>I have the following code to average embeddings for list of item-ids.
(Embedding is trained on review_meta_id_input, and used as look up for pirors_input and for getting average embedding)</p>

<pre><code> review_meta_id_input = tf.keras.layers.Input(shape=(1,), dtype='int32', name='review_meta_id')
 priors_input = tf.keras.layers.Input(shape=(None,), dtype='int32', name='priors') # array of ids
 item_embedding_layer = tf.keras.layers.Embedding(
     input_dim=100,      # max number
     output_dim=self.item_embedding_size,
     name='item')

 review_meta_id_embedding = item_embedding_layer(review_meta_id_input)
 selected = tf.nn.embedding_lookup(review_meta_id_embedding, priors_input)
 non_zero_count =  tf.cast(tf.math.count_nonzero(priors_input, axis=1), tf.float32)
 embedding_sum = tf.reduce_sum(selected, axis=1)
 item_average = tf.math.divide(embedding_sum, non_zero_count)
</code></pre>

<p>I also have some feature columns such as.. 
(I just thought feature_column looked cool, but not many documents to look for..)</p>

<pre><code>  kid_youngest_month = feature_column.numeric_column(""kid_youngest_month"")
     kid_age_youngest_buckets = feature_column.bucketized_column(kid_youngest_month, boundaries=[12, 24, 36, 72, 96])
</code></pre>

<p>I'd like to define <code>[review_meta_id_iput, priors_input, (tensors from feature_columns)]</code> as an input to keras Model.</p>

<p>something like:</p>

<pre><code> inputs = [review_meta_id_input, priors_input] + feature_layer
 model = tf.keras.models.Model(inputs=inputs, outputs=o)
</code></pre>

<p>In order to get tensors from feature columns, the closest lead I have now is </p>

<pre><code>fc_to_tensor = {fc: input_layer(features, [fc]) for fc in feature_columns}
</code></pre>

<p>from <a href=""https://github.com/tensorflow/tensorflow/issues/17170"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/17170</a></p>

<p>However I'm not sure what the <code>features</code> are in the code.<br>
There's no clear example on <a href=""https://www.tensorflow.org/api_docs/python/tf/feature_column/input_layer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/feature_column/input_layer</a> either.  </p>

<p>How should I construct the <code>features</code> variable for <code>fc_to_tensor</code> ?</p>

<p>Or is there a way to use <code>keras.layers.Input</code> and <code>feature_column</code> at the same time?</p>

<p>Or is there an alternative than tf.feature_column to do the bucketing as above? then I'll just drop the feature_column for now;</p>
",0
55422537,Testing TF serving model fails with bytes as strings and strings as bytes confusion,"<p>I'm having a problem serving my text classification model on <code>Tensorflow 1.12</code>. I'm using <code>tf.estimator.inputs.pandas_input_fn</code> to read in my data, and <code>tf.estimator.DNNClassifier</code> to train/evaluate. I'd then like to serve my model.
(Apologies in advance, it's tough to provide a full working example here, but it's very much like the example TF provides at <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier</a>  )</p>

<p>I'm currently saving my model with ...</p>

<pre class=""lang-py prettyprint-override""><code>...
estimator.export_savedmodel(""./TEST_SERVING/"", self.serving_input_receiver_fn, strip_default_attrs=True)
...
def serving_input_receiver_fn(self):
      """"""An input receiver that expects a serialized tf.Example.""""""

      # feature spec dictionary  determines our input parameters for the model
      feature_spec = {
          'Headline': tf.VarLenFeature(dtype=tf.string),
          'Description': tf.VarLenFeature(dtype=tf.string)
      }

      # the inputs will be initially fed as strings with data serialized by
      # Google ProtoBuffers
      serialized_tf_example = tf.placeholder(
          dtype=tf.string, shape=None, name='input_example_tensor')
      receiver_tensors = {'examples': serialized_tf_example}

      # deserialize input
      features = tf.parse_example(serialized_tf_example, feature_spec)
      return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)


</code></pre>

<p>This actually fails to run with the error:</p>

<pre class=""lang-sh prettyprint-override""><code>TypeError: Failed to convert object of type &lt;class 'tensorflow.python.framework.sparse_tensor.SparseTensor'&gt; to Tensor. Contents: SparseTensor(indices=Tensor(""ParseExample/ParseExample:0"", shape=(?, 2), 
dtype=int64), values=Tensor(""ParseExample/ParseExample:2"", shape=(?,), dtype=string), dense_shape=Tensor(""ParseExample/ParseExample:4"", shape=(2,), dtype=int64)). Consider casting elements to a supported type.

</code></pre>

<p>I tried to save a second way doing:</p>

<pre class=""lang-py prettyprint-override""><code>def serving_input_receiver_fn(self):
  """"""Build the serving inputs.""""""
  INPUT_COLUMNS = [""Headline"",""Description""]
  inputs = {}
  for feat in INPUT_COLUMNS:
    inputs[feat] = tf.placeholder(shape=[None], dtype=tf.string, name=feat)
  return tf.estimator.export.ServingInputReceiver(inputs, inputs)
</code></pre>

<p>This actually works, until I try testing it with the <code>saved_model_cli</code>.
Some output for <code>saved_model_cli show --all --dir TEST_SERVING/1553879255/</code>:</p>

<pre class=""lang-sh prettyprint-override""><code>MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['predict']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['Description'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: Description:0
    inputs['Headline'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: Headline:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['class_ids'] tensor_info:
        dtype: DT_INT64
        shape: (-1, 1)
        name: dnn/head/predictions/ExpandDims:0
    outputs['classes'] tensor_info:
        dtype: DT_STRING
        shape: (-1, 1)
        name: dnn/head/predictions/str_classes:0
    outputs['logits'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 3)
        name: dnn/logits/BiasAdd:0
    outputs['probabilities'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 3)
        name: dnn/head/predictions/probabilities:0
  Method name is: tensorflow/serving/predict

</code></pre>

<p>But now I can't seem to test it.</p>

<pre class=""lang-sh prettyprint-override""><code>&gt;&gt;&gt; saved_model_cli run --dir TEST_SERVING/1553879255/ --tag_set serve --signature_def predict --input_examples 'inputs=[{""Description"":[""What is going on""],""Headline"":[""Help me""]}]'
Traceback (most recent call last):
 ...
  File ""/Users/Josh/miniconda3/envs/python36/lib/python3.6/site-packages/tensorflow/python/tools/saved_model_cli.py"", line 489, in _create_example_string
    feature_list)
TypeError: 'What is going on' has type str, but expected one of: bytes

</code></pre>

<p>Ok, lets turn it into a bytes object by changing to <code>b[""What is going on""]</code> and <code>b[""Help me""]</code>...</p>

<pre class=""lang-sh prettyprint-override""><code>ValueError: Type &lt;class 'bytes'&gt; for value b'What is going on' is not supported for tf.train.Feature.
</code></pre>

<p>Any ideas/thoughts??
Thanks!</p>
",1
55425811,Implementing Intersection over Union Loss Using Tensorflow,"<p>This may be more of a Tensorflow gradient question. I have been attempting to implement Intersection over Union (IoU) as losses and have been running into some problems. To the point, here is the snippet of my code that computes the IoU:</p>

<pre><code>def get_iou(masks, predictions):
    ious = []
    for i in range(batch_size):
        mask = masks[i]
        pred = predictions[i]
        masks_sum = tf.reduce_sum(mask)
        predictions_sum = tf.reduce_mean(pred)
        intersection = tf.reduce_sum(tf.multiply(mask, pred))
        union = masks_sum + predictions_sum - intersection
        iou = intersection / union
        ious.append(iou)
    return ious

iou = get_iou(masks, predictions)
mean_iou_loss = -tf.log(tf.reduce_sum(iou))
train_op = tf.train.AdamOptimizer(0.001).minimize(mean_iou_loss)
</code></pre>

<p>It works as predicted. However, the issue that I am having is the losses do not decrease. The model does train, though the results are less than ideal so I am wondering if I am implementing it correctly. Do I have to compute the gradients myself? I can compute the gradients for this IoU loss derived by <a href=""https://arxiv.org/pdf/1608.01471.pdf"" rel=""noreferrer"">this paper</a> using <code>tf.gradients()</code>, though I am not sure how to incorporate that with the <code>tf.train.AdamOptimizer()</code>. Reading the documentation, I feel like <code>compute_gradients</code> and <code>apply_gradients</code> are the commands that I need to use, but I can't find any examples on how to use them. My understanding is that the Tensorflow graph should be able to come up with the gradient itself via chain rule. So is a custom gradient even necessary in this problem? If the custom gradient is not necessary then I may just have an ill-posed problem and need to adjust some hyperparameters.</p>

<p><strong>Note:</strong> I have tried Tensorflow's implementation of the IoU, <code>tf.metrics.mean_iou()</code>, but it spits out <code>inf</code> every time so I have abandoned that.</p>
",1
55460885,Replacing TensorFlow Saver with Checkpoint,"<p>I've been using TensorFlow's Saver class to save model parameters, but that class is going away in TensorFlow 2, so I need to replace it with Checkpoint.  I can't figure out how to do that.  All the examples in the documentation for Checkpoint assume you're saving a <code>tf.keras.Model</code>.  I'm not using Keras, so that doesn't apply.</p>

<p>Saver just takes a list of variables to save, so that's what I'm starting from.  How do I pass that to Checkpoint?  It expects you to pass every checkpointable object as a named argument.  I was hoping I could just say <code>variables=[var1, var2, ...]</code>, but it doesn't accept lists.  I could pass every variable as a separate argument, but what do I use as the names?  The variable names?  That defeats the whole purpose of checkpoint, which is to be more robust by not depending on variable names.  What is the intended way of writing checkpoints in code that doesn't use Keras?</p>
",1
55505379,Where is tf.contrib.layers.recompute_grad in TensorFlow 2.0?,"<p>Does TensorFlow 2.0 have any support for making a computation as ""recompute during the gradient pass"" to save memory?  TensorFlow 1.x has <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/layers/recompute_grad"" rel=""nofollow noreferrer"">tf.contrib.layers.recompute_grad</a>, but <code>contrib</code> is gone in TF 2.0 and it doesn't look like anyone moved <code>recompute_grad</code>.</p>
",0
55544489,No Multi-GPU speed up for GAN,"<p>I've got a <a href=""https://github.com/LucasSloan/GAN"" rel=""nofollow noreferrer"">codebase</a> where I try to replicate GAN papers.  I recently bought a second gpu, and I'm trying to update my code to take advantage of the additional hardware.  I tried the approach outlined in the Tensorflow <a href=""https://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_multi_gpu_train.py"" rel=""nofollow noreferrer"">cifar10 multi-gpu example</a>.  However, when I run the my code with 2 gpus, it doesn't run any faster, in fact, it runs about 10% slower than if I run with a single gpu.  Looking at resource manager, it says that both of my gpus are running at about 50% capacity.</p>

<p>I'm running on Windows 10, with python 3.7, TF 1.13.  I'm using 2 2080ti's with a 2950 cpu.</p>

<p>My first thought was that there was trouble with my input pipeline, so I tried a number of variations, such as using multiple data iterators, using tf.data.experimental.prefetch_to_device(), not feeding in my latent vector, etc.  None had any affect, and since my CPU utilization was about 5% with each, I'm pretty sure I'm not bottle necked there.</p>

<p>I've also tried some variations in how I set up the variable scopes for the towers, but that didn't help.</p>

<p>I also tried doubling the batch size in case I was just not putting enough data through the gpus, but that resulted in taking 2x as long to compute each batch, with the same 50% gpu utilization.</p>

<p>My code is <a href=""https://github.com/LucasSloan/GAN/blob/master/gan.py"" rel=""nofollow noreferrer"">here</a>, and the relevant portion is:</p>

<pre class=""lang-py prettyprint-override""><code>        d_grads = []
        g_grads = []
        for i in range(FLAGS.num_gpus):
            with tf.device('/gpu:{:d}'.format(i)):
                with tf.variable_scope('D', reuse=tf.AUTO_REUSE):
                    Dx, Dx_logits = self.discriminator(xs[i], yxs[i])
                with tf.variable_scope('G', reuse=tf.AUTO_REUSE):
                    G = self.generator(z[i], labels[i])
                with tf.variable_scope('D', reuse=tf.AUTO_REUSE):
                    Dg, Dg_logits = self.discriminator(G, labels[i])

                loss_d, loss_g = self.losses(Dx_logits, Dg_logits, Dx, Dg)

                vars = tf.trainable_variables()
                for v in vars:
                    print(v.name)
                d_params = [v for v in vars if v.name.startswith('D/')]
                g_params = [v for v in vars if v.name.startswith('G/')]

                d_grads.append(d_adam.compute_gradients(loss_d, var_list=d_params))
                g_grads.append(g_adam.compute_gradients(loss_g, var_list=g_params))

        d_opt = d_adam.apply_gradients(average_gradients(d_grads))
        g_opt = g_adam.apply_gradients(average_gradients(g_grads))
</code></pre>
",0
55560676,How to use tf.while_loop with eager execution?,"<p>In the documentation, the body of a tf.while_loop needs to be a python callable.</p>

<pre><code>i = tf.constant(0)
b = lambda i: tf.add(i,1)
c = lambda i: tf.less(i,10)
tf.while_loop(c,b, [i])
</code></pre>

<p>works but</p>

<pre><code>def b(i):
    tf.add(i,1)

i = tf.constant(0)
c = lambda i: tf.less(i,10)
tf.while_loop(c,b, [i])
</code></pre>

<p>throws a ValueError: Attempt to convert a value (None) with an unsupported type() to a Tensor</p>

<p>In 2.0, eager execution is default, I wonder what's the problem?!</p>
",1
55582569,Customized convolutional layer in TensorFlow,"<p>Let's assume i want to make the following layer in a neural network: Instead of having a square convolutional filter that moves over some image, I want the shape of the filter to be some other shape, say a rectangle, circle, triangle, etc (this is of course a silly example; the real case I have in mind is something different). How would I implement such a layer in TensorFlow?</p>

<p>I found that one can define custom layers in Keras by extending <code>tf.keras.layers.Layer</code>, but the documentation is quite limited without many examples. A python implementation of a convolutional layer by for example extending the <code>tf.keras.layer.Layer</code> would probably help as well, but it seems that the convolutional layers are implemented in C. Does this mean that I have to implement my custom layer in C to get any reasonable speed or would Python TensorFlow operations be enough?</p>

<p>Edit: Perhaps it is enough if I can just define a tensor of weights, but where I can customize entries in the tensor that are identically zero and some weights showing up in multiple places in this tensor, then I should be able to by hand build a convolutional layer and other layers. How would I do this, and also include these variables in training?</p>

<p>Edit2: Let me add some more clarifications. We can take the example of building a 5x5 convolutional layer with one output channel from scratch. If the input is say 10x10 (plus padding so output is also 10x10)), I would imagine doing this by creating a matrix of size 100x100. Then I would fill in the 25 weights in the correct locations in this matrix (so some entries are zero, and some entries are equal, ie all 25 weights will show up in many locations in this matrix). I then multiply the input with this matrix to get an output. So my question would be twofold: 1. How do I do this in <code>TensorFlow</code>? 2. Would this be very inefficient and is some other approach recommended (assuming that I want to later customize what this filter looks like and thus the standard <code>conv2d</code> is not good enough).</p>

<p>Edit3: It seems doable by using sparse tensors and assigning values via a previously defined <code>tf.Variable</code>. However I don't know if this approach will suffer from performance issues.</p>
",1
55602703,"Can I add tf.keras.backend function in TensorFlow ""custom"" layer?","<p>TensorFlow can implement Keras with <code>tf.keras</code>.</p>

<p>Keras has three backend implementations available: the TensorFlow backend, the Theano backend, and the CNTK backend.</p>

<p>If I wanted to build a custom layer, can I add <code>tf.keras.backend.theano.tensor.dot</code> and <code>tf.keras.backend.theano.gradient.disconnected_grad</code> in call function?</p>

<p>For example, there is one program in TensorFlow（<a href=""https://www.tensorflow.org/tutorials/eager/custom_layers"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/eager/custom_layers</a>）：</p>

<pre class=""lang-py prettyprint-override""><code>class MyDenseLayer(tf.keras.layers.Layer):
    def __init__(self, num_outputs):
        super(MyDenseLayer, self).__init__()
        self.num_outputs = num_outputs

    def build(self, input_shape):
        self.kernel = self.add_variable(""kernel"", 
                                    shape=[int(input_shape[-1]), 
                                           self.num_outputs])

    def call(self, input):
        return tf.matmul(input, self.kernel)
</code></pre>

<p>Can I change call function into:</p>

<pre class=""lang-py prettyprint-override""><code>def call(self, input):
    return tf.keras.backend.theano.tensor.dot(self.kernel,tf.keras.backend.theano.gradient.disconnected_grad(input))
</code></pre>
",0
55619070,GraphKeys.TRAINABLE_VARIABLES vs tf.trainable_variables(),"<p>Is <code>GraphKeys.TRAINABLE_VARIABLES</code> is the same as <code>tf.trainable_variables()</code> ?</p>

<p>Is <code>GraphKeys.TRAINABLE_VARIABLES</code> actually <code>tf.GraphKeys.TRAINABLE_VARIABLES</code>?</p>

<p>Looks like networks successfully trains with:</p>

<pre><code>optimizer = tf.train.AdamOptimizer(config.LEARNING_RATE)
with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
    self.train_op = optimizer.minimize(self.loss, var_list=tf.trainable_variables())
</code></pre>

<p>but not with </p>

<pre><code>optimizer = tf.train.AdamOptimizer(config.LEARNING_RATE)
with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):
    self.train_op = optimizer.minimize(self.loss)
</code></pre>

<p>According to <a href=""https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer#minimize"" rel=""nofollow noreferrer"">documentation</a>:</p>

<pre><code>var_list: Optional list or tuple of Variable objects to update to minimize loss. Defaults to the list of variables collected in the graph under the key GraphKeys.TRAINABLE_VARIABLES.
</code></pre>

<p>Also as I can see in batch normalization example code <code>var_list</code> is omited:</p>

<pre class=""lang-py prettyprint-override""><code>  x_norm = tf.layers.batch_normalization(x, training=training)

  # ...

  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
  with tf.control_dependencies(update_ops):
    train_op = optimizer.minimize(loss)
</code></pre>
",1
55703097,Training while loop in Tensorflow,"<p>I've attempted converting a Python-side training loop to Tensorflow to (hypothetically) make the code run faster - not having to pass control over to cpu constantly. However, I can't manage using <code>tf.while_loop</code>.</p>

<p>Here's the code that works:</p>

<pre><code>import numpy as np
import tensorflow as tf

from tqdm import tqdm
from sklearn.datasets import load_iris
from sklearn.preprocessing import RobustScaler

x, y = load_iris(True)
x = RobustScaler().fit_transform(x)

shape = (10, 10)
max_epochs = 1000


graph = tf.Graph()
sess = tf.Session(graph=graph)

x = x.astype(np.float64)


# Construct graph
with graph.as_default():
    weights = tf.get_variable(
        'weights', shape, initializer=tf.constant_initializer, dtype=tf.float64
    )
    curr_epoch = tf.placeholder(dtype=tf.int64, shape=())

    with tf.name_scope('data'):
        data = tf.data.Dataset.from_tensor_slices(x)
        data = data.shuffle(buffer_size=10000)
        data = data.repeat(max_epochs)
        data = data.batch(1)
        data = data.make_one_shot_iterator().get_next()

    with tf.name_scope('update'):
        update_op = make_update_op(weights)

    init = tf.global_variables_initializer()


sess.run(init)

for i in tqdm(range(max_epochs)):
    for _ in range(x.shape[0]):
        sess.run(update_op, feed_dict={
            curr_epoch: i
        })

np_weights = sess.run(weights)
print(np_weights) # Correctly prints an array of 150's.
</code></pre>

<p>Now, if I create an update function to pass <code>tf.while_loop</code>, an error is thrown.</p>

<pre><code>def make_update_op(w):
    return w.assign(
        w + 0.001
    )

# In the code above:
update_op = tf.while_loop(lambda _: True, make_update_op, (weights,), maximum_iterations=x.shape[0])

# No inner loop:
for i in tqdm(range(max_epochs)):
    sess.run(update_op, feed_dict={
        curr_epoch: i
    })
</code></pre>

<blockquote>
  <p>Line 22, in make_update_op
      <code>return w.assign(</code>
  AttributeError: 'Tensor' object has no attribute 'assign'</p>
</blockquote>

<p>I don't quite understand what is happening even after reading the documentation. <code>weights</code> is a <code>Variable</code> after all. What could be done to correctly make the training loop?</p>
",1
55711355,How to restore dangling tf.py_func within the tf.data.Dataset() with tf.saved_model API?,"<p>After doing a research for restoring the <code>tf.py_func()</code> when using saved_model API in vain, I couldn't find other information than documented in <a href=""https://www.tensorflow.org/api_docs/python/tf/py_func"" rel=""nofollow noreferrer"">tensorflow</a>:</p>

<blockquote>
  <p>The operation must run in the same address space as the Python program that calls <code>tf.py_func()</code>. If you are using distributed TensorFlow, you must run a <code>tf.train.Server</code> in the same process as the program that calls <code>tf.py_func()</code> and you must pin the created operation to a device in that server (e.g. using with <code>tf.device()</code>:)</p>
</blockquote>

<p>Two save/load snippets help to illustrate the situation. </p>

<p><strong>Save part:</strong></p>

<pre><code>def wrapper(x, y):
    with tf.name_scope('wrapper'):
        return tf.py_func(Copy, [x, y], [tf.float32, tf.float32])

def Copy(x, y):
    return x, y

x_ph = tf.placeholder(tf.float32, [None], 'x_ph')
y_ph = tf.placeholder(tf.float32, [None], 'y_ph')

with tf.name_scope('input'):
    ds = tf.data.Dataset.from_tensor_slices((x_ph, y_ph))
    ds = ds.map(wrapper)
    ds = ds.batch(1)
    it = tf.data.Iterator.from_structure(ds.output_types, ds.output_shapes)
    it_init_op = it.make_initializer(ds, name='it_init_op')

x_it, y_it = it.get_next()

# Simple operation
with tf.name_scope('add'):
    res = tf.add(x_it, y_it)

with tf.Session() as sess:
    sess.run([tf.global_variables_initializer(), it_init_op], feed_dict={y_ph: [10] * 10, x_ph: [i for i in range(10)]})
    sess.run([res])
    tf.saved_model.simple_save(sess, './dummy/test', {'x_ph': x_ph, 'y_ph': y_ph}, {'res': res})
</code></pre>

<p><strong>Load part:</strong></p>

<pre><code>graph = tf.Graph()
graph.as_default()
with tf.Session(graph=graph) as sess:
    tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], './dummy/test')

    res = graph.get_tensor_by_name('add/Add:0')
    it_init_op = graph.get_operation_by_name('input/it_init_op')
    x_ph = graph.get_tensor_by_name('x_ph:0')
    y_ph = graph.get_tensor_by_name('y_ph:0')
    sess.run([it_init_op], feed_dict={x_ph: [5] * 5, y_ph: [i for i in range(5)]})

    for _ in range(5):
        sess.run([res])
</code></pre>

<p><strong>Error:</strong></p>

<blockquote>
  <p>ValueError: callback pyfunc_0 is not found</p>
</blockquote>

<p>It's well known that the function wrapped by the <code>tf.py_func()</code> isn't saved with the model. Does anybody has a solution to restore this by using the small hint given by the tf doc applying <code>tf.train.Server</code></p>
",1
55713550,Trying to convert pandas DataFrame structure with list values to tensorflow DataSet,"<p>I'm building an DNNRegressor using tensorflow and my problem comes from the process of converting my features which are stored as a pandas DataFrame into tensorflow Dataset structures. 
More specifically, from the use of both <code>tf.data.Dataset.from_tensor_slices</code> and the feature columns function I created. </p>

<p>I was led to believe from this <a href=""https://www.tensorflow.org/guide/feature_columns#numeric_column"" rel=""nofollow noreferrer"">tensorflow guide</a> that it would be possible to create a data structure with a multi-dimensional feature. In the guide it is mentioned that when creating a <code>feature_column</code>, it is possible to specify the shape of the feature. Two examples are given, a 10 element vector under the key <code>""Bowling""</code> and a 10x5 matrix under the key <code>""MyMatrix""</code>. Following the example, I created some multi-dimensional features. For simplicity, I created an example code which I post below with two multi-dimensional features using a pandas DataFrame, under <code>""matrix""</code> there are two examples of 2x2 matrices and under <code>""vector""</code> two examples of 3 element vectors. There are two outputs for each of the two examples, an x and y (I've seen nothing to support that this representation is possible, however this would be the most convenient in the context of the problem)  </p>

<pre><code>import tensorflow as tf
import pandas as pd

# Create features DataFrame
feature_header = ['matrix', 'vector']
feature_table = [ [ [[1, 2], [3,4]], [5,6,7] ],
                 [ [[8, 9], [10,11]], [12,13,14] ] ]
features = pd.DataFrame(feature_table, columns=feature_header)

# Create outputs DataFrame
header_targets = ['x', 'y']
table_targets = [ [7, 8],
                 [9, 10] ]
labels = pd.DataFrame(table_targets, columns=header_targets)

ds = tf.data.Dataset.from_tensor_slices((features, labels))
</code></pre>

<p>An error occurs in the last line when converting to a tensorflow Dataset</p>

<pre><code>TypeError: Expected binary or unicode string, got [[1, 2], [3, 4]]
</code></pre>

<p>My question is, is it possible to do this conversion? I am able to do the conversion if I create a new column for each of the element of the matrix/vector but this becomes rather chaotic when the matrix is of size 12x12. </p>

<p>I'm using tensorflow version 1.13.1 and python 3.6.4</p>

<p><strong>EDIT</strong></p>

<p>Tried using numpy multidimensional arrays instead of lists, changed the feature table declaration to the following</p>

<pre><code>feature_table = [ [ np.array([[1, 2], [3,4]]), np.array([5,6,7]) ],
                 [ np.array([[8, 9], [10,11]]), np.array([12,13,14]) ] ]
</code></pre>

<p>Got the following error</p>

<pre><code>TypeError: Expected binary or unicode string, got array([[1, 2],
       [3, 4]])
</code></pre>
",0
55730930,Tensorflow 2.0: No gradients provided for any variable but only when tf.math.square AND tf.function is used?,"<p>The following code runs if I omit either tf.math.square line or the @tf.function decoration. </p>

<p>Why is the square line triggering some gradient issue?</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.backend as K
import numpy as np

m = 1000
n = 1
X = np.random.randn(m, n).astype(np.float32)
y = (3 + 0 * np.random.randn(m)).astype(np.float32)

def create_model():
    a_input = keras.layers.Input(shape=(n,), dtype=np.float32)
    a = K.expand_dims(a_input, axis=2)
    q = keras.layers.Conv1D(1, 1)(a)
    q = - tf.math.square(q) # this breaks things, but only when using tf.function
    model = keras.models.Model(inputs=a_input, outputs=q)
    return model

model = create_model()
model.predict(X)

class Trainer():
    def __init__(self, epochs=10):
        self.epochs = epochs
        self.model = create_model()
        self.optimizer = tf.optimizers.Adam()
        self.step = 0
    def train(self, X, y, epochs=10):
        X = tf.convert_to_tensor(X, dtype=tf.float32)
        y = tf.convert_to_tensor(y, dtype=tf.float32)
        for epoch in range(epochs):
            l = self._train_one_step(X, y)
        return l
    @tf.function
    def _train_one_step(self, X, y):
        with tf.GradientTape() as tape:
            yp = self.model(X)
            loss = tf.reduce_mean(tf.math.square(y - yp))
        gradients = tape.gradient(loss, self.model.trainable_variables)
        l = self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
        d = dict(loss=loss)
        tf.print(yp[0], loss)
        self.step += 1

trainer = Trainer()
l = trainer.train(X, y, epochs=100)
</code></pre>

<p>Traceback as requested:</p>

<pre><code>2019-04-18 14:23:03.776224: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-1-8a9114e8a51b&gt; in &lt;module&gt;
----&gt; 1 import example_keras_tf2 as e

~/toplevelrepo/something-train/something/train/usr/HOME/example_keras_tf2.py in &lt;module&gt;
     44
     45 trainer = Trainer()
---&gt; 46 l = trainer.train(X, y, epochs=100)

~/toplevelrepo/something-train/something/train/usr/HOME/example_keras_tf2.py in train(self, X, y, epochs)
     30         y = tf.convert_to_tensor(y, dtype=tf.float32)
     31         for epoch in range(epochs):
---&gt; 32             l = self._train_one_step(X, y)
     33         return l
     34     @tf.function

~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    424     # This is the first call of __call__, so we have to initialize.
    425     initializer_map = {}
--&gt; 426     self._initialize(args, kwds, add_initializers_to=initializer_map)
    427     if self._created_variables:
    428       try:

~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    368     self._concrete_stateful_fn = (
    369         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 370             *args, **kwds))
    371
    372     def invalid_creator_scope(*unused_args, **unused_kwds):

~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   1311     if self._input_signature:
   1312       args, kwargs = None, None
-&gt; 1313     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   1314     return graph_function
   1315

~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   1578           or call_context_key not in self._function_cache.missed):
   1579         self._function_cache.missed.add(call_context_key)
-&gt; 1580         graph_function = self._create_graph_function(args, kwargs)
   1581         self._function_cache.primary[cache_key] = graph_function
   1582         return graph_function, args, kwargs

~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   1510             arg_names=arg_names,
   1511             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 1512             capture_by_value=self._capture_by_value),
   1513         self._function_attributes)
   1514

~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    692                                           converted_func)
    693
--&gt; 694       func_outputs = python_func(*func_args, **func_kwargs)
    695
    696       # invariant: `func_outputs` contains only Tensors, IndexedSlices,

~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    315         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    316         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 317         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    318     weak_wrapped_fn = weakref.ref(wrapped_fn)
    319

~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/eager/function.py in bound_method_wrapper(*args, **kwargs)
   2106     # If __wrapped__ was replaced, then it is always an unbound function
   2107     # that takes self as first argument.
-&gt; 2108     return wrapped_fn(weak_instance(), *args, **kwargs)
   2109   weak_bound_method_wrapper = weakref.ref(bound_method_wrapper)
   2110

~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    684                   optional_features=autograph_options,
    685                   force_conversion=True,
--&gt; 686               ), args, kwargs)
    687
    688         # Wrapping around a decorator allows checks like tf_inspect.getargspec

~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)
    390     return _call_unconverted(f, args, kwargs)
    391
--&gt; 392   result = converted_f(*effective_args, **kwargs)
    393
    394   # The converted function's closure is simply inserted into the function's

/var/folders/8s/kcb8_98n1pqbnwrv2h0b14rm0000gp/T/tmp9aj7e32f.py in tf___train_one_step(self, X, y)
      5     loss = ag__.converted_call('reduce_mean', tf, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (tf.math.square(y - yp),), {})
      6   gradients = ag__.converted_call('gradient', tape, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_2, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (loss, self.model.trainable_variables), {})
----&gt; 7   l = ag__.converted_call('apply_gradients', self.optimizer, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_3, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (zip(gradients, self.model.trainable_variables),), {})
      8   d = ag__.converted_call(dict, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_4, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (), {'loss': loss})
      9   ag__.converted_call('print', tf, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_5, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (yp[0], loss), {})

~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)
    265
    266   if not options.force_conversion and conversion.is_whitelisted_for_graph(f):
--&gt; 267     return _call_unconverted(f, args, kwargs)
    268
    269   # internal_convert_user_code is for example turned off when issuing a dynamic

~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs)
    186     return f.__self__.call(args, kwargs)
    187
--&gt; 188   return f(*args, **kwargs)
    189
    190

~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name)
    394       ValueError: If none of the variables have gradients.
    395     """"""
--&gt; 396     grads_and_vars = _filter_grads(grads_and_vars)
    397     var_list = [v for (_, v) in grads_and_vars]
    398

~/anaconda3/envs/myenv/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in _filter_grads(grads_and_vars)
    922   if not filtered:
    923     raise ValueError(""No gradients provided for any variable: %s."" %
--&gt; 924                      ([v.name for _, v in grads_and_vars],))
    925   if vars_with_empty_grads:
    926     logging.warning(

ValueError: No gradients provided for any variable: ['conv1d_1/kernel:0', 'conv1d_1/bias:0'].
</code></pre>

<p>Also, I'm not sure if this is useful as I don't know how to read it yet, but here is the python_function dump:</p>

<pre class=""lang-py prettyprint-override""><code>In [6]: print(tf.autograph.to_code(e.trainer._train_one_step.python_function))
from __future__ import print_function

def tf___train_one_step(*args, **kwargs):
  try:
    """"""Wraps either a dummy MethodType or a converted AutoGraph function.""""""
    with ag__.function_scope('bound_method_wrapper'):
      do_return = False
      retval_ = None
      strong_bound_method_wrapper = ag__.converted_call(weak_bound_method_wrapper, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=ag__.Feature.ALL, internal_convert_user_code=True), (), {})
      wrapped_fn = strong_bound_method_wrapper.__wrapped__
      cond_1 = ag__.is_(wrapped_fn, strong_bound_method_wrapper.__original_wrapped__)

      def if_true_1():
        with ag__.function_scope('if_true_1'):
          wrapped_fn = original_function.python_function
          cond = ag__.converted_call('ismethod', tf_inspect, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=ag__.Feature.ALL, internal_convert_user_code=True), (wrapped_fn,), {})

          def if_true():
            with ag__.function_scope('if_true'):
              wrapped_fn_1, = wrapped_fn,
              wrapped_fn_1 = ag__.converted_call('get_unbound_function', six, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=ag__.Feature.ALL, internal_convert_user_code=True), (wrapped_fn_1,), {})
              return wrapped_fn_1

          def if_false():
            with ag__.function_scope('if_false'):
              return wrapped_fn
          wrapped_fn = ag__.if_stmt(cond, if_true, if_false)
          do_return = True
          retval_ = ag__.converted_call(wrapped_fn, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=ag__.Feature.ALL, internal_convert_user_code=True), (weak_instance(),) + tuple(args), dict(kwargs, **{}))
          return retval_

      def if_false_1():
        with ag__.function_scope('if_false_1'):
          do_return = True
          retval_ = ag__.converted_call(wrapped_fn, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=ag__.Feature.ALL, internal_convert_user_code=True), (weak_instance(),) + tuple(args), dict(kwargs, **{}))
          return retval_
      retval_ = ag__.if_stmt(cond_1, if_true_1, if_false_1)
      return retval_
  except:
    ag__.rewrite_graph_construction_error(ag_source_map__)



tf___train_one_step.autograph_info__ = {}
</code></pre>
",0
55731549,How to use tf.keras Sequential with tf.distribute.ParameterServerStrategy and tf.train.MonitoredSession?,"<p>I'm trying to set up a really easy Minimal Working Example for the following: Use a model built with tf.keras in a tf.train.MonitoredSession using a tf.Server with a tf.distribute.ParameterServerStrategy.</p>

<p>My goal in the end is to use a tf.keras model in a distributed environment using two workers each having one GPU and a parameter server.</p>

<p>The model is built according to the example and documentation found here: <a href=""https://www.tensorflow.org/versions/r1.12/api_docs/python/tf/keras/models/Sequential"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r1.12/api_docs/python/tf/keras/models/Sequential</a></p>

<p>The parameter server strategy is used according to the documentation found here: <a href=""https://www.tensorflow.org/versions/r1.12/api_docs/python/tf/contrib/distribute/ParameterServerStrategy"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r1.12/api_docs/python/tf/contrib/distribute/ParameterServerStrategy</a></p>

<p>The overall setup including the device placement and the use of a MonitoredSession is taken from: <a href=""https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md"" rel=""nofollow noreferrer"">https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md</a></p>

<p>I'm already using the ""allow_soft_placement"" option and I'm ""emulating"" a distributed setup on my local machine having only a single CPU, since there are different problems in the real distributed setup which I'm trying to solve by using a MonitoredSession where the variable initialization is handled automatically.</p>

<p>This Code works with a ""normal"" (not monitored) tf.Session and variable initialization - global, local, model variables and tables etc.</p>

<p>The line which unfreezes the graph is necessary to be able to use a tf.data.Dataset in the tf.keras.Model's fit function, since an iterator has to be created - which causes an error in a frozen graph.</p>

<p>This the code I'm trying to run. I use tensorflow 1.12.0 and python 3.6.7. I've also tried python 2.7, same result.</p>

<p>The code requires no setup besides installing tensorflow.</p>

<pre class=""lang-py prettyprint-override""><code>import sys
import tensorflow as tf

def main(argv):

  # Create local cluster config for run_local_server.sh script.
  cluster = tf.train.ClusterSpec({""worker"": [""localhost:2222""], ""ps"": [""localhost:2223""]})
  task = 0
  job = str(argv[0])

  # Number of GPUs per worker
  GPU_PER_WORKER = 0

  config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)
  server = tf.train.Server(cluster, job_name=job, task_index=task,config=config)

  strategy = tf.contrib.distribute.ParameterServerStrategy(num_gpus_per_worker=GPU_PER_WORKER)
  strategy.configure(session_config=config, cluster_spec=cluster,task_type=job,task_id=task)

  with strategy.scope():
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

    x_train = x_train.astype('float32') / 255
    x_test = x_test.astype('float32') / 255

    # Reshape input data from (28, 28) to (28, 28, 1)
    w, h = 28, 28
    x_train = x_train.reshape(x_train.shape[0], w, h, 1)
    x_test = x_test.reshape(x_test.shape[0], w, h, 1)

    # One-hot encode the labels
    y_train = tf.keras.utils.to_categorical(y_train, 10)
    y_test = tf.keras.utils.to_categorical(y_test, 10)

    train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(x_train),tf.data.Dataset.from_tensor_slices(y_train))).repeat().shuffle(60000).batch(10)
    val_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(x_test),tf.data.Dataset.from_tensor_slices(y_test))).repeat().shuffle(10000).batch(10)

    with tf.device(tf.train.replica_device_setter(worker_device=""/job:worker/task:%d"" % task,cluster=cluster)):
      model = tf.keras.models.Sequential()

      conv0 = tf.keras.layers.Conv2D(filters=16, data_format='channels_last', padding=""valid"", kernel_size=4, strides=1, input_shape=(28,28,1), activation=tf.keras.activations.relu)
      model.add(conv0)

      flatten = tf.keras.layers.Flatten()
      model.add(flatten)

      dense1 = tf.keras.layers.Dense(10, activation=tf.keras.activations.softmax)
      model.add(dense1)

      model.compile(tf.contrib.optimizer_v2.AdamOptimizer(0.001), loss=tf.keras.metrics.mean_absolute_error,metrics=['accuracy'],distribute=strategy)   

    if job == ""ps"":
      server.join()
    elif job == ""worker"":
      with tf.train.MonitoredSession(session_creator=tf.train.ChiefSessionCreator(master=server.target,config=config)) as sess:        
        sess.graph._unsafe_unfinalize()
        history = model.fit(x=train_ds, validation_data=val_ds, validation_steps=1000, steps_per_epoch=100, epochs=60)

if __name__ == ""__main__"":
  main(sys.argv[1:])
</code></pre>

<p>The code requires no extensive setup since the dataset is loaded from the web and converted to a tf.data.Dataset since this is how I want to organize my pipeline with the real data. The MNIST data setup example is taken from 
<a href=""https://www.kaggle.com/margaretmz/mnist-with-tf-keras"" rel=""nofollow noreferrer"">https://www.kaggle.com/margaretmz/mnist-with-tf-keras</a>.</p>

<p>I expect the code to not fail due to wrong variable or operation placement since I'm basically leaving all these decisions to the implementation of tensorflow by using <code>strategy.scope()</code> and <code>tf.device(tf.train.replica_device_setter(worker_device=""/job:worker/task:%d"" % task,cluster=cluster))</code></p>
",1
55764694,How to use gradient_override_map in Tensorflow 2.0?,"<p>I'm trying to use <code>gradient_override_map</code> with Tensorflow 2.0. There is an <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/Graph#gradient_override_map"" rel=""noreferrer"">example in the documentation</a>, which I will use as the example here as well.</p>

<p>In 2.0, <code>GradientTape</code> can be used to compute gradients as follows:</p>

<pre><code>import tensorflow as tf
print(tf.version.VERSION)  # 2.0.0-alpha0

x = tf.Variable(5.0)
with tf.GradientTape() as tape:
    s_1 = tf.square(x)
print(tape.gradient(s_1, x))
</code></pre>

<p>There is also the <code>tf.custom_gradient</code> decorator, which can be used to define the gradient for a <em>new</em> function (again, using the <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/custom_gradient"" rel=""noreferrer"">example from the docs</a>):</p>

<pre><code>import tensorflow as tf
print(tf.version.VERSION)  # 2.0.0-alpha

@tf.custom_gradient
def log1pexp(x):
    e = tf.exp(x)

    def grad(dy):
        return dy * (1 - 1 / (1 + e))

    return tf.math.log(1 + e), grad

x = tf.Variable(100.)

with tf.GradientTape() as tape:
    y = log1pexp(x)

print(tape.gradient(y, x))
</code></pre>

<p>However, I would like to replace the gradient for standard functions such as <code>tf.square</code>. I tried to use the following code:</p>

<pre><code>@tf.RegisterGradient(""CustomSquare"")
def _custom_square_grad(op, grad):
  return tf.constant(0)

with tf.Graph().as_default() as g:
    x = tf.Variable(5.0)
    with g.gradient_override_map({""Square"": ""CustomSquare""}):
        with tf.GradientTape() as tape:
            s_2 = tf.square(x, name=""Square"")

    with tf.compat.v1.Session() as sess:
        sess.run(tf.compat.v1.global_variables_initializer())            
        print(sess.run(tape.gradient(s_2, x)))
</code></pre>

<p>However, there are two issues: The gradient replacement does not seem to work (it is evaluated to <code>10.0</code> instead of <code>0.0</code>) and I need to resort to <code>session.run()</code> to execute the graph. Is there a way to achieve this in ""native"" TensorFlow 2.0?</p>

<p>In TensorFlow 1.12.0, the following produces the desired output:</p>

<pre><code>import tensorflow as tf
print(tf.__version__)  # 1.12.0

@tf.RegisterGradient(""CustomSquare"")
def _custom_square_grad(op, grad):
  return tf.constant(0)

x = tf.Variable(5.0)

g = tf.get_default_graph()
with g.gradient_override_map({""Square"": ""CustomSquare""}):
    s_2 = tf.square(x, name=""Square"")
grad = tf.gradients(s_2, x)

with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  print(sess.run(grad))
</code></pre>
",1
55765447,Structure of initial state for stacked LSTM,"<p>What is the required structure for an initial state on a multilayer/stacked RNN in TensorFlow (1.13.1) using the <code>tf.keras.layers.RNN</code> API?</p>

<p>I tried the following:</p>

<pre><code>lstm_cell_sizes = [256, 256, 256]
lstm_cells = [tf.keras.layers.LSTMCell(size) for size in lstm_cell_sizes]

state_init = [tf.placeholder(tf.float32, shape=[None] + cell.state_size) for cell in lstm_cells]

tf.keras.layers.RNN(lstm_cells, ...)(inputs, initial_state=state_init)
</code></pre>

<p>This results in:</p>

<pre><code>ValueError: Could not pack sequence. Structure had 6 elements, but flat_sequence had 3 elements.  Structure: ([256, 256], [256, 256], [256, 256]), flat_sequence: [&lt;tf.Tensor 'player/Placeholder:0' shape=(?, 256, 256) dtype=float32&gt;, &lt;tf.Tensor 'player/Placeholder_1:0' shape=(?, 256, 256) dtype=float32&gt;, &lt;tf.Tensor 'player/Placeholder_2:0' shape=(?, 256, 256) dtype=float32&gt;].
</code></pre>

<p>If I change <code>state_init</code> to be a flattened list of tensors with shape <code>[None, 256]</code> instead, I am getting:</p>

<pre><code>ValueError: An `initial_state` was passed that is not compatible with `cell.state_size`. Received `state_spec`=[InputSpec(shape=(None, 256), ndim=2), InputSpec(shape=(None, 256), ndim=2), InputSpec(shape=(None, 256), ndim=2)]; however `cell.state_size` is [[256, 256], [256, 256], [256, 256]]
</code></pre>

<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN"" rel=""noreferrer"">Tensorflow RNN docs</a> are fairly vague on this:</p>

<blockquote>
  <p>""You can specify the initial state of RNN layers symbolically by
  calling them with the keyword argument <code>initial_state</code>. The value of
  <code>initial_state</code> should be a tensor or list of tensors representing
  the initial state of the RNN layer.""</p>
</blockquote>
",0
55767475,Keras is not learning anything,"<p>I am trying to learn keras and none of the code I use is learning. From the example code on Deep Learning with Python to the code on <a href=""https://medium.com/nybles/create-your-first-image-recognition-classifier-using-cnn-keras-and-tensorflow-backend-6eaab98d14dd"" rel=""nofollow noreferrer"">https://medium.com/nybles/create-your-first-image-recognition-classifier-using-cnn-keras-and-tensorflow-backend-6eaab98d14dd</a>. With the last link, I am not able to use the full 10000 dataset, but even with my 1589 size training dataset the accuracy still stays at .5 the whole time.</p>

<p>I'm almost starting to think the problem is my overclocked cpu, and ram, but thats more of a crazy guess.</p>

<p>I initially thought the problem was that I had the tensorflow2.0.0-alpha. However, even after I went to regular tensorflow-gpu still nothing is learning.</p>

<pre><code>#Convolutional Neural Network

# Importing the Keras libraries and packages
    from keras.models import Sequential
    from keras.layers import Convolution2D
    from keras.layers import MaxPooling2D
    from keras.layers import Flatten
    from keras.layers import Dense
    from keras.models import model_from_json
    import os
#initialize the cnn
    classifier = Sequential()

#Step 1 convolution
    classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))

#Step 2 Pooling
    classifier.add(MaxPooling2D(pool_size = (2,2)))


#Step 3 Flattening
    classifier.add(Flatten())

#Step 4 Full Connection
    classifier.add(Dense(output_dim = 128, activation = 'relu'))
    classifier.add(Dense(output_dim = 64, activation = 'relu'))
    classifier.add(Dense(output_dim = 32, activation = 'relu'))
    classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))

#Compiling the CNN
    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

#Part 2 Fitting the CNN to the images
    from keras.preprocessing.image import ImageDataGenerator

    train_datagen = ImageDataGenerator(
        rescale=1/.255,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True)

    test_datagen = ImageDataGenerator(rescale=1./255)

    training_set = train_datagen.flow_from_directory(
        'dataset/training_set',
        target_size=(64, 64),
        batch_size=32,
        class_mode='binary')

    test_set = test_datagen.flow_from_directory(
        'dataset/test_set',
        target_size=(64, 64),
        batch_size=32,
        class_mode='binary')

    from IPython.display import display
    from PIL import Image

    classifier.fit_generator(
        training_set,
        steps_per_epoch=1589,
        epochs=10,
        validation_data=test_set,
        validation_steps=378)

    import numpy as np
    from keras.preprocessing import image
    test_image = image.load_img('dataset/test_set/cats/cat.4012.jpg', target_size = (64,64))
    test_image = image.img_to_array(test_image)
    test_image = np.expand_dims(test_image, axis = 0)
    result = classifier.predict(test_image)
    training_set.class_indices
    if result[0][0] &gt;= 0.5:
        prediction = 'dog'
    else:
        prediction = 'cat'
    print(prediction)
</code></pre>

<p>examples from deep-learning with python</p>

<pre><code>    from keras.datasets import imdb

    (train_data, train_labels), (test_data, test_labels) = 
    imdb.load_data(num_words = 10000)
    import numpy as np

    def vectorize_sequences(sequences, dimension=10000):
        results = np.zeros((len(sequences), dimension))
        for i,sequence in enumerate(sequences):
            results[i, sequence]=1.
        return results
    x_train = vectorize_sequences(train_data)
    x_test =  vectorize_sequences(test_data)

    y_train = np.asarray(train_labels).astype('float32')
    y_test = np.asarray(test_labels).astype('float32')

    from keras import models
    from keras import layers

    model = models.Sequential()
    model.add(layers.Dense(16, activation='relu',input_shape=(10000,)))
    model.add(layers.Dense(16, activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    x_val = x_train[:10000]
    partial_x_train = x_train[10000:]
    y_val = y_train[:10000]
    partial_y_train = y_train[10000:]
    model.compile(optimizer='rmsprop',
             loss='binary_crossentropy',
             metrics=['acc'])
    history = model.fit(partial_x_train,
                   partial_y_train,
                    epochs=20,
                   batch_size=512,
                   validation_data=(x_val,y_val))
</code></pre>

<p>dogs an cats output:</p>

<pre><code> Epoch 1/10
1589/1589 [==============================] - 112s 70ms/step - loss: 7.8736 - acc: 0.5115 - val_loss: 7.9528 - val_acc: 0.4976
Epoch 2/10
1589/1589 [==============================] - 111s 70ms/step - loss: 7.8697 - acc: 0.5117 - val_loss: 7.9606 - val_acc: 0.4971
Epoch 3/10
1589/1589 [==============================] - 111s 70ms/step - loss: 7.8740 - acc: 0.5115 - val_loss: 7.9499 - val_acc: 0.4978
Epoch 4/10
1589/1589 [==============================] - 111s 70ms/step - loss: 7.8674 - acc: 0.5119 - val_loss: 7.9634 - val_acc: 0.4969
Epoch 5/10
1589/1589 [==============================] - 111s 70ms/step - loss: 7.8765 - acc: 0.5113 - val_loss: 7.9499 - val_acc: 0.4977
Epoch 6/10
1589/1589 [==============================] - 111s 70ms/step - loss: 7.8737 - acc: 0.5115 - val_loss: 7.9634 - val_acc: 0.4970
Epoch 7/10
1589/1589 [==============================] - 129s 81ms/step - loss: 7.8623 - acc: 0.5122 - val_loss: 7.9626 - val_acc: 0.4970
Epoch 8/10
1589/1589 [==============================] - 112s 71ms/step - loss: 7.8758 - acc: 0.5114 - val_loss: 7.9508 - val_acc: 0.4977
Epoch 9/10
1589/1589 [==============================] - 115s 72ms/step - loss: 7.8708 - acc: 0.5117 - val_loss: 7.9519 - val_acc: 0.4976
Epoch 10/10
1589/1589 [==============================] - 112s 70ms/step - loss: 7.8738 - acc: 0.5115 - val_loss: 7.9614 - val_acc: 0.4971
cat
</code></pre>

<p>deeplearning imdb example output:
    WARNING:tensorflow:From C:\Users\Mike\Anaconda3\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.</p>

<p>Instructions for updating:
Use tf.cast instead.
Train on 15000 samples, validate on 10000 samples</p>

<pre><code>Epoch 1/20
15000/15000 [==============================] - 4s 246us/step - loss: 0.6932 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.4947
Epoch 2/20
15000/15000 [==============================] - 2s 115us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947
Epoch 3/20
15000/15000 [==============================] - 2s 115us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947
Epoch 4/20
15000/15000 [==============================] - 2s 119us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947
Epoch 5/20
15000/15000 [==============================] - 2s 120us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947
Epoch 6/20
15000/15000 [==============================] - 2s 119us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.4947
Epoch 7/20
15000/15000 [==============================] - 2s 113us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947
Epoch 8/20
15000/15000 [==============================] - 2s 113us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947
Epoch 9/20
15000/15000 [==============================] - 2s 119us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.4947
Epoch 10/20
15000/15000 [==============================] - 2s 122us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.4947
Epoch 11/20
15000/15000 [==============================] - 2s 116us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.4947
Epoch 12/20
15000/15000 [==============================] - 2s 116us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.4947
Epoch 13/20
15000/15000 [==============================] - 2s 121us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6933 - val_acc: 0.4947
Epoch 14/20
15000/15000 [==============================] - 2s 127us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947
Epoch 15/20
15000/15000 [==============================] - 2s 121us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947
Epoch 16/20
15000/15000 [==============================] - 2s 113us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947
Epoch 17/20
15000/15000 [==============================] - 2s 115us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947
Epoch 18/20
15000/15000 [==============================] - 2s 114us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947
Epoch 19/20
15000/15000 [==============================] - 2s 114us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947
Epoch 20/20
15000/15000 [==============================] - 2s 119us/step - loss: 0.6931 - acc: 0.5035 - val_loss: 0.6932 - val_acc: 0.4947
</code></pre>
",0
55770009,How to use a pre-trained embedding matrix in tensorflow 2.0 RNN as initial weights in an embedding layer?,"<p>I'd like to use a pretrained GloVe embedding as the initial weights for an embedding layer in an RNN encoder/decoder. The code is in Tensorflow 2.0. Simply adding the embedding matrix as a weights = [embedding_matrix] parameter to the tf.keras.layers.Embedding layer won't do it because the encoder is an object and I'm not sure now to effectively pass the embedding_matrix to this object at training time.</p>

<p>My code closely follows the <a href=""https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention"" rel=""noreferrer"">neural machine translation example in the Tensorflow 2.0 documentation</a>. How would I add a pre-trained embedding matrix to the encoder in this example? The encoder is an object. When I get to training, the GloVe embedding matrix is unavailable to the Tensorflow graph. I get the error message: </p>

<p>RuntimeError: Cannot get value inside Tensorflow graph function. </p>

<p>The code uses the GradientTape method and teacher forcing in the training process. </p>

<p>I've tried modifying the encoder object to include the embedding_matrix at various points, including in the encoder's <strong>init</strong>, call and initialize_hidden_state. All of these fail. The other questions on stackoverflow and elsewhere are for Keras or older versions of Tensorflow, not Tensorflow 2.0.</p>

<pre><code>class Encoder(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):
        super(Encoder, self).__init__()
        self.batch_sz = batch_sz
        self.enc_units = enc_units
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, weights=[embedding_matrix])
        self.gru = tf.keras.layers.GRU(self.enc_units,
                                       return_sequences=True,
                                       return_state=True,
                                       recurrent_initializer='glorot_uniform')

    def call(self, x, hidden):
        x = self.embedding(x)
        output, state = self.gru(x, initial_state = hidden)
        return output, state

    def initialize_hidden_state(self):
        return tf.zeros((self.batch_sz, self.enc_units))

encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)

# sample input
sample_hidden = encoder.initialize_hidden_state()
sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)
print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))
print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))

# ... Bahdanau Attention, Decoder layers, and train_step defined, see link to full tensorflow code above ...

# Relevant training code

EPOCHS = 10

training_record = pd.DataFrame(columns = ['epoch', 'training_loss', 'validation_loss', 'epoch_time'])


for epoch in range(EPOCHS):
    template = 'Epoch {}/{}'
    print(template.format(epoch +1,
                 EPOCHS))
    start = time.time()

    enc_hidden = encoder.initialize_hidden_state()
    total_loss = 0
    total_val_loss = 0

    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):
        batch_loss = train_step(inp, targ, enc_hidden)
        total_loss += batch_loss

        if batch % 100 == 0:
            template = 'batch {} ============== train_loss: {}'
            print(template.format(batch +1,
                            round(batch_loss.numpy(),4)))
</code></pre>
",0
55778682,fix/freeze individual kernel weights in a convolutional operation,"<p>I'm trying out a workaround for fixing individual kernel weights in a convolutional operation in TensorFlow using Python 3.7. I do it by creating </p>

<ol>
<li>a trainable variable, </li>
<li>an identical non-trainable variable and </li>
<li>a ""mask"" tensor consisting of <strong>1</strong>s and <strong>0s</strong> with the same shape as the created variables in step 1 and 2 above.</li>
</ol>

<p>A <strong>1</strong> in the ""mask"" tensor indicates that I want to fix/freeze that specific weight during training, i.e. not update it in the backward pass.</p>

<p>Now, this workaround works perfectly fine when applied to a fully connected layer but fails when applied to a convolutional layer and I can't figure out why or how to make it work.</p>

<p>Something seems to be happening in the <strong>tf.nn.conv2d()</strong> function call (see code example below) and according to the documentation this is what they do:</p>

<blockquote>
  <p>Given an input tensor of shape <code>[batch, in_height, in_width, in_channels]</code><br>
    and a filter / kernel tensor of shape<br>
   <code>[filter_height, filter_width, in_channels, out_channels]</code>, this op<br>
    performs the following:<br>
    1. Flattens the filter to a 2-D matrix with shape<br>
       <code>[filter_height * filter_width * in_channels, output_channels]</code>.<br>
    2. Extracts image patches from the input tensor to form a <em>virtual</em><br>
       tensor of shape <code>[batch, out_height, out_width,&lt;br&gt;
       filter_height * filter_width * in_channels]</code>.<br>
    3. For each patch, right-multiplies the filter matrix and the image patch<br>
       vector.</p>
</blockquote>

<p>But since I use <strong>weights_frozen</strong> which is a tensor and depends on the trainable variable, non-trainable variable and <strong>mask_weights</strong> it should get zero-valued gradients on the positions where I have a 1 in the <strong>mask_weights</strong> tensor.</p>

<pre class=""lang-py prettyprint-override""><code>def conv(input_, layer_name...):

    weights = tf.get_variable(shape=[filter_height, filter_width, in_channels, out_channels], dtype=tf.float32, initializer=tf.glorot_uniform_initializer(), trainable=True)

    weights_fixed = tf.Variable(tf.identity(weights), trainable=False)

    mask_weights = tf.placeholder(tf.float32, weights.shape)


    weights_frozen = tf.add(tf.multiply(mask_weights, weights_fixed), tf.multiply((1 - mask_weights), weights))


    out_conv = tf.nn.conv2d(input=input_, filter=weights_frozen, strides=strides_, padding='SAME')
    out_add = tf.nn.bias_add(value=out_conv, bias=biases_frozen)

    out = tf.nn.relu(features=out_add)

    return out
</code></pre>

<p>As mentioned, I expect to get zero-valued gradients on the positions where I have a <strong>1</strong> in the <strong>mask_weights</strong> tensor, but instead they are non-zero and therefore those weights are being trained, which is not the behavior I'm trying to achieve.</p>
",1
55788007,Unexpected results when using tfrecords loaded using tf.data.Dataset.list_files() with shuffle argument,"<p>I'm hoping to get clarification on how the <code>shuffle</code> argument in <code>tf.data.Dataset.list_files()</code> works. The documentation states that when <code>shuffle=True</code>, the filenames will be shuffled randomly. I've made model predictions using a tfrecords dataset that has been loaded using <code>tf.data.Dataset.list_files()</code>, and I would've expected the accuracy metric to be the same no matter the order of the files (i.e. whether shuffle is True or False), but am seeing otherwise. </p>

<p>Is this expected behavior or is there something wrong with my code or intepretation? I have reproducible example code below.</p>

<p>Oddly, as long as <code>tf.random.set_random_seed()</code> is set initially (and it seems it doesn't even matter what seed value is set), then the predictions results are the same no matter whether shuffle is True or False in <code>list_files()</code>.</p>

<p>tensorflow==1.13.1, keras==2.2.4</p>

<p>Thanks for any clarifications!</p>

<p>Edit: re-thinking it through and wondering if <code>Y = [y[0] for _ in range(steps) for y in sess.run(Y)]</code> is a separate and independent call?</p>

<pre><code># Fit and Save a Dummy Model
import numpy as np
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import np_utils
from sklearn import datasets, metrics

seed = 7
np.random.seed(seed)
tf.random.set_random_seed(seed)

dataset = datasets.load_iris()

X = dataset.data
Y = dataset.target
dummy_Y = np_utils.to_categorical(Y)

# 150 rows
print(len(X))

model = Sequential()
model.add(Dense(8, input_dim=4, activation='relu'))
model.add(Dense(3, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, dummy_Y, epochs=10, batch_size=10,  verbose=2)
model.save('./iris/iris_model')

predictions = model.predict(X)
predictions = np.argmax(predictions, axis=1)

# returns accuracy = 0.3466666666666667
print(metrics.accuracy_score(y_true=Y, y_pred=predictions))
</code></pre>

<p>Split dataset into multiple tfrecords files so we can reload it with list_files() later:</p>

<pre><code>numrows = 15
for i, j in enumerate(range(0, len(X), numrows)):
    with tf.python_io.TFRecordWriter('./iris/iris{}.tfrecord'.format(i)) as writer:
        for x, y in zip(X[j:j+numrows, ], Y[j:j+numrows, ]):
            features = tf.train.Features(feature=
                {'X': tf.train.Feature(float_list=tf.train.FloatList(value=x)), 
                'Y': tf.train.Feature(int64_list=tf.train.Int64List(value=[y]))
                })
            example = tf.train.Example(features=features)
            writer.write(example.SerializeToString())
</code></pre>

<p>At this point, I exit (ipython) and restart again:</p>

<pre><code>import numpy as np
import tensorflow as tf
from keras.models import load_model
from sklearn import metrics

model = load_model('./iris/iris_model')

batch_size = 10
steps = int(150/batch_size)
file_pattern = './iris/iris*.tfrecord'

feature_description = {
    'X': tf.FixedLenFeature([4], tf.float32),
    'Y': tf.FixedLenFeature([1], tf.int64)
}

def _parse_function(example_proto):
    return tf.parse_single_example(example_proto, feature_description)

def load_data(filenames, batch_size):
    raw_dataset = tf.data.TFRecordDataset(filenames)
    dataset = raw_dataset.map(_parse_function)
    dataset = dataset.batch(batch_size, drop_remainder=True)
    dataset = dataset.prefetch(2)
    iterator = dataset.make_one_shot_iterator()
    record = iterator.get_next()
    return record['X'], record['Y']

def get_predictions_accuracy(filenames):
    X, Y = load_data(filenames=filenames, batch_size=batch_size)

    predictions = model.predict([X], steps=steps)
    predictions = np.argmax(predictions, axis=1)
    print(len(predictions))

    with tf.Session() as sess:
        Y = [y[0] for _ in range(steps) for y in sess.run(Y)]

    print(metrics.accuracy_score(y_true=Y, y_pred=predictions))
</code></pre>

<pre><code># No shuffle results:
# Returns expected accuracy = 0.3466666666666667
filenames_noshuffle = tf.data.Dataset.list_files(file_pattern=file_pattern, shuffle=False)
get_predictions_accuracy(filenames_noshuffle)
</code></pre>

<pre><code># Shuffle results, no seed value set:
# Returns UNEXPECTED accuracy (non-deterministic value)
filenames_shuffle_noseed = tf.data.Dataset.list_files(file_pattern=file_pattern, shuffle=True)
get_predictions_accuracy(filenames_shuffle_noseed)
</code></pre>

<pre><code># Shuffle results, seed value set:
# Returns expected accuracy = 0.3466666666666667
# It seems like it doesn't even matter what seed value you set, as long as you you set it
seed = 1000
tf.random.set_random_seed(seed)
filenames_shuffle_seed = tf.data.Dataset.list_files(file_pattern=file_pattern, shuffle=True)
get_predictions_accuracy(filenames_shuffle_seed)
</code></pre>
",1
55813556,TensorFlow AlphaDropout: rank undefined,"<p>I am trying to set-up a neural network using TensorFlow's <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/nn/alpha_dropout"" rel=""nofollow noreferrer""><code>tf.contrib.nn.alpha_dropout</code></a> (as implemented in TensorFlow version <code>1.12.0</code>). Please consider the following example:</p>

<pre><code>import tensorflow as tf
from tensorflow.contrib.layers import fully_connected
from tensorflow.contrib.nn import alpha_dropout
import numpy as np

N_data = 100
x_in = tf.placeholder(tf.float32, shape=[None, N_data], name=""x_in"")
keep_prob = tf.placeholder(tf.float32)

fc = fully_connected(inputs=x_in, num_outputs=N_data)
drop = alpha_dropout(fc, keep_prob=keep_prob)
x_out = fully_connected(inputs=drop, num_outputs=N_data)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    fd = {
        x_in: np.random.rand(2, N_data),
        keep_prob: 0.5,
    }

    output = x_out.eval(feed_dict=fd)
</code></pre>

<p>When evaluating the output of the dropout layer, everything seems normal, but when the output from the dropout layer is linked to a second dense layer, I get the following error message:</p>

<pre><code>Traceback (most recent call last):
  File ""/***/problem_alpha_dropout.py"", line 14, in &lt;module&gt;
    x_out = fully_connected(inputs=drop, num_outputs=N_data)
  File ""/***/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 182, in func_with_args
    return func(*args, **current_args)
  File ""/***/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1854, in fully_connected
    outputs = layer.apply(inputs)
  File ""/***/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 817, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/***/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 374, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/***/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 730, in __call__
    self._assert_input_compatibility(inputs)
  File ""/***/anaconda3/envs/TensorFlow/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1465, in _assert_input_compatibility
    self.name + ' is incompatible with the layer: '
ValueError: Input 0 of layer fully_connected_1 is incompatible with the layer: its rank is undefined, but the layer requires a defined rank.
</code></pre>

<p>This behaviour does not emerge when <code>tf.contrib.nn.alpha_dropout</code> is replaced by <code>tf.nn.dropout</code> (same usage).</p>

<p><strong>Additional information:</strong></p>

<ul>
<li>TensorFlow version: <code>1.12.0</code> (GPU)</li>
<li>Python version: <code>3.6</code> (through Anaconda)</li>
<li>OS: Linux Mint</li>
</ul>
",0
55857522,How to create a serving_input_fn in Tensorflow 2.0 for image preprocessing?,"<p>I am using Tensorflow 2.0 and am able to train a CNN for image classification of 3-channel images. I perform image preprocessing within the data input pipeline (shown below) and would like to include the preprocessing functionality in the served model itself. My model is served with a TF Serving Docker container and the Predict API.</p>

<p>The data input pipeline for training is based on the documentation at <a href=""https://www.tensorflow.org/alpha/tutorials/load_data/images"" rel=""nofollow noreferrer"">https://www.tensorflow.org/alpha/tutorials/load_data/images</a>.</p>

<p>My pipeline image preprocessing function is <em>load_and_preprocess_from_path_label</em>:</p>

<pre><code>def load_and_preprocess_path(image_path):

    # Load image
    image = tf.io.read_file(image_path)
    image = tf.image.decode_png(image)

    # Normalize to [0,1] range
    image /= 255

    # Convert to HSV and Resize
    image = tf.image.rgb_to_hsv(image)
    image = tf.image.resize(image, [HEIGHT, WIDTH])

    return image

def load_and_preprocess_from_path_label(image_path, label):

    return load_and_preprocess_path(image_path), label
</code></pre>

<p>With lists of image paths, the pipeline prefetches and performs image preprocessing using tf functions within <em>load_and_preprocess_from_path_label</em>:</p>

<pre><code>all_image_paths, all_image_labels = parse_labeled_image_paths()
x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(all_image_paths, all_image_labels, test_size=0.2)

# Create a TensorFlow Dataset of training images and labels
ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))
image_label_ds = ds.map(load_and_preprocess_from_path_label)

BATCH_SIZE = 32
IMAGE_COUNT = len(all_image_paths)

ds = image_label_ds.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=IMAGE_COUNT))
ds = ds.batch(BATCH_SIZE)
ds = ds.prefetch(buffer_size=AUTOTUNE)

# Create image pipeline for model
image_batch, label_batch = next(iter(ds))
feature_map_batch = model(image_batch)

# Train model
model.fit(ds, epochs=5)
</code></pre>

<p>Previous Tensorflow examples I've found use <em>serving_input_fn()</em>, and utilized <em>tf.placeholder</em> which seems to no longer exist in Tensorflow 2.0. </p>

<p>An example for <em>serving_input_fn</em> in Tensorflow 2.0 is shown on <a href=""https://www.tensorflow.org/alpha/guide/saved_model"" rel=""nofollow noreferrer"">https://www.tensorflow.org/alpha/guide/saved_model</a>. Since I am using the Predict API, it looks like I would need something similar to:</p>

<pre><code>serving_input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(...)

# Save the model with the serving preprocessing function
model.export_saved_model(MODEL_PATH, serving_input_fn)

</code></pre>

<p>Ideally, the served model would accept a 4D Tensor of 3-channel image samples of any size and would perform the initial image preprocessing on them (decode image, normalize, convert to HSV, and resize) before classifying.</p>

<p>How can I create a serving_input_fn in Tensorflow 2.0 with a preprocessing function similar to my <em>load_and_preprocess_path</em> function?</p>
",0
55909188,How can I apply a TensorFlow 2D Convolution (tf.nn.conv2d) to a single (non-batch) 2D image?,"<p>I would like to use the function <code>tf.nn.conv2d()</code> on a <strong>single</strong> image example, but the TensorFlow documentation seems to only mention applying this transformation to a <strong>batch</strong> of images. </p>

<p>The docs mention that the input image must be of shape <code>[batch, in_height, in_width, in_channels]</code> and the kernel must be of shape <code>[filter_height, filter_width, in_channels, out_channels]</code>. However, what is the most straightforward way to achieve 2D convolution with input shape <code>[in_height, in_width, in_channels]</code>?</p>

<p>Here is an example of the current approach, where <code>img</code> has shape (height, width, channels):</p>

<pre><code>img = tf.random_uniform((10,10,3))  # a single image
img = tf.nn.conv2d([img], kernel)[0] # creating a batch of 1, then indexing the single example
</code></pre>

<p>I am reshaping the input as follows:</p>

<p><code>[in_height, in_width, in_channels]-&gt;[1, in_height, in_width, in_channels]-&gt;[in_height, in_width, in_channels]</code> </p>

<p>This feels like an unnecessary and costly operation when I am only interested in transforming one example.</p>

<p>Is there a simple/standard way to do this that doesn't involve reshaping?</p>
",1
55916743,How to get gradients with respect to input and change input (rather than trainable vars) to minimize loss in TF 2?,"<p>I want to use a trained model to change the input so it minimizes the loss (rather than changing the trainable variables) a la Deep Dreaming in Tensorflow 2.0 but I am not having success.</p>

<p>Say I have a basic NN as the one in the docs</p>

<pre><code>class MyModel(Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.conv1 = Conv2D(32, 3, activation='relu')
    self.flatten = Flatten()
    self.d1 = Dense(128, activation='relu')
    self.d2 = Dense(10, activation='softmax')

  def call(self, x):
    x = self.conv1(x)
    x = self.flatten(x)
    x = self.d1(x)
    return self.d2(x)

model = MyModel()

</code></pre>

<p>Which I train using a simple tf.GradientTape function</p>

<pre><code>@tf.function
def train_step(image, label):
  with tf.GradientTape() as tape:
    predictions = model(image)
    loss = loss_object(label, predictions)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))
</code></pre>

<p>What's the idiomatic way to create a function that will instead calculate and apply the gradients to the input - images.</p>

<p>I assumed it will be as simple as</p>

<pre><code>def train_step(image, label):
  with tf.GradientTape() as tape:
    predictions = model(image)
    loss = loss_object(label, predictions)
  gradients = tape.gradient(loss, image)
  optimizer.apply_gradients(zip(gradients, image))
</code></pre>

<p>However, that doesn't work.</p>
",1
55986982,What is the way to use Tensor flow 2.0 object in open cv2 python and why is it so circuitous?,"<p>I load an image using tensor flow api (2.0) like so : </p>

<pre><code>def load(image_file):
  image = tf.io.read_file(image_file)
  image = tf.image.decode_jpeg(image)
</code></pre>

<p>Now that I have this object, I want to show this image, I can simply use matplotlib.pyplot, and this works. </p>

<pre><code>plt.figure()
plt.imshow(re/255.0)
plt.show()
</code></pre>

<p>However attempting this with OpenCV2 is problematic from the start, most of the examples are from 1.0 with .eval() session based suggestion for numpy conversion. One way would be to first convert tensor flow object to numpy, here is the function to do that from API documentation :</p>

<pre><code>TensorFlow
API r2.0
TensorFlow Core 2.0a
Python
tf.make_ndarray
Create a numpy ndarray from a tensor.
</code></pre>

<p>I dont understand why this does not works and I get a number of errors while all I want is to do something simple and then use some open cv2 functions like remap, resize etc.:</p>

<blockquote>
  <p>File
  ""C:\Python\Python37\lib\site-packages\tensorflow\python\eager\def_function.py"",
  line 426, in <strong>call</strong>
      self._initialize(args, kwds, add_initializers_to=initializer_map)   File
  ""C:\Python\Python37\lib\site-packages\tensorflow\python\eager\def_function.py"",
  line 370, in _initialize
      *args, **kwds))   File ""C:\Python\Python37\lib\site-packages\tensorflow\python\eager\function.py"",
  line 1313, in _get_concrete_function_internal_garbage_collected
      graph_function, _, _ = self._maybe_define_function(args, kwargs)   File
  ""C:\Python\Python37\lib\site-packages\tensorflow\python\eager\function.py"",
  line 1580, in _maybe_define_function
      graph_function = self._create_graph_function(args, kwargs)   File ""C:\Python\Python37\lib\site-packages\tensorflow\python\eager\function.py"",
  line 1512, in _create_graph_function
      capture_by_value=self._capture_by_value),   File ""C:\Python\Python37\lib\site-packages\tensorflow\python\framework\func_graph.py"",
  line 694, in func_graph_from_py_func
      func_outputs = python_func(*func_args, **func_kwargs)   File ""C:\Python\Python37\lib\site-packages\tensorflow\python\eager\def_function.py"",
  line 317, in wrapped_fn
      return weak_wrapped_fn().<strong>wrapped</strong>(*args, **kwds)   File ""C:\Python\Python37\lib\site-packages\tensorflow\python\framework\func_graph.py"",
  line 686, in wrapper
      ), args, kwargs)   File ""C:\Python\Python37\lib\site-packages\tensorflow\python\autograph\impl\api.py"",
  line 392, in converted_call
      result = converted_f(*effective_args, **kwargs)   File ""C:\Users\syeda\AppData\Local\Temp\tmpnahp3og4.py"", line 32, in
  tf__random_deform
      im2 = ag__.converted_call('make_ndarray', tf, ag__.ConversionOptions(recursive=True, verbose=0,
  strip_decorators=(tf.function, defun_9, ag__.convert,
  ag__.do_not_convert, ag__.converted_call), force_conversion=False,
  optional_features=(), internal_convert_user_code=True), (real_image,),
  {})   File
  ""C:\Python\Python37\lib\site-packages\tensorflow\python\autograph\impl\api.py"",
  line 267, in converted_call
      return _call_unconverted(f, args, kwargs)   File ""C:\Python\Python37\lib\site-packages\tensorflow\python\autograph\impl\api.py"",
  line 188, in _call_unconverted
      return f(*args, **kwargs)   File ""C:\Python\Python37\lib\site-packages\tensorflow\python\framework\tensor_util.py"",
  line 596, in MakeNdarray
      shape = [d.size for d in tensor.tensor_shape.dim] AttributeError: 'Tensor' object has no attribute 'tensor_shape'</p>
</blockquote>

<p><strong>Update 5/5/2018 :</strong> After searching more I found out that this has something to do with Tensorflow graph execution. 
I have a function </p>

<pre><code>def load_image_train(image_file):
  input_image, real_image = load(image_file)
 print(type(real_image))
  print(real_image.shape)
  some_image = Open CV operations like filtering, jitter etc performed on real_image
return some_image
</code></pre>

<p>This works nicely when called eagerly with .numpy() attribute, however when called like following code and when you try to inspect what real_image is and its type returns</p>

<blockquote>
  <p>class 'tensorflow.python.framework.ops.Tensor'   (None, None, None)</p>
</blockquote>

<p>Please advice.</p>

<pre><code># Input pipeline
train_dataset = tf.data.Dataset.list_files(PATH+'train/*.jpg')
train_dataset = train_dataset.shuffle(BUFFER_SIZE)
train_dataset = train_dataset.map(load_image_train,
                               num_parallel_calls=tf.data.experimental.AUTOTUNE)
train_dataset = train_dataset.batch(1)
</code></pre>

<p><strong>Update 5/5/2018 :</strong> I decided to do a preprocessing of the data so I don't have to worry about the using any opencv functionality during the load time of the data. However during training time I still want to do some openCV operations. Now as per the suggestion of @giser_yugang I tried using py_function, I wrap opencv operations in py_function and call that function in a wrapper tf.function. This wrapper tf.function I call in train step. However the output I get from this wrapper function is like so : </p>

<pre><code>class 'tensorflow.python.framework.ops.Tensor'
unknown
</code></pre>

<p>Then if I try to consume this tensor in the next train step operation I get a </p>

<pre><code>incompatible with the layer: its rank is undefined, but the layer requires a defined rank.
</code></pre>

<p>If I don't use this py_function wrapper in my train step and directly try the numpy operations using opencv I get another error </p>

<pre><code>AttributeError: 'Tensor' object has no attribute 'numpy'
</code></pre>

<p>I guess both ways you cant win !</p>
",1
56047272,Explicit vs implicit type definition in TensorFlow,"<p>I'm just beginning to learn TensorFlow. Quoting from the <a href=""https://www.tensorflow.org/guide/low_level_intro#graph"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p>Let's build a simple computational graph. The most basic operation is a constant. The Python function that builds the operation takes a tensor value as input. The resulting operation takes no inputs. When run, it outputs the value that was passed to the constructor. We can create two floating point constants a and b as follows:</p>
</blockquote>

<pre><code>a = tf.constant(3.0, dtype=tf.float32)
b = tf.constant(4.0) # also tf.float32 implicitly
total = a + b
print(a)
print(b)
print(total)
</code></pre>

<p>The second constant is implicitly typed as a float32. Is that based on the explicit typing of the first constant? And does that imply that the first <code>dtype</code> is required? <a href=""https://www.tensorflow.org/api_docs/python/tf/constant"" rel=""nofollow noreferrer"">tf.constant documentation</a> would imply that it does not:</p>

<blockquote>
  <p>If the argument dtype is not specified, then the type is inferred from the type of <code>value</code>.</p>
</blockquote>

<p>But then it would be unnecessary to explicitly type the 3.0 constant above.</p>

<p>I'm just looking for some clarification on this, since, like I said, I'm just starting out.</p>
",1
56088294,tf.keras.models.Sequential model cannot fit with input type tf.Tensor,"<p>I have written a simple tf.keras.models.Sequential model. When I try to fit it with data and labels as tf.Tensor, it gives me some error. However I can fit it with numpy array with exactly the same underlying data. Why is it?</p>

<p>I am using tensorflow 1.13 with only CPU. I checked the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit"" rel=""nofollow noreferrer"">fit</a> function of tf.keras.models.Sequential but it says both tf.Tensor and numpy array can be used as data and label as long as their types match.</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
tf.enable_eager_execution()

# very simple keras Sequential model
model = tf.keras.Sequential([
tf.keras.layers.Dense(3, activation='relu'),
tf.keras.layers.Dense(3, activation='softmax')])

model.compile(optimizer=tf.train.AdamOptimizer(0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# use tf.Tensor as data and label
data = tf.constant([[0,0,1],[0,1,0],[1,0,0]])
label = tf.constant([[0,0,1],[0,1,0],[1,0,0]])
# This throws the following error
# InvalidArgumentError: Index out of range using input dim 2; input has only 2 dims [Op:StridedSlice] name: strided_slice/
model.fit(data, label, epochs=10)

# use numpy array with the same underlying data and label
data = data.numpy()
label = label.numpy()
# This works
model.fit(data, label, epochs=10)


</code></pre>

<p>The first fit does not work and throws the following error. But the second works. This is interesting because they have exacly the same underlying data</p>
",0
56092824,TF 2.0: Where can I find the upgrade of tf.contrib.training?,"<p>I want to use the HParams class from <strong>tf.contrib.training</strong> in tensorflow 2.0 version, but I can't find the replacement for this class neither in <em>tensorflow alpha</em> documentation nor in <em>tensorflow_addons</em></p>
",0
56104841,Will it impair the performance to explicitly feed data to 'tf.data.Dataset',"<p>I'm implementing an RL algorithm and using <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">tf.data.Dataset</a>(with <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch"" rel=""nofollow noreferrer"">prefetch</a>) to feed data to the neural network. However, in order to interact with the environment, I have to explicitly feed data through <code>feed_dict</code> to take action. I'm wondering if using <code>feed_dict</code> with <code>Dataset</code> would impair the speed.</p>

<p>Here's a simplified version of my code</p>

<pre class=""lang-py prettyprint-override""><code># code related to Dataset
ds = tf.data.Dataset.from_generator(buffer, sample_types, sample_shapes)
ds = ds.prefetch(5)
iterator = ds.make_one_shot_iterator()
samples = iterator.get_next(name='samples')
# pass samples to network
</code></pre>

<pre class=""lang-py prettyprint-override""><code># network training, no feed_dict is needed because of Dataset
sess.run([self.opt_op])
</code></pre>

<pre class=""lang-py prettyprint-override""><code># run the actor network to choose an action at the current state.
# manually feed the current state to samples
# will this impair the performance?
action = sess.run(self.action, feed_dict={samples['state']: state})
</code></pre>
",0
56151482,"tensorflow how to use tf.map_fn to process two input tensor shape of (?, 40,30,128) and (?,40,30) in batch wise, ? is my batch size","<p>I am using TF1.12.</p>

<p>I got a function input size is <code>fn([40, 30, 128],[40, 30])</code> and return <code>tf.float32</code></p>

<p>But with batch size, I don't know what to do. Then I found this <a href=""https://www.tensorflow.org/versions/r1.12/api_docs/python/tf/map_fn"" rel=""nofollow noreferrer"">function</a> <code>tf.map_fn</code>. I got two input so where shall I put my input.
<code>input1 [?, 40, 30, 128]</code>and <code>input2 [?, 40, 30]</code></p>

<p>The batch size is <code>None</code>, but they are equal.</p>

<p>Anything related will be thankful. I am new to tensorflow, so anything is helpful.</p>

<p>Big Thanks!</p>
",0
56176475,"In Keras, is there documentation describing the string name to class mappings for initializers, optimizers, etc?","<p>Is there any documentation describing what string names map to what objects in Keras? For example, below I create an Embedding layer from <code>tf.keras.layers</code> and I can use <code>'uniform'</code> to map to the <code>tf.keras.initializers.RandomUniform</code> class. </p>

<pre><code>tf.keras.layers.Embedding(1000, 64, embeddings_initializer='uniform')
</code></pre>

<p>But I only know that by seeing examples of that usage. I presume the supported string forms are documented somewhere, but I can't seem to find such documentation, and digging through the code got too abstract to follow easily.</p>

<p>Version: TF 1.13.1</p>
",1
56201490,What happens when @tf.function decorator compiles a function into graph? Why is it faster than that in eager mode?,"<p>According to the TensorFlow <a href=""https://www.tensorflow.org/alpha/guide/autograph"" rel=""nofollow noreferrer"">document</a>, <code>@tf.function</code> compiles a function into a graph, and ""makes you get the benefits of faster execution, running on GPU or TPU, or exporting to SavedModel.""</p>

<p>The example in the document demonstrates such a benefit:</p>

<pre><code>lstm_cell = tf.keras.layers.LSTMCell(10)

@tf.function
def lstm_fn(input, state):
  return lstm_cell(input, state)

input = tf.zeros([10, 10])
state = [tf.zeros([10, 10])] * 2
# warm up
lstm_cell(input, state); lstm_fn(input, state)
print(""eager lstm:"", timeit.timeit(lambda: lstm_cell(input, state), number=10))
print(""function lstm:"", timeit.timeit(lambda: lstm_fn(input, state), number=10))
</code></pre>

<p>output:</p>

<pre><code>eager lstm: 0.032440788112580776 
function lstm: 0.004768412094563246
</code></pre>

<p>What's the difference between a 'compiled graph' and a 'function in eager mode'? Why is the former faster when executed?</p>
",0
56212366,TensorFlow tf.data processing dev set after each epoch,"<pre class=""lang-py prettyprint-override""><code>batch_size = 2
x_dim = 2
m = 5
m_dev = 4
epochs = 2

# Toy data
X_train = np.random.randn(m, x_dim)
Y_train = np.random.randint(0, 5, size=m).reshape(-1, 1)
X_dev = np.random.randn(m_dev, x_dim)
Y_dev = np.random.randint(0, 5, size=m_dev).reshape(-1, 1)

X = tf.placeholder(X_train.dtype, shape=[None, x_dim], name='X')
Y = tf.placeholder(Y_train.dtype, shape=[None, 1], name='Y')

# Create two separate datasets
train_dataset = tf.data.Dataset.from_tensor_slices((X, Y)).batch(batch_size)
dev_dataset = tf.data.Dataset.from_tensor_slices((X, Y)).batch(X_dev.shape[0])

# Create a generic Iterator
iterator = tf.data.Iterator.from_structure(train_dataset.output_types,
                                           train_dataset.output_shapes)

# Create two init ops
train_init_op = iterator.make_initializer(train_dataset)
dev_init_op = iterator.make_initializer(dev_dataset)

next_data = iterator.get_next()

with tf.Session() as sess:
    for epoch in range(epochs):
        # Training data
        sess.run(train_init_op, feed_dict={X: X_train, Y: Y_train})
        while True:
            try:
                X_batch, Y_batch = sess.run(next_data)
                # process training data
            except tf.errors.OutOfRangeError:
                break

        # Epoch done: process the dev data
        sess.run(dev_init_op, feed_dict={X: X_dev, Y: Y_dev})
        X_dev_all, Y_dev_all = sess.run(next_data)
</code></pre>

<p>I am using <code>tf.data</code> with reinitializable iterator to handle training and dev set data. For each epoch, I initialize the training data set. <a href=""https://www.tensorflow.org/guide/datasets#creating_an_iterator"" rel=""nofollow noreferrer"">The official documentation</a> has similar structure. I think this is not efficient especially if the training set is large. </p>

<p>Some of the resources I found online has <code>sess.run(train_init_op, feed_dict={X: X_train, Y: Y_train})</code> before the for loop to avoid this issue. But then we can't process the dev set after each epoch; we can only process it after we are done iterating over <code>epochs</code> epochs.</p>

<p>Is there a way to efficiently process the dev set after each epoch?</p>
",1
56216553,Customize Input to Tensorflow Hub module,"<p>I know how to load a pre-trained image models from <a href=""https://www.tensorflow.org/hub/basics"" rel=""nofollow noreferrer"">Tensorflow Hub</a>. like so:  </p>

<pre><code>#load model
image_module = hub.Module('https://tfhub.dev/google/imagenet/mobilenet_v2_035_128/feature_vector/2')

#get predictions
features = image_module(batch_images)
</code></pre>

<p>I also know how to customize the output of this model (fine-tune on new dataset). The existing <code>Modules</code> expect input <code>batch_images</code> to be a RGB image tensor.</p>

<p><strong>My question</strong>: Instead of the input being a RGB image of certain dimensions, I would like to use a tensor (dim 20x20x128, from a different model) as input to the Hub model. This means I need to by-passing the initial layers of the tf-hub model definition (i don't need them). Is this possible in tf-hub module api's? Documentation is not clear on this aspect.</p>

<p>p.s.: I can do this easily be defining my own layers but trying to see if i can use the Tf-Hub API's.</p>
",0
56231695,When should tf.losses.add_loss() be used in TensorFlow?,"<p>I cannot find an answer to this question in the TensorFlow documentation. I once read that one should add losses from <code>tf.nn</code> functions but it isn't necessary for functions from <code>tf.losses</code>. Therefore:</p>

<p>When should I use <code>tf.losses.add_loss()</code>?</p>

<p>Example:</p>

<pre><code>loss = tf.reduce_mean(tf.nn.sparse_softmax_corss_entropy_with_logits
                       (labels=ground_truth, logits=predictions))

tf.losses.add_loss(loss) &lt;-- when is this required?
</code></pre>

<p>Thank yoou.</p>
",1
56271275,Constructing discrete table-based CPDs in tensorflow-probablity?,"<p>I'm trying to construct the simplest example of Bayesian network with several discrete random variables and conditional probabilities (the ""Student Network"" from Koller's book, see <a href=""https://uol.de/fileadmin/_processed/a/1/csm_Koller_Fig_3.4_Bayesian-student-network_a00d23f6fb.png"" rel=""nofollow noreferrer"">1</a>)</p>

<p>Although a bit unwieldy, I managed to build this network using pymc3. Especially, creating the CPDs is not that straightforward in pymc3, see the snippet below:</p>

<pre class=""lang-py prettyprint-override""><code>import pymc3 as pm

...

with pm.Model() as basic_model:
    # parameters for categorical are indexed as [0, 1, 2, ...]
    difficulty = pm.Categorical(name='difficulty', p=[0.6, 0.4])

    intelligence = pm.Categorical(name='intelligence', p=[0.7, 0.3])

    grade = pm.Categorical(name='grade',
        p=pm.math.switch(
            theano.tensor.eq(intelligence, 0),
                pm.math.switch(
                    theano.tensor.eq(difficulty, 0),
                        [0.3, 0.4, 0.3],  # I=0, D=0
                        [0.05, 0.25, 0.7]   # I=0, D=1
                    ),
                    pm.math.switch(
                        theano.tensor.eq(difficulty, 0),
                            [0.9, 0.08, 0.02],  # I=1, D=0
                            [0.5, 0.3, 0.2]  # I=1, D=1
                    )
            )
        )

    letter = pm.Categorical(name='letter', p=pm.math.switch(
    ...
</code></pre>

<p>But I have no idea how to build this network using tensoflow-probability (versions: <code>tfp-nightly==0.7.0.dev20190517</code>, <code>tf-nightly-2.0-preview==2.0.0.dev20190517</code>)</p>

<p>For the unconditioned binary variables, one can use categorical distribution, such as</p>

<pre class=""lang-py prettyprint-override""><code>from tensorflow_probability import distributions as tfd
from tensorflow_probability import edward2 as ed

difficulty = ed.RandomVariable(
                 tfd.Categorical(
                     probs=[0.6, 0.4],
                     name='difficulty'
                 )
             )
</code></pre>

<p>But how to construct the CPDs?</p>

<p>There are few classes/methods in tensorflow-probability that might be relevant (in <code>tensorflow_probability/python/distributions/deterministic.py</code> or the deprecated <code>ConditionalDistribution</code>) but the documentation is rather sparse (one needs deep understanding of tfp).</p>

<p>--- Updated question ---</p>

<p>Chris' answer is a good starting point. However, things are still a bit unclear even for a very simple two-variable model.</p>

<p>This works nicely:</p>

<pre class=""lang-py prettyprint-override""><code>jdn = tfd.JointDistributionNamed(dict(
    dist_x=tfd.Categorical([0.2, 0.8], validate_args=True),
    dist_y=lambda dist_x: tfd.Bernoulli(probs=tf.gather([0.1, 0.9], indices=dist_x), validate_args=True)
))
print(jdn.sample(10))
</code></pre>

<p>but this one fails</p>

<pre class=""lang-py prettyprint-override""><code>jdn = tfd.JointDistributionNamed(dict(
    dist_x=tfd.Categorical([0.2, 0.8], validate_args=True),
    dist_y=lambda dist_x: tfd.Categorical(probs=tf.gather_nd([[0.1, 0.9], [0.5, 0.5]], indices=[dist_x]))
))
print(jdn.sample(10))
</code></pre>

<p>(I'm trying to model categorical explicitly in the second example just for learning purposes)</p>

<p>-- Update: solved ---</p>

<p>Obviously, the last example wrongly used <code>tf.gather_nd</code> instead of <code>tf.gather</code> as we only wanted to select the first or the second row based on the <code>dist_x</code> outome. This code works now:</p>

<pre class=""lang-py prettyprint-override""><code>jdn = tfd.JointDistributionNamed(dict(
    dist_x=tfd.Categorical([0.2, 0.8], validate_args=True),
    dist_y=lambda dist_x: tfd.Categorical(probs=tf.gather([[0.1, 0.9], [0.5, 0.5]], indices=[dist_x]))
))
print(jdn.sample(10))
</code></pre>
",1
56271551,Tensorflow not utilizing GPU,"<p>I am currently trying to train a chat bot, more specifically <a href=""https://github.com/daniel-kukiela/nmt-chatbot"" rel=""nofollow noreferrer"">this one</a>. But when I start to train the chat bot it utilizes 100% of my CPU and roughly 10% of my GPU. Does someone possibly have an idea why.</p>

<p><a href=""https://i.stack.imgur.com/dreSx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dreSx.png"" alt=""GPU utilization""></a>
<a href=""https://i.stack.imgur.com/i4XLa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i4XLa.png"" alt=""CPU utilization""></a></p>

<p>I have installed <code>tensorflow-gpu</code> and have made sure I have the correct version of CUDA and cuDNN. I have also made sure that I do not have the base <code>tensorflow</code> pip package installed. I also have the latest Nvidia drivers for my GPU. I have also tried uninstalling and re-installing all my drivers, CUDA, cuDNN, tensorflow-gpu and all its dependencies and python itself - which none of it worked. </p>

<p>I can create a python script and include <code>with tf.device('/gpu:0');</code> and create a graph with it without issue, so it is definitely detecting the GPU but just doesn't seem to utilize it.</p>

<p>When running <code>sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))</code> I get an output of the following:</p>

<blockquote>
  <p>2019-05-22 16:47:00.168170: I
  C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137]
  Your CPU supports instructions that this TensorFlow binary was not
  compiled to use: AVX AVX2</p>
  
  <p>2019-05-22 16:47:00.433514: I
  C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1105]
  Found device 0 with properties:</p>
  
  <p>name: GeForce GTX 1060 with Max-Q Design major: 6 minor: 1
  memoryClockRate(GHz): 1.48</p>
  
  <p>pciBusID: 0000:01:00.0</p>
  
  <p>totalMemory: 6.00GiB freeMemory: 4.97GiB</p>
  
  <p>2019-05-22 16:47:00.450094: I
  C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1195]
  Creating TensorFlow device (/device:GPU:0) -> (device: 0, name:
  GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute
  capability: 6.1)</p>
  
  <p>Device mapping:</p>
  
  <p>/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name:
  GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute
  capability: 6.1</p>
  
  <p>2019-05-22 16:47:01.391802: I
  C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\direct_session.cc:297]
  Device mapping:</p>
  
  <p>/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name:
  GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute
  capability: 6.1</p>
</blockquote>
",0
56272954,Using different data types in EagerTensor,"<p>Using the <em>Tensorflow 2.0 alpha</em>, I received the error <code>ValueError: Can't convert Python sequence with mixed types to Tensor</code>, when I was trying to create a <code>tf.data.Dataset</code> using the following data:</p>

<p><a href=""https://i.stack.imgur.com/is5U7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/is5U7.png"" alt=""enter image description here""></a></p>

<p><em><a href=""https://www.kaggle.com/c/titanic/data"" rel=""nofollow noreferrer"">Inspect the complete dataset on Kaggle</a></em></p>

<p>Obviously, there are mixed data types. <code>Sex</code> is a string, <code>Age</code> a float/double, <code>SibSp</code> and <code>Parch</code> are Integers and so on. </p>

<p>My (Python 3) code to transform this <em>Pandas Dataframe</em> into a <code>tf.data.Dataset</code> is based on Tensorflow's tutorial on <a href=""https://www.tensorflow.org/alpha/tutorials/keras/feature_columns"" rel=""nofollow noreferrer"">How to classify structured data</a>, and looks like the following:</p>

<pre><code>def df_to_dataset(dataframe, shuffle=True, batch_size=32):
  dataframe = dataframe.copy()

  # the 'Survived' column is the label (not shown in the image of the Dataframe but exists in the Dataframe)
  label = dataframe.pop('Survived')

  # create the dataset from the dataframe
  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), label))

  # if shuffle == true, randomize the entries
  if shuffle:
    ds = ds.shuffle(buffer_size=len(dataframe))
  ds = ds.batch(batch_size)

  return ds
</code></pre>

<p>As already mentioned above, this function will throw the error <strong><code>ValueError: Can't convert Python sequence with mixed types to Tensor</code></strong> when executing it with, for instance:</p>

<pre><code>train_ds = df_to_dataset(df_train, batch_size=32) 
</code></pre>

<p>(while <code>df_train</code> is the pandas dataframe you can see in the image)</p>

<p>Now I wonder if I am missing something because Tensorflow's tutorial (mentioned above) is using a dataframe with mixed types, as well, but I ran into no errors when trying this example with exactly the same <code>df_to_dataset</code> function.</p>
",0
56283606,Keras Extraction of Informative Features in Text Using Weights,"<p>I am working on a text classification project, and I would like to use <code>keras</code> to rank the importance of each word (token). My intuition is that I should be able to sort weights from the Keras model to rank the words.</p>

<p>Possibly I am having a simple issue using <code>argsort</code> or <code>tf.math.top_k</code>.</p>

<p>The complete code is from <a href=""https://github.com/PacktPublishing/Natural-Language-Processing-with-Python-Cookbook/blob/master/Chapter09/9.2%20Email_Classification.py"" rel=""nofollow noreferrer"">Packt</a></p>

<p>I start by using <code>sklearn</code> to compute TF-IDF using the 10,000 most frequent words.</p>

<pre><code>vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english',
                             max_features=10000, strip_accents='unicode', norm='l2')

x_train_2 = vectorizer.fit_transform(x_train_preprocessed).todense()
x_test_2 = vectorizer.transform(x_test_preprocessed).todense()
</code></pre>

<p>I can view the list of words like this:</p>

<pre><code>print(vectorizer.get_feature_names()[:10])
</code></pre>

<p>I then build and fit a model using Keras. Keras is using the tensorflow backend. </p>

<pre><code># Deep Learning modules
import numpy as np
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.optimizers import Adadelta, Adam, RMSprop
from keras.utils import np_utils

# Definiting hyper parameters
np.random.seed(1337)
nb_classes = 20
batch_size = 64
nb_epochs = 20

Y_train = np_utils.to_categorical(y_train, nb_classes)
model = Sequential()

model.add(Dense(1000, input_shape=(10000,)))
model.add(Activation('relu'))
model.add(Dropout(0.5))

model.add(Dense(500))
model.add(Activation('relu'))
model.add(Dropout(0.5))

model.add(Dense(50))
model.add(Activation('relu'))
model.add(Dropout(0.5))

model.add(Dense(nb_classes))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam')

print(model.summary())

# Model Training
model.fit(x_train_2, Y_train, batch_size=batch_size, epochs=nb_epochs, verbose=1)
</code></pre>

<p>I can then get weights like this:</p>

<pre><code>weight = model.weights[0]
# Returns &lt;tf.Variable 'dense_1/kernel:0' shape=(10000, 1000) dtype=float32_ref&gt;
</code></pre>

<p>Since the number of rows (10,000) is equal to the number of features, I think I am on the right track. I need to get a list of indices I can use to get feature names: <code>informative_features = vectorizer.get_feature_names()[sorted_indices]</code>.</p>

<p>I have tried to build a list using two different techniques:</p>

<ol>
<li><p><code>tf.nn.top_k</code></p>

<pre><code>sorted_indices = tf.nn.top_k(weight)
# Returns TopKV2(values=&lt;tf.Tensor 'TopKV2_2:0' shape=(10000, 1) dtype=float32&gt;, indices=&lt;tf.Tensor 'TopKV2_2:1' shape=(10000, 1) dtype=int32&gt;)
</code></pre>

<p>I have not determined how to get a list from this result.</p></li>
<li><p><code>argsort</code></p>

<pre><code>sorted_indices = model.get_weights()[0].argsort(axis=0)
print(sorted_indices.shape)
# Returns (10000, 1000)
</code></pre>

<p>Function <code>argsort</code> returns a matrix, but what I need is a one-dimensional list.</p></li>
</ol>

<p>How can I use weights to rank text features?</p>
",0
56284927,tf.keras equivalent code block written in tf.contrib.slim,"<p>I'm trying to re-implement a research paper code in tf.keras, in init block it was written as:</p>

<pre><code>with slim.arg_scope([slim.conv2d,separable_conv],activation_fn=tf.nn.relu6, normalizer_fn=slim.batch_norm):
    with slim.arg_scope([slim.batch_norm], is_training=is_training, activation_fn=None):
        with tf.variable_scope(name):
            net = slim.conv2d(inputs, num_outputs=depth, kernel_size=3, stride=2, scope=""conv"") #padding same
</code></pre>

<p>I didn't find a equivalent in tf.keras.layer.Conv2D arguments for normalizer_fn=slim.batch_norm. How to achieve this in keras?</p>

<p>I tried:</p>

<pre><code>model.add(Conv2D(""some arguments"") #0
model.add(BatchNormalization())
</code></pre>

<p>Is this a valid equivalent to the above tf.contrib.slim code. With limited documentation of tf.contrib.slim, I'm really confused.</p>
",1
56286350,tf.keras.metrics.SpecificityAtSensitivity num_thresholds interpretation,"<p>I'm trying to get my head around <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/metrics/SensitivityAtSpecificity"" rel=""nofollow noreferrer"">tf.keras.metrics.SensitivityAtSpecificity</a>. I'm fine with the concept of sensity and specificity in isolation, but I'm unsure how the two are related in this single metric.</p>

<p>More specifically, I'm unsure how to interpret the <code>num_thresholds</code> argument. The example in documentation has <code>num_thresholds=1</code>. Setting <code>num_thresholds</code> greater than 1 with the same input data seems to always return a metric value of 1.0.</p>

<pre class=""lang-py prettyprint-override""><code>def print_metric_value(num_thresholds):
    # other values based on docs example
    m = tf.keras.metrics.SensitivityAtSpecificity(
        0.4, num_thresholds=num_thresholds)
    m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
    print('Result with num_thresholds = %d: %.1f' %
          (num_thresholds, m.result().numpy()))

print_metric_value(1)    # 0.5 - same as docs
print_metric_value(2)    # 1.0
print_metric_value(200)  # 1.0
</code></pre>
",1
56312032,How to use Keras `add_loss` exclusively,"<p>I am implementing a model that has a complicated loss term which requires the Keras <code>add_loss</code> function. I wish to implement my model as a <code>tf.keras.Model</code> (see <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/models/Model"" rel=""nofollow noreferrer"">documentation</a>). Unfortunately, it seems like Keras cannot deal with the case where <code>add_loss</code> is used, but <code>compile</code> does not get a loss function specification.</p>

<p>Here is a minimal example that ""works"" (doesn't do anything interesting, but no errors are raised):</p>

<pre><code>import tensorflow as tf
import numpy as np    

class Model(tf.keras.Model):

    def __init__(self, *args, **kwargs):
        super(Model, self).__init__(*args, **kwargs)    
        self.layer = tf.keras.layers.Dense(3, 
                       activation=tf.keras.activations.relu)

    def call(self, inputs, **kwargs):    
        output = self.layer(inputs)
        return output


if __name__ == '__main__':
    tf.random.set_random_seed(1)
    m = Model()    
    x = np.array([[1., 2., 3.]])
    y = np.array([[0., 1., 0.5]])    
    m.compile(tf.keras.optimizers.SGD(), 
              loss=tf.keras.losses.mean_squared_error)
    m.fit(x, y, epochs=100, verbose=0)
    print(m.predict(x))
    # [[0.         0.99999666 0.5000101 ]]
</code></pre>

<p>However, I don't need the ""external"" loss function, only one specified with <code>add_loss</code>, and this does not work. 
I had hoped that the following code does exactly the same thing as the one above:</p>

<pre><code># Same imports...

class Model(tf.keras.Model):

    # def __init__ ...

    def call(self, inputs, **kwargs):    
        output = self.layer(inputs)
        error = tf.keras.losses.mean_squared_error([0., 1., 0.5], output)
        self.add_loss(error)
        return output


if __name__ == '__main__':    
    tf.random.set_random_seed(1)
    m = Model()    
    x = np.array([[1., 2., 3.]])    
    m.compile(tf.keras.optimizers.SGD())
    m.fit(x, epochs=100, verbose=0)
    print(m.predict(x))
</code></pre>

<p>but it does not work. 
In particular, from <code>m.fit(x)</code> I get the error message</p>

<pre><code>AttributeError: 'Model' object has no attribute 'total_loss'
</code></pre>

<p>One possible workaround is to add a trivial loss function</p>

<pre><code>def no_loss(*args):
    return tf.keras.backend.constant(0.0)
</code></pre>

<p>but I had hoped for a more elegant solution.</p>

<p>I am using Tensorflow 1.13.1. </p>
",0
56344827,"in TF2, how do you save models/weights when not using the tf.keras API?","<p>In the documentation it seems they focus on how to save and restore tf.keras.models, but i was wondering how do you save and restore models trained customly through some basic iteration loop?</p>

<p>Now that there isnt a graph or a session, how do we save structure defined in a tf function that is customly built without using layer abstractions?</p>
",1
56386901,Example for tf. group_by_reducer?,"<p>Can someone show me an example of tf.data.experimental.group_by_reducer? I find the documentation tricky and couldn't understand fully.</p>

<p>How can I use it for calculating average?</p>
",1
56491633,What is the difference between tf.scatter_add and tf.scatter_nd when indices is a matrix?,"<p>Both <a href=""https://www.tensorflow.org/api_docs/python/tf/scatter_add"" rel=""nofollow noreferrer"">tf.scatter_add</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/scatter_nd"" rel=""nofollow noreferrer"">tf.scatter_nd</a> allow <code>indices</code> to be a matrix. It is clear from the documentation of tf.scatter_nd that the last dimension of <code>indices</code> contains values that are used to index a tensor of shape <code>shape</code>. The other dimensions of <code>indices</code> define the number of elements/slices to be scattered. Suppose <code>updates</code> has a rank <code>N</code>. First <code>k</code> dimensions of <code>indices</code> (except the last dimension) should match with first <code>k</code> dimensions of <code>updates</code>.  The last <code>(N-k)</code> dimensions of <code>updates</code> should match with the last <code>(N-k)</code> dimensions of <code>shape</code>.</p>

<p>This implies that <code>tf.scatter_nd</code> can be used to perform an <code>N</code>-dimensional scatter. However, <code>tf.scatter_add</code> also takes matrices as <code>indices</code>. But, its not clear which dimensions of <code>indices</code> correspond to the number of scatters to be performed and how do these dimensions align with <code>updates</code>. Can someone provide a clear explanation possibly with examples?</p>
",1
56506616,Output tensors to a Model must be the output of a TensorFlow `Layer` (thus holding past layer metadata) in Model Api Tensorfow,"<pre><code>def generator_model(self):

        input_images = Input(shape=[64,64,1])
        layer1= Conv2D(self.filter_size,self.kernel_size,(2,2),padding='same',use_bias=False,kernel_initializer='random_uniform')(input_images)
        layer1=LeakyReLU(0.2)(layer1)

        layer2= Conv2D(self.filter_size*2,self.kernel_size,(2,2),padding='same',use_bias=False,kernel_initializer='random_uniform')(layer1)
        layer2=BatchNormalization()(layer2)
        layer2=LeakyReLU(0.2)(layer2)

        layer3=Conv2D(self.filter_size*4,self.kernel_size,(2,2),padding='same',use_bias=False,kernel_initializer='random_uniform')(layer2)
        layer3=BatchNormalization()(layer3)
        layer3=LeakyReLU(0.2)(layer3) 

        layer4=Conv2D(self.filter_size*8,self.kernel_size,(2,2),padding='same',use_bias=False,kernel_initializer='random_uniform')(layer3)
        layer4=BatchNormalization()(layer4)
        layer4=LeakyReLU(0.2)(layer4)  

        layer5=Conv2D(self.filter_size*16,self.kernel_size,(2,2),padding='same',use_bias=False,kernel_initializer='random_uniform')(layer4)
        layer5=BatchNormalization()(layer5)
        layer5=LeakyReLU(0.2)(layer5)            

        up_layer5 = Conv2DTranspose(self.filter_size*8,self.kernel_size,strides = (2,2),padding='same',use_bias=False)(layer5)
        up_layer5=BatchNormalization()(up_layer5)
        up_layer5=LeakyReLU(0.2)(up_layer5)
        #shape = 4*4*512
        up_layer5_concat = tf.concat([up_layer5,layer4],0)

        up_layer6 = Conv2DTranspose(self.filter_size*4,self.kernel_size,strides = (2,2),padding='same',use_bias=False)(up_layer5_concat)
        up_layer6 =BatchNormalization()(up_layer6)
        up_layer6 =LeakyReLU(0.2)(up_layer6)
                    up_layer_6_concat = tf.concat([up_layer6,layer3],0)

        up_layer7 = Conv2DTranspose(self.filter_size*2,self.kernel_size,strides = (2,2),padding='same',use_bias=False)(up_layer_6_concat)
        up_layer7 =BatchNormalization()(up_layer7)
        up_layer7 =LeakyReLU(0.2)(up_layer7)
        up_layer_7_concat = tf.concat([up_layer7,layer2],0)

        up_layer8 = Conv2DTranspose(self.filter_size,self.kernel_size,strides = (2,2),padding='same',use_bias=False)(up_layer_7_concat)
        up_layer8 =BatchNormalization()(up_layer8)
        up_layer8 =LeakyReLU(0.2)(up_layer8)
        up_layer_8_concat = tf.concat([up_layer8,layer1],0)    
        output = Conv2D(3,self.kernel_size,strides = (1,1),padding='same',use_bias=False)(up_layer_8_concat)
        final_output = LeakyReLU(0.2)(output)

        model = Model(input_images,output)
        model.summary()
        return model
</code></pre>

<p>This is how my generator_model looks like, and I have followed a research paper to make the architecture. But, I am in problem with the error. I have checked the other solutions to given problem here in SO, but none of them worked for me as they are little bit different maybe. My guess, the problem is there with the <code>tf.concat()</code> function which should be put as tensorflow keras layer of Lambda, but I tried that too and of no help. Any help regarding this issue? Bugging me for 2 days now. </p>
",0
56553579,How to export Estimator's best model?,"<p>I am training a simple CNN based on a Custom Estimator with TF Records.
I am trying to export the best model in terms of validation loss during the <code>train_and_evaluate</code> phase. </p>

<p>According to the documentation of the <code>tf.estimator.BestExporter</code>, I should feed a function that returns a <code>ServingInputReceiver</code> but after doing so, the <code>train_and_evaluate</code> phase crashes with a <code>NotFoundError: model/m01/eval; No such file or directory</code>.</p>

<p>Seems like if the BestExporter does not permit saving the evaluation results as it would do without the exporter. I tried with different <code>ServingInputReceiver</code> but I keep getting the same error.</p>

<p>As defined <a href=""https://www.tensorflow.org/guide/saved_model#using_savedmodel_with_estimators"" rel=""nofollow noreferrer"">here</a>:</p>

<pre><code>feature_spec = {
        'shape': tf.VarLenFeature(tf.int64),
        'image_raw': tf.FixedLenFeature((), tf.string),
        'label_raw': tf.FixedLenFeature((43), tf.int64)
    }

def serving_input_receiver_fn():
  serialized_tf_example = tf.placeholder(dtype=tf.string,
                                         shape=[120, 120, 3],
                                         name='input_example_tensor')
  receiver_tensors = {'image': serialized_tf_example}
  features = tf.parse_example(serialized_tf_example, feature_spec)
  return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)
</code></pre>

<p>and <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/BestExporter#__init__"" rel=""nofollow noreferrer"">here</a></p>

<pre><code>def serving_input_receiver_fn():
    feature_spec = {
            'image': tf.FixedLenFeature((), tf.string)
        }
    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)
</code></pre>

<p>Here are my exporter and training procedure:</p>

<pre><code>exporter = tf.estimator.BestExporter(
    name=""best_exporter"",
    serving_input_receiver_fn=serving_input_receiver_fn,
    exports_to_keep=5)

train_spec = tf.estimator.TrainSpec(
    input_fn=lambda: imgs_input_fn(train_path, True, epochs, batch_size))

eval_spec = tf.estimator.EvalSpec(
    input_fn=lambda: imgs_input_fn(eval_path, perform_shuffle=False, batch_size=1),
    exporters=exporter)

tf.estimator.train_and_evaluate(ben_classifier, train_spec, eval_spec)
</code></pre>

<p><a href=""https://gist.github.com/hichameyessou/f2710391066f6ed5786693892ac93dbe"" rel=""nofollow noreferrer"">This is a gist</a> with the output.
What's the correct way to define a <code>ServingInputReceiver</code> for the <code>BestExporter</code>?</p>
",1
56573062,how can I set the data type of parameters of Dense layer to float16?,"<p>I want to use Tensorflow <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/dense"" rel=""nofollow noreferrer"">Dense</a> layer with float16 parameters. The default data types of bias and weights are both float32, I tried setting the data type by setting the initializer <code>tf.truncated_normal_initializer(dtype=tf.float16)</code> but it doesn't seem to have any effect.</p>

<pre><code>import tensorflow as tf
A = tf.get_variable(name='foo', shape=[3, 3])
dense = tf.layers.dense(inputs=A, units=3, kernel_initializer=tf.truncated_normal_initializer(dtype=tf.float16))
varis = tf.trainable_variables(scope=None)
print(varis[1])  # &lt;tf.Variable 'dense/kernel:0' shape=(3, 3) dtype=float32_ref&gt;
</code></pre>

<p>How can I use Tensorflow <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/dense"" rel=""nofollow noreferrer"">Dense</a> with float16 parameters?</p>
",0
56606757,Tensorflow: output of multi-step decay function returns a TypeError,"<p>We are trying to write a multi-step decay function in Tensorflow using tf.train.piecewise_constant() as suggested <a href=""https://stackoverflow.com/a/47174243/5079359"">here</a>. Tensorflow documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/train/piecewise_constant_decay"" rel=""nofollow noreferrer"">here</a> states that:</p>

<p>""When eager execution is enabled, this function returns a function which in turn returns the decayed learning rate Tensor""</p>

<p>However, when we tried running the code, it returned a  TypeError. 
It returns the same error even when lr() is used.</p>

<pre><code>import tensorflow as tf
tf.enable_eager_execution()
import numpy as np

def conv3x3(out_planes, data_format ='channels_last',  stride=1, padding='same', dilation=1, name = None,use_bias = False):
    """"""3x3 convolution with padding""""""
    return  tf.keras.layers.Conv2D(filters = out_planes, kernel_size = 3,data_format= data_format,
                                   strides=(stride, stride), padding='same', use_bias=use_bias,
                                   dilation_rate = (dilation,dilation) , kernel_initializer=tf.initializers.he_normal(),name = name)


def conv1x1(out_planes,data_format ='channels_last', padding = 'same', stride=1):
    """"""1x1 convolution""""""
    return tf.keras.layers.Conv2D(filters = out_planes, kernel_size = 1, strides=(stride, stride),data_format= data_format,
                                  padding=padding, use_bias=False, kernel_initializer=tf.initializers.he_normal())

class BasicBlock(tf.keras.Model):
    expansion = 1

    def __init__(self, planes=1, stride=1, data_format= 'channels_last', downsample=None,  dilation=(1, 1), residual=True, key=None, stage = None):
        super(BasicBlock, self).__init__()
        self.data_format = data_format
        bn_axis = 1 if self.data_format == 'channels_first' else 3
        self.conv1 = conv3x3(out_planes= planes, stride = stride, padding='same' ,
                             data_format = self.data_format, dilation=dilation[0], name = '{}_{}_conv0'.format(key,stage))

        self.bn1 = tf.keras.layers.BatchNormalization(axis=bn_axis, name = '{}_{}_BN0'.format(key,stage))

        self.conv2 = conv3x3(out_planes =planes, padding='same',
                             data_format = self.data_format, dilation=dilation[0],name = '{}_{}_conv1'.format(key,stage))

        self.bn2 = tf.keras.layers.BatchNormalization(axis=bn_axis,name = '{}_{}_BN1'.format(key,stage))

        self.downsample = downsample
        self.relu = tf.keras.layers.ReLU(name = '{}_{}_Relu'.format(key,stage))
        self.stride = stride
        self.residual = residual

    def get_config(self):
        base_config = {}
        base_config['conv1'] = self.conv1.get_config()
        base_config['bn1'] = self.bn1.get_config()
        base_config['conv2'] = self.conv2.get_config()
        base_config['bn2'] = self.bn2.get_config()
        if self.downsample is not None:
            base_config['downsample'] = self.downsample.get_config()
        return base_config


    def call(self, inputs, training=None):
        residual = inputs
        out = self.conv1(inputs)
        out = self.bn1(out,training = training)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            residual = self.downsample(inputs)
        if self.residual:
            out += residual
        out = self.relu(out)
        return out


class Bottleneck(tf.keras.Model):
    expansion = 4

    def __init__(self, planes, stride=1, data_format = 'channels_last',downsample=None,dilation=(1, 1)):
        super(Bottleneck, self).__init__()

        bn_axis = 1 if data_format == 'channels_first' else 3
        self.conv1 = conv1x1(planes, data_format = data_format)
        self.bn1 = tf.keras.layers.BatchNormalization(axis=bn_axis)
        self.relu = tf.keras.layers.ReLU()
        self.conv2 = conv3x3(planes, stride, padding= 'same', bias=False,  data_format = data_format, dilation=dilation[1])
        self.bn2 = tf.keras.layers.BatchNormalization(axis=bn_axis)
        self.conv3 =conv1x1( planes * 4, data_format = data_format, )
        self.bn3 =  tf.keras.layers.BatchNormalization(axis=bn_axis) # nn.BatchNorm2d(planes * self.expansion)
        self.downsample = downsample
        self.stride = stride

    def get_config(self):
        base_config = {}
        base_config['conv1'] = self.conv1.get_config()
        base_config['bn1'] = self.bn1.get_config()
        base_config['conv2'] = self.conv2.get_config()
        base_config['bn2'] = self.bn2.get_config()
        base_config['conv3'] = self.conv3.get_config()
        base_config['bn3'] = self.bn3.get_config()
        if self.downsample is not None:
            base_config['downsample'] = self.downsample.get_config()
        return base_config



    def call(self, inputs, training=None):
        identity = inputs
        out = self.conv1(inputs)
        out = self.bn1(out,training = training)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out,training = training)
        out = tf.nn.relu(out)
        out = self.conv3(out)
        out = self.bn3(out,training = training)
        if self.downsample is not None:
            identity = self.downsample(inputs)
        out += identity
        out = self.relu(out)
        return out

class pooling (tf.keras.Model):
    def __init__(self, pool_size, stride = None, data_format='channels_last'):
        super(pooling, self).__init__()
        self.pool_size = pool_size
        self.data_format = data_format
        if stride is None:
            self.stride =self.pool_size
        else:
            self.stride = stride


    def call(self, inputs):
        return tf.layers.average_pooling2d(inputs, strides =self.stride, pool_size = self.pool_size, data_format = self.data_format)


class DRN(tf.keras.Model):
    def __init__(self, block, layers, data_format='channels_last', num_classes=7,channels=(16, 32, 64, 128, 256, 512, 512, 512),
                 out_map=False, out_middle=False, pool_size=28, arch='D'):
        super(DRN, self).__init__()
        self.inplanes = channels[0]
        self.out_map = out_map
        self.out_dim = channels[-1]
        self.out_middle = out_middle
        self.arch = arch
        self.poolsize = pool_size
        self.data_format = data_format
        self.bn_axis = 1 if data_format == 'channels_first' else 3

        self.conv0 = tf.keras.layers.Conv2D(filters=channels[0], kernel_size=7, strides=1,  padding='same',
                                               use_bias=False, data_format = self.data_format, kernel_initializer=tf.initializers.he_normal(), name ='L0_conv0' )
        self.bn0 = tf.keras.layers.BatchNormalization(axis=self.bn_axis,name ='L0_BN0')
        self.relu0 = tf.keras.layers.ReLU(name ='L0_Relu0')


        if arch == 'C':
            self.layer1 = self._make_layer(block = BasicBlock, planes = channels[0], blocks = layers[0], stride=1, data_format = self.data_format, key='CL1')
            self.layer2 = self._make_layer(block = BasicBlock, planes =  channels[1], blocks = layers[1], stride=2, data_format = self.data_format, key='CL2')
        elif arch == 'D':
            self.layer1 = self._make_conv_layers(channels = channels[0],convs = layers[0], stride=1, data_format = self.data_format, key='DL1')
            self.layer2 = self._make_conv_layers(channels = channels[1],convs = layers[1], stride=2, data_format = self.data_format, key='DL2')


        self.layer3 = self._make_layer(block = block, planes = channels[2], blocks = layers[2], stride=2, data_format = self.data_format, key='L3')
        self.layer4 = self._make_layer(block = block, planes = channels[3], blocks = layers[3], stride=2, data_format = self.data_format, key='L4')
        self.layer5 = self._make_layer(block = block, planes = channels[4], blocks = layers[4], dilation=2, new_level=False, data_format = self.data_format, key='L5')
        self.layer6 = None if layers[5] == 0 else self._make_layer(block, channels[5], layers[5], dilation=4, new_level=False, data_format = self.data_format, key='L6')

        if arch == 'C':
            self.layer7 = None if layers[6] == 0 else self._make_layer(BasicBlock, channels[6], layers[6], dilation=2, new_level=False, residual=False, data_format = self.data_format, key='CL7')
            self.layer8 = None if layers[7] == 0 else self._make_layer(BasicBlock, channels[7], layers[7], dilation=1, new_level=False, residual=False, data_format = self.data_format, key='CL8')
        elif arch == 'D':
            self.layer7 = None if layers[6] == 0 else self._make_conv_layers(channels[6], layers[6], dilation=2, data_format = self.data_format, key='DL7')
            self.layer8 = None if layers[7] == 0 else self._make_conv_layers(channels[7], layers[7], dilation=1, data_format = self.data_format, key='DL8')

        if num_classes &gt; 0:
            self.avgpool = tf.keras.layers.GlobalAveragePooling2D(data_format = self.data_format)
            self.fc = tf.keras.layers.Dense(units=num_classes)


    def _make_layer(self, block, planes, blocks, stride=1,dilation=1, new_level=True, data_format = 'channels_last', residual=True, key=None):
        assert dilation == 1 or dilation % 2 == 0
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = tf.keras.Sequential([conv1x1(out_planes = planes * block.expansion,stride = stride, data_format = data_format),
                      tf.keras.layers.BatchNormalization(axis=self.bn_axis)], name = 'downsample')

#
        layers = []
        layers.append(block(planes= planes, stride =  stride, downsample = downsample, dilation=(1, 1) if dilation == 1 else (
                dilation // 2 if new_level else dilation, dilation), data_format=data_format, residual=residual, key = key, stage = '0'))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(planes, residual=residual,dilation=(dilation, dilation), data_format=data_format, key = key, stage = i))
        return tf.keras.Sequential(layers, name = key)


    def _make_conv_layers(self, channels, convs, stride=1, dilation=1 ,data_format = 'channels_last', key = None):
        modules = []
        for i in range(convs):
            modules.extend([
                conv3x3(out_planes= channels, stride=stride if i == 0 else 1,
                          padding= 'same' , use_bias=False, dilation=dilation,  data_format = data_format,name ='{}_{}_Conv'.format(key,i)),
                tf.keras.layers.BatchNormalization(axis=self.bn_axis,name ='{}_{}_BN'.format(key,i)),
                tf.keras.layers.ReLU(name ='{}_{}_Relu'.format(key,i))])
            self.inplanes = channels
        return tf.keras.Sequential(modules,name=key)


    def call(self, x, training=None):
        x = self.conv0(x)
        x = self.bn0(x,training = training)
        x = self.relu0(x)
        x = self.layer1(x,training = training)
        x = self.layer2(x,training = training)
        x = self.layer3(x,training = training)
        x = self.layer4(x,training = training)
        x = self.layer5(x,training = training)

        if self.layer6 is not None:
            x = self.layer6(x,training = training)

        if self.layer7 is not None:
            x = self.layer7(x)
        if self.layer8 is not None:
            x = self.layer8(x)
        if self.out_map:
            x = self.fc(x)
        else:
            x = self.avgpool(x)
            x = self.fc(x)
        return x

def loss(logits, labels):
  return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))

def make_scheduler(policy, init_lr, n_step_epoch, global_step):
    total_steps= n_step_epoch * 10 #10 epochs
    milestones = policy.split('_')
    milestones.pop(0)
    milestones = list(map(lambda x: int(x), milestones))
    boundaries = np.multiply(milestones,n_step_epoch)
    values = [init_lr] + [init_lr/(0.1**-i) for i in  range(1,len(milestones)+1)]
    learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)
    return learning_rate


def train(model, optimizer, step_counter ):
  """"""Trains model on `dataset` using `optimizer`.""""""

  for (batch, i) in enumerate(range(10)):
      print('Training Loop {}'.format(i))
      images = tf.random.uniform((4, 224, 224,3))
      labels = tf.constant(np.random.randint(4, size=4))
      with tf.contrib.summary.record_summaries_every_n_global_steps(10, global_step=step_counter):
          with tf.GradientTape() as tape:
            logits = model(images, training=True)
            loss_value = loss(logits, labels)
          grads = tape.gradient(loss_value, model.variables)
          optimizer.apply_gradients(zip(grads, model.variables), global_step=step_counter)


def test(model):
  """"""Perform an evaluation of `model` on the examples from `dataset`.""""""
  for  i in (range(10)):
    images = tf.random.uniform((4, 225, 225,3))
    logits = model(images, training=False)
    print(logits)

def main():
    model =  DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 1, 1], arch='C',num_classes = 4)
    device = '/gpu:0'
    step_counter = tf.train.get_or_create_global_step()
    lr = make_scheduler(policy='multistep_2_5',init_lr=0.1,n_step_epoch = 10,global_step= step_counter)
    optimizer = tf.train.MomentumOptimizer(lr,momentum=0.5)

    with tf.device(device):
        for _ in range(10):
           train(model, optimizer,step_counter)
           print(optimizer._lr_t)
           test(model)

if __name__ == '__main__':
  main()

</code></pre>

<blockquote>
  <p>File """", line 1, in 
      runfile('/home/srijith/work/Tensorflow/SkinCaner_tensorflow/debug/stackoverflow.py', wdir='/home/srijith/work/Tensorflow/SkinCaner_tensorflow/debug')</p>
  
  <p>File ""/home/srijith/anaconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py"", line 709, in runfile
      execfile(filename, namespace)</p>
  
  <p>File ""/home/srijith/anaconda3/lib/python3.7/site-packages/spyder_kernels/customize/spydercustomize.py"", line 108, in execfile
      exec(compile(f.read(), filename, 'exec'), namespace)</p>
  
  <p>File ""/home/srijith/work/Tensorflow/SkinCaner_tensorflow/debug/stackoverflow.py"", line 311, in 
      main()</p>
  
  <p>File ""/home/srijith/work/Tensorflow/SkinCaner_tensorflow/debug/stackoverflow.py"", line 305, in main
      train(model, optimizer,step_counter)</p>
  
  <p>File ""/home/srijith/work/Tensorflow/SkinCaner_tensorflow/debug/stackoverflow.py"", line 284, in train
      optimizer.apply_gradients(zip(grads, model.variables), global_step=step_counter)</p>
  
  <p>File ""/home/srijith/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/optimizer.py"", line 598, in apply_gradients
      self._prepare()</p>
  
  <p>File ""/home/srijith/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/momentum.py"", line 87, in _prepare
      learning_rate = learning_rate()</p>
  
  <p>File ""/home/srijith/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/learning_rate_decay_v2.py"", line 171, in decayed_lr
      boundaries = ops.convert_n_to_tensor(boundaries)</p>
  
  <p>File ""/home/srijith/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1273, in convert_n_to_tensor
      as_ref=False)</p>
  
  <p>File ""/home/srijith/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 1228, in internal_convert_n_to_tensor
      raise TypeError(""values must be a list."")</p>
  
  <p>TypeError: values must be a list.</p>
</blockquote>

<p>The code works as expected when we provide a constant learning rate. Is there something that we are missing?</p>
",1
56635027,Feeding array (shape with rank 1) to TensorFlow tf.case,"<p>Following this example from the <code>tf.case</code> documentation:</p>

<pre><code>def f1(): return tf.constant(17)
def f2(): return tf.constant(23)
def f3(): return tf.constant(-1)
r = tf.case({tf.less(x, y): f1, tf.greater(x, z): f2},
            default=f3, exclusive=True)
</code></pre>

<p>I want to do the same, but allow to use a feed_dict as input, illustrated by this snipped:</p>

<pre><code>x = tf.placeholder(tf.float32, shape=[None])
y = tf.placeholder(tf.float32, shape=[None])
z = tf.placeholder(tf.float32, shape=[None])
def f1(): return tf.constant(17)
def f2(): return tf.constant(23)
def f3(): return tf.constant(-1)
r = tf.case({tf.less(x, y): f1, tf.greater(x, z): f2},
            default=f3, exclusive=True)
print(sess.run(r, feed_dict={x: [0, 1, 2, 3], y: [1, 1, 1, 1], z: [2, 2, 2, 2]}))
# result should be [17, -1, -1, 23]
</code></pre>

<p>So, basically I want to feed three <code>int</code>-arrays of equal length and receive an array of <code>int</code>-values containing either 17, 23, or -1. Unfortunately, there code above gives and error:</p>

<blockquote>
  <p>ValueError: Shape must be rank 0 but is rank 1 for 'case/cond/Switch' (op: 'Switch') with input shapes: [?], [?].</p>
</blockquote>

<p>I understand, that <code>tf.case</code> requires boolean scalar tensor input values but is there any way to achieve what I want? I also tried <code>tf.cond</code> without success.</p>
",1
56639621,how to use tensorflow tf.losses.softmax_cross_entropy?,"<p>I am doing some semantic segmentation problem and need to define loss function.</p>

<p>Does any one know how to use tensorflow ""tf.losses.softmax_cross_entropy""?</p>

<p>It is said in the documentation that the first input of the function is 
onehot_labels, so do we need to first transfer the pixel-wise class label into one hot encode format and input one hot encode into this function?</p>

<p>Or we can directly input the pixel class label like tf.losses.sigmoid_cross_entropy in this post <a href=""https://stackoverflow.com/questions/52046971/sigmoid-cross-entropy-loss-function-from-tensorflow-for-image-segmentation"">sigmoid_cross_entropy loss function from tensorflow for image segmentation</a>?</p>

<p>Thank you so much!</p>
",1
56655403,How to use a remote machine's GPU in jupyter notebook,"<p>I am trying to run tensorflow on a remote machine's GPU through Jupyter notebook. However, if I print the available devices using tf, I only get CPUs. I have never used a GPU before and am relatively new at using conda / jupyter notebook remotely as well, so I am not sure how to set up using the GPU in jupyter notebook. </p>

<p>I am using an environment set up by someone else who already executed the same code on the same GPU, but they did it via python script, not in a jupyter notebook. </p>

this is the only code in the other person's file that had to do with the GPU

<p>config = tf.ConfigProto()</p>

<p>config.gpu_options.allow_growth=True</p>

<p>set_session(tf.Session(config=config))</p>
",0
56671159,What does tf.get_operations actually return?,"<p><em>""<a href=""https://www.tensorflow.org/api_docs/python/tf/Graph#get_operations"" rel=""nofollow noreferrer"">tf.get_operations</a> return the list of operations in the graph.</em></p>

<p><em>You can modify the operations in place, but modifications to the list such as inserts/delete have no effect on the list of operations known to the graph.
This method may be called concurrently from multiple threads.</em></p>

<p><em>Returns:
A list of Operations.""</em></p>

<p>Can anyone give me an idiots guide to what this <em>actually</em> means please? </p>

<p>I know tf.Operations is an object which produces an output of a tensor, given an input of a tensor. I am guessing it reads these operation from the tensor graph then? </p>

<p>Printing the variable returned by <code>ops = get_operations()</code> produces a whole bunch of text like so, is this all from the frozen graph?</p>

<pre><code> &lt;tf.Operation 'FirstStageFeatureExtractor/InceptionV2/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/Conv2D' type=Conv2D&gt;, &lt;tf.Operation 'FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/gamma' type=Const&gt;
</code></pre>

<p>Thanks.</p>

<p>Bonus : </p>

<pre><code>all_tensor_names = {output.name for op in ops for output in op.outputs}
</code></pre>

<p>How does this work?</p>
",0
56672331,Plotting a Tensor in Python,"<p>I am following the tutorial from <a href=""https://www.tensorflow.org/beta/tutorials/generative/dcgan"" rel=""nofollow noreferrer"">https://www.tensorflow.org/beta/tutorials/generative/dcgan</a></p>

<p>I want to be able to see the image that is being generated using plt.imshow() but for some reason the line </p>

<pre><code>generator = make_generator_model()

noise = tf.random.normal([1, 100])
generated_image = generator(noise, training=False)
#type = tensorflow.python.framework.ops.Tensor

plt.imshow(generated_image[0, :, :, 0], cmap='gray')
</code></pre>

<p>doesn't work for me and I get an error :</p>

<pre><code>TypeError: Image data cannot be converted to float
</code></pre>

<p>I followed a few threads on StackOverflow and tried to cast the Tensor using tf.cast, but even that didn't help.</p>

<p>The model as on the website is different from my code (only slightly)</p>

<pre><code>def make_generator_model():
    model = Sequential()
    model.add(Dense(9*9*256, use_bias=False, input_shape=(100,)))
#     model.add(BatchNormalization())
    model.add(LeakyReLU())

    model.add(Reshape((9, 9, 256)))
    assert model.output_shape == (None, 9, 9, 256) # Note: None is the batch size

    model.add(Conv2DTranspose(128, (3, 3), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 9, 9, 128)
#     model.add(BatchNormalization())
    model.add(LeakyReLU())

    model.add(Conv2DTranspose(64, (3,3), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 9, 9, 64)
#     model.add(BatchNormalization())
    model.add(LeakyReLU())

    model.add(Conv2DTranspose(1, (3,3), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 9,9,1)

    return model
</code></pre>
",0
56680233,TF gradient returning zero for simple problem,"<p>In TF 2.0, I traced back a bug and isolated to this simple problem:
tf.gradient is giving zero gradient, it should be infinity.</p>

<pre><code>bxe = tf.keras.losses.BinaryCrossentropy(); 
a=tf.Variable(1.0) 
with tf.GradientTape() as tape: 
   loss = bxe([0.], a) 
grads = tape.gradient(loss, a) # compute gradients 
print(loss, grads) 

tf.Tensor(15.333239, shape=(), dtype=float32) 
tf.Tensor(0.0, shape=(), dtype=float32)
</code></pre>

<p>As you can see, the loss should be infinite cause -log(1-1) in the binary cross entropy. However, I looked up documentation and I noticed that they clip the numbers before applying the log, that's why 15.333 show up instead of infinity. That's great, and help avoiding many troubles during training, <strong>BUT</strong>, what about the gradient? It is producing zero! 
According to my math it should be infinite, so, at least it should be producing something very high, rather than zero. As such, my training is stuck! Why is that happening? How did I even arrive at this point? and how come that this doesn't happen when I use Keras high level built classifier models?</p>
",0
56692251,tf.estimator.BoostedTreesRegressor SavedModel Restore Issue,"<p>I am having an issue restoring a tf.estimator.BoostedTreesRegressor model using tf.SavedModel. When reloading the model from the saved model directory using tf.contrib.predictor.from_saved_model() I receive the following error:</p>

<blockquote>
  <p>KeyError: ""The name 'boosted_trees/QuantileAccumulator/' refers to an
  Operation not in the graph.""</p>
</blockquote>

<p>This error only occurs when using numeric features (e.g. tf.feature_column.numeric_column). Reloading the model works fine when using only categorical columns</p>

<p>When I'm not saving/restoring, BoostedTreesRegressor evaluates and predicts successfully with all features. </p>

<p>The following estimator save/restore scenarios have worked successfully:<br>
- DNNRegressor with numeric and categorical features<br>
- LinearRegressor with numeric and categorical features<br>
- BoostedTreeRegressor with just categorical features  </p>

<pre><code>fc = tf.feature_column
feature_columns = [
fc.numeric_column('f1', dtype=tf.int64),
fc.numeric_column('f2', dtype=tf.int64),
fc.indicator_column(
               fc.categorical_column_with_vocabulary_list('f3',f3)),
fc.indicator_column(
               fc.categorical_column_with_vocabulary_list('f4',f4))
]

feature_spec = fc.make_parse_example_spec(feature_columns)

params = {
    'feature_columns' : feature_columns,
    'n_batches_per_layer' : n_batches,
    'n_trees': 200,
    'max_depth': 6,
    'learning_rate': 0.01
}

regressor = tf.estimator.BoostedTreesRegressor(**params)
regressor.train(train_input_fn, max_steps=400)

serving_input_receiver_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)

regressor.export_saved_model('saved_model', serving_input_receiver_fn)

.
.
.
# latest is path to saved model
predict_fn = predictor.from_saved_model(latest[:-4])
</code></pre>

<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-101-ee20beae4424&gt; in &lt;module&gt;
----&gt; 1 predict_fn = predictor.from_saved_model(latest[:-4])
/usr/local/anaconda3/envs/zume/lib/python3.7/site-packages/tensorflow/contrib/predictor/predictor_factories.py in from_saved_model(export_dir, signature_def_key, signature_def, input_names, output_names, tags, graph, config)
    151       tags=tags,
    152       graph=graph,
--&gt; 153       config=config)
/usr/local/anaconda3/envs/zume/lib/python3.7/site-packages/tensorflow/contrib/predictor/saved_model_predictor.py in __init__(self, export_dir, signature_def_key, signature_def, input_names, output_names, tags, graph, config)
    151     with self._graph.as_default():
    152       self._session = session.Session(config=config)
--&gt; 153       loader.load(self._session, tags.split(','), export_dir)
    154 
    155     if input_names is None:
/usr/local/anaconda3/envs/zume/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    322               'in a future version' if date is None else ('after %s' % date),
    323               instructions)
--&gt; 324       return func(*args, **kwargs)
    325     return tf_decorator.make_decorator(
    326         func, new_func, 'deprecated',
/usr/local/anaconda3/envs/zume/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py in load(sess, tags, export_dir, import_scope, **saver_kwargs)
    267   """"""
    268   loader = SavedModelLoader(export_dir)
--&gt; 269   return loader.load(sess, tags, import_scope, **saver_kwargs)
    270 
    271 
/usr/local/anaconda3/envs/zume/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py in load(self, sess, tags, import_scope, **saver_kwargs)
    418     with sess.graph.as_default():
    419       saver, _ = self.load_graph(sess.graph, tags, import_scope,
--&gt; 420                                  **saver_kwargs)
    421       self.restore_variables(sess, saver, import_scope)
    422       self.run_init_ops(sess, tags, import_scope)
/usr/local/anaconda3/envs/zume/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py in load_graph(self, graph, tags, import_scope, **saver_kwargs)
    348     with graph.as_default():
    349       return tf_saver._import_meta_graph_with_return_elements(  # pylint: disable=protected-access
--&gt; 350           meta_graph_def, import_scope=import_scope, **saver_kwargs)
    351 
    352   def restore_variables(self, sess, saver, import_scope=None):
/usr/local/anaconda3/envs/zume/lib/python3.7/site-packages/tensorflow/python/training/saver.py in _import_meta_graph_with_return_elements(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)
   1455           import_scope=import_scope,
   1456           return_elements=return_elements,
-&gt; 1457           **kwargs))
   1458 
   1459   saver = _create_saver_from_imported_meta_graph(
/usr/local/anaconda3/envs/zume/lib/python3.7/site-packages/tensorflow/python/framework/meta_graph.py in import_scoped_meta_graph_with_return_elements(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate, return_elements)
    850           for value in field.value:
    851             col_op = graph.as_graph_element(
--&gt; 852                 ops.prepend_name_scope(value, scope_to_prepend_to_names))
    853             graph.add_to_collection(key, col_op)
    854         elif kind == ""int64_list"":
/usr/local/anaconda3/envs/zume/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in as_graph_element(self, obj, allow_tensor, allow_operation)
   3476 
   3477     with self._lock:
-&gt; 3478       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
   3479 
   3480   def _as_graph_element_locked(self, obj, allow_tensor, allow_operation):
/usr/local/anaconda3/envs/zume/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _as_graph_element_locked(self, obj, allow_tensor, allow_operation)
   3536         if name not in self._nodes_by_name:
   3537           raise KeyError(""The name %s refers to an Operation not in the ""
-&gt; 3538                          ""graph."" % repr(name))
   3539         return self._nodes_by_name[name]
   3540 
KeyError: ""The name 'boosted_trees/QuantileAccumulator/' refers to an Operation not in the graph.""
</code></pre>
",0
56739714,Tensorflow contrib.summary API - recording scalars every n-th step does not work properly,"<p>Recently I have started playing out with TensorBoard. Firstly, I've just wanted to do a simple visualization of the loss function over a few hundred steps. For that, I've wanted to use the <code>tf.contrib.summary</code> API. </p>

<p>My code works except for a slight annoyance - let's say that I want to perform 250 steps of optimizer and I want to record the loss on each of these steps, so, I will do something like this (some chunks of code are missing).</p>

<pre class=""lang-py prettyprint-override""><code>graph = tf.Graph()
sess = tf.Session(graph=graph)

with sess.graph.as_default():
    ... # lines that define the computation graph as well as input dataset and predictions
    global_step = tf.train.create_global_step()

    rmse = tf.math.sqrt(tf.losses.mean_squared_error(labels=Y, predictions=Y_PRED))
    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(rmse, global_step=global_step)

    # create summary writer, tensor for recording scalar and initialize everything
    summary_writer = tf.contrib.summary.create_file_writer(args.logdir, flush_millis=10 * 1000)
    summaries = {}

    with summary_writer.as_default(), tf.contrib.summary.always_record_summaries():
        summaries[""train_rmse""] = tf.contrib.summary.scalar(""train/RMSE"", rmse)

    sess.run(tf.global_variables_initializer())
    with summary_writer.as_default():
        tf.contrib.summary.initialize(session=sess, graph=graph)

for i in range(250):
    train_X_batch, train_Y_batch = # ... retrieve batch of data from dataset
    sess.run(optimizer, feed_dict={X : train_X_batch, Y : train_Y_batch})
    sess.run(summaries[""train_rmse""], {X: train_X, Y: train_Y})
</code></pre>

<p>But when I do this, and then visualize results in tensorboard, my <code>train_rmse</code> was recorded only 241 times instead of 250 times as I've used the <code>tf.contrib.summary.always_record_summaries()</code>, right? (See the image). </p>

<p><a href=""https://i.stack.imgur.com/c8z40.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c8z40.png"" alt=""enter image description here""></a></p>

<p>This issue seems to be data dependent. When I try similiar thing on the mnist dataset and try to record some scalars for the same amount of steps, the number of recorded steps was something like 200.</p>

<p>I've tried to find the answer in the tensorflow documentation but without success. I've also checked things like not having enough data for the 250 steps - this should not be an issue.</p>

<p>One more thing is that this happens even when I use the <code>record_summaries_every_n_global_steps(n)</code> call. For example, calling it with <code>n = 5</code> records steps only up to the 215th step.</p>

<p>Could anyone help me with this please? </p>
",1
56777704,"how to fix ""There is at least 1 reference to internal data in the interpreter in the form of a numpy array or slice"" and run inference on tf.lite","<p>I'm trying to run inference using tf.lite on an mnist keras model that I optimized by doing post-training-quantization according to <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_quant.ipynb"" rel=""noreferrer"">this</a></p>

<pre><code>RuntimeError: There is at least 1 reference to internal data
in the interpreter in the form of a numpy array or slice. Be sure to
only hold the function returned from tensor() if you are using raw
data access.
</code></pre>

<p>It happens after I resize either the images to be in 4 dimensions, or the interpreter itself as seen in the commented line; since the error before this was something like ""expected 4 dimensions but found 3"". Here is the code:</p>

<pre><code>import tensorflow as tf
tf.enable_eager_execution()
import numpy as np
from tensorflow.keras.datasets import mnist
import matplotlib.pyplot as plt
%matplotlib inline

mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()
images, labels = tf.cast(mnist_test[0], tf.float32)/255.0, mnist_test[1]
images = np.reshape(images,[images.shape[0],images.shape[1],images.shape[2],1])
mnist_ds = tf.data.Dataset.from_tensor_slices((images, labels)).batch(1, drop_remainder = True)

interpreter = tf.lite.Interpreter(model_path=""C:\\Users\\USER\\Documents\\python\\converted_quant_model_cnn_5_100.tflite"")
#tf.lite.Interpreter.resize_tensor_input(interpreter, input_index=""index"" , tensor_size=([1,28,28,1]) )

interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
input_index = interpreter.get_input_details()[0][""index""]
output_index = interpreter.get_output_details()[0][""index""]

for img, label in mnist_ds.take(1):
  break
#print(img.get_shape)
interpreter.set_tensor(input_index, img)
interpreter.invoke()
predictions = interpreter.get_tensor(output_index)
</code></pre>
",0
56795623,How to change/insert the shape of/into BatchDataset?,"<p>I'm replicating deep voice 3 paper from a github repo with TF 2.0.
[<a href=""https://github.com/Kyubyong/deepvoice3]"" rel=""nofollow noreferrer"">https://github.com/Kyubyong/deepvoice3]</a>
I have done all preprocessing.
I have also created from tensor (shape is (13066,)) to TensorSliceDataset (shape is (None,)), mapped some functions and already batched it.
Now, I have BatchDataset which shape is (None, None).
<strong>I have no idea how to change that (None, None) shape into desire shape (16, 180)?</strong></p>

<p>The repo is implemented with TF 1.3.
I am using TF 2.0.
When the old code is tf.train.batch and I head to the tensorflow website and found to use tf.data.Dataset in TF 2.0.
But, it doesn't have an option to shape the dataset.
Below is TF 1.3 code.</p>

<pre><code># TF old version
texts = tf.train.batch([text], shapes=[(hp.Tx,)],
                               num_threads=32,
                               batch_size=hp.batch_size,
                               capacity=hp.batch_size*32,   
                               dynamic_pad=False) # (16, 180)
# TF 2.0
texts = texts.batch(hp.batch_size) # (None, None)
</code></pre>

<p>The shape of BatchDataset before applying shape is (None, None).
The shape of BatchDataset after applying shape should be (16, 180).</p>

<p>Thank you!</p>
",0
56865487,tf.lookup.StaticVocabularyTable with num_oov_buckets doesn't work in TF Serving,"<p>I have created a TF model that uses <code>tf.lookup.StaticVocabularyTable</code> for creating a vocab map inside TF Graph. It reads mapping from a text file and assigns <code>num_oov_buckets=500</code>. Below is a part of the code -    </p>

<pre class=""lang-py prettyprint-override""><code>num_oov_buckets = 500
table_init = tf.lookup.TextFileInitializer('resmap.txt', tf.int64, 0, tf.int64, 1, delimiter="" "")
table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)
</code></pre>

<p>Using this it runs fine at the time of training and prediction.<br>
I convert this TF model into a Tensorflow serving using below code -   </p>

<pre class=""lang-py prettyprint-override""><code>from model import ModelWDN

with tf.Session() as sess:

    tf.app.flags.DEFINE_string('f', '', 'kernel')
    tf.app.flags.DEFINE_integer('model_version', 1, 'version number of the model.')
    tf.app.flags.DEFINE_string('save_dir', '/home/abhilash', 'Saving directory.')
    FLAGS = tf.app.flags.FLAGS

    export_path = os.path.join(tf.compat.as_bytes(FLAGS.save_dir), tf.compat.as_bytes(str(FLAGS.model_version)))
    print('Exporting trained model to', export_path)

    # Creating Model object and initializing all the global variables in TF Graph.
    model = ModelWDN(res_count=21663)
    sess.run(tf.global_variables_initializer())
    sess.run(tf.local_variables_initializer())
    sess.run(tf.tables_initializer())

    tf.train.Saver().restore(sess, os.path.join('/home/abhilash', 'wdn'))
    print(""Model restored."")

    # SavedModel Builder Object
    builder = tf.saved_model.builder.SavedModelBuilder(export_path)

    # Converting Tensor to TensorInfo Objects so that they can be used in SignatureDefs
    tensor_info_click_hist_str = tf.saved_model.utils.build_tensor_info(model.click_hist_str)
    tensor_info_res_to_predict_str = tf.saved_model.utils.build_tensor_info(model.res_to_predict_str)
    tensor_info_prob = tf.saved_model.utils.build_tensor_info(model.logits_all)

    # SignatureDef
    prediction_signature = (
          tf.saved_model.signature_def_utils.build_signature_def(
              inputs={'click_hist_str':tensor_info_click_hist_str,
                      'res_to_predict_str':tensor_info_res_to_predict_str},
              outputs={'probs': tensor_info_prob},
              method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))

    builder.add_meta_graph_and_variables(
                sess=sess,
                tags=[tf.saved_model.tag_constants.SERVING],
                signature_def_map={'predict_ad_view_prob': prediction_signature},
                main_op=tf.tables_initializer(), 
                strip_default_attrs=False,
                )

    # Export the model
    builder.save()
    print('Done exporting TF Model to SavedModel format!')
</code></pre>

<p>It is converted without any error and gives correct prediction when I provide any value that exists in the <code>resmap.txt</code> which I gave while defining <code>tf.lookup.TextFileInitializer</code>. Any value which doesn't exist in this map gives an error in serving when doing curl request but doesn't give any error otherwise (i.e when predicting from TF model inside session). 
Curl request - </p>

<pre><code>curl -X POST http://localhost:8501/v1/models/1:predict -d '{""signature_name"": ""predict_ad_view_prob"", ""inputs"":{""res_to_predict_str"": [""9 18788418 19039855 18771619""], ""click_hist_str"": [""18198449 18656271 18198449""]}}'
</code></pre>

<p>Here <code>9</code> is the id that is not present in the <code>resmap.txt</code></p>

<p>Below is the error I get when doing a curl request - </p>

<pre><code>{ ""error"": ""indices[0] = 21748 is not in [0, 21663)\n\t [[{{node GatherV2_5}}]]"" }
</code></pre>

<p><code>resmap.txt</code> has 21663 key-values and <code>num_oov_buckets</code> is set to be 500.    </p>

<p>Same input while predicting inside TF session gives correct result - </p>

<pre><code>[[0.10621755 0.50749264 0.08582641 0.00173556]]
</code></pre>

<p>So clearly there is some problem with <code>num_oov_buckets</code> &amp; graph having this are not correctly implemented in serving or if I am missing something/incorrectly building <code>TF SavedModel</code> then let me know.</p>

<p><strong>UPDATE - Adding saved_model_cli show and run commands</strong>      </p>

<pre><code>saved_model_cli show --dir 1 --all
</code></pre>

<pre><code>MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['predict_ad_view_prob']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['click_hist_str'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: Placeholder_3:0
    inputs['res_to_predict_str'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: Placeholder_5:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['probs'] tensor_info:
        dtype: DT_DOUBLE
        shape: (-1, -1)
        name: Sigmoid:0
  Method name is: tensorflow/serving/predict
</code></pre>

<pre><code>saved_model_cli run --dir 1 --tag_set serve --signature_def predict_ad_view_prob --input_exprs 'click_hist_str=[""50 50""];res_to_predict_str=[""50 303960 1 2""]'
</code></pre>

<pre><code>2019-07-18 10:18:54.805220: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2019-07-18 10:18:54.810121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-18 10:18:54.811041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
2019-07-18 10:18:54.811492: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-18 10:18:54.813643: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-18 10:18:54.815415: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-18 10:18:54.815914: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-18 10:18:54.818528: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-18 10:18:54.820856: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-18 10:18:54.826085: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-18 10:18:54.826234: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-18 10:18:54.827152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-18 10:18:54.827807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-18 10:18:54.828138: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-07-18 10:18:54.856561: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300065000 Hz
2019-07-18 10:18:54.857004: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5635e1749450 executing computations on platform Host. Devices:
2019-07-18 10:18:54.857037: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;
2019-07-18 10:18:54.984822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-18 10:18:54.985784: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5635e36188b0 executing computations on platform CUDA. Devices:
2019-07-18 10:18:54.985823: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7
2019-07-18 10:18:54.986072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-18 10:18:54.987021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
pciBusID: 0000:00:1e.0
2019-07-18 10:18:54.987099: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-18 10:18:54.987152: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
2019-07-18 10:18:54.987202: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0
2019-07-18 10:18:54.987250: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0
2019-07-18 10:18:54.987300: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0
2019-07-18 10:18:54.987362: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0
2019-07-18 10:18:54.987413: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2019-07-18 10:18:54.987554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-18 10:18:54.988526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-18 10:18:54.989347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2019-07-18 10:18:54.989418: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0
2019-07-18 10:18:54.995160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-07-18 10:18:54.995475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2019-07-18 10:18:54.995629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
2019-07-18 10:18:54.995938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-18 10:18:54.996963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-07-18 10:18:54.997884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8895 MB memory) -&gt; physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
WARNING: Logging before flag parsing goes to stderr.
W0718 10:18:54.999173 140274532570944 deprecation.py:323] From /home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/tools/saved_model_cli.py:339: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
W0718 10:18:55.271977 140274532570944 deprecation.py:323] From /home/ubuntu/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2019-07-18 10:18:56.953677: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2019-07-18 10:18:56.979903: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0
Result for output key probs:
[[0.14920072 0.07349582 0.12342736 0.12342736]]
</code></pre>
",0
56879198,Tensorflow Transform: How to find the mean of a variable over the entire dataset,"<p>I often see in many Tensorflow <a href=""https://www.tensorflow.org/beta/tutorials/load_data/csv"" rel=""noreferrer"">tutorials</a> text like:</p>

<blockquote>
  <p>To do this calculation, you need the column means. You would obviously
  need to compute these in real life, but for this example we'll just
  provide them.</p>
</blockquote>

<p>For small or medium sized CSV datasets computing the mean is as easy as a <code>pandas</code> method on a dataframe or using `scikit-learn</p>

<p>BUT, if we have large dataset, say a CSV file that is 50GB, then how do you calculate the mean or other similar statistics. <code>Tensorflow Transform</code> claims that it can calculate global summary statistics, but they don't really explain how this work or how to integrate this into a workflow.</p>

<p>Here is the code example from their <a href=""https://www.tensorflow.org/tfx/transform/get_started"" rel=""noreferrer"">getting started guide</a>.</p>

<pre><code>import tensorflow as tf
import tensorflow_transform as tft

def preprocessing_fn(inputs):
  x = inputs['x']
  y = inputs['y']
  s = inputs['s']
  x_centered = x - tft.mean(x)
  y_normalized = tft.scale_to_0_1(y)
  s_integerized = tft.compute_and_apply_vocabulary(s)
  x_centered_times_y_normalized = x_centered * y_normalized
  return {
      'x_centered': x_centered,
      'y_normalized': y_normalized,
      'x_centered_times_y_normalized': x_centered_times_y_normalized,
      's_integerized': s_integerized
  }
</code></pre>

<p>The documentation says that this code will run <code>tft.mean(x)</code> over the entire dataset, but it is not clear how that will happen since <code>x</code> is limited to just the scope of the batch? Yet here is the claim in the documentation. </p>

<blockquote>
  <p>While not obvious in the example above, the user defined preprocessing
  function is passed tensors representing batches and not individual
  instances, as happens during training and serving with TensorFlow. On
  the other hand, analyzers perform a computation over the entire
  dataset that returns a single value and not a batch of values. x is a
  Tensor with a shape of (batch_size,), while tft.mean(x) is a Tensor
  with a shape of ().</p>
</blockquote>

<p>So the questions are </p>

<ol>
<li><p>Does <code>tft.mean()</code> run over the entire dataset first, and only after computing the global mean does it begin to load batches?</p></li>
<li><p>Are there any more detailed or complete examples of using <code>tft.transforms</code> in a workflow? Like can these tranforms be included in a single batch <code>preprocessing</code> function on a <code>tf.data.Dataset.map()</code> call, or how?</p></li>
</ol>

<p>So if I was trying to write some code to calculate the average <code>age</code> of individuals in my tensorflow dataset. Here is the code I have so far. Is this the best way to do something like this, or is there a better way? </p>

<p>I used the tensorflow-2.0 <code>make_csv_dataset()</code> which takes care of stacking the examples from the CSV file into a column structure. Note I took the code for the <code>make_csv_dataset()</code> from the new tutorial on the tensorflow website referenced in the link above. </p>

<pre><code>  dataset = tf.data.experimental.make_csv_dataset(
      file_path,
      batch_size=32, 
      label_name=LABEL_COLUMN,
      na_value=""?"",
      num_epochs=1,
      ignore_errors=True)

 ds_iter = dataset.make_one_shot_iterator()

 list_of_batch_means = []

 for ex_features, ex_labels in ds_iter:
    batch_length = len(ex_features)
    batch_sum = tf.reduce_sum(ex_features['age'])
    list_of_batch_means.append(batch_sum/len(ex_features)

 average_age = np.mean(list_of_batch_means)
</code></pre>

<p>As a caveat, I divided the <code>batch_sum/len(ex_features)</code> since the final batch will not necessarily be the same size as the other batches, hence I did that calculate manually instead of using <code>tf.reduce_mean()</code>.This might be a minor issue if you have a lot of batches, but just wanted to be as accurate as possible. </p>

<p>Any suggestions would be appreciated. </p>
",1
56899105,How to use regularizer argument in tf.get_variable?,"<p>The syntax of the usage is clear:</p>

<pre><code>decay = tf.constant(0.001, dtype=tf.float32)
w = tf.get_variable(name='weight', shape=[512, 512],
                    regularizer=tf.contrib.layers.l2_regularizer(decay))
</code></pre>

<p>However, in the documentation only the following is stated:</p>

<blockquote>
  <p><code>regularizer</code>: A (Tensor -> Tensor or None) function; the result of applying it on a newly created variable will be added to the collection <code>tf.GraphKeys.REGULARIZATION_LOSSES</code> and can be used for regularization.</p>
</blockquote>

<p>The above does not imply that the regularization loss is automatically minimized. So do we need to manually get the variable from the collection  <code>tf.GraphKeys.REGULARIZATION_LOSSES</code> and add it to our main loss in order for it to be applied?</p>
",1
56905939,Effective way to read images from a csv file and return a tf.data.Dataset object,"<p>I have a csv file that contains two columns:</p>

<ol>
<li>the file path of the image which is stored as <code>numpy</code> arrays</li>
<li>the label of the image</li>
</ol>

<p>Each row in the csv corresponds to one item (sample).</p>

<p>I want to create a <code>tf.data</code> pipeline that reads the file path and loads the numpy array and the label associated with it. How would I go about doing so so that I can return a <code>tf.data.Dataset</code> object?</p>

<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/data"" rel=""nofollow noreferrer"">documentation</a> on the website is not very informative and I cannot figure out where to start from.</p>
",1
56939282,How do you feed a tf.data.Dataset dynamically in eager execution mode where initializable_iterator isn't available?,"<p>What is the new approach (under eager execution) to feeding data through a dataset pipeline in a dynamic fashion, when we need to feed it sample by sample? </p>

<p>I have a <code>tf.data.Dataset</code> which performs some preprocessing steps and reads data from a generator, drawing from a large dataset during training. </p>

<p>Let's say that dataset is represented as:</p>

<pre><code>ds = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6])
ds = ds.map(tf.square).shuffle(2).batch(2)
iterator = tf.data.make_one_shot_iterator(ds)
</code></pre>

<p>After training I want to produce various visualizations which require that I feed one sample at a time through the network for inference. I've now got this dataset preprocessing pipeline that I need to feed my raw sample through to be sized and shaped appropriately for the network input.</p>

<p>This seems like a use case for the initializable iterator:</p>

<pre><code>placeholder = tf.placeholder(tf.float32, shape=None)
ds = tf.data.Dataset.from_tensor_slices(placeholder)
ds = ds.map(tf.square).shuffle(2).batch(2)
iterator = tf.data.make_initializable_iterator(ds) 
# now re-initialize for each sample
</code></pre>

<blockquote>
  <p>Keep in mind that the map operation in this example represents a long sequence of preprocessing operations that can't be duplicated for each new data sample being feed in.</p>
</blockquote>

<p><strong>This doesn't work with eager execution</strong>, you can't use the placeholder. The documentation examples all seem to assume a static input such as in the first example here.</p>

<p>The only way I can think of doing this is with a queue and <code>tf.data.Dataset.from_generator(...)</code> which reads from the queue that I push to before predicting on the data. But this feels both hacky, and appears prone to deadlocks that I've yet to solve.</p>

<p>TF 1.14.0</p>
",1
56961856,How to write to TensorBoard in TensorFlow 2,"<p>I'm quite familiar in TensorFlow 1.x and I'm considering to switch to TensorFlow 2 for an upcoming project. I'm having some trouble understanding <strong>how to write scalars to TensorBoard logs with eager execution</strong>, using a <a href=""https://www.tensorflow.org/beta/tutorials/eager/custom_training_walkthrough"" rel=""noreferrer"">custom training loop</a>.</p>

<h3>Problem description</h3>

<p>In tf1 you would create some summary ops (one op for each thing you would want to store), which you would then merge into a single op, run that merged op inside a session and then write this to a file using a FileWriter object. Assuming <code>sess</code> is our <code>tf.Session()</code>, an example of how this worked can be seen below:</p>

<pre class=""lang-py prettyprint-override""><code># While defining our computation graph, define summary ops:
# ... some ops ...
tf.summary.scalar('scalar_1', scalar_1)
# ... some more ops ...
tf.summary.scalar('scalar_2', scalar_2)
# ... etc.

# Merge all these summaries into a single op:
merged = tf.summary.merge_all()

# Define a FileWriter (i.e. an object that writes summaries to files):
writer = tf.summary.FileWriter(log_dir, sess.graph)

# Inside the training loop run the op and write the results to a file:
for i in range(num_iters):
    summary, ... = sess.run([merged, ...], ...)
    writer.add_summary(summary, i)
</code></pre>

<p>The problem is that sessions don't exist anymore in tf2 and I would prefer not disabling eager execution to make this work. The <a href=""https://www.tensorflow.org/guide/summaries_and_tensorboard"" rel=""noreferrer"">official documentation</a> is written for tf1 and all references I can find suggest using the Tensorboard keras callback. However, as far as I know, this only works if you train the model through <code>model.fit(...)</code> and not through a <a href=""https://www.tensorflow.org/beta/tutorials/eager/custom_training_walkthrough"" rel=""noreferrer"">custom training loop</a>.</p>

<h3>What I've tried</h3>

<ul>
<li>The tf1 version of <code>tf.summary</code> functions, outside of a session. Obviously any combination of these functions fails, as FileWriters, merge_ops, etc. don't even exist in tf2.</li>
<li><a href=""https://medium.com/tensorflow/effective-tensorflow-2-0-best-practices-and-whats-changed-a0ca48767aff"" rel=""noreferrer"">This medium post</a> states that there has been a ""cleanup"" in some tensorflow APIs including <code>tf.summary()</code>. They suggest using <code>from tensorflow.python.ops.summary_ops_v2</code>, which doesn't seem to work. This <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/summary_ops_v2.py#L125"" rel=""noreferrer"">implies</a> using a <code>record_summaries_every_n_global_steps</code>; more on this later.</li>
<li>A series of other posts <a href=""https://medium.com/coinmonks/8-things-to-do-differently-in-tensorflows-eager-execution-mode-47cf429aa3ad"" rel=""noreferrer"">1</a>, <a href=""https://stackoverflow.com/questions/49711324/tf-contrib-summary-generic-or-tf-summary-text-in-eager-mode"">2</a>, <a href=""https://stackoverflow.com/questions/50257614/tensorflow-eager-and-tensorboard-graphs"">3</a>, suggest using the <code>tf.contrib.summary</code> and <code>tf.contrib.FileWriter</code>. However, <code>tf.contrib</code> <a href=""https://github.com/tensorflow/community/pull/18"" rel=""noreferrer"">has been removed from the core TensorFlow repository and build process</a>. </li>
<li><a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tf2_showcase/mnist.py"" rel=""noreferrer"">A TensorFlow v2 showcase from the official repo</a>, which again uses the <code>tf.contrib</code> summaries along with the <code>record_summaries_every_n_global_steps</code> mentioned previously. I couldn't make this to work either (even without using the contrib library).</li>
</ul>

<h3>tl;dr</h3>

<p>My questions are:</p>

<ul>
<li>Is there a way to properly use <code>tf.summary</code> in TensroFlow 2?</li>
<li>If not, is there another way to write TensorBoard logs in TensorFlow 2, when using a custom training loop (not <code>model.fit()</code>)?</li>
</ul>
",1
56969703,How to use `tf.scatter_nd` with multi-dimensional tensors,"<p>I'm trying to create a new tensor (<code>output</code>) with the values of another tensor (<code>updates</code>) placed according to <code>idx</code> tensor. The shape of <code>output</code> should be <code>[batch_size, 1, 4, 4]</code> (like an image of 2x2 pixels and one channel) and <code>update</code> has shape <code>[batch_size, 3]</code>.</p>

<p>I've read Tensorflow documentation (I'm working with gpu version 1.13.1) and found <code>tf.scatter_nd</code> should work for my problem. The issue is that I cannot make it work, I think I'm having problems understanding how I have to arange <code>idx</code>. </p>

<p>Let's consider <code>batch_size = 2</code>, so what I'm doing is:</p>

<pre class=""lang-py prettyprint-override""><code>updates = tf.constant([[1, 2, 3], [4, 5, 6]])  # shape [2, 3]
output_shape = tf.constant([2, 1, 4, 4])
idx = tf.constant([[[1, 0], [1, 1], [1, 0]], [[0, 0], [0, 1], [0, 2]]])  # shape [2, 3, 2]
idx_expanded = tf.expand_dims(idx, 1)  # so I have shape [2, 1, 3, 2]
output = tf.scatter_nd(idx_expanded, updates, output_shape)
</code></pre>

<p>I expect it to work, but it doesn't, it gives me this error:</p>

<p><code>ValueError: The outer 3 dimensions of indices.shape=[2,1,3,2] must match the outer 3 dimensions of updates.shape=[2,3]: Shapes must be equal rank, but are 3 and 2 for 'ScatterNd_7' (op: 'ScatterNd') with input shapes: [2,1,3,2], [2,3], [4]</code></p>

<p>I don't understand why it's expecting <code>updates</code> to have dimension 3. I thought <code>idx</code> has to make sense with <code>output_shape</code> (that's why I used <code>expand_dims</code>) and also with <code>updates</code> (specify the two indices for the three points), but it's obvious I'm missing something here.</p>

<p>Any help would be appreciated.</p>
",1
56970612,Fitted values and weights in tensorflow (tesorflow DNNRegressor),"<p>I am using tensorflow version 2.0.0-beta1. I have created the input function to feed into tf.estimator.DNNRegressor.</p>

<pre><code>input_func = tf.compat.v1.estimator.inputs.pandas_input_fn(x=X_train,
                                                y=y_train,
                                                batch_size=10,
                                                num_epochs=1000,
                                                shuffle=True)
</code></pre>

<p>Below is the model that I am creating using DNNRegressor. </p>

<pre><code>model = tf.estimator.DNNRegressor(feature_columns=feature_col, hidden_units=[1024, 800, 512, 256])
model.train(input_fn=input_func, steps=10000)
</code></pre>

<p>Now I want to identify </p>

<p><strong>1) Fitted values of my model on training data.</strong></p>

<p><strong>2) Weights associated with each variable in model(i.e. tf.estimator.DNNRegressor)</strong></p>

<p>I have search through the documentation of tensorflow and other sources but didn't get this information. </p>
",1
56973875,How to slice the data of tf.data.Dataset type to certain length in TF2.0?,"<p>I am following this tutorial (<a href=""https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches"" rel=""nofollow noreferrer"">https://www.tensorflow.org/beta/tutorials/load_data/text#split_the_dataset_into_text_and_train_batches</a>), in this I want to trim the length of each sentence stored in (sentence, label) tuple of <code>tf.data.Dataset</code> type to length 8 or n. I tried using tf.map function with lambda but can not do it since lambda only accepts one argument and it is receiving <strong>sentences</strong> and <strong>label</strong> arguments. <code>train_data = train_data.map(lambda x : x[:4])</code></p>

<p>I will be thankful if anyone can give their ideas on how to do this.</p>
",0
56978161,What are the differences between Tensorflow's methods for exporting computational graph?,"<p>I understand one might want to have an overloaded method for coarser or finer grained control, but why to have them all named in a different way, scattered around the API? I am afraid I might be missing some differences between them.</p>

<p>So far I have found <code>tf.io.write_graph</code>,<code>tf.train.Saver.export_meta_graph</code> and <code>tf.train.export_meta_graph</code> (increasing number of parameters, not listing aliases). Can someone explain to me what is the rationale for having them all and when one is preferred over the other? Is any of them legacy? If so, it is not clearly marked in the documentation.</p>
",1
56980488,Convert dense tensor to ragged tensor,"<p>I have a Sparse Tensor which I converted into Dense Tensor. After converting from sparse to dense using <code>tf.sparse.to_dense</code>, I got an output which looks like this:</p>

<p><a href=""https://i.stack.imgur.com/3Uklm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3Uklm.png"" alt=""Dense tensor""></a></p>

<p>I want to remove default values i.e zero and convert it to ragged tensor like this:
<a href=""https://i.stack.imgur.com/GPO34.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GPO34.png"" alt=""Ragged tensor""></a>
<strong>Data in the above two images is different</strong><br>
<br>
I went through Tensorflow documentation and I am unable to find any method to do this. I tried using <code>boolean_mask</code>, the major problem is to compute <code>boolean_mask</code> for each row individually.<br>
So, I am wondering if there is an efficient way to achieve this.</p>

<p>Thank you.</p>
",1
57014236,How to use the Embedding Projector in Tensorflow 2.0,"<p>With the tf.contrib module gone from Tensorflow, and with tf.train.Saver() also gone, I cannot find a way to store a set of embeddings and their corresponding thumbnails, so that the Tensorboard Projector can read them.</p>

<p>The <a href=""https://www.tensorflow.org/tensorboard/r2/get_started"" rel=""noreferrer"">Tensorboard documentation</a> for Tensorflow 2.0 explains how to create plots and summaries, and how to use the summary tool in general, but nothing about the projector tool. Has anyone found how to store datasets for visualization?</p>

<p>If possible, I would appreciate a (minimal) code example.</p>
",1
57043401,"What is the difference between the properties ""kernel_initializer"" and ""kernel"" in layers.Dense?","<p>In the <a href=""https://www.tensorflow.org/api_docs/python/tf/layers/Dense?hl=en"" rel=""nofollow noreferrer"">documentation</a> on the TensorFlow website for <code>tf.layers.Dense</code>, it lists <code>kernel_initializer</code> and <code>kernel</code> as its properties. From what I understand, the <code>kernel_initializer</code> is an argument where you can pass the weight matrix. However, <code>kernel</code> also is a weight matrix.</p>

<p>What is the difference between these two and why would you pick one over the other when initializing the weights?</p>
",1
57081006,Create a Keras Layer Subclass using Conv Layers,"<p>I would like to create a custom <code>tf.keras.layers.Layer</code> resembling the below function:</p>

<pre><code>def conv_block(inputs, filters, kernel_size, strides=(1, 1, 1),
                 padding='valid', activation=True, block_name='conv3d'):

    with tf.name_scope(block_name):
      conv = Conv3D(filters=filters, kernel_size=kernel_size, strides=strides,
                    padding=padding, activation=None,
                    name='{}_conv'.format(block_name))(inputs)
      batch_norm = BatchNormalization(
          name='{}_batch_norm'.format(block_name))(conv)

      if activation:
        relu = ReLU(max_value=6, name='{}_relu'.format(block_name))(batch_norm)
        res_layer = relu
      else:
        res_layer = batch_norm
    return res_layer
</code></pre>

<p>I went through the documentation available <a href=""https://www.tensorflow.org/beta/guide/keras/custom_layers_and_models#the_layer_class"" rel=""nofollow noreferrer"">here</a> and <a href=""https://www.tensorflow.org/guide/keras#custom_layers"" rel=""nofollow noreferrer"">here</a> and subsequently I created the below class:</p>

<pre><code>class ConvBlock(tf.keras.layers.Layer):

    def __init__(self, filters, kernel_size, strides=(1, 1, 1), padding='valid', activation=True, **kwargs):
        super(ConvBlock, self).__init__()
        self.filters = filters
        self.kernel_size = kernel_size
        self.strides = strides
        self.padding = padding
        self.activation = activation

        self.conv_1 = Conv3D(filters=self.filters, 
                             kernel_size=self.kernel_size, 
                             strides=self.strides, 
                             padding=self.padding, 
                             activation=None)

        self.batch_norm_1 = BatchNormalization()
        self.relu_1 = ReLU(max_value=6)

    def call(self, inputs):
        conv = self.conv_1(inputs)
        batch_norm = self.batch_norm_1(conv)

        if self.activation:
            relu = self.relu_1(batch_norm)
            return relu
        else:
            return batch_norm
</code></pre>

<p>I want to use this <code>Layer</code> several times throughout my model. I have several questions around this:</p>

<ol>
<li>The documentation mentions using <code>add_weights()</code> in the <code>build()</code> method. However would it be necessary in this case?</li>
<li>Do I need to include a <code>build()</code>method at all? </li>
<li><p>How do I get the output shape of the layer? The documentation mentions using the below function:</p>

<p>def compute_output_shape(self, input_shape):
    shape = tf.TensorShape(input_shape).as_list()
    shape[-1] = self.output_dim
    return tf.TensorShape(shape)</p></li>
</ol>

<p>How can I use this function to compute the shape of the output layer?</p>
",0
57083881,Tensorflow 2.0 Saving trained parameters to be restored in a new file,"<p>I need to save trained variables of a TensorFlow 2.0 model using one of TF's built in functions like tf.train.Checkpoint or any other, and want to call them in a new file. I am not using tf.Keras.Sequantial and don't want to use something like model.save_weights()</p>

<p>I have tried tf.train.Checkpoint to save variables, but not sure how to restore them. I used to work with tf.train.Saver() in TF 1.0 to save variables using sessions and restore them using tf.train.import_meta_graph and tf.train.latest_checkpoint. However, I haven't been able to find equivalent functionalities in TF 2.0 documentation so far. </p>

<h1>try checkpoint saver in tensorflow 2.0 format to save trained parameters W, b_v, b_h</h1>

<p>saver = tf.train.Checkpoint()</p>

<p>saver.listed = [W, b_v, b_h]</p>

<p>saver.mapped = {'W':saver.listed[0],'b_v':saver.listed[1],
 'b_h':saver.listed[2]}</p>

<p>save_path = saver.save('trained_parameters')</p>

<h1>in a new file:</h1>

<p>restorer = tf.train.Checkpoint()</p>

<p>restorer.restore('trained_parameters')</p>

<h1>calling the parameters by their previously mapped names doesn't work, not sure how to go about this</h1>
",1
57120680,Deep copy of tensor in tensorflow python,"<p>In some of my code, I have created a neural network using tensorflow and have access to a tensor representing that network's output. I want to make a copy of this tensor so that even if I train the neural network more, I can access the original value of the tensor.</p>

<p>Following other answers and tensorflow documentation, I have tried the tf.identity() function, but it does not seem to be doing what I need. Some other links suggested the use of tf.tile(), but this did not help either. I do not wish to use sess.run(), evaluate the tensor, and store it elsewhere.</p>

<p>Here is a toy example that describes what I need to do:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np

t1 = tf.placeholder(tf.float32, [None, 1])
t2 = tf.layers.dense(t1, 1, activation=tf.nn.relu)
expected_out = tf.placeholder(tf.float32, [None, 1])

loss = tf.reduce_mean(tf.square(expected_out - t2))
train_op = tf.train.AdamOptimizer(1e-4).minimize(loss)

sess = tf.Session()

sess.run(tf.global_variables_initializer())

print(sess.run(t2, feed_dict={t1: np.array([1]).reshape(-1,1)}))
t3 = tf.identity(t2) # Need to make copy here
print(sess.run(t3, feed_dict={t1: np.array([1]).reshape(-1,1)}))

print(""\nTraining \n"")

for i in range(1000):
    sess.run(train_op, feed_dict={t1: np.array([1]).reshape(-1,1), expected_out: np.array([1]).reshape(-1,1)})

print(sess.run(t2, feed_dict={t1: np.array([1]).reshape(-1,1)}))
print(sess.run(t3, feed_dict={t1: np.array([1]).reshape(-1,1)}))
</code></pre>

<p>The result of the above code is that <code>t2</code> and <code>t3</code> have the same value. </p>

<pre><code>[[1.5078927]]
[[1.5078927]]

Training

[[1.3262703]]
[[1.3262703]]
</code></pre>

<p>What I want is for <code>t3</code> to keep its value from being copied.</p>

<pre><code>[[1.5078927]]
[[1.5078927]]

Training

[[1.3262703]]
[[1.5078927]]
</code></pre>

<p>Thanks in advance for your help.</p>
",1
57134808,tf.keras.optimizers.Adam with tf.estimator model in Tensorflow 2.0.beta is crashing,"<p>I am using <code>Tensorflow 2.0.beta</code> with <code>Python 3.6.6</code> on <code>Mac OS</code> (nightly: <code>tf-nightly-2.0-preview</code> <code>2.0.0.dev20190721</code> but I never managed to have it working with compat module in <code>Tensorflow 2.0</code>).</p>

<p>I am traying to migrate a <code>tf.estimator</code> model from <code>Tensorflow 1.12</code> (fully working) to <code>Tensorflow 2.0</code>. Here is the code:</p>

<pre><code># estimator model
def baseline_estimator_model(features, labels, mode, params):
    """"""
    Model function for Estimator
    """"""
    print('model based on keras layer but return an estimator model')

    # gettings the bulding blocks
    model = keras_building_blocks(params['dim_input'], params['num_classes'])

    dense_inpout = features['dense_input']

    # Logits layer
    if mode == tf.estimator.ModeKeys.TRAIN:
        logits = model(dense_inpout, training=True)
    else:
        logits = model(dense_inpout, training=False)


    # Compute predictions
    probabilities = tf.nn.softmax(logits)
    classes = tf.argmax(input=probabilities, axis=1, )

    # made prediction
    predictions = {
        'classes': classes,
        'probabilities': probabilities,
    }

    # to be tested
    predictions_output = tf.estimator.export.PredictOutput(predictions)

    # Provide an estimator spec for `ModeKeys.PREDICT`
    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(mode=mode,
                                          predictions=predictions,
                                          export_outputs={tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY: predictions_output})

    # Compute loss for both TRAIN and EVAL modes
    # old -&gt; loss = tf.compat.v1.losses.softmax_cross_entropy(onehot_labels=labels, logits=logits)
    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)(labels, logits)

    # Generate necessary evaluation metrics
    # old -&gt; accuracy = tf.compat.v1.metrics.accuracy(labels=tf.argmax(input=labels, axis=1), predictions=classes, name='accuracy')
    accuracy = tf.keras.metrics.CategoricalAccuracy()
    accuracy.update_state(labels, logits)

    eval_metrics = {'accuracy': accuracy}

    tf.summary.scalar('accuracy', accuracy.result())

    # Provide an estimator spec for `ModeKeys.EVAL`
    if mode == tf.estimator.ModeKeys.EVAL:
        return tf.estimator.EstimatorSpec(mode=mode,
                                          loss=loss,
                                          eval_metric_ops=eval_metrics)

    # Provide an estimator spec for `ModeKeys.TRAIN`
    if mode == tf.estimator.ModeKeys.TRAIN:

        # old but working -&gt; optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001, beta1=0.9)
        # crashing
        optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, epsilon=1e-07)

        # old -&gt; train_op = optimizer.minimize(loss, tf.compat.v1.train.get_or_create_global_step())
        train_op = optimizer.minimize(loss,var_list=model.weights)

        return tf.estimator.EstimatorSpec(mode=mode,
                                          loss=loss,
                                          train_op=train_op)
</code></pre>

<p>predictions=predictions, loss=loss, train_op=train_op, export_outputs=predictions_output)</p>

<p>If I keep the compat.v1 module it is working:</p>

<pre><code>optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001, beta1=0.9)
</code></pre>

<p>If I try to use something without compat.v1 it is crashing:</p>

<pre><code>optimizer = tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9,epsilon=1e-07)
</code></pre>

<p>with the following error (I am running the code locally for the moment, not on <code>GCP</code>):</p>

<pre><code>I0721 17:33:04.812453 4526515648 estimator.py:209] Using config: {'_model_dir': 'results/Models/Mnist/tf_1_12/estimator/v3/ckpt/', '_tf_random_seed': None, '_save_summary_steps': 10, '_save_checkpoints_steps': 10, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 3, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 50, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x1c37b11b70&gt;, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
I0721 17:33:04.815697 4526515648 estimator_training.py:186] Not using Distribute Coordinator.
I0721 17:33:04.817899 4526515648 training.py:612] Running training and evaluation locally (non-distributed).
I0721 17:33:04.818665 4526515648 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 10 or save_checkpoints_secs None.
I0721 17:33:04.834385 4526515648 model.py:211] input_dataset_fn: TRAIN, train

using keras layer and estimator (recommended way)
exporter &lt;tensorflow_estimator.python.estimator.exporter.LatestExporter object at 0x1c37b115f8&gt;

I0721 17:33:05.117963 4526515648 estimator.py:1145] Calling model_fn.

model based on keras layer but return an estimator model

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;timed exec&gt; in &lt;module&gt;

~/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/src/model_mnist_2_0_v1/trainer/model.py in train_and_evaluate(FLAGS, use_keras)
    589                                       exporters=exporter)
    590 
--&gt; 591     tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
    592 
    593 def train_and_evaluate_old(FLAGS, use_keras):

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)
    471         '(with task id 0).  Given task id {}'.format(config.task_id))
    472 
--&gt; 473   return executor.run()
    474 
    475 

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py in run(self)
    611         config.task_type != run_config_lib.TaskType.EVALUATOR):
    612       logging.info('Running training and evaluation locally (non-distributed).')
--&gt; 613       return self.run_local()
    614 
    615     # Distributed case.

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/training.py in run_local(self)
    712         max_steps=self._train_spec.max_steps,
    713         hooks=train_hooks,
--&gt; 714         saving_listeners=saving_listeners)
    715 
    716     eval_result = listener_for_eval.eval_result or _EvalResult(

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    365 
    366       saving_listeners = _check_listeners_type(saving_listeners)
--&gt; 367       loss = self._train_model(input_fn, hooks, saving_listeners)
    368       logging.info('Loss for final step: %s.', loss)
    369       return self

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
   1156       return self._train_model_distributed(input_fn, hooks, saving_listeners)
   1157     else:
-&gt; 1158       return self._train_model_default(input_fn, hooks, saving_listeners)
   1159 
   1160   def _train_model_default(self, input_fn, hooks, saving_listeners):

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)
   1186       worker_hooks.extend(input_hooks)
   1187       estimator_spec = self._call_model_fn(
-&gt; 1188           features, labels, ModeKeys.TRAIN, self.config)
   1189       global_step_tensor = training_util.get_global_step(g)
   1190       return self._train_with_estimator_spec(estimator_spec, worker_hooks,

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)
   1144 
   1145     logging.info('Calling model_fn.')
-&gt; 1146     model_fn_results = self._model_fn(features=features, **kwargs)
   1147     logging.info('Done calling model_fn.')
   1148 

~/Desktop/Work/Data_Science/Tutorials_Codes/Python/proj_DL_models_and_pipelines_with_GCP/src/model_mnist_2_0_v1/trainer/model.py in baseline_estimator_model(features, labels, mode, params)
    442         #train_op = optimizer.minimize(loss, tf.compat.v1.train.get_or_create_global_step())
    443         #train_op = optimizer.minimize(loss, tf.train.get_or_create_global_step())
--&gt; 444         train_op = optimizer.minimize(loss,var_list=model.weights)
    445 
    446         print('step 8')

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in minimize(self, loss, var_list, grad_loss, name)
    315     """"""
    316     grads_and_vars = self._compute_gradients(
--&gt; 317         loss, var_list=var_list, grad_loss=grad_loss)
    318 
    319     return self.apply_gradients(grads_and_vars, name=name)

~/anaconda-release/conda-env/env_gcp_dl_2_0_nightly/lib/python3.6/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py in _compute_gradients(self, loss, var_list, grad_loss)
    349       if not callable(var_list):
    350         tape.watch(var_list)
--&gt; 351       loss_value = loss()
    352     if callable(var_list):
    353       var_list = var_list()

TypeError: 'Tensor' object is not callable
</code></pre>

<p>Any idea how to fix that ? The error messages was changing over time since <code>Tensorflow 2.0 alpha</code>.</p>

<p>I am also looking for a full working example of tf.estimator working with <code>Tensorflow 2.0</code>. I have issue to export the model as well. In the official documentation of <code>Tensorflow 2.0</code> they only use in their example <code>compat.v1</code> and don't export the model. All the online course on tf.estimator from GCP are using  older version of Tensorflow (1.12 - 1.14).</p>
",1
57153123,Remove stdout output from tf.Session due to init in a loop,"<p>I've come to stack overflow because no one else seems to have asked this question before. When running my tensorflow model I use it inside a loop to do regression analysis. I use tf.Session inside the lop</p>

<p>loop:
    with tf.Session as sess:
          ...code rest of code</p>

<p>When I run the neural network it spits out all the tensorflow/core information in the loop and makes my actual output (for me to visualize) useless. I'd like to either re-direct the output or find a way to make it less verbose or completely silent.</p>

<p>I've gone through most of the documentation from tensorflow and the Session documentation doesn't reveal anything to me. Here is the current output to the terminal.</p>

<blockquote>
  <p>2019-07-22 16:09:46.813546: I >tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu >devices: 0
  2019-07-22 16:09:46.815548: I >tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect >StreamExecutor with strength 1 edge matrix:
  2019-07-22 16:09:46.817433: I >?>tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0
  2019-07-22 16:09:46.818613: I >tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N
  2019-07-22 16:09:46.819902: I >tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow >device (/job:localhost/replica:0/task:0/device:GPU:0 with 8698 MB memory) -> >physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:09:00.0, >compute capability: 7.5)</p>
</blockquote>
",1
57188207,"Is there a Pytorch equivalent of Tensorflow class ""tf.nn.rnn_cell.MultiRNNCell""?","<p>guys.
Is there pytorch equivalent of “<a href=""https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/MultiRNNCell"" rel=""nofollow noreferrer"">tf.nn.rnn_cell.MultiRNNCell</a>” that stacks multiple cells?
Could it be torch.nn.LSTM()?
Then how to use it to create same network as MultiRNNCell?</p>
",0
57270125,"Conceptual understanding of tf.nn.dynamic_rnn() ""outputs"" vs. ""state""","<h2>Context</h2>

<p>I'm reading through part II of <a href=""https://rads.stackoverflow.com/amzn/click/com/1491962291"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">Hands on ML</a> and am looking for some clarity on when to use ""outputs"" and when to use ""state"" in the loss calculation for a RNN.</p>

<p>In the book (p.396 for those that have the book), the author says, ""Note that the fully connected layer is connected to the <code>states</code> tensor, which contains only the final states of the RNN,"" referring to a sequence classifier that is unrolled over 28 steps. Since the <code>states</code> variable will have <code>len(states) == &lt;number_of_hidden_layers&gt;</code>, when building a deep RNN I have been using states[-1] to only connect to the final state of the final layer. For example:</p>

<pre class=""lang-py prettyprint-override""><code># hidden_layer_architecture = list of ints defining n_neurons in each layer
# example: hidden_layer_architecture = [100 for _ in range(5)]
layers = []
for layer_id, n_neurons in enumerate(hidden_layer_architecture):

    hidden_layer = tf.contrib.rnn.BasicRNNCell(n_neurons, 
                                               activation=tf.nn.tanh,                                                                                                                                                                     
                                               name=f'hidden_layer_{layer_id}')

    layers.append(hidden_layer)

recurrent_hidden_layers = tf.contrib.rnn.MultiRNNCell(layers)
outputs, states = tf.nn.dynamic_rnn(recurrent_hidden_layers,
                                    X_, dtype=tf.float32)

logits = tf.layers.dense(states[-1], n_outputs, name='outputs')
</code></pre>

<p>This works as expected given the author's previous statement. However, I don't understand when one would use the <code>outputs</code> variable (first output of <code>tf.nn.dynamic_rnn()</code>)</p>

<p>I have looked at <a href=""https://stackoverflow.com/questions/44162432/analysis-of-the-output-from-tf-nn-dynamic-rnn-tensorflow-function"">this question</a>, which does a pretty good job of answering the minutia, and mentioned that, ""If you are only interested in the last output of the cell, you can just slice the time dimension to pick just the last element (e.g. <code>outputs[:, -1, :]</code>)."" I inferred this to mean something along the lines of <code>states[-1] == outputs[:, -1, :]</code>, which when tested was false. Why would this not be the case? If the outputs are the outputs of the cell at each time step, why wouldn't this be the case? In general...</p>

<h2>Question</h2>

<p>When does one use the <code>outputs</code> variable from <code>tf.nn.dynamic_rnn()</code> in the loss function and when would one use the <code>states</code> variable? How does this change the abstracted architecture of the network?</p>

<p>Any clarity would be greatly appreciated.</p>
",0
57349824,"Recurrent neural network, time series prediction with newer Tensorflow 1.14","<p>How to use new tf.keras API with recurrent neural network? I have checked the documentation but there is no example of such a situation.
There is this great book Hands on machine learning from 2017. Since that year the API of tensorflow has evolved and I am trying to rewrite recurrent neural network for time series prediction with using version <code>1.14</code> code.
The code from the book is using older <code>tf.nn.dynamic_rnn</code> and <code>tf.nn.rnn_cell.BasicRNNCell</code>:</p>

<pre><code>n_steps = 20
n_inputs = 1
n_neurons = 100
n_outputs = 1
learning_rate = 0.001

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])
cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu)
rnn_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)
stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])
stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
loss = tf.reduce_mean(tf.square(outputs - y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(loss)

init = tf.global_variables_initializer()
saver = tf.train.Saver()
n_iterations = 500
batch_size = 50

with tf.Session() as sess:
    init.run()
        for iteration in range(n_iterations):
        X_batch, y_batch = next_batch(batch_size, n_steps)
        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
        if iteration % 100 == 0:
            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})
            print(iteration, ""\tMSE:"", mse)

    X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))
    y_pred = sess.run(outputs, feed_dict={X: X_new})
</code></pre>

<p>And this code works just fine (except that it throws warnings about deprecation left and right). I wanted to use <code>tf.keras</code> API as suggested in warning. My code is the same except:</p>

<pre><code>cell =  tf.keras.layers.SimpleRNNCell(units=n_neurons, activation=tf.nn.relu)  
rnn_outputs = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"")(X)
</code></pre>

<p>But this yields following exception:</p>

<pre><code>InvalidArgumentError: Input to reshape is a tensor with 50 values, but the requested shape requires a multiple of 20
 [[node Reshape_1 (defined at &lt;ipython-input-9-879361be49dd&gt;:3) ]]
</code></pre>

<p>so I understand that the problematic line is</p>

<pre><code>outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
</code></pre>

<p>After checking and comparing documentation for both cells <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn</a> and 
<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN</a> I can't find the culprit.</p>

<p><strong>What is the difference with these two cells? How to use tf.keras API with time series?</strong></p>

<p>Full old code: <a href=""https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb"" rel=""nofollow noreferrer"">https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb</a></p>

<p>Full ""my"" code:</p>

<pre><code>import numpy as np
import tensorflow as tf
from datetime import datetime
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import pandas as pd
from utils import shuffle_batch, variable_summaries
import os


dir_path = os.getcwd()

now = datetime.utcnow().strftime(""%Y%m%d%H%M%S"")
root_logdir = ""tf_logs""
logdir = ""{}/run-{}/"".format(root_logdir, now)
print(dir_path)


t_min, t_max = -5, 5
section_start = (t_max + t_min) / 2
resolution = 0.1
n_steps = 20

def time_series(t):
    return np.sin(t)

def next_batch(batch_size, n_steps):
    t0 = np.random.rand(batch_size, 1) * (t_max - t_min - n_steps * resolution)
    Ts = t0 + np.arange(0., n_steps + 1) * resolution
    ys = time_series(Ts)
    return ys[:, :-1].reshape(-1, n_steps, 1), ys[:, 1:].reshape(-1, n_steps, 1)


t = np.linspace(t_min, t_max, int((t_max - t_min) / resolution))

t_instance = np.linspace(start = section_start, stop = section_start + resolution * (n_steps + 1),num = n_steps + 1)

plt.figure(figsize=(11,4))
plt.subplot(121)
plt.title(""A time series (generated)"", fontsize=14)
plt.plot(t, time_series(t), label=r""original"")
plt.plot(t_instance[:-1], time_series(t_instance[:-1]), ""b-"", linewidth=3, label=""A training instance"")
plt.legend(loc=""lower left"", fontsize=14)
#plt.axis([-10, 10, -17, 13])
plt.xlabel(""Time"")
plt.ylabel(""Value"")

plt.subplot(122)
plt.title(""A training instance"", fontsize=14)
plt.plot(t_instance[:-1], time_series(t_instance[:-1]), ""bo"", markersize=10, label=""instance"")
plt.plot(t_instance[1:], time_series(t_instance[1:]), ""c*"", markersize=10, label=""target"")
plt.legend(loc=""upper left"")
plt.xlabel(""Time"")


# In[6]:


n_steps = 20
n_inputs = 1
n_neurons = 100
n_outputs = 1

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])


# In[7]:


cell =  tf.keras.layers.SimpleRNNCell(units=n_neurons, activation=tf.nn.relu)                        


rnn_outputs = tf.keras.layers.RNN(cell,dtype=tf.float32, name=""hidden1"")(X)
print(rnn_outputs.get_shape())


stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons], name='reshape1')
stacked_outputs = tf.keras.layers.Dense(n_outputs,name=""hidden2"")(stacked_rnn_outputs)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs], name='reshape2')


learning_rate = 0.001

loss = tf.reduce_mean(tf.square(outputs - y)) # MSE
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(loss)

init = tf.global_variables_initializer()
saver = tf.train.Saver()

n_iterations = 1500
batch_size = 50
save_path =os.path.join(dir_path,""model"",""recurrent_sinus_model"")

with tf.Session() as sess:
    init.run()
    for iteration in range(n_iterations):
        X_batch, y_batch = next_batch(batch_size, n_steps)
        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
        if iteration % 100 == 0:
            mse = loss.eval(feed_dict={X: X_batch, y: y_batch})
            print(iteration, ""\tMSE:"", mse)

    saver.save(sess, save_path)


with tf.Session() as sess:                      
    saver.restore(sess, save_path)  

    X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs)))
    y_pred = sess.run(outputs, feed_dict={X: X_new})


plt.title(""Testing the model"", fontsize=14)
plt.plot(t_instance[:-1], time_series(t_instance[:-1]), ""bo"", markersize=10, label=""instance"")
plt.plot(t_instance[1:], time_series(t_instance[1:]), ""w*"", markersize=10, label=""target"")
plt.plot(t_instance[1:], y_pred[0,:,0], ""r."", markersize=10, label=""prediction"")
plt.legend(loc=""upper left"")
plt.xlabel(""Time"")

plt.show()


# In[ ]:


with tf.Session() as sess:                      
    saver.restore(sess, save_path)  

    X_new = time_series(np.array(t.reshape(-1, n_steps, n_inputs)))
    y_pred = sess.run(outputs, feed_dict={X: X_new})



plt.title(""A time series (generated)"", fontsize=14)
plt.plot(t, time_series(t), label=r""original"",linewidth=5,c='r')
plt.plot(t[:-1], time_series(t[:-1]), ""b-"", linewidth=3, label=""A training instance"")
plt.legend(loc=""lower left"", fontsize=14)

plt.xlabel(""Time"")
plt.ylabel(""Value"")
</code></pre>
",1
57392510,TensorFlow simple example help - custom gradient,"<p>How do you pass a custom gradient into a gradient optimization function in TensorFlow.</p>

<p>I have illustrated what I am trying to do, with a simple example (trying to minimize z = 2x^2 + y^2 + 2).</p>

<p>I have been looking at:
<a href=""https://www.tensorflow.org/api_docs/python/tf/train/Optimizer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/train/Optimizer</a></p>

<p>The problem seems to work if you pass in <code>optimizer = tf.train.GradientDescentOptimizer(0.55)</code> and <code>train = optimizer.minimize(z)</code></p>

<p>This code works:</p>

<pre><code>import tensorflow as tf

x = tf.Variable(11, name='x', dtype=tf.float32)
y = tf.Variable(11, name='x', dtype=tf.float32)
const = tf.constant(2.0, dtype=tf.float32)

z = x**2 + y**2 + const


optimizer = tf.train.GradientDescentOptimizer(0.55)
train = optimizer.minimize(z)

init = tf.global_variables_initializer()

def optimize():
  with tf.Session() as session:
    session.run(init)
    print(""starting at"", ""x:"", session.run(x), ""y:"", session.run(y), ""z:"", session.run(z))
    for step in range(10):  
      session.run(train)
      print(""step"", step, ""x:"", session.run(x), ""y:"", session.run(y), ""z:"", session.run(z))


optimize()
</code></pre>

<p>But I want to specify the gradient in the problem.
aka I am trying to do this:</p>

<pre><code>def function_to_minimize(x,y, const):
    # z = 2x^2 + y^2 + constant
    z = 2*x**2 + y**2 + const
    return z

def calc_grad(x,y):
    # z = 2x^2 + y^2 + constant
    dz_dx = 4*x
    dz_dy = 2*y
    return [(dz_dx, x), (dz_dy, y)]

x = tf.Variable(3, name='x', dtype=tf.float32)
y = tf.Variable(3, name='y', dtype=tf.float32)
const = tf.constant(2.0, dtype=tf.float32)


z = function_to_minimize(x,y, const)
grad = calc_grad(x,y)


init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
print(sess.run(z))
print(sess.run(grad))


optimizer = tf.train.GradientDescentOptimizer(0.5)

grads_and_vars = calc_grad(x,y)

optimizer.apply_gradients(grads_and_vars)

# minimize() takes care of both computing the gradients and applying them to the variables.
#If you want to process the gradients before applying them you can instead use the optimizer in three steps:
#     1. Compute the gradients with compute_gradients().
#     2. Process the gradients as you wish.
#     3. Apply the processed gradients with apply_gradients()
</code></pre>

<p>How do you do this properly?</p>
",0
57395645,How do I prune over the highest weights in tensorflow layer? tfmot.sparsity.keras.prune_low_magnitude,"<p>I want to prune over the highest weight values in a tf layer. I'm thinking about using <code>tf.nn.top_k</code> but I'm not exactly sure how I would go about doing this. </p>

<p>Documentation: <a href=""https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/prune_low_magnitude"" rel=""nofollow noreferrer"">https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/sparsity/keras/prune_low_magnitude</a>
Code: </p>

<pre><code>pruning_params = {
    'pruning_schedule': PolynomialDecay(initial_sparsity=0.2,
        final_sparsity=0.8, begin_step=1000, end_step=2000),
    'block_size': (2, 3),
    'block_pooling_type': 'MAX'
}

model = keras.Sequential([
    layers.Dense(10, activation='relu', input_shape=(100,)),
    prune_low_magnitude(layers.Dense(2, activation='tanh'), **pruning_params)
])

</code></pre>
",1
57396474,Difference between Profiling and Trace in Tensorflow?,"<p>I was looking for a solution for Profiling in tensorflow. See <a href=""https://stackoverflow.com/questions/56756028/tensorflow-profiling-in-tf2-0"">Tensorflow profiling in TF2.0</a>.</p>

<p>Since I could not find a solution for tensorflow2.0, I am using <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/summary/trace_on"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/summary/trace_on</a></p>

<p>As indicated in the documentation, it incurrs high memory overload, but between two given points in code, I am able to trace the runtime and memory.</p>

<p>What are the other differences between these methods(i.e, Profiler and Trace)?</p>
",1
57423567,How to add operations to tensorflow graph which are distributed and defined inside functions?,"<p>I would like to define 2 class and each class with a different graph. Thereby, as a first step I defined a graph for the <code>class Model</code>. </p>

<p>The below code snippet will work if the line <code>with self.graph.as_default():</code> is removed and run with with a default graph i.e.<code>with tf.Session() as sess:</code> .</p>

<p>But I want to define in it in a graph so that I can add another class with a new graph and make those 2 graphs to run in parallel or sequential.</p>

<p>I am new to tensorflow so am still not sure whether the below way of adding operations to the graph is correct or not.</p>

<pre><code>import functools
import tensorflow as tf
tf.reset_default_graph()
from tensorflow.examples.tutorials.mnist import input_data


def doublewrap(function):
    """"""
    A decorator decorator, allowing to use the decorator to be used without
    parentheses if not arguments are provided. All arguments must be optional.
    """"""
    @functools.wraps(function)
    def decorator(*args, **kwargs):
        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):
            return function(args[0])
        else:
            return lambda wrapee: function(wrapee, *args, **kwargs)
    return decorator


@doublewrap
def define_scope(function, scope=None, *args, **kwargs):
    """"""
    A decorator for functions that define TensorFlow operations. The wrapped
    function will only be executed once. Subsequent calls to it will directly
    return the result so that operations are added to the graph only once.
    The operations added by the function live within a tf.variable_scope(). If
    this decorator is used with arguments, they will be forwarded to the
    variable scope. The scope name defaults to the name of the wrapped
    function.
    """"""
    attribute = '_cache_' + function.__name__
    name = scope or function.__name__
    @property
    @functools.wraps(function)
    def decorator(self):
        if not hasattr(self, attribute):
            with tf.variable_scope(name, *args, **kwargs):
                setattr(self, attribute, function(self))
        return getattr(self, attribute)
    return decorator


class Model:

    def __init__(self, image, label):
        self.graph = tf.Graph()
        self.image = image
        self.label = label
        self.prediction
        self.optimize
        self.error

    @define_scope(initializer=tf.contrib.slim.xavier_initializer())
    def prediction(self):
        with self.graph.as_default():
            x = self.image
            x = tf.contrib.slim.fully_connected(x, 200)
            x = tf.contrib.slim.fully_connected(x, 200)
            x = tf.contrib.slim.fully_connected(x, 10, tf.nn.softmax)
        return x

    @define_scope
    def optimize(self):
        with self.graph.as_default():
            current_error=self.error
            logprob = tf.log(self.prediction + 1e-12) *(1-current_error)  #Here changed ????????????

            cross_entropy = -tf.reduce_sum(self.label * logprob)
            optimizer = tf.train.RMSPropOptimizer(0.03)
            trainop = optimizer.minimize(cross_entropy) 
        return trainop

    @define_scope
    def error(self):
        with self.graph.as_default():
            mistakes = tf.not_equal(
                tf.argmax(self.label, 1), tf.argmax(self.prediction, 1))
            me = tf.reduce_mean(tf.cast(mistakes, tf.float32))
        return me

    # @define_scope
    # def accuracy(self):
    #     accuracy = tf.reduce_sum()


def main():
    mnist = input_data.read_data_sets('../../MNIST_data/', one_hot=True)
    image = tf.placeholder(tf.float32, [None, 784])
    label = tf.placeholder(tf.float32, [None, 10])
    model = Model(image, label)

    with tf.Session(graph=model.graph) as sess:
        sess.run(tf.initialize_all_variables())

        for _ in range(10):
          images, labels = mnist.test.images, mnist.test.labels
          error = sess.run(model.error, {image: images, label: labels})
          print('Test error {:6.2f}%'.format(100 * error))
          for _ in range(60):
            images, labels = mnist.train.next_batch(100)
            sess.run(model.optimize, {image: images, label: labels})


if __name__ == '__main__':
    main()
</code></pre>

<p>when the above code is executed am getting the following error message</p>

<blockquote>
  <p>ValueError: Tensor(""error/Const:0"", shape=(1,), dtype=int32) must be
  from the same graph as Tensor(""optimize/Cast:0"", shape=(?,),
  dtype=float32).</p>
</blockquote>
",0
57438371,tf.global_variable_initializer() with regard to session?,"<p>My understandings on Sessions in Tensorflow still seem to be flawed even after reading the <a href=""https://www.tensorflow.org/guide/graphs"" rel=""nofollow noreferrer"">official documentation</a> and <a href=""https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/"" rel=""nofollow noreferrer"">this</a> tutorial. </p>

<p>In particular, does <code>tf.global_variable_initializer()</code> initialize global variables with regard to a particular session, or for all the sessions in the program? Are there ways to ""uninitialize"" a variable in / during a session?</p>

<p>Can a <code>tf.variable</code> be used in multiple sessions? The answer seems to be yes (e.g. the following code), but then are there good cases where we want multiple sessions in a program, instead of a single one?</p>

<pre><code>#!/usr/bin/env python
import tensorflow as tf

def main():
    x = tf.constant(0.)
    with tf.Session() as sess:
        print(sess.run(x))
    with tf.Session() as sess:
        print(sess.run(x))

if __name__ == '__main__':
    main()
</code></pre>
",1
57449484,What is trainable parameter in tensorflow?,"<p>tf.compat.v1.layers.batch_normalization takes <code>trainable</code> as an input. The documentation says:</p>

<blockquote>
  <p>Boolean, if True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable).</p>
</blockquote>

<p>I think only scaling factor (gamma) and offset (beta) should be added to trainable variables and I am skeptical if even moving averages will get added to GraphKeys.TRAINABLE_VARIABLES. Can somebody tell me how trainable input is influencing the behavior of batch_normalization</p>
",1
57460127,Tensorflow 2 creating custom dataset,"<p>I am trying to build a custom dataset-loader, which laods <a href=""https://rrc.cvc.uab.es/?ch=4"" rel=""nofollow noreferrer"">ICDAR</a>-Dataset.
My frist step was to embed a dataset inside my loader as suggested also
<a href=""https://stackoverflow.com/questions/54373780/create-tensorflow-dataset-with-custom-file-format"">here</a> in this post, but the problem is that you have to implement all the nice features that the tenfsoflow-2 class ""Dataset"" offers manually.</p>

<p>My second try was to subclass the Dataset-Class, something like:</p>

<pre><code>class MyDataset(tf.data.Dataset):
  def __init__(self):
    super(MyDataset, self).init()

  def preprocess_images(self):
    pass
</code></pre>

<p>But the problem is i did not find any documentation what dataset-class internally really does, the only implementation i found was <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/dataset_ops.py#L2349"" rel=""nofollow noreferrer"">this one</a>.</p>

<p>So question is does anybody know how to build a custom ""dataset"" in tf2 by subclassing tf.data.Dataset.</p>

<p>By the way i also tried tensorflow_datasets, bit it does not really worked, shince it will downlaod the dataset, and split them manually which is in this is alreay seperated by train and test and also ICDAr can not be downlaoded without registration.</p>

<p><strong>The content of the ICDAR-Dataset is as following:</strong></p>

<blockquote>
  <p>An Image </p>
  
  <p>A List of all texts in each image </p>
  
  <p>A List of Bouding-boxes for each text in each image</p>
</blockquote>

<p><strong>Image:</strong> 
@<a href=""https://rrc.cvc.uab.es/?ch=4"" rel=""nofollow noreferrer"">https://rrc.cvc.uab.es/?ch=4</a> owns the copyrights of this image.
<a href=""https://i.stack.imgur.com/YCVX2.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YCVX2.jpg"" alt=""enter image description here""></a></p>

<p>Words and bounding boxes for the above image:</p>

<pre><code>377,117,463,117,465,130,378,130,Genaxis Theatre
493,115,519,115,519,131,493,131,[06]
374,155,409,155,409,170,374,170,###
492,151,551,151,551,170,492,170,62-03
376,198,422,198,422,212,376,212,Carpark
494,190,539,189,539,205,494,206,###
374,1,494,0,492,85,372,86,###
</code></pre>

<p>Thanks
does anyone know how to </p>
",1
57481282,How to use multiple GPU for a DCGAN using Tensorflow 2.0 - RuntimeError: Replica-local variables may only be assigned in a replica context,"<p>I would like to develop a DCGAN with resolution of 256x256. To do so I need to use multiple GPU since only one it is not enough and it will probably take too much time.</p>

<p>I followed the procedure explained in the documentation at this link
<a href=""https://www.tensorflow.org/beta/guide/distribute_strategy"" rel=""nofollow noreferrer"">https://www.tensorflow.org/beta/guide/distribute_strategy</a></p>

<p>At the top of the script I used</p>

<p><code>strategy = tf.distribute.MirroredStrategy()</code></p>

<p>Then inside the Generator, Discriminator, and Loss functions I used</p>

<p><code>with strategy.scope():</code></p>

<p>The error I get is:</p>

<p><code>RuntimeError: Replica-local variables may only be assigned in a replica context.</code></p>

<pre class=""lang-py prettyprint-override""><code>
strategy = tf.distribute.MirroredStrategy()

path = '/my/dataset/path/'
file_paths = [f for f in glob.glob(path + ""**/*.jpg"", recursive=True)]

tensor_data = np.zeros((len(file_paths), 256, 256, 3)).astype('float32')

for i in range(len(file_paths)): 
  img_tensor = tf.image.decode_image(tf.io.read_file(file_paths[i]))
  tensor_data[i] = img_tensor

for i in range(tensor_data.shape[0]):
  tensor_data[i] = ((tensor_data[i] - 127.5) / 127.5)

BUFFER_SIZE = len(file_paths)
BATCH_SIZE = 256

train_dataset = tf.data.Dataset.from_tensor_slices(tensor_data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

def make_generator_model():
    with strategy.scope():
        model = tf.keras.Sequential()
        model.add(layers.Dense(64*64*1536, use_bias=False, input_shape=(100,)))
        model.add(layers.BatchNormalization())
        model.add(layers.LeakyReLU())

        model.add(layers.Reshape((64, 64, 1536)))
        assert model.output_shape == (None, 64, 64, 1536) # Note: None is the batch size

        model.add(layers.Conv2DTranspose(1536, (5, 5), strides=(1, 1), padding='same', use_bias=False))
        assert model.output_shape == (None, 64, 64, 1536)
        model.add(layers.BatchNormalization())
        model.add(layers.LeakyReLU())

        model.add(layers.Conv2DTranspose(768, (5, 5), strides=(2, 2), padding='same', use_bias=False))
        assert model.output_shape == (None, 128, 128, 768)
        model.add(layers.BatchNormalization())
        model.add(layers.LeakyReLU())

        model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
        assert model.output_shape == (None, 256, 256, 3)

        return model

generator = make_generator_model()

noise = tf.random.normal([1, 100])
generated_image = generator(noise, training=False)

sample = generated_image[0, :, :, :];
sample = tf.cast(sample, tf.int32)

plt.imshow(sample, cmap=None)

def make_discriminator_model():
    with strategy.scope():
        model = tf.keras.Sequential()
        model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same', input_shape=[256, 256, 3]))
        model.add(layers.LeakyReLU())
        model.add(layers.Dropout(0.3))

        model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
        model.add(layers.LeakyReLU())
        model.add(layers.Dropout(0.3))

        model.add(layers.Flatten())
        model.add(layers.Dense(1))

    return model

discriminator = make_discriminator_model()
decision = discriminator(generated_image)
print (decision)

cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real_output, fake_output):
    with strategy.scope():
        real_loss = cross_entropy(tf.ones_like(real_output), real_output)
        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
        total_loss = real_loss + fake_loss
        return total_loss

def generator_loss(fake_output):
    with strategy.scope():
        return cross_entropy(tf.ones_like(fake_output), fake_output)

generator_optimizer = tf.keras.optimizers.Adam(1e-4)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

checkpoint_dir = './training_checkpoints/'
checkpoint_prefix = os.path.join(checkpoint_dir, ""ckpt"")
checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,
                                 discriminator_optimizer=discriminator_optimizer,
                                 generator=generator,
                                 discriminator=discriminator)

EPOCHS = 2000
noise_dim = 100
num_examples_to_generate = 16

seed = tf.random.normal([num_examples_to_generate, noise_dim])

# Notice the use of `tf.function`
# This annotation causes the function to be ""compiled"".
@tf.function
def train_step(images):
    noise = tf.random.normal([BATCH_SIZE, noise_dim])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = generator(noise, training=True)

        real_output = discriminator(images, training=True)
        fake_output = discriminator(generated_images, training=True)

        gen_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

def train(dataset, epochs):
  for epoch in range(epochs):
    start = time.time()

    for image_batch in dataset:
      train_step(image_batch)

    # Produce images for the GIF as we go
    display.clear_output(wait=True)
    generate_and_save_images(generator,
                             epoch + 1,
                             seed)

    # Save the model every 15 epochs
    os.makedirs(os.path.dirname(checkpoint_prefix), exist_ok=True)

    if (epoch + 1) % 50 == 0:
      checkpoint.save(file_prefix = checkpoint_prefix)

    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))

  # Generate after the final epoch
  display.clear_output(wait=True)
  generate_and_save_images(generator,epochs,seed)

def generate_and_save_images(model, epoch, test_input):
    # Notice `training` is set to False.
    # This is so all layers run in inference mode (batchnorm).
    predictions = model(test_input, training=False)

    fig = plt.figure(figsize=(4,4))

    for i in range(predictions.shape[0]):
      plt.subplot(8, 8, i+1)
      sample = predictions[i, :, :, :] * 127.5 + 127.5
      sample = tf.cast(sample, tf.int32)
      plt.imshow(sample, cmap=None)
      plt.axis('off')

    filename = './screens/eye-256x256/1/image_at_epoch_{:04d}.png'
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    if (epoch + 1) % 10 == 0:
        plt.savefig(filename.format(epoch))
        plt.show()

get_ipython().run_cell_magic('time', '', 'train(train_dataset, EPOCHS)')
</code></pre>

<p>The error is the following</p>

<pre><code>Executing op ExperimentalRebatchDataset in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op ExperimentalAutoShardDataset in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op OptimizeDataset in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op ModelDataset in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op MultiDeviceIterator in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op MultiDeviceIteratorInit in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op MultiDeviceIteratorToStringHandle in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op GeneratorDataset in device /job:localhost/replica:0/task:0/device:GPU:1
Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op PrefetchDataset in device /job:localhost/replica:0/task:0/device:GPU:1
Executing op AnonymousIteratorV2 in device /job:localhost/replica:0/task:0/device:GPU:1
Executing op MakeIterator in device /job:localhost/replica:0/task:0/device:GPU:1
Executing op IteratorGetNextSync in device /job:localhost/replica:0/task:0/device:GPU:0
Executing op IteratorGetNextSync in device /job:localhost/replica:0/task:0/device:GPU:1
Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0
Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:GPU:1
Executing op DeleteIterator in device /job:localhost/replica:0/task:0/device:GPU:0
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;timed exec&gt; in &lt;module&gt;

&lt;ipython-input-20-88a9879432c7&gt; in train(dataset, epochs)
      4 
      5     for image_batch in dataset:
----&gt; 6       train_step(image_batch)
      7 
      8     # Produce images for the GIF as we go

/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    414     # This is the first call of __call__, so we have to initialize.
    415     initializer_map = {}
--&gt; 416     self._initialize(args, kwds, add_initializers_to=initializer_map)
    417     if self._created_variables:
    418       try:

/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    357     self._concrete_stateful_fn = (
    358         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 359             *args, **kwds))
    360 
    361     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   1358     if self.input_signature:
   1359       args, kwargs = None, None
-&gt; 1360     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   1361     return graph_function
   1362 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   1646       graph_function = self._function_cache.primary.get(cache_key, None)
   1647       if graph_function is None:
-&gt; 1648         graph_function = self._create_graph_function(args, kwargs)
   1649         self._function_cache.primary[cache_key] = graph_function
   1650       return graph_function, args, kwargs

/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   1539             arg_names=arg_names,
   1540             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 1541             capture_by_value=self._capture_by_value),
   1542         self._function_attributes)
   1543 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    714                                           converted_func)
    715 
--&gt; 716       func_outputs = python_func(*func_args, **func_kwargs)
    717 
    718       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.5/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    307         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    308         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 309         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    310     weak_wrapped_fn = weakref.ref(wrapped_fn)
    311 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    704           except Exception as e:  # pylint:disable=broad-except
    705             if hasattr(e, ""ag_error_metadata""):
--&gt; 706               raise e.ag_error_metadata.to_exception(type(e))
    707             else:
    708               raise

RuntimeError: in converted code:

    &lt;ipython-input-19-d2ffe8a85706&gt;:9 train_step  *
        generated_images = generator(noise, training=True)
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py:667 __call__
        outputs = call_fn(inputs, *args, **kwargs)
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/sequential.py:248 call
        return super(Sequential, self).call(inputs, training=training, mask=mask)
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py:753 call
        return self._run_internal_graph(inputs, training=training, mask=mask)
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/network.py:895 _run_internal_graph
        output_tensors = layer(computed_tensors, **kwargs)
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py:667 __call__
        outputs = call_fn(inputs, *args, **kwargs)
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/normalization.py:782 call
        self.add_update(mean_update)
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:507 new_func
        return func(*args, **kwargs)
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py:1095 add_update
        update()
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/normalization.py:775 mean_update
        return tf_utils.smart_cond(training, true_branch, false_branch)
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/tf_utils.py:58 smart_cond
        pred, true_fn=true_fn, false_fn=false_fn, name=name)
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/smart_cond.py:54 smart_cond
        return true_fn()
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/normalization.py:773 &lt;lambda&gt;
        true_branch = lambda: _do_update(self.moving_mean, new_mean)
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/normalization.py:769 _do_update
        inputs_size)
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/normalization.py:458 _assign_moving_average
        return state_ops.assign_sub(variable, update_delta, name=scope)
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/state_ops.py:164 assign_sub
        return ref.assign_sub(value)
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/distribute/values.py:1394 assign_sub
        _assert_replica_context(self._distribute_strategy)
    /usr/local/lib/python3.5/dist-packages/tensorflow/python/distribute/values.py:1381 _assert_replica_context
        ""Replica-local variables may only be assigned in a replica context."")

    RuntimeError: Replica-local variables may only be assigned in a replica context.
</code></pre>
",0
57517992,Can I use dictionary in keras customized model?,"<p>I recently read a paper about UNet++，and I want to implement this structure with tensorflow-2.0 and keras customized model. As the structure is so complicated, I decided to manage the keras layers by a dictionary. Everything went well in training, but an error occurred while saving the model. Here is a minimum code to show the error:</p>

<pre><code>class DicModel(tf.keras.Model):
    def __init__(self):
        super(DicModel, self).__init__(name='SequenceEECNN')
        self.c = {}
        self.c[0] = tf.keras.Sequential([
            tf.keras.layers.Conv2D(32, 3,activation='relu',padding='same'),
            tf.keras.layers.BatchNormalization()]
        )
        self.c[1] = tf.keras.layers.Conv2D(3,3,activation='softmax',padding='same')
    def call(self,images):
        x = self.c[0](images)
        x = self.c[1](x)
        return x

X_train,y_train = load_data()
X_test,y_test = load_data()

class_weight.compute_class_weight('balanced',np.ravel(np.unique(y_train)),np.ravel(y_train))

model = DicModel()
model_name = 'test'
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs/'+model_name+'/')
early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=100,mode='min')

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss=tf.keras.losses.sparse_categorical_crossentropy,
              metrics=['accuracy'])

results = model.fit(X_train,y_train,batch_size=4,epochs=5,validation_data=(X_test,y_test),
                    callbacks=[tensorboard_callback,early_stop_callback],
                    class_weight=[0.2,2.0,100.0])

model.save_weights('model/'+model_name,save_format='tf')
</code></pre>

<p>The error information is:</p>

<pre><code>Traceback (most recent call last):

  File ""/media/xrzhang/Data/ZHS/Research/CNN-TF2/learn_tf2/test_model.py"", line 61, in \&lt;module&gt;

    model.save_weights('model/'+model_name,save_format='tf')

  File ""/media/xrzhang/Data/ZHS/Research/CNN-TF2/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 1328, in save_weights

    self.\_trackable_saver.save(filepath, session=session)

  File ""/media/xrzhang/Data/ZHS/Research/CNN-TF2/venv/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py"", line 1106, in save

    file_prefix=file_prefix_tensor, object_graph_tensor=object_graph_tensor)

  File ""/media/xrzhang/Data/ZHS/Research/CNN-TF2/venv/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py"", line 1046, in \_save_cached_when_graph_building

    object_graph_tensor=object_graph_tensor)

  File ""/media/xrzhang/Data/ZHS/Research/CNN-TF2/venv/lib/python3.6/site-packages/tensorflow/python/training/tracking/util.py"", line 1014, in \_gather_saveables

    feed_additions) = self.\_graph_view.serialize_object_graph()

  File ""/media/xrzhang/Data/ZHS/Research/CNN-TF2/venv/lib/python3.6/site-packages/tensorflow/python/training/tracking/graph_view.py"", line 379, in serialize_object_graph

    trackable_objects, path_to_root = self.\_breadth_first_traversal()

  File ""/media/xrzhang/Data/ZHS/Research/CNN-TF2/venv/lib/python3.6/site-packages/tensorflow/python/training/tracking/graph_view.py"", line 199, in \_breadth_first_traversal

    for name, dependency in self.list_dependencies(current_trackable):

  File ""/media/xrzhang/Data/ZHS/Research/CNN-TF2/venv/lib/python3.6/site-packages/tensorflow/python/training/tracking/graph_view.py"", line 159, in list_dependencies

    return obj.\_checkpoint_dependencies

  File ""/media/xrzhang/Data/ZHS/Research/CNN-TF2/venv/lib/python3.6/site-packages/tensorflow/python/training/tracking/data_structures.py"", line 690, in \_\_getattribute\_\_

    return object.\_\_getattribute\_\_(self, name)

  File ""/media/xrzhang/Data/ZHS/Research/CNN-TF2/venv/lib/python3.6/site-packages/tensorflow/python/training/tracking/data_structures.py"", line 732, in \_checkpoint_dependencies

    ""ignored."" % (self,))

ValueError: Unable to save the object {0: \&lt;tensorflow.python.keras.engine.sequential.Sequential object at 0x7fb5c6c36588&gt;, 1: \&lt;tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7fb5c6c36630&gt;} (a dictionary wrapper constructed automatically on attribute assignment). The wrapped dictionary contains a non-string key which maps to a trackable object or mutable data structure.



If you don't need this dictionary checkpointed, wrap it in a tf.contrib.checkpoint.NoDependency object; it will be automatically un-wrapped and subsequently ignored.

</code></pre>

<p>The tf.contrib.checkpoint.NoDependency seems has been removed from Tensorflow-2.0 (<a href=""https://medium.com/tensorflow/whats-coming-in-tensorflow-2-0-d3663832e9b8"" rel=""nofollow noreferrer"">https://medium.com/tensorflow/whats-coming-in-tensorflow-2-0-d3663832e9b8</a>). How can I fix this issue? Or should I just give up using dictionary in customized Keras Model. Thank you for your time and helps!</p>
",0
57527609,How to show in tensorboard the tf.data.Dataset.map subgraph in Tensorflow 2.0?,"<p>According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map"" rel=""nofollow noreferrer"">documentation</a>, the <code>tf.data.Datasets</code> work in graph mode (in both eager and graph mode):</p>

<blockquote>
  <p>Note that irrespective of the context in which map_func is defined (eager vs. graph), tf.data traces the function and executes it as a graph</p>
</blockquote>

<p>In Tensorflow 1.X, we can easily plot this graph in Tensorboard: The processing functions are plotted in a subgraph.</p>

<p>For example,</p>

<pre><code>def _parse_function(x):
    return x * 2

x = tf.constant([0 , 1])
dataset = tf.data.Dataset.from_tensor_slices(x)
dataset = dataset.map(_parse_function)
</code></pre>

<p>In Tensorboard, a subgraph appears, corresponding to the _parse_function:
<a href=""https://i.stack.imgur.com/ykIRx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ykIRx.png"" alt=""enter image description here""></a></p>

<p>However, in Tensorflow 2.0, this does not produce any visible element in the Tensorboard graph.
The following code does not produce any graph according to Tensorboard:</p>

<pre><code>def _parse_function(x):
    return x * 2

logdir = 'logs'
writer = tf.summary.create_file_writer(logdir)

tf.summary.trace_on(graph=True, profiler=True)
x = tf.constant([0 , 1])
dataset = tf.data.Dataset.from_tensor_slices(x)
dataset = dataset.map(_parse_function)

with writer.as_default():
  tf.summary.trace_export(
      name=""trace"",
      step=0,
      profiler_outdir=logdir)
</code></pre>

<p>So, since a graph is created when calling the <code>map</code>, is there a way to access/visualize this graph?</p>
",0
57529534,How is tf.data.Dataset use optimised by tf.function in Tensorflow 2.0?,"<p>The <a href=""https://www.tensorflow.org/beta/guide/effective_tf2#combine_tfdatadatasets_and_tffunction"" rel=""noreferrer"">official documentation</a> of Tensorflow 2.0 advices to use <code>tf.data.Dataset</code> along with <code>tf.function</code>.</p>

<p>There are two examples of such uses:</p>

<ul>
<li>Using a <code>Dataset</code> as an argument of a <code>tf.function</code>, as described <a href=""https://www.tensorflow.org/beta/guide/effective_tf2#combine_tfdatadatasets_and_tffunction"" rel=""noreferrer"">here</a>:</li>
</ul>

<pre class=""lang-py prettyprint-override""><code>@tf.function
def train(model, dataset, optimizer):
  for x, y in dataset:
      ....
</code></pre>

<ul>
<li>Creating a local <code>Dataset</code> in a <code>tf.function</code> body, as described <a href=""https://www.tensorflow.org/beta/guide/autograph"" rel=""noreferrer"">here</a>:</li>
</ul>

<pre class=""lang-py prettyprint-override""><code>@tf.function
def train(model, optimizer):
  train_ds = mnist_dataset()
  ...
</code></pre>

<p>Finally, the <a href=""https://www.tensorflow.org/beta/tutorials/eager/tf_function#autograph_and_loops"" rel=""noreferrer"">autograph doc</a> states that the iterating on a <code>Dataset</code> is optimized by <code>tf.function</code>.
<a href=""https://stackoverflow.com/questions/56038372/does-wrapping-tf-data-dataset-into-tf-function-improve-performance"">This SO answer</a> shows indeed that using a <code>Dataset</code> as a parameter of <code>tf.function</code> increases performances.</p>

<p>So, how does <code>tf.data.Dataset</code> benefit from <code>tf.function</code>, and how can it explain the speedup of <a href=""https://stackoverflow.com/questions/56038372/does-wrapping-tf-data-dataset-into-tf-function-improve-performance"">this SO answer</a>:</p>

<ul>
<li>How is the <a href=""https://www.tensorflow.org/beta/tutorials/eager/tf_function#autograph_and_loops"" rel=""noreferrer"">""foor loop optimization""</a> creating a speedup by itself?</li>
<li>How is the <code>Dataset</code> object treated by the <code>tf.function</code> tracer. Like in the parameter or as a local variable examples, how can we use <code>Dataset</code> in a <code>tf.function</code>, even if the <a href=""https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function"" rel=""noreferrer""><code>tf.function</code> documentation</a> suggests that the original function should only accept <code>Tensor</code>?</li>
<li>Is there some kind of communication/sharing between the <code>Dataset</code> subgraph (that can then be used in <code>tf.function</code> subgraph), or the two are fully separated (meaning that <code>Dataset</code> is just used as an black-box iterator from <code>tf.function</code>)? </li>
</ul>
",0
57588164,Can't choose framework for segmentation_models library in python,"<p>I am trying to build a ResNet34 model using <strong>segmentation_models</strong>(sm) library in python. The <strong>sm</strong> library uses <strong>keras framework</strong> by default when importing, but I am working with <strong>tf.keras</strong> to build my datasets used for training and testing.</p>

<p>The documentation says that in order to change the default framework I should either use an environmental variable <code>SM_FRAMEWORK=tf.keras</code> before importing (which I tried but it didn't work) or set it using the method <code>set_framework</code> (which doesn't show up in the suggestions/it says it doesn't exist when I try to execute it).</p>

<p>Is there any other way to overcome this problem?</p>
",1
57651287,How to swap tensor axes efficiently in tensorflow?,"<p>I have to swap tensor's axes using <code>tf.transpose</code> to do the batch matrix multiplication (as the code shown below). </p>

<p>tensor input_a: shape [10000, 10000] </p>

<p>tensor input_b: shape [batch_size, 10000, 10] </p>

<p>tensor output:  shape [batch_size, 10000, 10] </p>

<pre><code># reshape_input_b: shape [10000, batch_size, 10]
transpose_input_b = tf.transpose(input_b, [1, 0, 2])

# transpose_input_b : shape [10000, batch_size * 10]
reshape_input_b = tf.reshape(transpose_input_b , [10000, -1])

# ret: shape [10000, batch_size * 10]
ret = tf.matmul(input_a, reshape_input_b, a_is_sparse = True)

# reshape_ret: [10000, batch_size, 10]
reshape_ret = tf.reshape(ret, [10000, -1, 10])

# output : [batch_size, 10000, 10]
output = tf.transpose(reshape_ret, [1, 0, 2])
</code></pre>

<p>However, it seems very slow. I noticed this in the document page of <code>tf.transpose</code>:</p>

<blockquote>
  <p>In numpy transposes are memory-efficient constant time operations as they simply return a new view of the same data with adjusted strides.</p>
  
  <p>TensorFlow does not support strides, <b>so transpose returns a new tensor with the items permuted</b>.</p>
</blockquote>

<p>So, I think it might be the reason why my code run slowly? Is there any way to swap tensor's axes, or do the batch matrix multiplication efficiently?</p>
",1
57717004,Tensorflow: Modern way to load large data,"<p>I want to train a convolutional neural network (using tf.keras from Tensorflow version 1.13) using numpy arrays as input data. The training data (which I currently store in a single &gt;30GB '.npz' file) does not fit in RAM all at once. <strong>What is the best way to save and load large data-sets into a neural network for training?</strong> Since I didn't manage to find a good answer to this (surely ubiquitous?) problem, I'm hoping to hear one here. Thank you very much in advance for any help!</p>
<h3>Sources</h3>
<p>Similar questions seem to have been asked many times (e.g. <a href=""https://stackoverflow.com/questions/49169016/training-classifier-from-tfrecords-in-tensorflow"">training-classifier-from-tfrecords-in-tensorflow</a>, <a href=""https://stackoverflow.com/questions/40467990/tensorflow-synchronize-readings-from-tfrecord"">tensorflow-synchronize-readings-from-tfrecord</a>, <a href=""https://stackoverflow.com/questions/51357223/how-to-load-data-parallelly-in-tensorflow"">how-to-load-data-parallelly-in-tensorflow</a>) but are several years old and usually contain no conclusive answer.</p>
<p>My current understanding is that using TFRecord files is a good way to approach this problem. The most promising tutorial I found so far explaining how to use TFRecord files with keras is <a href=""https://medium.com/@moritzkrger/speeding-up-keras-with-tfrecord-datasets-5464f9836c36"" rel=""noreferrer"">medium.com</a>. Other helpful sources were <a href=""http://machinelearninguru.com/deep_learning/data_preparation/tfrecord/tfrecord.html"" rel=""noreferrer"">machinelearninguru.com</a> and <a href=""https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564"" rel=""noreferrer"">medium.com_source2</a> and sources therin.</p>
<p>The official tensorflow documentation and tutorials (on <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""noreferrer"">tf.data.Dataset</a>, <a href=""https://www.tensorflow.org/guide/datasets"" rel=""noreferrer"">Importing Data</a>, <a href=""https://www.tensorflow.org/tutorials/load_data/tf_records"" rel=""noreferrer"">tf_records</a> etc.) did not help me. In particular, several of the examples given there didn't work for me even without modifications.</p>
<h3>My Attempt at using TFRecord files</h3>
<p>I'm assuming TFRecords are a good way to solve my problem but I'm having a hard time using them. Here is an example I made based on the tutorial <a href=""https://medium.com/@moritzkrger/speeding-up-keras-with-tfrecord-datasets-5464f9836c36"" rel=""noreferrer"">medium.com</a>. I stripped down the code as much as I could.</p>
<pre><code># python 3.6, tensorflow 1.13.
# Adapted from https://medium.com/@moritzkrger/speeding-up-keras-with-tfrecord-datasets-5464f9836c36
import tensorflow as tf
import numpy as np
from tensorflow.python import keras as keras


# Helper functions (see also https://www.tensorflow.org/tutorials/load_data/tf_records)
def _int64_feature(value):
    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))


def _bytes_feature(value):
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))


def writeTFRecords():
    number_of_samples = 100  # create some random data to play with
    images, labels = (np.random.sample((number_of_samples, 256, 256, 1)), np.random.randint(0, 30, number_of_samples))

    writer = tf.python_io.TFRecordWriter(&quot;bla.tfrecord&quot;)

    for index in range(images.shape[0]):
        image = images[index]
        label = labels[index]

        feature = {'image':  _bytes_feature(tf.compat.as_bytes(image.tostring())),
                   'label':  _int64_feature(int(label))}

        example = tf.train.Example(features=tf.train.Features(feature=feature))
        writer.write(example.SerializeToString())
    writer.close()


def loadTFRecord(data_path):
    with tf.Session() as sess:
        feature = {'train/image': tf.FixedLenFeature([], tf.string),
                   'train/label': tf.FixedLenFeature([], tf.int64)}
        # Create a list of filenames and pass it to a queue
        filename_queue = tf.train.string_input_producer([data_path], num_epochs=1)
        # Define a reader and read the next record
        reader = tf.TFRecordReader()
        _, serialized_example = reader.read(filename_queue)
        # Decode the record read by the reader
        features = tf.parse_single_example(serialized_example, features=feature)
        # Convert the image data from string back to the numbers
        image = tf.decode_raw(features['train/image'], tf.float32)

        # Cast label data into int32
        label = tf.cast(features['train/label'], tf.int32)
        # Reshape image data into the original shape
        image = tf.reshape(image, [256, 256, 1])

        return image, label  # I'm not 100% sure that's how this works...


# ######### generate a TFRecords file in the working directory containing random data. #################################
writeTFRecords()
# ######## Load the TFRecords file and use it to train a simple example neural network. ################################
image, label = loadTFRecord(&quot;bla.tfrecord&quot;)

model_input = keras.layers.Input(tensor=image)
model_output = keras.layers.Flatten(input_shape=(-1, 256, 256, 1))(model_input)
model_output = keras.layers.Dense(16, activation='relu')(model_output)

train_model = keras.models.Model(inputs=model_input, outputs=model_output)
train_model.compile(optimizer=keras.optimizers.RMSprop(lr=0.0001),  
                    loss='mean_squared_error',
                    target_tensors=[label])

print(&quot;\n \n start training \n \n&quot;) # Execution gets stuck on fitting
train_model.fit(epochs=1, steps_per_epoch=10)  # no output or error messages.

</code></pre>
<p>The code creates a TFRecord file and starts fitting, then just gets stuck with no output or error messages. I don't know what the problem is or how I could try to fix it.</p>
",1
57719398,Unable to save model with tensorflow 2.0.0 beta1,"<p>I have tried all the options described in the documentation but none of them allowed me to save my model in tensorflow 2.0.0 beta1. I've also tried to upgrade to the (also unstable) TF2-RC but that ruined even the code I had working in beta so I quickly rolled back for now to beta.</p>

<p>See a minimal reproduction code below.</p>

<p>What I have tried: </p>

<ol>
<li><pre><code>model.save(""mymodel.h5"") 
</code></pre></li>
</ol>

<blockquote>
  <p>NotImplementedError: Saving the model to HDF5 format requires the
  model to be a Functional model or a Sequential model. It does not work
  for subclassed models, because such models are defined via the body of
  a Python method, which isn't safely serializable. Consider saving to
  the Tensorflow SavedModel format (by setting save_format=""tf"") or
  using <code>save_weights</code>.</p>
</blockquote>

<ol start=""2"">
<li><pre><code>model.save(""mymodel"", format='tf')
</code></pre></li>
</ol>

<blockquote>
  <p>ValueError: Model &lt;<strong>main</strong>.CVAE object at 0x7f1cac2e7c50> cannot be
  saved because the input shapes have not been set. Usually, input
  shapes are automatically determined from calling .fit() or .predict().
  To manually set the shapes, call model._set_inputs(inputs).</p>
</blockquote>

<p>3.</p>

<pre><code>model._set_input(input_sample)
model.save(""mymodel"", format='tf') 
</code></pre>

<blockquote>
  <p>AssertionError: tf.saved_model.save is not supported inside a traced
  @tf.function. Move the call to the outer eagerly-executed context.</p>
</blockquote>

<p>And this is where I am stuck now because it gives me no reasonable hint whatsoever. That's because I am NOT calling the save() function from a @tf.function, I'm already calling it from the outermost scope possible. In fact, I have no @tf.function at all in this minimal reproduction script below and still getting the same error.</p>

<p>So I really have no idea how to save my model, I've tried every options and they all throw errors and provide no hints.</p>

<p>The minimal reproduction example below works fine if you set save_model=False and it reproduces the error when save_model=True. </p>

<p>It may seem unnecessary in this simplified auto-encoder code example to use a subclassed model but I have lots of custom functions added to it in my original VAE code that I need it for.</p>

<p>Code:</p>

<pre><code>import tensorflow as tf

save_model = True

learning_rate = 1e-4
BATCH_SIZE = 100
TEST_BATCH_SIZE = 10
color_channels = 1
imsize = 28

(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()

train_images = train_images[:5000, ::]
test_images = train_images[:1000, ::]
train_images = train_images.reshape(-1, imsize, imsize, 1).astype('float32')
test_images = test_images.reshape(-1, imsize, imsize, 1).astype('float32')
train_images /= 255.
test_images /= 255.
train_dataset = tf.data.Dataset.from_tensor_slices(train_images).batch(BATCH_SIZE)
test_dataset = tf.data.Dataset.from_tensor_slices(test_images).batch(TEST_BATCH_SIZE)

class AE(tf.keras.Model):
    def __init__(self):
        super(AE, self).__init__()
        self.network = tf.keras.Sequential([
            tf.keras.layers.InputLayer(input_shape=(imsize, imsize, color_channels)),
            tf.keras.layers.Flatten(),
            tf.keras.layers.Dense(50),
            tf.keras.layers.Dense(imsize**2 * color_channels),
            tf.keras.layers.Reshape(target_shape=(imsize, imsize, color_channels)),
        ])
    def decode(self, input):
        logits = self.network(input)
        return logits

optimizer = tf.keras.optimizers.Adam(learning_rate)
model = AE()

def compute_loss(data):
    logits = model.decode(data)
    loss = tf.reduce_mean(tf.losses.mean_squared_error(logits, data))
    return loss

def train_step(data):
    with tf.GradientTape() as tape:
        loss = compute_loss(data)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss, 0

def test_step(data):
    loss = compute_loss(data)
    return loss

input_shape_set = False
epoch = 0
epochs = 20
for epoch in range(epochs):
    for train_x in train_dataset:
        train_step(train_x)
    if epoch % 1 == 0:
        loss = 0.0
        num_batches = 0
        for test_x in test_dataset:
            loss += test_step(test_x)
            num_batches += 1
        loss /= num_batches
        print(""Epoch: {}, Loss: {}"".format(epoch, loss))

        if save_model:
            print(""Saving model..."")
            if not input_shape_set:
                # Note: Why set input shape manually and why here:
                # 1. If I do not set input shape manually: ValueError: Model &lt;main.CVAE object at 0x7f1cac2e7c50&gt; cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling .fit() or .predict(). To manually set the shapes, call model._set_inputs(inputs).
                # 2. If I set input shape manually BEFORE the first actual train step, I get: RuntimeError: Attempting to capture an EagerTensor without building a function.
                model._set_inputs(train_dataset.__iter__().next())
                input_shape_set = True
            # Note: Why choose tf format: model.save('MNIST/Models/model.h5') will return NotImplementedError: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=""tf"") or using save_weights.
            model.save('MNIST/Models/model', save_format='tf')
</code></pre>
",1
57858794,"Why doesn't a Lambda function that returns the result of tf.py_func/tf.numpy_function have an output shape, and how can I correct this behavior?","<p>For example purposes, I've created two Python functions. One takes a value and just returns it; the other is a lambda function that returns a Tensor to evaluate the function:</p>

<pre><code>import tensorflow as tf
import numpy as np
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input,Lambda,Dense
def test_func(value):
  return value
test_func = np.vectorize(test_func)
test_op = lambda x: tf.numpy_function(test_func,[x],tf.float32)
input = Input(shape=(1,))
test_layer = Lambda(test_op)
model = Sequential()
model.add(input)
model.add(test_layer)
model.summary()
</code></pre>

<p>When I run this code on Tensorflow 1.14.x + Keras 2.2.x + Python 3.6.x, and even if I set the output_shape parameter of the Lambda layer, I get a Lambda layer with a ""None"" output shape:</p>

<pre class=""lang-none prettyprint-override""><code>Model: ""sequential_1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lambda_1 (Lambda)            None                      0         
=================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
_________________________________________________________________
</code></pre>

<p>When I test the code using a test tensor, I get the results I might expect:</p>

<pre><code>with tf.Session() as session:
    X = tf.constant([[1.0,2.0],[3.0,4.0]])
    Y = tf.constant(2.0)
    X_out = test_op(X)
    Y_out = test_op(Y)
    print(""X:"",session.run(X))
    print(""X shape: "",session.run(tf.shape(X_out)))
    print(""Y:"",session.run(Y_out))
    print(""Y shape: "", session.run(tf.shape(Y_out)))
</code></pre>

<p>Output:</p>

<blockquote>
  <p>X: [[1. 2.]  [3. 4.]]
  X shape:  [2 2]
  Y: 2.0
  Y shape:  <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda"" rel=""nofollow noreferrer"">1</a>
  <strong>EDIT:</strong></p>
</blockquote>

<p>For comparison purposes, I also created a neural network using the function found in <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda"" rel=""nofollow noreferrer"">this page of documentation</a>:</p>

<pre><code>from tensorflow.keras import Model
from tensorflow.keras.layers import Input,Lambda
input = Input(shape=(1,))
def antirectifier(x):
    x -= K.mean(x, axis=1, keepdims=True)
    x = K.l2_normalize(x, axis=1)
    pos = K.relu(x)
    neg = K.relu(-x)
    return K.concatenate([pos, neg], axis=1)
lambda_ = Lambda(antirectifier)
lambda_ = Lambda(antirectifier)(input)
model = Model(inputs=[input],outputs=[lambda_])
model.summary()
</code></pre>

<p>As somewhat expected, this prints:</p>

<pre class=""lang-none prettyprint-override""><code>Model: ""model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 1)]               0         
_________________________________________________________________
lambda_1 (Lambda)            (None, 2)                 0         
=================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
_________________________________________________________________
</code></pre>

<p>Now - if I understand correctly - both tf.py_func and K.concatenate return a TensorFlow Tensor object. So, the question is, Why does all the code I posted here work, EXCEPT my tf.numpy_function layer?</p>
",0
57914611,Has tf.Estimator become obsolete in Tensorflow 2.0?,"<p>Today I've set up a custom model using its tf.Estimator high-level API in Tensorflow 2.0.
It was a pain in the *** to get it running, and there are very few complete examples online that implement custom Estimators in Tensorflow 2, which made me questioning the reasons for using this API.</p>

<p>According to the docs, the main advantages of using the tf.Estimator API are:</p>

<ol>
<li><p>You can run Estimator-based models on a local host or on a distributed multi-server environment without changing your model. Furthermore, you can run Estimator-based models on CPUs, GPUs, or TPUs without recoding your model.</p></li>
<li><p>You no longer have to worry about creating the computational graph or sessions since Estimators handle all the ""plumbing"" for you</p></li>
</ol>

<p>Advantage 2. clearly doesn't apply to Tensorflow 2.0 anymore, as it runs in eager mode by default, so you don't have to worry about sessions anyways.</p>

<p>Advantage 1. also seems quite irrelevant in Tensorflow 2.0 - using tf.distribute.Strategy, you can now easily run even high level tf.Keras models in a distributed fashion and on CPUs/GPUs/TPUs.
tf.Keras models are so much easier and faster to set up, so why did they even bother to keep the tf.Estimator API in Tensorflow 2.0? Are there other advantages of using this API?</p>
",0
57921463,How tf.data.experimental.group_by_window() operates in Tensorflow 2.0,"<p>I am trying to understand the tf.data.experimental.group_by_window() method in Tensorflow 2 but I have some difficulties.</p>

<p>For a reproducible example I use the one presented in the documentation:</p>

<pre><code>components = np.arange(100).astype(np.int64)
dataset20 = tf.data.Dataset.from_tensor_slices(components)
dataset20 = dataset.apply(tf.data.experimental.group_by_window(key_func=lambda x: x%2, reduce_func=lambda _,\
                                                          els: els.batch(10), window_size=100))

i = 0

for elem in dataset20:

    print('i is {0}\n'.format(i))

    print('elem is {0}'.format(elem.numpy()))

    i += 1

    print('\n--------------------------------\n')

i is 0

elem is [0 2 4 6 8]

--------------------------------

i is 1

elem is [1 3 5 7 9]

--------------------------------
</code></pre>
",1
57925817,TensorFlow 2.0 scatter add,"<p>I would like to implement the following design in TensorFlow 2.0.</p>

<p>Given a <code>memory</code> tensor of shape <code>[a, b, c]</code>,<br>
an <code>indices</code> tensor of shape <code>[a, 1]</code>,<br>
and an <code>updates</code> tensor of shape <code>[a, c]</code>,  </p>

<p>I want to increment <code>memory</code> at the positions indicated by <code>indices</code> with the values in <code>updates</code>.</p>

<p><code>tf.tensor_scatter_nd_add</code> does not seem to work:</p>

<p><code>tf.tensor_scatter_nd_add(memory, indices, updates)</code> returns <code>{InvalidArgumentError}Inner dimensions of output shape must match inner dimensions of updates shape. Output: [a,b,c] updates: [a,c] [Op:TensorScatterAdd]</code>.</p>

<p>Is it really necessary for <code>updates</code> to have as many inner dimensions as <code>memory</code> ? In my logic, <code>memory[indices]</code> (as a pseudocode) should already be a tensor of shape <code>[a, c]</code>. Furthermore, the shape of <code>tf.gather_nd(params=memory, indices=indices, batch_dims=1)</code> is already <code>[a, c]</code>.</p>

<p>Could you please recommend an alternative ?</p>

<p>Thanks.</p>
",0
57926860,"tensorflow/serving - Input to reshape is a tensor with 100 values, but the requested shape has 10000","<p>When I train my tf.keras Functional API model and serve it with the tensorflow/serving docker image I'm getting a shape error when I call the API.</p>

<p><strong>How I build my model:</strong></p>

<pre><code>from tensorflow.keras.layers import Dense, DenseFeatures, Input
from tensorflow.keras.models import Model
from tensorflow import feature_column, string as tf_string
import os

feature_layer_inputs = {}
feature_columns = []
for header in ['categorical_one', 'categorical_two', 'categorical_three']:
    feature_columns.append(feature_column.indicator_column(
        feature_column.categorical_column_with_hash_bucket(header, hash_bucket_size=100)))
    feature_layer_inputs[header] = Input(shape=(1,), name=header, dtype=tf_string)


fc_layer = DenseFeatures(feature_columns)
fc = fc_layer(feature_layer_inputs)

''' Feature Columns to Dense and merge with attention output '''

fc = Dense(300, activation='relu')(fc)
fc = Dense(324, activation='relu')(fc)

pred = Dense(num_classes, activation='softmax')(fc)

inputs = [v for v in feature_layer_inputs.values()]

model = Model(inputs=inputs, outputs=pred)

model.compile(...)

model.fit(...)

saved_model_path = ""C:/Temp/saved_models/{}"".format(int(time.time()))
os.mkdir(saved_model_path)

model.save(saved_model_path)
</code></pre>

<p><strong>How my Serving Signature Definition looks like:</strong></p>

<pre><code>The given SavedModel SignatureDef contains the following input(s):
inputs['categorical_one'] tensor_info:
dtype: DT_STRING
shape: (-1, 1)
name: serving_default_categorical_one:0
inputs['categorical_two'] tensor_info:
dtype: DT_STRING
shape: (-1, 1)
name: serving_default_categorical_two:0
inputs['categorical_three'] tensor_info:
dtype: DT_STRING
shape: (-1, 1)
name: serving_default_categorical_three:0
The given SavedModel SignatureDef contains the following output(s):
outputs['dense'] tensor_info:
dtype: DT_FLOAT
shape: (-1, num_classes)
name: StatefulPartitionedCall:0
Method name is: tensorflow/serving/predict
</code></pre>

<p><strong>How I call the Serving API:</strong></p>

<pre><code>curl -d '{""instances"": [ {""categorical_one"": ""ABC"", ""categorical_two"": ""DEF"", ""categorical_three"": ""GHI""} ] }' -X POST http://localhost:8501/v1/models/my-model-name/versions/1:predict
</code></pre>

<p><strong>The error message I get:</strong></p>

<pre><code>{ ""error"": ""Input to reshape is a tensor with 100 values, but the requested shape has 10000\n\t [[{{node StatefulPartitionedCall/StatefulPartitionedCall/model/dense_features/categorical_one_indicator/Reshape}}]]"" }
</code></pre>

<p><em>Please notice that I just posted the crucial parts of my code and not every line.</em></p>

<p>Any idea is welcome!</p>
",0
57929803,What is the proper way to convert Tracing Code using RunOptions to Tensorflow 2.0?,"<p>I'm having difficulty finding any documentation on how to migrate tracing code from 1.x to 2.0.</p>

<p>In tensorflow 1.x you could do the following:</p>

<pre><code>run_options = tf.compat.v1.RunOptions(trace_level=tf.compat.v1.RunOptions.FULL_TRACE)
run_metadata = tf.compat.v1.RunMetadata()
final_result = sess.run(result, feed_dict={...},
                        options=run_options,
                        run_metadata=run_metadata)

trace = fetched_timeline = timeline.Timeline(run_metadata.step_stats)
chrome_trace = fetched_timeline.generate_chrome_trace_format()
with open('timeline.json', 'w') as f:
    f.write(chrome_trace)
</code></pre>

<p>How can you do a similar thing with a @tf.function call?</p>

<pre><code>@tf.function
def predict(x1, x2):
    ...
#=============
# Set run options and RunMetadata variables
#=============
#=============
final_result = predict(x1_val, x2_val)

#=============
# Dump Trace (assuming run_metadata is the RunMetaData object we configured previously)
#=============
trace = fetched_timeline = timeline.Timeline(run_metadata.step_stats)
chrome_trace = fetched_timeline.generate_chrome_trace_format()
with open('timeline.json', 'w') as f:
    f.write(chrome_trace)
#=============

</code></pre>
",1
57970717,Using pretrained convolutional network as a GAN discriminator,"<p>I've pulled some code from TF2.0 documentation to generate images from a custom dataset. The code is <a href=""https://www.tensorflow.org/beta/tutorials/generative/dcgan"" rel=""nofollow noreferrer"">here</a> </p>

<p>Since the documentation uses Keras i figured i might change the discriminator network to a pretrained network e.g InceptionV3, and only train the top layers. I've found <a href=""https://keras.io/applications/"" rel=""nofollow noreferrer"">this</a> code (Fine-tune InceptionV3 on a new set of classes). I cant seem to figure out how to replace the the one with the other. I understand that im trying to replace Sequential mode with the Functional API. But i guess they are somehow interconnected. However, im not a frequent Keras user.</p>

<p>My questions is: How do i replace a custom CNN in Sequential mode with a pretrained one from the Functional API to use as a discriminator?</p>

<p>EDIT: I would be happy if anyone has examples of doing it with the GANEstimator instead as im more used to TF.</p>

<p><strong>Use the generator to generate a random image</strong></p>

<pre><code>def make_generator_model():
    model = tf.keras.Sequential()
    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Reshape((7, 7, 256)))
    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size

    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))
    assert model.output_shape == (None, 7, 7, 128)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))
    assert model.output_shape == (None, 14, 14, 64)
    model.add(layers.BatchNormalization())
    model.add(layers.LeakyReLU())

    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))
    assert model.output_shape == (None, 28, 28, 3)

    return model

generator = make_generator_model()
noise = tf.random.normal([1, 100])
generated_image = generator(noise, training=False)
</code></pre>

<p><strong>The current discriminator and helpers (Outputs tf.Tensor([[-0.0003378]], shape=(1, 1), dtype=float32))</strong></p>

<pre><code>def make_discriminator_model():
    model = tf.keras.Sequential()
    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',
                                     input_shape=[28, 28, 1]))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
    model.add(layers.LeakyReLU())
    model.add(layers.Dropout(0.3))

    model.add(layers.Flatten())
    model.add(layers.Dense(1))

    return model

cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    total_loss = real_loss + fake_loss
    return total_loss

discriminator = make_discriminator_model()
decision = discriminator(generated_image)
print (decision)
</code></pre>

<p><strong>The desired discriminator</strong></p>

<pre><code>def make_discriminator_model():
    # create the base pre-trained model
    model = InceptionV3(weights='imagenet', include_top=False)

    # ADD TOP LAYERS

    # FREEZE ALL LAYERS EXCEPT TOP LAYERS

    return model

# COMPILE

def discriminator_loss(real_output, fake_output):
    real_loss = ??? # Real Loss
    fake_loss = ??? # Fake loss
    total_loss = real_loss + fake_loss
    return total_loss

noise = tf.random.normal([1, 100])
generated_image = generator(noise, training=False)

discriminator = make_discriminator_model()
decision = discriminator(generated_image)
print (decision)
</code></pre>

<p><strong>All imports</strong></p>

<pre><code>  from __future__ import absolute_import, division, print_function, unicode_literals

try:
  # %tensorflow_version only exists in Colab.
  %tensorflow_version 2.x
except Exception:
  pass
import tensorflow as tf
print('TF version: {}'.format(tf.__version__))

import glob
import imageio
import matplotlib.pyplot as plt
import numpy as np
import os
import PIL
from PIL import Image
from tensorflow.keras import layers
import time

from IPython import display
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications import vgg16
import os.path
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras import backend as K
</code></pre>

<p><strong>EDIT:
This was the discriminator i ended up with! Thanks to @pandrey</strong></p>

<pre><code>def make_discriminator_model():
    pre_trained = tf.keras.applications.InceptionV3(
        weights='imagenet', include_top=False, input_shape=IMG_SHAPE
    )
    pre_trained.trainable = False  # mark all weights as non-trainable
    model = tf.keras.Sequential([pre_trained])
    model.add(layers.GlobalAveragePooling2D())
    model.add(layers.Dense(1))   
    return model
</code></pre>
",1
57995171,Need a clear simple approach to distributed learning in Tensorflow/Keras,"<p>I have multiple machines, some with GPUs and some others not. I also have a keras model that works fine on a single machine but I want to train it in a distributed mode because I want to test it with a huge dataset and with a bigger number of layers.
There is quite a lot of pages discussing the distribution strategy in tf.distribute, but at the same time there are a lot other pages showing how to do it with encapsulating keras model with an estimator, setting up the TF_CONFIG parameter and then call <code>tf.estimator.train_and_evaluate</code>.
I used the second approach personally as it was more straightforward and am struggling to tune and debug it. It works anyway, but I am very confused what is the point in all of strategy-related stuff as I don't see any use them in the second approach, and the documentation is not helping to clear it.</p>

<p>I also have some doubt if my file setting environment is correct: My understanding is that the PS server is going to hold the model parameters and the chief server is going to administer the whole training process, distributing data, and saving summaries and checkpoints. So I assume that:</p>

<p>0- I need only one chief server, at least one PS server, possibly some workers, and one evaluator. The data and parameter sharing and communication between all of these servers is done by system and I am not engaged in it.</p>

<p>1- The main python code for all machines should be exactly the same, except in TF_CONFIG definition that defines the task and index for that specific machine.</p>

<p>2- I should have one shared copy of data in a folder available to all chief and workers.</p>

<p>3- I should have one shared log directory accessible by all machines as defined in tf.estimator.RunConfig.</p>

<p>4- Having this setting then a piece of code such as below will do the job (assuming the <code>model</code> has been defined elsewhere and the <code>read_datasets</code> function returns features and labels for running the model):</p>

<pre><code>runConfig = tf.estimator.RunConfig(
        session_config=config,
        model_dir=log_dir,
        save_summary_steps=1,
        save_checkpoints_steps=train_steps
        )
estimator = tf.keras.estimator.model_to_estimator(model, model_dir=log_dir, config=runConfig)
train_spec = tf.estimator.TrainSpec(input_fn=lambda: read_datasets(...), max_steps=epochs*train_steps)
eval_spec = tf.estimator.EvalSpec(input_fn=lambda: read_datasets(...), start_delay_secs=1, throttle_secs=1)
tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
</code></pre>

<p>Although the above approach seems to work fine, I still have some difficulties understanding how the chief is partitioning the dataset among the workers and how to set the train_step and batch_size in this approach. Also I don't know how can I report accuracy and other metrics such as precision/recall/F1 in addition to loss when running the <code>tf.estimator.evaluate</code> without writing a custom model_fn for my encapsulated keras estimator.</p>
",1
58012185,Combining map and padded_batch in tensorflow,"<p>To increase the performance on input pipeline tensorflow documentation recommends using <code>tf.contrib.data.map_and_batch</code> instead of separate <code>.map</code> and <code>.batch</code> functions (<a href=""https://www.tensorflow.org/guide/performance/datasets"" rel=""nofollow noreferrer"">Data input pipeline performance</a>). But how to combine <code>.map</code> and <code>.padded_batch</code> functions in input pipeline?</p>
",1
58035125,How to provide input layer to tf.layers.Conv3D?,"<p>tf.layers have two classes for 3d convolution. 1) tf.layers.conv3d and 2) tf.layers.Conv3D</p>

<p>According to the documentation, tf.layers.conv3d is going to be deprecated. So I started using tf.layers.Conv3D.</p>

<p>But tf.layers.Conv3D doesn't have a parameter to specify the input layer where as tf.layers.conv3d has. </p>

<p>Can someone help me figure out how to give inputs to tf.layers.Conv3D layer ?</p>
",0
58090424,"Trying to use tensorflow.io.gfile.GFile and tensorflow.dataset.map, getting ""Expected binary or unicode string, got Tensor dtype=string","<p>I'm quite new to using Tensorflow, and imagine someone will quickly tell me I'm doing something stupid so here goes. </p>

<p>I'm working with the MSTAR dataset and trying to get it read in. The files have a very strange format, but suffice it to say that if eager execution is on the following code reads and displays an image from the dataset. </p>

<pre><code>import tensorflow as tf
import matplotlib.pyplot as plt
tf.enable_eager_execution()

img1Path='HB15000.018'
img2Path='HB15001.018'

def pathToImgTF(path):
    with tf.io.gfile.GFile(path,'rb') as filePath:
        step1=filePath.readlines()
        step2=[x.strip(b'\n') for x in step1]
        for x in step2:
            if b'PhoenixHeaderLength' in x:
                line=x.strip().split(b'=')
                PhoenixHeaderLength=int(line[1])
            elif b'native_header_length' in x:
                line=x.strip().split(b'=')
                native_header_length=int(line[1])
            elif b'NumberOfColumns' in x:
                line=x.strip().split(b'=')
                NumberOfColumns=int(line[1])
            elif b'NumberOfRows' in x:
                line=x.strip().split(b'=')
                NumberOfRows=int(line[1])
        filePath.seek(PhoenixHeaderLength+native_header_length)
        step3=tf.decode_raw(filePath.read(),out_type=tf.float32,little_endian=False)
        depth_major=tf.reshape(step3,[2,NumberOfRows,NumberOfColumns])
        image=tf.transpose(depth_major,[1,2,0])[:,:,0] #Cut off phase for now
    return image

img=pathToImgTF(imgPath)
plt.imshow(img,cmap='gray')
</code></pre>

<p>I would like to use tf.dataset.from_tensor_slices, but it appears that isn't an option because the following code:</p>

<pre><code>ds=tf.data.Dataset.from_tensor_slices([img1Path,img2Path])
ds=ds.map(pathToImgTF)
</code></pre>

<p>Gives the error ""TypeError: Expected binary or unicode string, got tf.Tensor 'args_0:0' shape=() dtype=string""</p>

<p>The traceback looks to me like it's breaking specifically on 'filePath.readlines()', any help would be greatly appreciated. </p>

<p>Full error output:</p>

<blockquote>
  <p>--------------------------------------------------------------------------- TypeError                                 Traceback (most recent call
  last) ipython-input-6-e12909fb73cd in module
        1 ds=tf.data.Dataset.from_tensor_slices([img1Path,img2Path])
  ----> 2 ds=ds.map(pathToImgTF)</p>
  
  <p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py
  in map(self, map_func, num_parallel_calls)    1770     if
  num_parallel_calls is None:    1771       return DatasetV1Adapter(
  -> 1772           MapDataset(self, map_func, preserve_cardinality=False))    1773     else:    1774       return
  DatasetV1Adapter(</p>
  
  <p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py
  in <strong>init</strong>(self, input_dataset, map_func, use_inter_op_parallelism,
  preserve_cardinality, use_legacy_function)    3188<br>
  self._transformation_name(),    3189         dataset=input_dataset,
  -> 3190         use_legacy_function=use_legacy_function)    3191     variant_tensor = gen_dataset_ops.map_dataset(    3192<br>
  input_dataset._variant_tensor,  # pylint: disable=protected-access</p>
  
  <p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py
  in <strong>init</strong>(self, func, transformation_name, dataset, input_classes,
  input_shapes, input_types, input_structure, add_to_graph,
  use_legacy_function, defun_kwargs)    2553       resource_tracker =
  tracking.ResourceTracker()    2554       with
  tracking.resource_tracker_scope(resource_tracker):
  -> 2555         self._function = wrapper_fn._get_concrete_function_internal()    2556         if
  add_to_graph:    2557<br>
  self._function.add_to_graph(ops.get_default_graph())</p>
  
  <p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\eager\function.py
  in _get_concrete_function_internal(self, *args, **kwargs)    1353<br>
  """"""Bypasses error checking when getting a graph function.""""""    1354<br>
  graph_function =
  self._get_concrete_function_internal_garbage_collected(
  -> 1355         *args, **kwargs)    1356     # We're returning this concrete function to someone, and they may keep a    1357     #
  reference to the FuncGraph without keeping a reference to the</p>
  
  <p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\eager\function.py
  in _get_concrete_function_internal_garbage_collected(self, *args,
  **kwargs)    1347     if self.input_signature:    1348       args, kwargs = None, None
  -> 1349     graph_function, _, _ = self._maybe_define_function(args, kwargs)    1350     return graph_function    1351 </p>
  
  <p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\eager\function.py
  in _maybe_define_function(self, args, kwargs)    1650<br>
  graph_function = self._function_cache.primary.get(cache_key, None)<br>
  1651       if graph_function is None:
  -> 1652         graph_function = self._create_graph_function(args, kwargs)    1653         self._function_cache.primary[cache_key] =
  graph_function    1654       return graph_function, args, kwargs</p>
  
  <p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\eager\function.py
  in _create_graph_function(self, args, kwargs,
  override_flat_arg_shapes)    1543             arg_names=arg_names,<br>
  1544             override_flat_arg_shapes=override_flat_arg_shapes,
  -> 1545             capture_by_value=self._capture_by_value),    1546         self._function_attributes)    1547 </p>
  
  <p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\func_graph.py
  in func_graph_from_py_func(name, python_func, args, kwargs, signature,
  func_graph, autograph, autograph_options, add_control_dependencies,
  arg_names, op_return_value, collections, capture_by_value,
  override_flat_arg_shapes)
      713                                           converted_func)
      714 
  --> 715       func_outputs = python_func(*func_args, **func_kwargs)
      716 
      717       # invariant: <code>func_outputs</code> contains only Tensors, CompositeTensors,</p>
  
  <p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py
  in wrapper_fn(*args)    2547           attributes=defun_kwargs)<br>
  2548       def wrapper_fn(*args):  # pylint: disable=missing-docstring
  -> 2549         ret = _wrapper_helper(*args)    2550         ret = self._output_structure._to_tensor_list(ret)    2551         return
  [ops.convert_to_tensor(t) for t in ret]</p>
  
  <p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py
  in _wrapper_helper(*args)    2487         nested_args = (nested_args,)
  2488 
  -> 2489       ret = func(*nested_args)    2490       # If <code>func</code> returns a list of tensors, <code>nest.flatten()</code> and    2491       #
  <code>ops.convert_to_tensor()</code> would conspire to attempt to stack</p>
  
  <p> in pathToImgTF(path)
        9 def pathToImgTF(path):
       10     with tf.io.gfile.GFile(path,'rb') as filePath:
  ---> 11         step1=filePath.readlines()
       12         step2=[x.strip(b'\n') for x in step1]
       13         for x in step2:</p>
  
  <p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\lib\io\file_io.py
  in readlines(self)
      181   def readlines(self):
      182     """"""Returns all lines from the file in a list.""""""
  --> 183     self._preread_check()
      184     lines = []
      185     while True:</p>
  
  <p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\lib\io\file_io.py
  in _preread_check(self)
       82                                            ""File isn't open for reading"")
       83       self._read_buf = pywrap_tensorflow.CreateBufferedInputStream(
  ---> 84           compat.as_bytes(self.__name), 1024 * 512)
       85 
       86   def _prewrite_check(self):</p>
  
  <p>~\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\util\compat.py
  in as_bytes(bytes_or_text, encoding)
       63   else:
       64     raise TypeError('Expected binary or unicode string, got %r' %
  ---> 65                     (bytes_or_text,))
       66 
       67 </p>
  
  <p>TypeError: Expected binary or unicode string, got tf.Tensor
  'args_0:0' shape=() dtype=string</p>
</blockquote>
",0
58112355,"What, exactly, is eager execution from a programming point of view?","<p>I am trying to understand eager execution. Pages returned by Google describe what it does for you, and I'm ok with that. I am trying to understand it from the point of view of program code. Here is an example from <a href=""https://towardsdatascience.com/eager-execution-tensorflow-8042128ca7be"" rel=""nofollow noreferrer"">this article</a>.</p>

<pre><code>a = tf.constant([[1,2],[3,4]])
</code></pre>

<p>The article says this statement does something different depending on whether you are in eager mode or not. Without eager mode, print(a) gives:</p>

<pre><code>Tensor(""Const:0"", shape=(2, 2), dtype=int32)
</code></pre>

<p>With eager mode, print(a) gives:</p>

<pre><code>tf.Tensor(
[[1 2]
 [3 4]], shape=(2, 2), dtype=int32)
</code></pre>

<p>Please could someone explain what these two return values are. If they are two different object types, a Tensor and a tf.Tensor, what is the difference between these objects?</p>

<p>I have searched the TensorFlow documentation and can't see anything that addresses this distinction. Any pointers gratefully received.</p>

<p>Thanks,</p>

<p>Julian</p>
",1
58118334,When should we inherits keras.Model instead of keras.layers.Layer even if we don't use model.fit?,"<p>In some Tensorflow tutorials with tf2 (e.g. <a href=""https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention"" rel=""nofollow noreferrer"">Neural Machine Translation with Attention</a> and <a href=""https://www.tensorflow.org/beta/guide/eager"" rel=""nofollow noreferrer"">Eager essentials</a>), they define custom <code>tf.keras.Model</code>s instead of <code>tf.keras.layers.Layer</code>s (e.g. <code>BahdanauAttention(tf.keras.Model):</code>)</p>

<p>Also, <a href=""https://www.tensorflow.org/tutorials/eager/custom_layers#models_composing_layers"" rel=""nofollow noreferrer"">Models: composing layers</a> doc uses <code>tf.keras.Model</code> explicitly. The section says:</p>

<blockquote>
  <p>The main class used when creating a layer-like thing which contains other layers is tf.keras.Model. Implementing one is done by inheriting from tf.keras.Model.</p>
</blockquote>

<p>It sounds we need to inherit <code>tf.keras.Model</code> to define a layer which compose child layers.</p>

<p>However, as far as I checked, this code works even if I define <code>ResnetIdentityBlock</code> as a child class of <code>tf.keras.layers.Layer</code>. Other two tutorials work with <code>Layer</code> too.
In addition to that, <a href=""https://www.tensorflow.org/beta/guide/keras/custom_layers_and_models"" rel=""nofollow noreferrer"">another tutorial</a> says</p>

<blockquote>
  <p>Model is just like a Layer, but with added training and serialization utilities.</p>
</blockquote>

<p>Thus, I have no idea what is the real difference between <code>tf.keras.Model</code> and <code>tf.keras.layers.Layer</code> and why those three tutorial with Eager execution uses <code>tf.keras.Model</code> though they don't use <em>training and serialization utilities</em> of <code>tf.keras.Model</code>.</p>

<p>Why do we need to inherit <code>tf.keras.Model</code> in those tutorials?</p>

<p><strong>Additional comment</strong></p>

<p>utilities of <code>Model</code> work only with special subsets of <code>Layer</code> (<a href=""https://github.com/tensorflow/tensorflow/blob/c8ef33dd913463ced8cc347c03945a88b34da7f8/tensorflow/python/keras/engine/training.py#L1461"" rel=""nofollow noreferrer"">Layers whose <code>call</code> receive only one input</a>). Thus, I think the idea like <em>""Always extend Model because Model has more features""</em> is not correct. Also, it violates a basic programming program like <a href=""https://en.wikipedia.org/wiki/Single_responsibility_principle"" rel=""nofollow noreferrer"">SRP</a>.</p>
",0
58160789,How to handle none value error with custom matrix multiplication in Tensorflow neural network,"<p>I've written my own matrix multiplication implementation in TensorFlow that works with tf placeholders when their dimensions are properly defined. But many times in neural networks you want to leave one of the dimensions undefined so that the batch size or size of the input can change such as: </p>

<pre><code>X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=""X"")
y = tf.placeholder(tf.int64, shape=(None), name=""y"")
</code></pre>

<p>In my implementation, I first check the dimensions of the matrices so that they match and so that I can initialize a matrix with the correct dimensions with:</p>

<pre><code>(m, n) = A.get_shape().as_list()
</code></pre>

<p>But then when I use it in a tf neural network I get the error:</p>

<pre><code>raise ValueError(""None values not supported."")
</code></pre>

<p>Because my matrix multiplication only works for defined matrices. Is there a way to handle this error so that it will work when the dimensions are eventually defined? Tensorflow's tf.matmul() must have a way but I can't figure it out.
<a href=""https://www.tensorflow.org/api_docs/python/tf/linalg/matmul"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/linalg/matmul</a></p>
",0
58162110,How to port a tf.Session to a tf.train.MonitoredSession call while allowing graph modifications,"<p>The code I'm working on is <a href=""https://github.com/tensorflow/tensorrt/blob/master/tftrt/examples/object_detection/object_detection.py"" rel=""nofollow noreferrer"">this</a>.<br>
The code uses tf.session call to take in a graph for object detection tasks.  <a href=""https://github.com/tensorflow/tensorrt/blob/526f3650a550d6ffcceddfd73d112391080066ad/tftrt/examples/object_detection/object_detection.py#L593-L676"" rel=""nofollow noreferrer"">Link</a><br>
My aim here is to profile this code for Nvidia GPUs using the nvtx-plugins-tf to analyze the time taken for different ops. <a href=""https://nvtx-plugins.readthedocs.io/en/latest/templates/api.html#session-hooks"" rel=""nofollow noreferrer"">Link to docs</a></p>

<p>The plugin library provides a function hook for a tf.train.MonitoredSession as given in their example code <a href=""https://github.com/NVIDIA/nvtx-plugins/blob/a5d47ba6cb0548f2a6c15814d409a14aca4d33f8/examples/tf_session_example.py#L19-L95"" rel=""nofollow noreferrer"">here</a>.<br>
The code linked above uses tf.session along with a tf.config and when I try to modify the tf.session call to a tf.train.MonitoredSession call, I can't get my code to work and it fails with an error that graph can't be modified. I went through the tensorflow APIs and it turns out that tf.session doesn't support hook callbacks and tf.train.MonitoredSession doesn't support tf_config as a function argument.</p>

<pre><code>Traceback (most recent call last):
  File ""/home/mayroy13/anaconda3/envs/trt-py36/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/mayroy13/anaconda3/envs/trt-py36/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/mayroy13/Mayank/Mayank/test/tensorrt/tftrt/examples/object_detection/test.py"", line 105, in &lt;module&gt;
    test(args.test_config_path)
  File ""/home/mayroy13/Mayank/Mayank/test/tensorrt/tftrt/examples/object_detection/test.py"", line 81, in test
    **test_config['benchmark_config'])
  File ""/home/mayroy13/Mayank/Mayank/test/tensorrt/tftrt/examples/object_detection/object_detection.py"", line 608, in benchmark_model
    tf.import_graph_def(frozen_graph, name='')
  File ""/home/mayroy13/anaconda3/envs/trt-py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/home/mayroy13/anaconda3/envs/trt-py36/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 443, in import_graph_def
    _ProcessNewOps(graph)
  File ""/home/mayroy13/anaconda3/envs/trt-py36/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 236, in _ProcessNewOps
    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access
  File ""/home/mayroy13/anaconda3/envs/trt-py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3751, in _add_new_tf_operations
    for c_op in c_api_util.new_tf_operations(self)
  File ""/home/mayroy13/anaconda3/envs/trt-py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3751, in &lt;listcomp&gt;
    for c_op in c_api_util.new_tf_operations(self)
  File ""/home/mayroy13/anaconda3/envs/trt-py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3640, in _create_op_from_tf_operation
    self._check_not_finalized()
  File ""/home/mayroy13/anaconda3/envs/trt-py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3225, in _check_not_finalized
    raise RuntimeError(""Graph is finalized and cannot be modified."")
RuntimeError: Graph is finalized and cannot be modified.
</code></pre>

<p>Any directions to go in would be appreciated. If there are ways in tensorflow to use hooks in conjunction with tf.session, that will also work for me.</p>
",0
58176215,Tensorflow difference between tf.stop_gradient and feed variables to optimizer?,"<p>I'm trying to train a model in <strong>self-supervised learning</strong>. The flow chart is something like the following:
<a href=""https://i.stack.imgur.com/g2yMn.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/g2yMn.jpg"" alt=""enter image description here""></a></p>

<p>Let's assume that <code>N1</code> is already trained and we want to train just <code>N2</code>. This is my current implementation:</p>

<pre><code>x_1 = tf.placeholder(tf.float32, [None, 128, 128, 1])
x_2 = tf.placeholder(tf.float32, [None, 128, 128, 1])

s_t1 = tf.stop_gradient(N1(x_1))  # treat s_t1 as a constant
s_t2_pred = N2(s_t1)) 
s_t2 = tf.stop_gradient(N1(x_2))  # treat s_t2 as a constant

loss = some_loss_function(s_t2, s_t2_pred)
train_op = tf.train.AdamOptimizer(lr).minimize(loss)
</code></pre>

<p>In this way, I should be optimizing only <code>N2</code>. What makes me confused is the fact that if I were to use the following code I would obtain very different results (much better than the above): </p>

<pre><code># treat everything as a variable:
s_t1 = N1(x_1)
s_t2_pred = N2(s_t1)
s_t2 = N1(x_2)

loss = some_loss_function(s_t2, s_t2_pred)
var_list = take_all_variables_in_N2()
train_op = tf.train.AdamOptimizer(lr).minimize(loss, var_list)
</code></pre>

<p>I wonder what is the problem with the first implementation. What is exactly the behaviour of <code>tf.stop_gradient</code> (the documentation is a bit poor)? How does this differ from the second approach?</p>

<p>From a practical perspective in semi-supervised learning: what is the difference between the two? Which one is the correct approach?</p>

<p>Thank you :) </p>

<hr>

<hr>

<p><strong>I added a possible solution to the problem in the comments below. I would still be happy to receive any feedback from more experienced users and to share some opinions on the best approach to structure a self-supervised learning problem in tensorflow.</strong></p>

<p>Bye, G.</p>
",1
58203947,"How to do the element-wise sum of two tf.data.Datasets, both iterating indefinitely, with tf.data.Dataset.map?","<p>I would like to write a <i>mixup</i> data augmentation [1] function in my <strong>tf.data</strong>-based pipeline.</p>

<p>I generate one <strong>tf.data.Dataset</strong> with my training examples and one with the examples I want to use to augment my training examples.</p>

<p>I want to map the elements <strong><em>feat_train, label_train</em></strong> of dataset_train to <strong><em>feat_train + feat_aug, label_train, label_aug</em></strong>, <em>feat_aug</em> and <em>label_aug</em> being the elements of <em>dataset_aug</em>, such that both datasets are indefinitely iterated, e.g. for a dataset_train with 3 elements and dataset_aug with 2 elements:</p>

<p>feat_train[0], label_train[0] -> feat_train[0] + feat_aug[0], label_train[0] + label_aug[0]<br>
feat_train[1], label_train[1] -> feat_train[1] + feat_aug[1], label_train[1] + label_aug[1]<br>
feat_train[2], label_train[2] -> feat_train[2] + feat_aug[0], label_train[2] + label_aug[0]<br>
feat_train[0], label_train[0] -> feat_train[0] + feat_aug[1], label_train[0] + label_aug[1]<br>
feat_train[1], label_train[1] -> feat_train[1] + feat_aug[0], label_train[1] + label_aug[0]<br>
...</p>

<p>How can I get this behavior in my mixup fonction ? Is there any other recommended way to perform element-wise operations on 2 <strong>tf.data.Datasets</strong> iterating indefinitely ?</p>

<p>[1] Zhang, Hongyi, et al. ""mixup: Beyond empirical risk minimization."" arXiv preprint arXiv:1710.09412 (2017).</p>

<pre class=""lang-py prettyprint-override""><code># files_train and files_aug are lists of TFRecord files.

# parse TFRecords to get training example features and
# one-hot encoded labels
dataset_train = tf.data.TFRecordDataset(files_train)
dataset_train = dataset_train.map(
    lambda x: serialized2data(x, feature_shape, class_list))
dataset_train = dataset_train.shuffle(10000)
dataset_train = dataset_train.repeat()  # Repeat indefinitely.

# parse TFRecords to get augmentation example features and
# one-hot encoded labels
dataset_aug = tf.data.TFRecordDataset(files_aug)
dataset_aug = dataset_aug.map(
    lambda x: serialized2data(x, feature_shape, class_list))
dataset_aug = dataset_aug.repeat()  # Repeat indefinitely.

# augment data (mixup)
# Here how can I write a map function so that the features of every item
# of dataset_train is mixed with an item of dataset_aug ?
# something like
# dataset_train = dataset_train.map(
#     lambda feat_train, label_train: mixup(
#         feat_train, label_train, feat_aug, label_aug)
# )
# ?
# but how can I iterate dataset_aug to get feat_aug and label_aug ?

# make batch
dataset_train = dataset_train.batch(batch_size, drop_remainder=True)

return dataset


def mixup(feat_train, label_train, feat_aug, label_aug):
    # Shown as an example. This will be more complicated...
    return (feat_train + feat_aug,
            label_train + label_aug)


def serialized2data(
        serialized_data,
        feature_shape,
        class_list,
        data_format='channels_first',
        training=True):
    """"""Generate features, labels and, if training is False, filenames and times.
    Labels are indices of original label in class_list.

    Args:
        serialized_data: data serialized using utils.tf_utils.serialize_data
        feature_shape: shape of the features. Can be obtained with
            feature_extractor.feature_shape (see utils.feature_utils)
        class_list: list of class ids (used for one-hot encoding the labels)
        data_format: 'channels_first' (NCHW) or 'channels_last' (NHWC).
            Default is set to 'channels_first' because it is faster on GPU
            (https://www.tensorflow.org/guide/performance/overview#data_formats).
    """"""

    features = {
        'filename': tf.io.FixedLenFeature([], tf.string),
        'times': tf.io.FixedLenFeature([2], tf.float32),
        'data': tf.io.FixedLenFeature(feature_shape, tf.float32),
        'labels': tf.io.FixedLenFeature([], tf.string),
    }
    example = tf.io.parse_single_example(serialized_data, features)

    # reshape data to channels_first format
    if data_format == 'channels_first':
        data = tf.reshape(example['data'], (1, feature_shape[0], feature_shape[1]))
    else:
        data = tf.reshape(example['data'], (feature_shape[0], feature_shape[1], 1))

    # one-hot encode labels
    labels = tf.strings.to_number(
        tf.string_split([example['labels']], '#').values,
        out_type=tf.int32
    )

    # get intersection of class_list and labels
    labels = tf.squeeze(
        tf.sparse.to_dense(
            tf.sets.intersection(
                tf.expand_dims(labels, axis=0),
                tf.expand_dims(class_list, axis=0)
            )
        ),
        axis=0
    )

    # sort class_list and get indices of labels in class_list
    class_list = tf.sort(class_list)
    labels = tf.where(
        tf.equal(
            tf.expand_dims(labels, axis=1),
            class_list)
    )[:,1]

    tf.cond(
        tf.math.logical_and(training, tf.equal(tf.size(labels), 0)),
        true_fn=lambda:myprint(tf.strings.format('File {} has no label', example['filename'])),
        false_fn=lambda:1
    )

    one_hot = tf.cond(
        tf.equal(tf.size(labels), 0),
        true_fn=lambda: tf.zeros(tf.size(class_list)),
        false_fn=lambda: tf.reduce_max(tf.one_hot(labels, tf.size(class_list)), 0)
    )

    if training:
        return (data, one_hot)
    else:
        return (data, one_hot, example['filename'], example['times'])
</code></pre>
",0
58227659,Tensorflow structured data model.predict() returns incorrect probabilities,"<p>I'm trying to follow a Tensorflow tutorial (i'm a beginner) for <a href=""https://www.tensorflow.org/tutorials/structured_data/feature_columns"" rel=""nofollow noreferrer"">structured data models</a> with some changes along the way.</p>

<p>My purpose is to create a model to which i provide data (in csv format) that looks something like this (the example has only 2 features but i want to extend it after i figure it out):</p>

<pre><code>power_0,power_1,result
0.2,0.3,draw
0.8,0.1,win
0.3,0.1,draw
0.7,0.2,win
0.0,0.4,lose
</code></pre>

<p>I created the model using the following code:</p>

<pre><code>def get_labels(df, label, mapping):
    raw_y_true = df.pop(label)
    y_true = np.zeros((len(raw_y_true)))
    for i, raw_label in enumerate(raw_y_true):
        y_true[i] = mapping[raw_label]
    return y_true


tf.compat.v1.enable_eager_execution()

mapping_to_numbers = {'win': 0, 'draw': 1, 'lose': 2}

data_frame = pd.read_csv('data.csv')
data_frame.head()

train, test = train_test_split(data_frame, test_size=0.2)
train, val = train_test_split(train, test_size=0.2)

train_labels = np.array(get_labels(train, label='result', mapping=mapping_to_numbers))
val_labels = np.array(get_labels(val, label='result', mapping=mapping_to_numbers))
test_labels = np.array(get_labels(test, label='result', mapping=mapping_to_numbers))

train_features = np.array(train)
val_features = np.array(val)
test_features = np.array(test)

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(train_features.shape[-1],)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(3, activation='sigmoid'),
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy'],
    run_eagerly=True)

epochs = 10
batch_size = 100
history = model.fit(
    train_features,
    train_labels,
    epochs=epochs,
    validation_data=(val_features, val_labels))

input_data_frame = pd.read_csv('input.csv')
input_data_frame.head()

input_data = np.array(input_data_frame)

print(model.predict(input_data))
</code></pre>

<p>input.csv looks as following:</p>

<pre><code>power_0,power_1
0.8,0.1
0.7,0.2
</code></pre>

<p>And the actual result is:</p>

<pre><code>[[0.00604381 0.00242573 0.00440606]
 [0.01321151 0.00634229 0.01041476]]
</code></pre>

<p>I expected to get the probability for each label ('win', 'draw' and 'lose'), can anyone please help me with this?</p>

<p>Thanks in advance</p>
",0
58268936,Taking all combinations of records from two TFRecords,"<p>I have two TFRecords <code>A</code> and <code>B</code> of different sizes and containing different data elements. </p>

<p>I need to take all possible pairs of records from <code>A</code> and <code>B</code>. Therefore, during training or testing, I would like the signal of epoch to end only when all combinations have been exhausted, after which the process should resume for the next epoch.</p>

<p>In doing this, of course, I would like to specify a <code>batchsize</code>.</p>

<p>I have gone through the documentation of <code>tf.data.Dataset</code> and have found nothing which does something like this.</p>

<p>Of course, if I were to write a python generator, this could be accomplished. But unfortunately, this is not useful because according to documentation, python generators will be bounded by the <code>GIL</code> i.e the <code>global interpreter lock</code>. </p>

<p>Thus, suppose that,</p>

<p><code>A</code> contains <code>{image1, image2, image3}</code>, while <code>B</code> contains <code>{im1, im2, im3, im4, im5, im6}</code>. And I have specified a batchsize of <code>2</code>. Then I would like the output to be something like following :</p>

<p><code>(image1, im1) and (image2, im4)</code></p>

<p><code>(image3, im2) and (image1, im2)</code></p>

<p><code>(image2, im1) and (image2, im3)</code></p>

<p><code>..............</code></p>

<p><code>15 more combinations</code></p>

<p>and then the next epoch starts.</p>

<p>How can that be achieved in TensorFlow ?</p>
",1
58384884,'tensorflow.python.keras.api._v1.keras.losses' has no attribute 'Reduction',"<p>I am using Huber loss implementation in tf.keras in tensorflow 1.14.0 as follows:</p>

<pre><code>huber_keras_loss = tf.keras.losses.Huber(
        delta=delta,
        reduction=tf.keras.losses.Reduction.SUM,
        name='huber_loss'
    )
</code></pre>

<p>I am getting the error 
AttributeError: module 'tensorflow.python.keras.api._v1.keras.losses' has no attribute 'Reduction'</p>

<p>I have tried using tf.losses.Reduction, tf.compat.v2.losses.Reduction nothing seems to work.</p>

<p>Did tensorflow remove Reduction from tf.keras.losses, it is strange if they did so because their documentation still shows:
<a href=""https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/losses/Huber#args"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/losses/Huber#args</a></p>
",1
58499116,How to raise an error based on condition in Tensorflow?,"<p>I am writing a function <strong>my_function</strong> to be used as tf.data.Dataset.map argument, using tf.py_function, as recommended in <a href=""https://www.tensorflow.org/guide/data#applying_arbitrary_python_logic"" rel=""nofollow noreferrer"">the doc</a>.</p>

<p>How can I raise a <strong>tf.errors.InvalidArgumentError</strong> in my_function ?</p>

<p><b>my_function</b></p>

<pre><code>def my_function(data, param):
    _, n_freq_bins, n_time_bins, channels = tf.shape(data)
    tf.cond(
        param &lt; n_time_bins,
        lambda: 1,
        &lt;raise tf.errors.InvalidArgumentError&gt;
    )
    ...
</code></pre>

<p><b>In my data pipeline:</b></p>

<pre><code>dataset = dataset.map(lambda data, labels: (tf.py_function(my_function, [data, 100], tf.float32), labels))
</code></pre>
",0
58505602,How to create windowed multivariate dataset from SequenceExample TFRecord,"<p>I am trying to set up a Tensorflow pipeline using tf.data.datasets in order to load some TFRecord into a Keras model. These data are multivariate timeseries.</p>

<p>I am currently using Tensorflow 2.0</p>

<p>First I get my dataset from the TFRecord and parse it :</p>

<pre class=""lang-py prettyprint-override""><code>dataset = tf.data.TFRecordDataset('...')

context_features = {...}
sequence_features = {...}

def _parse_function(example_proto):
  _, sequence =  tf.io.parse_single_sequence_example(example_proto,context_features, sequence_features)
  return sequence


dataset = dataset.map(_parse_function)
</code></pre>

<p>The problem right now is that it gives me a MapDataset with dict of EagerTensor inside : </p>

<pre class=""lang-py prettyprint-override""><code>for data in dataset.take(3):
  print(type(data))

&lt;class 'dict'&gt;
&lt;class 'dict'&gt;
&lt;class 'dict'&gt;

# which look like : {feature1 : EagerTensor, feature2 : EagerTensor ...}
</code></pre>

<p>Because of these dictionaries, I cannot seem to manage to get these data to be batched, shuffled ... in order to use them in an LSTM layer afterwards. For instance this :</p>

<pre class=""lang-py prettyprint-override""><code>def make_window_dataset(ds, window_size=5, shift=1, stride=1):
  windows = ds.window(window_size, shift=shift, stride=stride)

  def sub_to_batch(sub):
    return sub.values().batch(window_size, drop_remainder=True)

  windows = windows.flat_map(sub_to_batch)
  return windows

ds = make_window_dataset(dataset, 10)

gives me :

AttributeError: 'dict_values' object has no attribute 'batch'
</code></pre>

<p>Thank you for your help. I am basing my research on this and other Tensorflow helpers :</p>

<p><a href=""https://www.tensorflow.org/guide/data#time_series_windowing"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/data#time_series_windowing</a></p>

<p><strong>EDIT :</strong></p>

<p>I found the solution to my problem. I ended up converting the dictionary given by the parsing to a (None,11) shaped Tensor using stack in my parse function :</p>

<pre class=""lang-py prettyprint-override""><code>def _parse_function(example_proto):
  # Parse the input `tf.Example` proto using the dictionary above.
  _, sequence =  tf.io.parse_single_sequence_example(example_proto,context_features, sequence_features)
  return tf.stack(list(sequence.values()), axis=1)
</code></pre>
",0
58520594,"tf.Data.Dataset - On each Epoch, only train with a sub sample of the full dataset","<p>I have an image dataset with a large imbalance of positive and negatives samples (many more negatives). I would like to create a tf.data.Dataset where each epoch it will train with all of the positive samples but only (ratio * len(positive) ) of the negative samples. </p>

<p>I am currently using a datagen inherited from keras.util.Sequence to achieve this and using this subsampling policy is performing much better than training on all data.</p>

<p>However reading the docs on Dataset, I cannot seem to find a way to do it, is it possible?</p>

<p>In my existing data generator, I am doing this:</p>

<pre class=""lang-py prettyprint-override""><code># List if indicies of the positive and negative samples
positives = np.where(self.labels == 1)[0]
negatives = np.where(self.labels == 0)[0]
# How many of the negatives do we want to use?
n_negatives = np.clip(int(len(positives) * self.config.DATASET_NEGSUBSAMPLE_RATIO), 1, len(negatives))
# Choose random negatives
subsampled_negatives = np.random.choice(negatives, n_negatives, replace=False)
# Create the incidies array from the positive and subsamples negative indicies
self.indexes = np.concatenate((positives, subsampled_negatives))
# Shuffle them together
np.random.shuffle(self.indexes)
</code></pre>
",1
58543649,How to fix the tf.app.run() bug with app not recognized?,"<p>I started to learn tensorflow with the tutorial on their official website (<a href=""https://www.tensorflow.org/hub/tutorials/image_retraining"" rel=""nofollow noreferrer"">https://www.tensorflow.org/hub/tutorials/image_retraining</a>). I followed the steps by understanding them and I'm stuck at this point. </p>

<pre><code>Traceback (most recent call last):
  File ""retrain.py"", line 1349, in &lt;module&gt;
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
AttributeError: 'module' object has no attribute 'app'
</code></pre>

<p>You can find the retrain.py file in the following link (<a href=""https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/hub/blob/master/examples/image_retraining/retrain.py</a>).</p>

<p>I searched in forums, and all questions was about the args. None of them were about the tf.app.run function itself. So I read this function (<a href=""https://github.com/tensorflow/tensorflow/blob/9dc6c17797c065796603d9259b2aa57b3c07ff71/tensorflow/python/platform/app.py#L31-L48"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/9dc6c17797c065796603d9259b2aa57b3c07ff71/tensorflow/python/platform/app.py#L31-L48</a>), and I didn't found my answer there. </p>

<p>Should I replace this line, by something else instead? </p>

<p>I don't know if this (official) tutorial is deprecated, I don't think so.
By the way, if you have some good tutorials to recommend, I would love to do them. </p>
",0
58550146,How to use the tf.keras.layers.BatchNormalization() in custom training loop?,"<p>I went back to tensorflow after quite a while and it seems the landscape is completely changed.</p>

<p>However, previously I used to use <code>tf.contrib....batch_normalization</code> with the following in the training loop:</p>

<pre><code>update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    train_op = optimizer.minimize(cnn.loss, global_step=global_step)
</code></pre>

<p>But it seems, <code>contrib</code> is nowhere to be found and <code>tf.keras.layers.BatchNormalization</code> does not work the same way. Also, I couldn't find any training instruction in their <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization"" rel=""nofollow noreferrer"">documentation</a>.</p>

<p>So, any information of help is appreciated.</p>
",1
58631390,What is the purpose of tf.compat?,"<p>What's the purpose of tf.compat module? It looks like just the entire Tensorflow API is replicated inside this module.
The documentation states</p>

<blockquote>
  <p>Functions for Python 2 vs. 3 compatibility.</p>
</blockquote>

<p>So why there is a ""v1"" and a ""v2"" submodule? What are the compatibility problems address by tf.compat specifically?</p>
",1
58660613,How to add another layer on a pre-loaded network?,"<p>I'm loading a neural network using tensorflow and colab notbook from google. And I want to remove the fully connected layer of the output layer and add another fully connected with only one neuron, and I want to freeze the other layers and train just this added output layer. 
I'm using <code>tf.keras.application.MobileNetV2</code> and I'm using the mledu-<code>datasets/cats_and_dogs</code>.</p>

<p>I've serarch in the tensorflow API and testeded the methods to add and I've got no success. My code is the following</p>

<pre><code>
Original file is located at
    https://colab.research.google.com/drive/16VdqQFBfY_jp5-5kRQvWQ0Y0ytN9W1kN

https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/classification.ipynb#scrollTo=3f0Z7NZgVrWQ

This tutorial follows a basic machine learning workflow:

1.   Examine and understand data
2.   Build an input pipeline
3.   Build the model
4.   Train the model
5.   Test the model
6.   Improve the model and repeat the process

## Import packages

Let's start by importing the required packages. The `os` package is used to read files and directory structure, NumPy is used to convert python list to numpy array and to perform required matrix operations and `matplotlib.pyplot` to plot the graph and display images in the training and validation data.
""""""

from __future__ import absolute_import, division, print_function, unicode_literals

""""""Import Tensorflow and the Keras classes needed to construct our model.""""""

# try:
#   # %tensorflow_version only exists in Colab.
#   %tensorflow_version 2.x
# except Exception:
#   pass

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator

import os
import numpy as np
import matplotlib.pyplot as plt

import keras
from keras import backend as K
from keras.layers.core import Dense, Activation
from keras.metrics import categorical_crossentropy
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing import image
from keras.models import Model
from keras.applications import imagenet_utils
from keras.layers import Dense,GlobalAveragePooling2D
from keras.applications import MobileNet
from keras.applications.mobilenet import preprocess_input
from IPython.display import Image
from keras.optimizers import Adam

""""""## Load data
Begin by downloading the dataset. This tutorial uses a filtered version of Dogs vs Cats dataset from Kaggle. Download the archive version of the dataset and store it in the ""/tmp/"" directory.
""""""

_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'

path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)

PATH = os.path.join(os.path.dirname(path_to_zip), 'cats_and_dogs_filtered')

""""""The dataset has the following directory structure:

&lt;pre&gt;
&lt;b&gt;cats_and_dogs_filtered&lt;/b&gt;
|__ &lt;b&gt;train&lt;/b&gt;
    |______ &lt;b&gt;cats&lt;/b&gt;: [cat.0.jpg, cat.1.jpg, cat.2.jpg ....]
    |______ &lt;b&gt;dogs&lt;/b&gt;: [dog.0.jpg, dog.1.jpg, dog.2.jpg ...]
|__ &lt;b&gt;validation&lt;/b&gt;
    |______ &lt;b&gt;cats&lt;/b&gt;: [cat.2000.jpg, cat.2001.jpg, cat.2002.jpg ....]
    |______ &lt;b&gt;dogs&lt;/b&gt;: [dog.2000.jpg, dog.2001.jpg, dog.2002.jpg ...]
&lt;/pre&gt;



After extracting its contents, assign variables with the proper file path for the training and validation set.
""""""

train_dir = os.path.join(PATH, 'train')
validation_dir = os.path.join(PATH, 'validation')

train_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures
train_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures
validation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures
validation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures

""""""### Understand the data
Let's look at how many cats and dogs images are in the training and validation directory:
""""""

num_cats_tr = len(os.listdir(train_cats_dir))
num_dogs_tr = len(os.listdir(train_dogs_dir))

num_cats_val = len(os.listdir(validation_cats_dir))
num_dogs_val = len(os.listdir(validation_dogs_dir))

total_train = num_cats_tr + num_dogs_tr
total_val = num_cats_val + num_dogs_val

print('total training cat images:', num_cats_tr)
print('total training dog images:', num_dogs_tr)

print('total validation cat images:', num_cats_val)
print('total validation dog images:', num_dogs_val)
print(""--"")
print(""Total training images:"", total_train)
print(""Total validation images:"", total_val)

""""""For convenience, set up variables to use while pre-processing the dataset and training the network.""""""

batch_size = 32
epochs = 15
IMG_HEIGHT = 160
IMG_WIDTH = 160

""""""### Data preparation

Format the images into appropriately pre-processed floating point tensors before feeding to the network:

1. Read images from the disk.
2. Decode contents of these images and convert it into proper grid format as per their RGB content.
3. Convert them into floating point tensors.
4. Rescale the tensors from values between 0 and 255 to values between 0 and 1, as neural networks prefer to deal with small input values.

Fortunately, all these tasks can be done with the `ImageDataGenerator` class provided by `tf.keras`. It can read images from disk and preprocess them into proper tensors. It will also set up generators that convert these images into batches of tensors—helpful when training the network.
""""""

train_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our training data
validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data

""""""After defining the generators for training and validation images, the `flow_from_directory` method load images from the disk, applies rescaling, and resizes the images into the required dimensions.""""""

train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,
                                                            directory=train_dir,
                                                            shuffle=True,
                                                            target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                            class_mode='binary')

val_data_gen = validation_image_generator.flow_from_directory(batch_size=batch_size,
                                                                directory=validation_dir,
                                                                target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                                class_mode='binary')

""""""### Visualize training images
Visualize the training images by extracting a batch of images from the training generator—which is 32 images in this example—then plot five of them with `matplotlib`.
""""""

sample_training_images, _ = next(train_data_gen)

""""""The `next` function returns a batch from the dataset. The return value of `next` function is in form of `(x_train, y_train)` where x_train is training features and y_train, its labels. Discard the labels to only visualize the training images.""""""

# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.
def plotImages(images_arr):
    fig, axes = plt.subplots(1, 5, figsize=(20,20))
    axes = axes.flatten()
    for img, ax in zip( images_arr, axes):
        ax.imshow(img)
        ax.axis('off')
    plt.tight_layout()
    plt.show()

plotImages(sample_training_images[:5])

""""""## Create the model
The model consists of three convolution blocks with a max pool layer in each of them. There's a fully connected layer with 512 units on top of it thatr is activated by a `relu` activation function. The model outputs class probabilities based on binary classification by the `sigmoid` activation function.
""""""

# model = Sequential([
#     Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)),
#     MaxPooling2D(),
#     Conv2D(32, 3, padding='same', activation='relu'),
#     MaxPooling2D(),
#     Conv2D(64, 3, padding='same', activation='relu'),
#     MaxPooling2D(),
#     Flatten(),
#     Dense(512, activation='relu'),
#     Dense(1, activation='sigmoid')
# ])

""""""Carregando o modelo o modelo `keras.applications.MobileNetV2`, com pesos treinados para a base imagenet e sem as camadas totalmente conectadas.""""""

# from keras.layers import Input
# input_tensor = Input(shape=(IMG_HEIGHT, IMG_WIDTH ,32))
model = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(IMG_HEIGHT,
                                                                    IMG_WIDTH,
                                                                    3),
                                                                    alpha=1.0,
                                                                    include_top=False,
                                                                    weights='imagenet',
                                                                    input_tensor=None,
                                                                    pooling='max',
                                                                    classes=2)
model.trainable = False
</code></pre>

<p>I expect to add the fully connected layer in the network but it's not adding at all.</p>
",0
58663198,Does `tf.data.Dataset.take()` return random sample?,"<p>Different calls of <code>tf.data.Dataset.take()</code> return different batches from the given dataset. Are those samples chosen randomly or is there another mechanism at play? </p>

<p>This is all the more confusing that the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take"" rel=""noreferrer"">documentation</a> makes no reference as to the <em>randomness</em> of the sampling.</p>
",1
58672774,Training using tf.Dataset in TensorFlow 2.0,"<p>I'm having difficulty training my TensorFlow model using a <code>tf.Dataset</code> rather than, say, a <code>pd.DataFrame</code> (which works fine). </p>

<p>I have created a dummy example below that I would expect to work given what I have read online/on the <a href=""https://www.tensorflow.org/guide/data"" rel=""nofollow noreferrer"">TensorFlow website</a>.</p>

<pre class=""lang-py prettyprint-override""><code>!pip install tensorflow==2.0.0 &gt; /dev/null

import numpy as np
import tensorflow as tf

features, target = np.random.rand(100, 30), np.random.randint(0, 2, 100)
dataset = tf.data.Dataset.from_tensor_slices((features, target))

model = tf.keras.Sequential([
    tf.keras.layers.Dense(30, activation='relu', input_shape=(30,)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.5),

    tf.keras.layers.Dense(30, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.5),

    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.fit(
    dataset, 
    epochs=10,
)
</code></pre>

<p>which returns the following error message</p>

<pre><code>...

ValueError: Error when checking input: expected dense_input to have shape (30,) but got array with shape (1,)
</code></pre>

<p>Is there anything obviously wrong in the above? Why is TensorFlow grabbing an input with shape <code>(1,)</code>?</p>
",0
58712020,How to print the content of a variable of type <class 'tensorflow.python.framework.ops.Tensor'> in pyhton 3,"<p>I was reading the following code from Keras Documentation (<a href=""https://keras.io/examples/mnist_siamese/"" rel=""nofollow noreferrer"">https://keras.io/examples/mnist_siamese/</a>). I would like to print the values of x and y which are of type <strong>class 'tensorflow.python.framework.ops.Tensor'</strong> in the ""euclidean_distance"" function. I tried tf.print(x) but it prints <strong>tf.Operation 'lambda_3/PrintV2_1' type=PrintV2</strong> and the type(tf.print(x)) yields 
<strong>class 'tensorflow.python.framework.ops.Operation'</strong></p>

<p>How can I see the output of ""euclidean_distance"" function?</p>

<pre><code>from __future__ import absolute_import
from __future__ import print_function
import numpy as np

import random
from keras.datasets import mnist
from keras.models import Model
from keras.layers import Input, Flatten, Dense, Dropout, Lambda
from keras.optimizers import RMSprop
from keras import backend as K

num_classes = 10
epochs = 20


def euclidean_distance(vects):
    x, y = vects
    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)
    return K.sqrt(K.maximum(sum_square, K.epsilon()))


def eucl_dist_output_shape(shapes):
    shape1, shape2 = shapes
    return (shape1[0], 1)


def contrastive_loss(y_true, y_pred):
    '''Contrastive loss from Hadsell-et-al.'06
    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf
    '''
    margin = 1
    square_pred = K.square(y_pred)
    margin_square = K.square(K.maximum(margin - y_pred, 0))
    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)


def create_pairs(x, digit_indices):
    '''Positive and negative pair creation.
    Alternates between positive and negative pairs.
    '''
    pairs = []
    labels = []
    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1
    for d in range(num_classes):
        for i in range(n):
            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]
            pairs += [[x[z1], x[z2]]]
            inc = random.randrange(1, num_classes)
            dn = (d + inc) % num_classes
            z1, z2 = digit_indices[d][i], digit_indices[dn][i]
            pairs += [[x[z1], x[z2]]]
            labels += [1, 0]
    return np.array(pairs), np.array(labels)


def create_base_network(input_shape):
    '''Base network to be shared (eq. to feature extraction).
    '''
    input = Input(shape=input_shape)
    x = Flatten()(input)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.1)(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.1)(x)
    x = Dense(128, activation='relu')(x)
    return Model(input, x)


def compute_accuracy(y_true, y_pred):
    '''Compute classification accuracy with a fixed threshold on distances.
    '''
    pred = y_pred.ravel() &lt; 0.5
    return np.mean(pred == y_true)


def accuracy(y_true, y_pred):
    '''Compute classification accuracy with a fixed threshold on distances.
    '''
    return K.mean(K.equal(y_true, K.cast(y_pred &lt; 0.5, y_true.dtype)))


# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
input_shape = x_train.shape[1:]

# create training+test positive and negative pairs
digit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]
tr_pairs, tr_y = create_pairs(x_train, digit_indices)

digit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]
te_pairs, te_y = create_pairs(x_test, digit_indices)

# network definition
base_network = create_base_network(input_shape)

input_a = Input(shape=input_shape)
input_b = Input(shape=input_shape)

# because we re-use the same instance `base_network`,
# the weights of the network
# will be shared across the two branches
processed_a = base_network(input_a)
processed_b = base_network(input_b)

distance = Lambda(euclidean_distance,
                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])

model = Model([input_a, input_b], distance)

# train
rms = RMSprop()
model.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])
model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,
          batch_size=128,
          epochs=epochs,
          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))

# compute final accuracy on training and test sets
y_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])
tr_acc = compute_accuracy(tr_y, y_pred)
y_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])
te_acc = compute_accuracy(te_y, y_pred)

print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))
print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))
</code></pre>
",0
58728086,Passing `training=true` when using Tensorflow 2's Keras Functional API,"<p>When operating in graph mode in TF1, I believe I needed to wire up <code>training=True</code> and <code>training=False</code> via feeddicts when I was using the functional-style API. What is the proper way to do this in TF2?</p>

<p>I believe this is automatically handled when using <code>tf.keras.Sequential</code>. For example, I don't need to specify <code>training</code> in the following example from the <a href=""https://www.tensorflow.org/guide/migrate"" rel=""noreferrer"">docs</a>:</p>

<pre><code>model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, 3, activation='relu',
                           kernel_regularizer=tf.keras.regularizers.l2(0.02),
                           input_shape=(28, 28, 1)),
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dropout(0.1),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Model is the full model w/o custom layers
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(train_data, epochs=NUM_EPOCHS)
loss, acc = model.evaluate(test_data)
print(""Loss {:0.4f}, Accuracy {:0.4f}"".format(loss, acc))
</code></pre>

<p>Can I also assume that keras automagically handles this when training with the functional api? Here is the same model, rewritten using the function api:</p>

<pre><code>inputs = tf.keras.Input(shape=((28,28,1)), name=""input_image"")
hid = tf.keras.layers.Conv2D(32, 3, activation='relu',
                           kernel_regularizer=tf.keras.regularizers.l2(0.02),
                           input_shape=(28, 28, 1))(inputs)
hid = tf.keras.layers.MaxPooling2D()(hid)
hid = tf.keras.layers.Flatten()(hid)
hid = tf.keras.layers.Dropout(0.1)(hid)
hid = tf.keras.layers.Dense(64, activation='relu')(hid)
hid = tf.keras.layers.BatchNormalization()(hid)
outputs = tf.keras.layers.Dense(10, activation='softmax')(hid)
model_fn = tf.keras.Model(inputs=inputs, outputs=outputs)

# Model is the full model w/o custom layers
model_fn.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model_fn.fit(train_data, epochs=NUM_EPOCHS)
loss, acc = model_fn.evaluate(test_data)
print(""Loss {:0.4f}, Accuracy {:0.4f}"".format(loss, acc))
</code></pre>

<p>I'm unsure if <code>hid = tf.keras.layers.BatchNormalization()(hid)</code> needs to be <code>hid = tf.keras.layers.BatchNormalization()(hid, training)</code>?</p>

<p>A colab for these models can be found <a href=""https://gist.github.com/justincosentino/d36d66cddb1d606f72818c63d90d5147"" rel=""noreferrer"">here</a>.</p>
",0
58772846,Custom TensorFlow Keras optimizer,"<p>Suppose I want to write a custom optimizer class that conforms to the <code>tf.keras</code> API (using TensorFlow version>=2.0). I am confused about the documented way to do this versus what's done in implementations.</p>

<p>The documentation for <code>tf.keras.optimizers.Optimizer</code> <a href=""https://github.com/tensorflow/tensorflow/blob/3ba8ba551fc371389d184ba3917cfd826bdefd89/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L217-L223"" rel=""noreferrer"">states</a>,</p>

<pre class=""lang-none prettyprint-override""><code>  ### Write a customized optimizer.
  If you intend to create your own optimization algorithm, simply inherit from
  this class and override the following methods:

    - resource_apply_dense (update variable given gradient tensor is dense)
    - resource_apply_sparse (update variable given gradient tensor is sparse)
    - create_slots (if your optimizer algorithm requires additional variables)
</code></pre>

<p>However, the current <code>tf.keras.optimizers.Optimizer</code> implementation does not define a <code>resource_apply_dense</code> method, but it <em>does</em> define a private-looking <a href=""https://github.com/tensorflow/tensorflow/blob/3ba8ba551fc371389d184ba3917cfd826bdefd89/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L916-L928"" rel=""noreferrer""><code>_resource_apply_dense</code> method stub</a>. Similarly, there are no <code>resource_apply_sparse</code> or <code>create_slots</code> methods, but there are a <a href=""https://github.com/tensorflow/tensorflow/blob/3ba8ba551fc371389d184ba3917cfd826bdefd89/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L958-L977"" rel=""noreferrer""><code>_resource_apply_sparse</code> method stub</a> and a <a href=""https://github.com/tensorflow/tensorflow/blob/3ba8ba551fc371389d184ba3917cfd826bdefd89/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L434"" rel=""noreferrer""><code>_create_slots</code> method call</a>.</p>

<p>In official <code>tf.keras.optimizers.Optimizer</code> subclasses (using <code>tf.keras.optimizers.Adam</code> as an example), there are <a href=""https://github.com/tensorflow/tensorflow/blob/b0a5ae9891da8b1c0224a45c2c0130623b9fb1ce/tensorflow/python/keras/optimizer_v2/adam.py#L192-L227"" rel=""noreferrer""><code>_resource_apply_dense</code></a>, <a href=""https://github.com/tensorflow/tensorflow/blob/b0a5ae9891da8b1c0224a45c2c0130623b9fb1ce/tensorflow/python/keras/optimizer_v2/adam.py#L229-L267"" rel=""noreferrer""><code>_resource_apply_sparse</code></a>, and <a href=""https://github.com/tensorflow/tensorflow/blob/b0a5ae9891da8b1c0224a45c2c0130623b9fb1ce/tensorflow/python/keras/optimizer_v2/adam.py#L150-L159"" rel=""noreferrer""><code>_create_slots</code></a> methods, and there are no such methods without the leading underscore.</p>

<p>There are similar leading-underscore methods in slightly-less-official <code>tf.keras.optimizers.Optimizer</code> subclasses (e.g., <code>tfa.optimizers.MovingAverage</code> from TensorFlow Addons: <a href=""https://github.com/tensorflow/addons/blob/999aebc0961ccddb8174cc5331cc23a7291a2255/tensorflow_addons/optimizers/average_wrapper.py#L73-L76"" rel=""noreferrer""><code>_resource_apply_dense</code></a>, <a href=""https://github.com/tensorflow/addons/blob/999aebc0961ccddb8174cc5331cc23a7291a2255/tensorflow_addons/optimizers/average_wrapper.py#L78-L82"" rel=""noreferrer""><code>_resource_apply_sparse</code></a>, <a href=""https://github.com/tensorflow/addons/blob/999aebc0961ccddb8174cc5331cc23a7291a2255/tensorflow_addons/optimizers/moving_average.py#L92-L95"" rel=""noreferrer""><code>_create_slots</code></a>).</p>

<p>Another confounding point for me is that some of the TensorFlow Addons optimizers <em>also</em> override the <code>apply_gradients</code> method (e.g., <a href=""https://github.com/tensorflow/addons/blob/999aebc0961ccddb8174cc5331cc23a7291a2255/tensorflow_addons/optimizers/average_wrapper.py#L55-L57"" rel=""noreferrer""><code>tfa.optimizers.MovingAverage</code></a>), whereas the <code>tf.keras.optimizers</code> optimizers do not.</p>

<p>Moreover, I noticed that the <code>apply_gradients</code> method of <code>tf.keras.optimizers.Optimizer</code> method <a href=""https://github.com/tensorflow/tensorflow/blob/4a7069416442a82270494ec5ad26955e3ff85758/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L434"" rel=""noreferrer"">calls <code>_create_slots</code></a>, but the base <code>tf.keras.optimizers.Optimizer</code> class does not have a <code>_create_slots</code> method.
So, it seems that a <code>_create_slots</code> method <em>must</em> be defined in an optimizer subclass if that subclass does not override <code>apply_gradients</code>.</p>

<hr>

<h1>Questions</h1>

<p>What is the correct way to subclass a <code>tf.keras.optimizers.Optimizer</code>? Specifically,</p>

<ol>
<li>Does the <code>tf.keras.optimizers.Optimizer</code> documentation listed at the top simply mean to override the leading-underscore versions of the methods they mention (e.g., <code>_resource_apply_dense</code> instead of <code>resource_apply_dense</code>)? If so, are there any API guarantees about these private-looking methods not changing their behavior in future versions of TensorFlow? What are the signatures of these methods?</li>
<li>When would one override <code>apply_gradients</code> in addition to the <code>_apply_resource_[dense|sparse]</code> methods?</li>
</ol>

<hr>

<p><strong>Edit.</strong> Opened issue on GitHub: <a href=""https://github.com/tensorflow/tensorflow/issues/36449"" rel=""noreferrer"">#36449</a></p>
",1
58802573,Pre processing keras dataset using keras tokenizer,"<p>I am trying to do some pre processing using the keras tokenizer on data I read using the following code:</p>

<pre><code> dataset = tf.data.Dataset.from_tensor_slices(filenames)
    dataset = dataset.interleave(lambda x:
        tf.data.TFRecordDataset(x).prefetch(params.num_parallel_readers),
                                     cycle_length=params.num_parallel_readers,
                                     block_length=1)
        dataset = dataset.map(_parse_example, num_parallel_calls = params.num_parallel_calls)
</code></pre>

<p>Now that I have the parsed example (output of _parse_example map function) I want to do some pre-processing on the text using <code>tf.keras.preprocessing.text.Tokenizer</code> method <code>texts_to_sequences</code>.
However, texts_to_sequences expects an input of python strings and I get Tensors in the parsed_example.</p>

<p>I can work around it by using <code>py_func</code> to wrap my code (see <strong>'emb': tf.py_func..</strong> in the code below), but then I will not be able to serialize my model (according to the <code>py_func</code> documentation).</p>

<pre><code>dataset = dataset.map(lambda features, labels: 
                              ({'window': features['window'],
                                'winSize': features['winSize'],
                                'LandingPage': features['LandingPage'],
                                'emb': tf.py_func(getEmb, [features['window']], tf.int32)},
                                tf.one_hot(labels, hparams.numClasses) ))
</code></pre>

<p>Looking for a way to do that (or a link to some similar example)</p>
",0
58826512,Tensorflow 2.0 does not iterate through entire dataset when tf.keras.model.fit is called,"<p>I am training a model in tf.keras with tensorflow 2.0. <strong>I am having an issue where my model appears to train successfully, but it is not iterating through the entire dataset.</strong> I restructured the code into tensorflow 1.15, and I do not have this issue in tensorflow 1.x. I am following <a href=""https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/#comment-510384"" rel=""nofollow noreferrer"">this tutorial</a> for Multiple Input Series. Below are more details:</p>

<p>I have a time-series dataset. It is very small so I am able to load it into memory, so I do not need the dataset API. I am windowing the time-series to produce two arrays, X and Y, for instance, </p>

<pre><code>X=[
   [[1,2,3],[4,5,6],   [7,8,9]],
   [[4,5,6],[7,8,9],   [10,11,12]],
   [[7,8,9],[10,11,12],[13,14,15]],
   ...
  ] 
Y = [
     [4],
     [7],
     [10],
     ...
    ]
</code></pre>

<p>(yes, I realize that I could just as easily only include one of the features and make <code>X=[[[1,2,3]], [[4,5,6]], [[7,8,9]], ...]</code>, but I am going to include many features which aren't this perfectly synced when the pipeline works. Also, even when I only include the 1st feature, I still see the problem I describe.)</p>

<p>Then, I build my model:</p>

<pre><code>model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')
</code></pre>

<p>and then I train it:</p>

<pre><code>model.fit([X],[Y],num_epochs=300,validation_split=0.2)
</code></pre>

<p>It correctly reports the number of train and validation samples, and then the progress bar pops up... but that's where the success stops. The val_loss and val_mean_squared_error is always 0, for every epoch, and it appears to never train more than a fraction (~1/1000) of the windows in my dataset. This is the print out:</p>

<pre><code>Epoch X/300   192/162636 [..............................] - ETA: 45:42 - loss: 0.4783 - mean_squared_error: 0.4783 - val_loss: 0.0000e+00 - val_mean_squared_error: 0.0000e+00
</code></pre>

<p>When I execute the same code in tf 1.15, it executes as I expect - the epochs take ~45 minutes (in tf 2.0 they take &lt; 3 seconds), and tf 1.15 reports a legitimate val_loss and val_mean_squared_error. I cannot figure out why the model does not train correctly in tf 2.0. This is my first time I wrote code in tf 2.0/did not migrate from tf 1.13, but all of the legacy code that I upgraded from tf 1.13 to tf 2.0 executed without any errors. None of the legacy code that I migrated had sequential models. </p>

<p>There are no errors, warnings, or info that is reported, it just stops iterating through my dataset early. <strong>Does anyone have any insights into the changes in tf.keras.Model.fit in tensorflow 2.0 that could be causing this? Or are there any mistakes in the path that I have taken? Any insight would be HUGELY appreciated.</strong> Thanks!</p>

<p>EDIT 11/25:</p>

<p>I have filed a GitHub issue for this bug <a href=""https://github.com/tensorflow/tensorflow/issues/34585"" rel=""nofollow noreferrer"">here</a>. Please see that post for updates on progress, and I'll try to remember to update this post when the issue is resolved.</p>
",0
58842107,How do I update a model using a pre-release version of Tensorflow to run in a Google Colab instance?,"<p>I'm trying to use the <a href=""https://github.com/google-research-datasets/wiki-reading"" rel=""nofollow noreferrer"">WikiReading</a> dataset and model in a project and train it using a Google Colaboratory instance. For that purpose, I'm adapting the code to a Jupyter Notebook, which also uses a more recent version of Tensorflow than the provided baseline Bag of Words model did. The original paper and accompanying baseline model was published in August 2016, which predates Tensorflow 1.0.0.</p>

<p>I've gone some way towards the process of updating the model to Tensorflow v1.x, but I appear to have hit a roadblock. From my (fairly limited) understanding, the last step in the model is to apply a softmax function on the results. In the original model, this was done using the following function call:</p>

<pre><code>softmax, loss = tf.contrib.learn.ops.softmax_classifier(
        joint_enc, answers, answer_embeddings, answer_biases)
</code></pre>

<p>This is then used in this statement for the optimization:</p>

<pre><code>train_op = tf.contrib.layers.optimize_loss(
        loss, tf.contrib.framework.get_global_step(),
        learning_rate=LEARNING_RATE,
        optimizer='Adam')
</code></pre>

<p>I can't seem to find any documentation relating to the <code>tf.contrib.learn.ops.softmax_classifier()</code> function online. I'm assuming it takes in 4 tensors in some order, most likely something like the first holds the batch to classify, the second one holds a list of the answers to predict with the 3rd and 4th holding the embedding and biases for each answer.</p>

<p>My problem is that I cannot find a function that maps neatly to that format with the same output and I'm not sure what transformations to apply to my tensors to get a similar result using something like <code>tf.nn.softmax()</code> without access to the documentation of <code>tf.contrib.learn.ops.softmax_classifier()</code>. How should I go about tackling this problem? Should I just rewrite the model_fcn()?</p>

<h2>Original model_fn()</h2>

<pre class=""lang-py prettyprint-override""><code>def bow_model(features, target):
    document = utils.prune_out_of_vocab_ids(features['document_sequence'], VOCAB_SIZE)
    question = utils.prune_out_of_vocab_ids(features['question_sequence'], VOCAB_SIZE)
    answers = tf.squeeze(tf.one_hot(target, ANSWER_NUM, 1.0, 0.0),
                         squeeze_dims=[1])
    embeddings = tf.get_variable('embeddings', [VOCAB_SIZE, EMBED_DIM])
    doc_enc = layers.safe_embedding_lookup_sparse(
        [embeddings], document, None, combiner='sum')
    question_enc = layers.safe_embedding_lookup_sparse(
        [embeddings], question, None, combiner='sum')
    joint_enc = tf.concat(1, [doc_enc, question_enc])
    answer_embeddings = tf.get_variable(
        'answer_embeddings', [ANSWER_DIM, ANSWER_NUM])
    answer_biases = tf.get_variable('answer_biases', [ANSWER_NUM])
    softmax, loss = learn.ops.softmax_classifier(
        joint_enc, answers, answer_embeddings, answer_biases)
    train_op = layers.optimize_loss(
        loss, tf.contrib.framework.get_global_step(),
        learning_rate=LEARNING_RATE,
        optimizer='Adam')
    return softmax, loss, train_op
</code></pre>

<h2>Partially updated model_fn():</h2>

<pre class=""lang-py prettyprint-override""><code>def bow_model(features, labels, mode):
    document = prune_out_of_vocab_ids(features['document_sequence'], VOCAB_SIZE)
    question = prune_out_of_vocab_ids(features['question_sequence'], VOCAB_SIZE)

    answers = tf.squeeze(tf.one_hot(labels, ANSWER_NUM, 1.0, 0.0))

    embeddings = tf.get_variable('embeddings',
                                 [VOCAB_SIZE, EMBED_DIM])
    answer_embeddings = tf.get_variable('answer_embeddings',
                                        [ANSWER_DIM, ANSWER_NUM])
    answer_biases = tf.get_variable('answer_biases',
                                    [ANSWER_NUM])

    doc_enc = layers.safe_embedding_lookup_sparse(
        [embeddings], document, None, combiner='sum')
    question_enc = layers.safe_embedding_lookup_sparse(
        [embeddings], question, None, combiner='sum')
    joint_enc = tf.concat(axis=1, values=[doc_enc, question_enc])

    # softmax, loss = tf.contrib.learn.ops.softmax_classifier(
    #     joint_enc, answers, answer_embeddings, answer_biases)
    # replaced by:
    logits = tf.nn.xw_plus_b(doc_enc, answer_embeddings, answer_biases)
    softmax = tf.nn.softmax(logits)
    loss = tf.losses.softmax_cross_entropy(onehot_labels=answers,
                                           logits=logits,
                                           weights=answer_embeddings)

    train_op = layers.optimize_loss(
        loss, tf.train.get_global_step(),
        learning_rate=LEARNING_RATE,
        optimizer='Adam')

    return learn.EstimatorSpec(mode=mode,
                               predictions=softmax,
                               loss=loss,
                               train_op=train_op)
</code></pre>

<h2>Original input_fn():</h2>

<pre class=""lang-py prettyprint-override""><code>def get_wikireading_input():
    filename = ""../train-*""
    feature_info = {k: tf.VarLenFeature(dtype=tf.int64) for k in SPARSE_FEATURES}
    feature_info['answer_ids'] = tf.VarLenFeature(dtype=tf.int64)
    def input_fn():
        features = learn.read_batch_features(
            filename, BATCH_SIZE, feature_info,
            reader=tf.TFRecordReader)
        target = features.pop('answer_ids')
        target = utils.resize_axis(tf.sparse_tensor_to_dense(target), 1, 1)
        return features, target
    return input_fn
</code></pre>

<h2>Updated input_fn()</h2>

<pre class=""lang-py prettyprint-override""><code>def input_fn():
    features = {'document_sequence': tf.VarLenFeature(dtype=tf.int64),
                'question_sequence': tf.VarLenFeature(dtype=tf.int64),
                'answer_ids': tf.VarLenFeature(dtype=tf.int64)}

    files = tf.data.Dataset.list_files(file_pattern=filename)
    dataset = files.interleave(tf.data.TFRecordDataset,
                          cycle_length=AUTOTUNE,
                          num_parallel_calls=AUTOTUNE)

    def parse_fn(serialized):
        example = tf.io.parse_single_sequence_example(serialized=serialized,
                                                      sequence_features=features)[1]
        labels = example.pop('answer_ids')
        labels = resize_axis(tf.sparse_tensor_to_dense(labels), 1, 1)
        return example, labels

    dataset = dataset.map(map_func=parse_fn, num_parallel_calls=AUTOTUNE)
    dataset = dataset.batch(batch_size=BATCH_SIZE)
    dataset = dataset.shuffle(buffer_size=BATCH_SIZE)
    dataset = dataset.prefetch(buffer_size=AUTOTUNE)
    return dataset
</code></pre>

<p>The full Jupyter notebook can be accessed <a href=""https://github.com/ThierrySt-Arnaud/wiki-reading/blob/colab-conversion/colab/wiki_reading_training_en.ipynb"" rel=""nofollow noreferrer"">here</a></p>

<h2>Edit:</h2>

<p>I have found the source for the mentioned function <a href=""https://github.com/tensorflow/tensorflow/blob/r0.8/tensorflow/contrib/learn/python/learn/ops/losses_ops.py"" rel=""nofollow noreferrer"">here</a>. The deprecation warning  says this about updating this function:</p>

<blockquote>
  <p>'Use <code>tf.losses.softmax_cross_entropy</code> and explicit logits computation.'</p>
</blockquote>

<p>Now, I'm replicating the operations of this function as follows:</p>

<pre class=""lang-py prettyprint-override""><code>  logits = tf.nn.xw_plus_b(doc_enc, answer_embeddings, answer_biases)
  softmax = tf.nn.softmax(logits)
  loss = tf.losses.softmax_cross_entropy(answers, logits, answer_embeddings)
</code></pre>

<p>but this gives me:</p>

<blockquote>
  <p>ValueError Dimensions must be equal, but are 20 and 40 for 'xw_plus_b/MatMul' (op: 'BatchMatMulV2') with input shapes: [?,?,20], [40,5000].</p>
</blockquote>

<p>This makes perfect sense when I look at the constant definitions:</p>

<pre class=""lang-py prettyprint-override""><code>VOCAB_SIZE = 10000
EMBED_DIM = 20
ANSWER_DIM = 2 * EMBED_DIM
ANSWER_NUM = 5000
BATCH_SIZE = 128
LEARNING_RATE = 0.01
HIDDEN_SIZE = 128
</code></pre>

<p>I guess my questions now are:</p>

<ul>
<li>Why is ANSWER_NUM and ANSWER_DIM not equal to VOCAB_SIZE and EMBED_DIM, respectively? Shouldn't they have the same size?</li>
<li>How could this have worked before?</li>
</ul>

<h3>Edit 2:</h3>

<p>As I try to update and train this model, I have grown more and more confused by the way it is defined. This is largely due to my inexperience with machine learning in general and TensorFlow in particular, but there are some things that don't make sense, at least to me. I have adjusted the ANSWER_NUM and ANSWER_DIM as I mention above and updated other functions and parameters (seen in the updated model_fn above) which gives me a valid data graph but the following error when trying to fit:</p>

<blockquote>
  <p>(0) Invalid argument: assertion failed: [weights can not be broadcast to values.] [weights.shape=] [answer_embeddings/read:0] [20 10000] [values.shape=] [softmax_cross_entropy_loss/xentropy/Reshape_2:0] [128 0] [is_scalar=] [0]</p>
</blockquote>

<p>This probably just requires a formatting step (that I'm not entirely sure of yet) before feeding running softmax_cross_entropy(). However, I also noticed that there is no definition of the hidden layers anywhere in the original model and HIDDEN_SIZE is unused. Is there an implicit definition of those layers somewhere in the model that I'm missing?</p>

<p>At this point, I feel like it will be easier to just use a Keras functional model in TensorFlow 2.x to get as close as possible to the <em>perceived</em> original model.</p>
",1
58857720,Is there an equivalent PyTorch function for `tf.nn.space_to_depth`,"<p>As the title says, is there an equivalent PyTorch function for <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/space_to_depth"" rel=""nofollow noreferrer"">tf.nn.space_to_depth</a>?</p>
",0
58873635,What is tensorflow.matmul?,"<p>From the output of print, it is function. But according to the <a href=""https://www.tensorflow.org/api_docs/python/tf/Operation"" rel=""nofollow noreferrer"">official document</a>:</p>

<blockquote>
  <p>An Operation is a node in a TensorFlow Graph that takes zero or more
  Tensor objects as input, and produces zero or more Tensor objects as
  output. Objects of type Operation are created by calling a Python op
  constructor (such as tf.matmul) or tf.Graph.create_op.</p>
</blockquote>

<p>it is a constructor. So I think it is a class name. But, printing the return value of tf.matmul shows it is a tensor, not an ""Object of type Operation"". Is the class Tensor inherited from the class Operation? I tried to find the definition of tf.matmul in tensorflow source code but could not get it. </p>
",1
58933545,Using Tensorflow 2.0 and eager execution without Keras,"<p>So this question might stem from a lack of knowledge about tensorflow. But I am trying to build a multilayer perceptron with <code>tensorflow 2.0</code>, but without <code>Keras</code>.</p>
<p>The reason being that it is a requirement for my machine learning course that we do not use keras. Why you might ask? I am not sure.</p>
<p>I already have implemented our model in <code>tensorflow 2.0</code> with Keras ease, and now I want to do the exact same thing without <code>keras</code>.</p>
<pre><code>model = Sequential()
model.add(Dense(64, activation='relu', input_dim=784))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(5, activation='softmax'))

sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy',
              optimizer=Adam(),
              metrics=['accuracy'])

X_train = X[:7000]
y_train = tf.keras.utils.to_categorical(y[:7000], num_classes=5)
X_dev = X[7000:]
y_dev = tf.keras.utils.to_categorical(y[7000:], num_classes=5)

model.fit(X_train, y_train,
          epochs=100,
          batch_size=128)
score = model.evaluate(X_dev, y_dev, batch_size=128)
print(score)
</code></pre>
<p>Here is my problem. Whenever I look up the documentation on <code>Tensorflow 2.0</code>, then even the guides on custom training are using Keras.</p>
<p>As placeholders and sessions are a thing of the past in <code>tensorflow 2.0</code>, as I understand it, then I am a bit unsure of how to structure it.</p>
<p>I can make tensor objects. I have the impression that I need to use eager execution and use gradient tape. But I still am unsure of how to put these things together.</p>
<p>Now my question is. Where should I look to get a better understanding? Which direction has the greatest descent?</p>
<p>Please do tell me if I am doing this stack overflow post wrong. It is my first time here.</p>
",1
58947679,No gradients provided for any variable in tensorflow2.0,"<p>I met a problem when I tried to use <code>tensorflow2.0</code> to create a transformer based on the official guidelines posted by the <code>TensorFlow</code> and when I add a full connected net it seems that both the classification loss and the translate loss as gradients on some of the variables. </p>

<p>But once I try to add the two loss the gradients to all variables disappear. I have no idea and I tried to figure to solved the problem for weeks. Could anyone give me some suggestions?</p>

<pre><code>@tf.function(input_signature=train_step_signature)
def train_step(group, inp, tar, label):
    tar_inp = tar[:, :-1]
    tar_real = tar[:, 1:]  # sess=tf.compat.v1.Session()
    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)
    with tf.GradientTape(persistent=True) as tape:
        classfication, predictions, _ = transformer(inp, tar_inp,
                                                    True,
                                                    enc_padding_mask,
                                                    combined_mask,
                                                    dec_padding_mask)
        loss = loss_function(tar_real, predictions)
        loss2 = tf.nn.softmax_cross_entropy_with_logits(label, classfication)

    #print(loss,loss2)
    a=tape.gradient(loss,trainsformer.trainable_variable)
    gradients = tape.gradient(loss+loss2, transformer.trainable_variables)

    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))
    class_loss(loss2)
    train_loss(loss)
    train_accuracy(tar_real, predictions)
</code></pre>

<p>below is my error infomation</p>

<pre><code>    ValueError                                Traceback (most recent call last)
&lt;ipython-input-2-81054f0385cb&gt; in &lt;module&gt;()
    999     # inp -&gt; portuguese, tar -&gt; english
   1000     for (batch, (group, inp, tar, label)) in enumerate(train_dataset):
-&gt; 1001         train_step(group, inp, tar, label)
   1002         if batch % 50 == 0:
   1003             print(

8 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--&gt; 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    501       # This is the first call of __call__, so we have to initialize.
    502       initializer_map = object_identity.ObjectIdentityDictionary()
--&gt; 503       self._initialize(args, kwds, add_initializers_to=initializer_map)
    504     finally:
    505       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    406     self._concrete_stateful_fn = (
    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 408             *args, **kwds))
    409 
    410     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   1846     if self.input_signature:
   1847       args, kwargs = None, None
-&gt; 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   1849     return graph_function
   1850 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2148         graph_function = self._function_cache.primary.get(cache_key, None)
   2149         if graph_function is None:
-&gt; 2150           graph_function = self._create_graph_function(args, kwargs)
   2151           self._function_cache.primary[cache_key] = graph_function
   2152         return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2039             arg_names=arg_names,
   2040             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 2041             capture_by_value=self._capture_by_value),
   2042         self._function_attributes,
   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    913                                           converted_func)
    914 
--&gt; 915       func_outputs = python_func(*func_args, **func_kwargs)
    916 
    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    357         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    359     weak_wrapped_fn = weakref.ref(wrapped_fn)
    360 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)
    903           except Exception as e:  # pylint:disable=broad-except
    904             if hasattr(e, ""ag_error_metadata""):
--&gt; 905               raise e.ag_error_metadata.to_exception(e)
    906             else:
    907               raise

ValueError: in converted code:

    &lt;ipython-input-1-81054f0385cb&gt;:856 train_step  *
        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:427 apply_gradients
        grads_and_vars = _filter_grads(grads_and_vars)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:1025 _filter_grads
        ([v.name for _, v in grads_and_vars],))

    ValueError: No gradients provided for any variable: ['transformer_1/encoder_1/embedding_2/embeddings:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_98/kernel:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_98/bias:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_99/kernel:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_99/bias:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_100/kernel:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_100/bias:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_101/kernel:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_101/bias:0', 'transformer_1/encoder_1/encoder_layer_6/sequential_12/dense_102/kernel:0', 'transformer_1/encoder_1/encoder_layer_6/sequential_12/dense_102/bias:0', 'transformer_1/encoder_1/encoder_layer_6/sequential_12/dense_103/kernel:0', 'transformer_1/encoder_1/encoder_layer_6/sequential_12/dense_103/bias:0', 'transformer_1/encoder_1/encoder_layer_6/layer_normalization_30/gamma:0', 'transformer_1/encoder_1/encoder_layer_6/layer_normalization_30/beta:0', 'transformer_1/encoder_1/encoder_layer_6/layer_normalization_31/gamma:0', 'transformer_1/encoder_1/encoder_layer_6/layer_normalization_31/beta:0', 'transformer_1/encoder_1/encoder_layer_7/multi_head_attention_19/dense_104/kernel:0', 'transformer_1/encoder_1/encoder...
</code></pre>
",0
58959582,"Saving, loading, and predicting from a TensorFlow Estimator model (2.0)","<p>Is there a guide anywhere for serializing and restoring <code>Estimator</code> models in TF2? The documentation is very spotty, and much of it not updated to TF2. I've yet to see a clear ands complete example anywhere of an <code>Estimator</code> being saved, loaded from disk and used to predict from new inputs. </p>

<p>TBH, I'm a bit baffled by how complicated this appears to be. Estimators are billed as simple, relatively high-level ways of fitting standard models, yet the process for using them in production seems very arcane. For example, when I load a model from disk via <code>tf.saved_model.load(export_path)</code> I get an <code>AutoTrackable</code> object:</p>

<p><code>&lt;tensorflow.python.training.tracking.tracking.AutoTrackable at 0x7fc42e779f60&gt;</code></p>

<p>Its not clear why I don't get my <code>Estimator</code> back. It looks like there used to be a useful-sounding function <code>tf.contrib.predictor.from_saved_model</code>, but since <code>contrib</code> is gone, it does not appear to be in play anymore (except, it appears, in TFLite).</p>

<p>Any pointers would be very helpful. As you can see, I'm a bit lost.</p>
",1
58963793,ValueError: Shapes must be equal rank in assign_add(),"<p>I am reading <a href=""https://www.tensorflow.org/api_docs/python/tf/Variable"" rel=""nofollow noreferrer"">tf.Variable in Tensorflow r2.0</a> in TF2: </p>

<pre><code>import tensorflow as tf

# Create a variable.
w = tf.constant([1, 2, 3, 4], tf.float32, shape=[2, 2])

# Use the variable in the graph like any Tensor.
y = tf.matmul(w,tf.constant([7, 8, 9, 10], tf.float32, shape=[2, 2]))
v= tf.Variable(w)
# The overloaded operators are available too.
z = tf.sigmoid(w + y)
tf.shape(z)
# Assign a new value to the variable with `assign()` or a related method.
v.assign(w + 1)
v.assign_add(tf.constant([1.0, 21]))
</code></pre>

<blockquote>
  <p>ValueError: Shapes must be equal rank, but are 2 and 1 for
  'AssignAddVariableOp_4' (op: 'AssignAddVariableOp') with input shapes:
  [], <a href=""https://youtu.be/Up9CvRLIIIw?t=113"" rel=""nofollow noreferrer"">2</a>.</p>
</blockquote>

<p>And also how come the following returns false?</p>

<pre><code>tf.shape(v) == tf.shape(tf.constant([1.0, 21],tf.float32))
</code></pre>

<p>My other question is that when we are in TF 2, we should not use tf.Session() anymore, correct? It seems <a href=""https://youtu.be/Up9CvRLIIIw?t=113"" rel=""nofollow noreferrer"">we should never run session.run()</a>, but the API document keys doing it with tf.compat.v1, etc. So why they are using it in TF2 docs?</p>

<p>Any help would be appreciated.</p>

<p>CS</p>
",1
58990432,What happens when we give an input other than the classes that we trained our model for?,"<p>I have modeled a Dog and Cat classifier using TensorFlow and Keras with the help of online tutorials. The model seems to work fine for both the cat and dog images. However, when I give a bird image as the input to the trained model, the output turns out to be a dog, which is not the desirable output. So, what should actually happen when we give an input that belongs to a class other than the classes that we train our model for? Isn't it supposed to give something like a 'not detected' error or something? If yes, how does that actually work? Please explain. 
Thanks in advance!</p>
",0
59033036,Create a checkpoint for class Model(object) in Tensorflow 2,"<p>I was just trying to use low level API of tensorflow2. I created my model based on this tutorial :
<a href=""https://www.tensorflow.org/tutorials/customization/custom_training#define_the_model"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/customization/custom_training#define_the_model</a></p>

<p>Then I want to create a checkpoint for my training process, and I follow this tutorial :
<a href=""https://www.tensorflow.org/guide/checkpoint"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/checkpoint</a></p>

<p>The problem is the checkpoint's tutorial use a class with tf.keras.Model as parameter, while I use object as my parameter. It gave me error, said that it was expecting a trackable object.</p>

<p>Here is the snippet of my code:</p>

<pre><code>class SimpleANN(object):
    def __init__(self):
        initializer = tf.initializers.glorot_uniform()
        self.w1 = tf.Variable(initializer([784, 360]), name = 'weight1', trainable = True, dtype = tf.float32)
        self.w2 = tf.Variable(initializer([360, 64]), name = 'weight2', trainable = True, dtype = tf.float32)
        self.w3 = tf.Variable(initializer([64, 10]), name = 'weight3', trainable = True, dtype = tf.float32)

    def __call__(self, x, leaky_relu_alpha = 0.2):
        fc1 = tf.nn.leaky_relu(tf.matmul(x, self.w1), alpha = leaky_relu_alpha)
        fc2 = tf.nn.leaky_relu(tf.matmul(fc1, self.w2), alpha = leaky_relu_alpha)
        logits = tf.matmul(fc2, self.w3)

        return logits

model = SimpleANN() 
optimizer = tf.keras.optimizers.Adam(learning_rate)

ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, model = model)
</code></pre>

<p>then I got this error :</p>

<pre><code>ValueError: `Checkpoint` was expecting a trackable object (an object
derived from `TrackableBase`), got &lt;__main__.SimpleANN object at
0x000001D859792748&gt;. If you believe this object should be trackable
(i.e. it is part of the TensorFlow Python API and manages state),
please open an issue.
</code></pre>

<p>I would like to know If it is able to implement tf.train.Checkpoint for the low level API, as what I was doing.</p>
",0
59056872,Why class name change after saving a keras model?,"<p>i wrote a basic keras model (tf.keras.__version = 2.2.4-tf) using tensorflow (2.0.0) :</p>

<pre><code>import tensorflow as tf

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1, activation='linear',input_shape=(1,),name='equation'))
model.compile(optimizer='RMSprop', loss='mean_squared_error')
model.save('c:\\tmp\\oneneuron')
print(""Model saved type : "", type(model))
loaded_model = tf.keras.models.load_model('c:\\tmp\\oneneuron')
print(""Model loaded type : "", type(loaded_model))
print(""compare object model with loaded_model type : "",isinstance(model,type(loaded_model)))
print(""compare object loaded_model with model type : "",isinstance(loaded_model,type(model)))
print(""compare sublclass loaded_model and model type : "",issubclass(type(loaded_model),type(model)))
</code></pre>

<p>Results are </p>

<pre><code>Python 3.6.6 (v3.6.6:4cf1f54eb7, Jun 27 2018, 03:37:03) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; exec(open(r'C:\tmp\myPython\test_type_model.py').read())
2019-11-26 18:49:39.071088: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-11-26 18:49:39.574113: W tensorflow/python/util/util.cc:299] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING: Logging before flag parsing goes to stderr.
W1126 18:49:39.627490 11772 deprecation.py:506] From F:\Program Files\Python\lib\site-packages\tensorflow_core\python\ops\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Model saved type :  &lt;class 'tensorflow.python.keras.engine.sequential.Sequential'&gt;
Model loaded type :  &lt;class 'tensorflow.python.keras.saving.saved_model.load.Sequential'&gt;
compare object model with loaded_model type :  False
compare object loaded_model with model type :  True
compare sublclass loaded_model and model type :  True
</code></pre>

<p>Where can I find the difference between tensorflow.python.keras.saving.saved_model.load.Sequential and tensorflow.python.keras.engine.sequential.Sequential in tensorflow or keras documentation?</p>
",1
59074659,Best practice for allocating GPU and CPU resources in TensorFlow,"<p>I'm wondering what is the correct way to set devices for creating/training a model in order to optimize resource usage for speedy training in TensorFlow with the Keras API? I have 1 CPU and 2 GPUs at my disposal. I was initially using a <code>tf.device</code> context to create my model and train on GPUs only, but then I saw in the TensorFlow documentation for <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/multi_gpu_model"" rel=""nofollow noreferrer""><code>tf.keras.utils.multi_gpu_model</code></a>, they suggest explicitly instantiating the model on the CPU:</p>

<pre><code># Instantiate the base model (or ""template"" model).
# We recommend doing this with under a CPU device scope,
# so that the model's weights are hosted on CPU memory.
# Otherwise they may end up hosted on a GPU, which would
# complicate weight sharing.
with tf.device('/cpu:0'):
    model = Xception(weights=None,
                     input_shape=(height, width, 3),
                     classes=num_classes)

# Replicates the model on 8 GPUs.
# This assumes that your machine has 8 available GPUs.
parallel_model = multi_gpu_model(model, gpus=8)
parallel_model.compile(loss='categorical_crossentropy',
                       optimizer='rmsprop')
</code></pre>

<p>I did this, and now when I train I see my CPU usage go way up with all 8 cores at about 70% usage each, and my GPU memory is maxed out. Would things go faster if the model were created on one of the GPUs? Even if I have just 1 GPU, is it still better to create model on CPU and use <code>tf.device</code> context to train the model on the GPU?</p>
",1
59092423,What is a nested structure in TensorFlow?,"<p>Going over the documentation for <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">tf.data.Datasets</a>, I see  that there is a frequent mention of ""nested structures"". What exactly is meant by that? Can any Python data type be considered a nested structure (e.g., <code>(1,3, (7,6, (0), 5))</code> or <code>dict(k=dict(3), 7, None</code>) or does this specifically refer to TensorFlow data types?</p>
",1
59109662,How to use Tensorflow Dataset for CNN Model Training,"<p>I wanted to feed my data using <code>tf.data.Dataset</code> class</p>

<pre><code>
from tensorflow_core.python.keras.datasets import cifar10
(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()

train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))
test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))

</code></pre>

<ul>
<li><p>I am doing this to use <code>Dataset</code> in my pipeline.</p></li>
<li><p>Utilize other features of <code>Dataset</code> further down.</p></li>
</ul>

<p>I am defining my model like this</p>

<pre><code>    model = models.Sequential()
    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
    model.add(layers.MaxPool2D((2, 2)))
    # more layers
</code></pre>

<p>But when I call to train the model</p>

<pre><code>model.fit(train_dataset, epochs=10, validation_data=test_dataset, callbacks=[ cp_callback])

</code></pre>

<p>I get an error</p>

<blockquote>
  <p>ValueError: Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (32, 32, 3)</p>
</blockquote>

<ul>
<li>What is really happening? How do I use <code>DataSet</code> in my Conv2D layer with input_shape=(32, 32, 3) ?</li>
</ul>

<p>The Tensorflow tutorial (<a href=""https://www.tensorflow.org/tutorials/load_data/numpy"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/load_data/numpy</a>) did not cover this scenario and I cannot find an explanation that will help me solve my problem.</p>
",0
59173505,Tensorflow 2.0 custom gradient function,"<p>I am trying to build a custom gradient for A piece-wise continuous function that I found in the paper</p>

<p>A continuum among logarithmic, linear, and exponential functions, and its potential to improve generalization in neural networks - Godfrey</p>

<p>Current problem: the gradient explodes for the learnable parameter in a custom layer of my network, causing <code>nans</code> to be generated. This custom layer is a learnable activation function defined in the paper above.</p>

<p>Tried solutions</p>

<ul>
<li>clip the gradient the optimizer -> This had no affect when I compiled the model with e.g <code>keras.optimizer.Adam(clipnorm=0.5)</code></li>
<li>clip the value of the learnable parameter in the custom layer -> This gave some errors that I am unable to recall</li>
<li>create a custom gradient calculation to control the gradients of the learnable parameter -> not sure how the gradient should be calculated.</li>
</ul>

<p>My current gradient calculation function is like this</p>

<pre><code>@tf.custom_gradient
def call_lt0(alpha_actv, x):
    def grad(dx):
        grad_x = 1/(1-alpha_actv * (alpha_actv + x))
        grad_alpha = (tf.math.log(1-alpha_actv * (alpha_actv + x)) - ((2 * alpha_actv ** 2 + alpha_actv * x)/(alpha_actv ** 2 + alpha_actv * x - 1))) / alpha_actv ** 2
        return grad_x * dx, grad_alpha * dx  # denk ik..?
    return (tf.math.exp(alpha_actv * x) - 1) / alpha_actv + alpha_actv, grad
</code></pre>

<p>where I give the gradient of x variable and the learnable parameter alpha. However, when I check these outputs with <code>tf.GradientTape</code> I get only one output.. where I expected two, because of my definition of <code>grad</code>.
Also, when I let keras/tensorflow calculate everything, and check the output with <code>tf.GradientTape</code> again, I get only one value.. which is what I would expect for the single learnable parameter.
SO somewhere I am doing something wrong, or misinterpreting the <code>grad</code> function.</p>

<p>Can someone point me the way?</p>
",0
59177677,Custom aggregation for tf.GradientTape().gradient? (TF2.0),"<p>As far as I know, the tf.gradients function provides option to choose the aggregation method for summarizing the gradients from multiple sources.
<a href=""https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/gradients"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/gradients</a></p>

<p>However according to the Tensorflow API documentation, the tf.GradientTape().gradient method has no such option.
<a href=""https://www.tensorflow.org/api_docs/python/tf/GradientTape"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/GradientTape</a></p>

<p>So my questions are as follows:</p>

<ol>
<li><p>Is there any way to change the aggregation method in tf.GradientTape().gradient?</p></li>
<li><p>If not, is there any way to obtain the gradient with a custom aggregation method which is compatible with eager execution?</p></li>
</ol>
",1
59177925,Tensorflow : How to build efficient NLP pipeline using tf Dataset,"<p>I am working on TensorFlow and trying to create an efficient training and inference pipeline using tf.dataset API but facing some error :</p>

<p>For example, a simple RNN network structure is like this :</p>

<pre><code>import tensorflow as tf
import numpy as np
# hyper parameters
vocab_size          = 20
word_embedding_dim  = 100
batch_size          = 2



tf.reset_default_graph()
# placeholders
sentences             = tf.placeholder(tf.int32, [None,None], name='sentences')
targets               = tf.placeholder(tf.int32, [None, None], name='labels' )
keep_prob             = tf.placeholder(tf.float32, [1,], name='dropout')
keep_prob             = tf.cast(keep_prob.shape[0],tf.float32)


# embedding
word_embedding         = tf.get_variable(name='word_embedding_',
                                             shape=[vocab_size, word_embedding_dim],
                                             dtype=tf.float32,
                                             initializer = tf.contrib.layers.xavier_initializer())
embedding_lookup = tf.nn.embedding_lookup(word_embedding, sentences)



#  bilstm model
with tf.variable_scope('forward'):
    fr_cell = tf.contrib.rnn.LSTMCell(num_units = 15)
    dropout_fr = tf.contrib.rnn.DropoutWrapper(fr_cell, output_keep_prob = 1. - keep_prob)

with tf.variable_scope('backward'):
    bw_cell = tf.contrib.rnn.LSTMCell(num_units = 15)
    dropout_bw = tf.contrib.rnn.DropoutWrapper(bw_cell, output_keep_prob = 1. - keep_prob)

with tf.variable_scope('bi-lstm') as scope:
    model,last_state = tf.nn.bidirectional_dynamic_rnn(dropout_fr,
                                                       dropout_bw,
                                                       inputs=embedding_lookup,
                                                       dtype=tf.float32)

logits             = tf.transpose(tf.concat(model, 2), [1, 0, 2])[-1]
linear_projection  = tf.layers.dense(logits, 5)



#loss
cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits = linear_projection, labels = tf.cast(targets,tf.float32))
loss = tf.reduce_mean(tf.reduce_sum(cross_entropy, axis=1))
optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)
</code></pre>

<p>And dummy data is :</p>

<pre><code>dummy_data    = [[1,3,4,5,5,12],[1,3,4,4,12,0],[12,4,12,0,0,0],[1,3,4,5,5,12]]
dummpy_labels = [[1,0,0,0,0],[0,1,0,1,0],[1,0,0,0,0],[0,1,0,1,0]]
</code></pre>

<p>Now How I typically train this network by defining slice and pad sequences manually :</p>

<pre><code>#  pad and slice 


def get_train_data(batch_size, slice_no):

    batch_data_j = np.array(dummy_data[slice_no * batch_size:(slice_no + 1) * batch_size])
    batch_labels = np.array(dummpy_labels[slice_no * batch_size:(slice_no + 1) * batch_size])

    max_sequence = max(list(map(len, batch_data_j)))

    # getting Max length of sequence
    padded_sequence = [i + [0] * (max_sequence - len(i)) if len(i) &lt; max_sequence else i for i in batch_data_j]
    return padded_sequence, batch_labels




# dropout 0.2 during training and 0.0 during inference
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    iteration = len(dummy_data) // batch_size

    for iter_ in range(iteration):

        sentences_, labels_ = get_train_data(2,iter_)
        loss_,_ = sess.run([loss,optimizer], feed_dict= {sentences: sentences_, targets: labels_, keep_prob : 0.2})
        print(loss_)
</code></pre>

<p>Now want to use tf dataset pipeline to build an efficient pipeline for training and inference. I went through some tutorials but couldn't find good answer.</p>

<p>I tried to use tf.dataset like :</p>

<pre><code>dataset = tf.data.Dataset.from_tensor_slices((sentences,targets,keep_prob))
dataset = dataset.batch(batch_size)

iterator = tf.data.Iterator.from_structure(dataset.output_types)
iterator_initializer_ = iterator.make_initializer(dataset, name='initializer')
sentec, labels, drop_  = iterator.get_next()



def initialize_iterator(sess, sentences_, labels_, drops_):

        feed_dict = {sentences: sentences_, targets: labels_, keep_prob : [np.random.randint(0,2,[1,]).astype(np.float32)]}

        return sess.run(iterator_initializer_, feed_dict = feed_dict)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    iteration = len(dummy_data) // batch_size

    for iter_ in range(iteration):
        initialize_iterator(sess, dummy_data, dummpy_labels, [0.0])
        los, _ = sess.run([loss, optimizer])
        print(los)
</code></pre>

<p>But I am getting an error.</p>

<p>So what should be an efficient pipeline for training RNN and encoding, padding with dropout sequences using dataset api?</p>
",0
59182162,Tensorflow 2.0: Can I change the settings on a Tf.data.Dataset -- in particular the `repeat()` feature?,"<p>I have a tensorflow 2.0 <code>tf.data.Dataset</code> created from some pandas data. Now I wanted to change a setting on the dataset, but it seems to not let me do that. Case in point, I wanted to change the <code>.repeat()</code> parameter on the dataset from repeat infinitely, to repeat only 1 time. But when I tried to make this change, the Dataset did not accept the change. </p>

<p>Here is an example with some code. The function is taken from one of the Tensorflow Tutorials on the TF website.</p>

<pre><code>URL = 'https://storage.googleapis.com/applied-dl/heart.csv'
df = pd.read_csv(URL)

def df_to_dataset(dataframe, shuffle=True, batch_size=32):
    dataframe = dataframe.copy()
    labels = dataframe.pop('target')
    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
    if shuffle:
        ds = ds.shuffle(buffer_size=len(dataframe))
    ds = ds.batch(batch_size).repeat() # &lt;-- NOTICE THE INFINITE REPEAT
    return ds

train_ds = df_to_dataset(df)
train_ds.repeat(1) # &lt;-- TRYING TO CHANGE TO A FIXED NUMBER OF REPETITIONS
</code></pre>

<p>So I tried to change the number of repeats on the Dataset, but this leads to a dataset that still repeats forever. It is like the if I were to set the dataset to repeat infinitely and then repeat 1 time, that I end up with a dataset that repeats infinitely, but 1 time--which is the same as repeating infinitely. </p>

<p>I imagine that the same behavior is probably true for other features of a dataset, such as the number of batches, or such. </p>

<p>Is there a way to reset the behavior on a dataset?</p>
",0
59232039,How does TensorFlow calculate the gradients of an FFT layer?,"<p>If I insert the function, e.g., <code>tf.fft(input, name=None)</code>, into a neural network, how does TensorFlow calculate the gradients in backpropagation?</p>

<p>I didn't find any documentation about this. </p>

<p>I am using TensorFlow 1.0.</p>
",1
59238519,Tensorflow 2 gradient gives nan results for pow,"<p>The following simplified code outputs nan for derivatives when x=0. I'm running tensorflow 2.0.0.</p>

<pre><code>import tensorflow as tf

x = tf.Variable([[-1.0], [0.0], [1.0]])

with tf.GradientTape(persistent=True) as t:
    t.watch(x)
    # case 1: y = x^4
    # y = tf.reduce_sum(tf.pow(x, 4), axis=1) # gives nan for 2nd to 5th derivative at x=0
    # case 2: y = x + x^2 + x^3 + x^4
    y = tf.reduce_sum(tf.pow(x, [[1, 2, 3, 4]]), axis=1) # gives nan for 2nd to 5th derivative at x=0
    dy_dx = t.gradient(y, x)
    d2y_dx2 = t.gradient(dy_dx, x)
    d3y_dx3 = t.gradient(d2y_dx2, x)
    d4y_dx4 = t.gradient(d3y_dx3, x)
    d5y_dx5 = t.gradient(d4y_dx4, x)
del t

tf.print(y)
tf.print(tf.transpose(dy_dx)) # transpose only to fit on one line when printed
tf.print(tf.transpose(d2y_dx2))
tf.print(tf.transpose(d3y_dx3))
tf.print(tf.transpose(d4y_dx4))
tf.print(tf.transpose(d5y_dx5))

</code></pre>

<p>This outputs correct values except when x=0:</p>

<pre><code>[0 0 4]
[[-2 1 10]]
[[8 -nan(ind) 20]]
[[-18 -nan(ind) 30]]
[[24 -nan(ind) 24]]
[[0 -nan(ind) 0]]
</code></pre>

<p>If you run the <code>tf.pow(x, 4)</code> case instead, the nan only shows up for the 5th derivative:</p>

<pre><code>[1 0 1]
[[-4 0 4]]
[[12 0 12]]
[[-24 0 24]]
[[24 24 24]]
[[-0 -nan(ind) 0]]
</code></pre>

<p>So my questions are:</p>

<ul>
<li><p>The tensorflow documentation doesn't explicitly say that the pow function supports two parameters of different size, but the first output y is correct. Anyone have experience with this? I'm expecting a matrix of all 3 input <code>x</code> values raised to all 4 powers.</p></li>
<li><p>Is the nan value returned from the gradient a bug I should report? I did find this previous possibly related issue, but it was fixed: <a href=""https://github.com/tensorflow/tfjs/issues/346"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tfjs/issues/346</a></p></li>
</ul>
",0
59262869,Kernel Constraint usage in Tensorflow v1.14,"<p>I am implementing a custom dense layer with weights of dimension <code>12x12</code> in which not all the neurons from one layer are connected to another layer. So I have defined a projection matrix like below:</p>

<pre class=""lang-py prettyprint-override""><code>projection_matrix = np.zeros((12, 12))
connections = [[2, 6, 9], [4, 7, 10], [0, 6, 9], [5, 8, 11], [1, 7, 10], [3, 8, 11], [0, 2, 9], [1, 4, 10],
               [3, 5, 11], [0, 2, 6], [1, 4, 7], [3, 5, 8]]
for i, connection in zip(range(projection_matrix.shape[0]), connections):
    for j in connection:
        projection_matrix[i, j] = 1
</code></pre>

<p>And then the idea is to multiply the weight matrix with this projection matrix:</p>

<pre class=""lang-py prettyprint-override""><code>new_weight_matrix = np.multiply(weight_matrix, projection_matrix)  # Might as well be tf.multiply
</code></pre>

<p>I was going through the documentation of <code>tf.layers.dense</code> from <a href=""https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/layers/dense"" rel=""nofollow noreferrer"">here</a>. There is a parameter called <code>kernel_constraint</code>, whose description reads:</p>

<blockquote>
  <p>An optional projection function to be applied to the bias after being updated by an Optimizer.</p>
</blockquote>

<p>My question is, does passing the <code>projection_matrix</code> to this parameter (<code>kernel_constraint</code>) achieve what I intend to achieve (connect only specific neurons defined by <code>projection_matrix</code>)?</p>
",1
59299060,TF 2.0 while_loop and parallel_iterations,"<p>I am trying to use <code>tf.while_loop</code> to run loops in parallel. However, in the following toy examples,loops don't appear to be running in parallel. </p>

<pre><code>iteration = tf.constant(0)
c = lambda i: tf.less(i, 1000)
def print_fun(iteration):
    print(f""This is iteration {iteration}"")
    iteration+=1
    return (iteration,)
r = tf.while_loop(c, print_fun, [iteration], parallel_iterations=10)
</code></pre>

<p>Or</p>

<pre><code>i = tf.constant(0)
c = lambda i: tf.less(i, 1000)
b = lambda i: (tf.add(i, 1),)
r = tf.while_loop(c, b, [i])
</code></pre>

<p>What is preventing the <code>tf.while_loop</code> from parallelizing the loop?</p>

<p>In addition, if anyone who maintain the Tensorflow documentation see this page, he/she should fix the bug in the first example. See the discussion <a href=""https://github.com/tensorflow/tensorflow/issues/18257"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Thanks.</p>
",1
59348648,How to load and run a TensorFlow model,"<p>Super simple question.</p>

<p>I'm following this tutorial: 
<a href=""https://github.com/tensorflow/hub/blob/master/examples/colab/tf2_image_retraining.ipynb"" rel=""nofollow noreferrer"">https://github.com/tensorflow/hub/blob/master/examples/colab/tf2_image_retraining.ipynb</a></p>

<p>I've followed the instructions (not the optional part) trained a model, and have it saved using tf.saved_model.save</p>

<p>Now I want to run the model against a new picture. How do I do this?</p>

<p>I tried the following: </p>

<pre><code>loaded = tf.saved_model.load(""/tmp/saved_flowers_model"")
print(list(loaded.signatures.keys()))  # [""serving_default""]

infer = loaded.signatures[""serving_default""]
print(infer.structured_outputs)

file = .../pic.jpg
output = loaded (file)
</code></pre>

<p>which returned the following error:</p>

<pre><code>ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (3 total):
    * /Users/dansafdie/Documents/Code Repos/Image Classifier/flower_photos/daisy/19834392829_7d697871f6.jpg
    * False
    * None
  Keyword arguments: {}

Expected these arguments to match one of the following 4 option(s):

Option 1:
  Positional arguments (3 total):
    * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')
    * True
    * None
  Keyword arguments: {}

Option 2:
  Positional arguments (3 total):
    * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='inputs')
    * False
    * None
  Keyword arguments: {}

Option 3:
  Positional arguments (3 total):
    * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')
    * False
    * None
  Keyword arguments: {}

Option 4:
  Positional arguments (3 total):
    * TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1')
    * True
    * None
  Keyword arguments: {}
</code></pre>
",0
59361689,Redundancies in tf.keras.backend and tensorflow libraries,"<p>I have been working in TensorFlow for about a year now, and I am transitioning from TF 1.x to TF 2.0, and I am looking for some guidance on how to use the <code>tf.keras.backend</code> library in TF 2.0. I understand that the transition to TF 2.0 is supposed to remove a lot of redundancies in modeling and building graphs, since there were many ways to create equivalent layers in earlier TensorFlow versions (and I'm insanely grateful for that change!), but I'm getting stuck on understanding when to use <code>tf.keras.backend</code>, because the operations appear redundant with other TensorFlow libraries. </p>

<p>I see that some of the functions in <code>tf.keras.backend</code> are redundant with other TensorFlow libraries. For instance, <code>tf.keras.backend.abs</code> and <code>tf.math.abs</code> are not aliases (or at least, they're not listed as aliases in the documentation), but both take the absolute value of a tensor. After examining the source code, it looks like <code>tf.keras.backend.abs</code> calls the <code>tf.math.abs</code> function, and so I really do not understand why they are not aliases. Other <code>tf.keras.backend</code> operations don't appear to be duplicated in TensorFlow libraries, but it looks like there are TensorFlow functions that can do equivalent things. For instance, <code>tf.keras.backend.cast_to_floatx</code> can be substituted with <code>tf.dtypes.cast</code> as long as you explicitly specify the dtype. I am wondering two things:</p>

<ol>
<li>when is it best to use the <code>tf.keras.backend</code> library instead of the equivalent TensorFlow functions?</li>
<li>is there a difference in these functions (and other equivalent <code>tf.keras.backend</code> functions) that I am missing?</li>
</ol>
",1
59427969,Mathematical definition of tensordot operation on TensorFlow tensor,"<p>I'm trying to reverse engineer the behavior of tf.tensordot axes parameter, but having a hard time.</p>

<p>Given the following code:</p>

<pre><code>a = tf.constant([[1., 2.], [3., 4.], [4., 5.]])
b = tf.constant([1., 2.])
c = tf.constant([[1., 2.], [2., 3.], [3., 4.]])

print(f'Shape of c: {c.shape}')

ct = tf.transpose(c)

print(f'Shape of ct: {ct.shape}')

print('.................')

d = tf.tensordot(a, ct, axes=1)
print(f'Shape of d: {d.shape}')
print(d)

print('.................')


e = tf.tensordot(a, ct, axes=0)
print(f'Shape of e: {e.shape}')
print(e)


print('.................')


f = tf.tensordot(a, ct, axes=2)
print(f'Shape of f: {f.shape}')
print(e)
</code></pre>

<p>I understand how ""d"" is produced, but I don't understand how ""e"" and ""f"" are produced. The <a href=""https://www.tensorflow.org/api_docs/python/tf/tensordot"" rel=""nofollow noreferrer"">TensorFlow Documentation</a> is not sufficient for me to understand.</p>
",1
59432671,Issue with tf.keras.backend.random_normal?,"<p>I am very new to TF2 and tried to customize the example code on the tensorflow guide documentation:</p>

<p><a href=""https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example</a></p>

<p>but my code requires the latent dimension to be =1, and my code returned the:
<strong>ValueError: The last dimension of the inputs to <code>Dense</code> should be defined. Found <code>None</code>.</strong> 
error. After trouble shooting I think the error is in the:</p>

<pre><code>class Sampling(layers.Layer):
  """"""Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.""""""

  def call(self, inputs):
    z_mean, z_log_var = inputs
    batch = tf.shape(z_mean)[0]
    dim = tf.shape(z_mean)[1]
    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
    return z_mean + tf.exp(0.5 * z_log_var) * epsilon
</code></pre>

<p><strong>where tf.keras.backend.random_normal sets epsilon always to dimensions [None,None]</strong></p>

<p>Then I just copied the example from the guide (<em>reference above</em>) and set the latent dimension to 1.
For training i used the given code:</p>

<pre><code>vae = VariationalAutoEncoder(784, 64, 1)

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())
vae.fit(x_train, x_train, epochs=3, batch_size=64)
</code></pre>

<p>same error:
<strong>ValueError: The last dimension of the inputs to <code>Dense</code> should be defined. Found <code>None</code>.</strong></p>

<p>The code works fine if latent dimension is more than 1!</p>

<p>Could somebody help me?</p>
",0
59497372,Is there an alternative to tf.py_function() for custom Python code?,"<p>I have started using TensorFlow 2.0 and have a little uncertainty with regard to one aspect.</p>

<p>Suppose I have this use case: while ingesting data with the <code>tf.data.Dataset</code> I want to apply some specific augmentation operations upon some images. However, the external libraries that I am using <strong>require</strong> that the <strong>image is a numpy array</strong>, <strong>not a tensor</strong>.</p>

<p>When using <code>tf.data.Dataset.from_tensor_slices()</code>, the flowing data needs to be of type Tensor. Concrete example:</p>

<pre><code>def my_function(tensor_image):
   print(tensor_image.numpy()
   return


data = tf.data.Dataset.from_tensor_slices(tensor_images).map(my_function)
</code></pre>

<p>The code above does not work yielding an </p>

<blockquote>
  <p>'Tensor' object has no attribute 'numpy' error.</p>
</blockquote>

<p>I have read the documentation on TensorFlow 2.0 stating that if one wants to use an arbitrary python logic, one should use <code>tf.py_function</code> <strong>or only TensorFlow primitives</strong> according to:
<a href=""https://stackoverflow.com/questions/56075037/how-to-convert-tensor-to-numpy-array-in-tensorflow"">How to convert &quot;tensor&quot; to &quot;numpy&quot; array in tensorflow?</a> </p>

<p><strong>My question is the following</strong>: Is there another way to use arbitrary python code in a function with a custom decorator/an easier way than to use <code>tf.py_function</code>?</p>

<p>To me honestly it seems that there must be a more elegant way than passing to a <code>tf.py_function</code>, transforming to a numpy array, perform operations A,B,C,D and then retransform to a tensor and yield the result. </p>
",1
59555206,keras to tf.keras Conversion: Dense layer dimensions not defined?,"<p>So I've built a convnet using pure <code>keras</code>. It compiles and operates exactly as intended, but I need to convert it to use <code>tf.keras</code> so that I can make use of <code>tfmot</code>. Having read documentation, I attempted to convert it, only to get the following error:</p>

<p><code>The last dimension of the inputs to Dense should be defined. Found None.</code> </p>

<p>Any idea what I'm doing wrong?</p>

<p>Thanks!</p>

<p>Original <code>keras</code> model:</p>

<pre><code>input_layer = keras.layers.Input(shape=(100,))
reshape_layer = keras.layers.Reshape((-1, 100, 1))(input_layer)
conv_layer_1 = keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=""same"", activation=""relu"")(reshape_layer)
conv_layer_2 = keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_1)
conv_layer_3 = keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_2)
conv_layer_4 = keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_3)
conv_layer_5 = keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_4)
flatten_layer = keras.layers.Flatten()(conv_layer_5)
label_layer = keras.layers.Dense(200, activation=""relu"")(flatten_layer)
output_layer = keras.layers.Dense(1, activation=""linear"")(label_layer)

model = keras.Model(inputs=input_layer, outputs=output_layer)
</code></pre>

<p>Converted <code>tf.keras</code> model:</p>

<pre><code>input_layer = tf.keras.layers.InputLayer(input_shape=(100,))
reshape_layer = tf.keras.layers.Reshape((-1, 100, 1))(input_layer)
conv_layer_1 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(10, 1), strides=(1, 1), padding=""same"", activation=""relu"")(reshape_layer)
conv_layer_2 = tf.keras.layers.Convolution2D(filters=30, kernel_size=(8, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_1)
conv_layer_3 = tf.keras.layers.Convolution2D(filters=40, kernel_size=(6, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_2)
conv_layer_4 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_3)
conv_layer_5 = tf.keras.layers.Convolution2D(filters=50, kernel_size=(5, 1), strides=(1, 1), padding=""same"", activation=""relu"")(conv_layer_4)
flatten_layer = tf.keras.layers.Flatten()(conv_layer_5)
label_layer = tf.keras.layers.Dense(200, activation=""relu"")(flatten_layer)
output_layer = tf.keras.layers.Dense(1, activation=""linear"")(label_layer)

model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
</code></pre>

<p>EDIT 1:</p>

<p>I thought maybe I could get around the issue by saving the <code>keras</code> model after creation and loading it as a <code>tf.keras</code> model immediately before compilation / training. That throws the same error! </p>
",1
59578283,tensorflow_probability: AttributeError: module 'tensorflow_probability.* has no attribute '*',"<p>I'm trying to reproduce a snippet of code I found on google colaboratory (<a href=""https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Gaussian_Process_Regression_In_TFP.ipynb#scrollTo=jw-_1yC50xaM"" rel=""nofollow noreferrer"">here</a>) and I have problems with the methods used (tfb.Shift and tfp.util.TransformedVariable) that I can't find in the documentation.</p>

<pre><code>import numpy as np
import tensorflow.compat.v2 as tf
import tensorflow_probability as tfp
tfb = tfp.bijectors
tfd = tfp.distributions
tf.enable_v2_behavior()

constrain_positive = tfb.Shift(np.finfo(np.float64).tiny)(tfb.Exp())

amplitude_var = tfp.util.TransformedVariable(
    initial_value=1.,
    bijector=constrain_positive,
    name='amplitude',
    dtype=np.float64)
</code></pre>

<p>I get </p>

<pre><code>AttributeError: module 'tensorflow_probability.python.bijectors' has no attribute 'Shift'
</code></pre>

<p>and</p>

<pre><code>AttributeError: module 'tensorflow_probability.python.util' has no attribute 'TransformedVariable'
</code></pre>

<p>Could be a version issue?</p>

<p>I'm using  tensorflow 2.0.0 and tensorflow-probability 0.8.0 and I see that in colab notebook is used <code>@tf.function</code> so I thought it used tensorflow 2.0.0 too.</p>

<p>What can I use instead?</p>

<p>Thank you very much</p>
",1
59578808,what is the difference between `tf.multiply` and `*`?,"<p>After <code>import tensorflow.kera.backend as K</code>  </p>

<p><strong>what is the difference between <code>tf.multiply</code> and <code>*</code>?</strong> </p>

<p><strong>Similarly, What is the difference between <code>K.pow(x, -1)</code> and <code>1/x</code>??</strong></p>

<p>I write the following codes of a customized metrics function based on some other's codes.</p>

<pre><code>def dice_coef_weight_sub(y_true, y_pred):
    """"""
    Returns the product of dice coefficient for each class
    """"""
    y_true_f = (Lambda(lambda y_true: y_true[:, :, :, :, 0:])(y_true))
    y_pred_f = (Lambda(lambda y_pred: y_pred[:, :, :, :, 0:])(y_pred))

    product = tf.multiply([y_true_f, y_pred_f]) # multiply should be import from tf or tf.math

    red_y_true = K.sum(y_true_f, axis=[0, 1, 2, 3]) # shape [None, nb_class]
    red_y_pred = K.sum(y_pred_f, axis=[0, 1, 2, 3])
    red_product = K.sum(product, axis=[0, 1, 2, 3])

    smooth = 0.001
    dices = (2. * red_product + smooth) / (red_y_true + red_y_pred + smooth)

    ratio = red_y_true / (K.sum(red_y_true) + smooth)
    ratio = 1.0 - ratio
    # ratio =  K.pow(ratio + smooth, -1.0) # different method to get ratio

    return K.sum(multiply([dices, ratio]))
</code></pre>

<p>In the codes, can I replace <code>tf.multiply</code> by <code>*</code>? Can I replace <code>K.pow(x,-1)</code> by <code>1/x</code>??</p>

<p>(From tensorflow's document, I know the difference between <code>tf.pow</code> and <code>K.pow</code>: <code>tf.pow(x,y)</code> receives 2 tensors to compute x^y for corresponding elements in <code>x</code> and <code>y</code>, while <code>K.pow(x,a)</code> receives a tensor <code>x</code> and a integer <code>a</code> to compute x^a. But I do not know why in the above code <code>K.pow</code> receives a float number 1.0 and it still works norally)</p>
",0
59583308,How switch tensorflow versions between 2.0 and 1.x?,"<p>Is there a way to switch versions of tensorflow?, when I installed tensorflow 2.0 using conda, it updated many things even python. On runing <code>conda list</code> it shows both versions; </p>

<blockquote>
  <p>tensorflow  2.0.0   mkl_py37h66b46cc_0<br>
  tensorflow    1.13.1   &lt; pip></p>
</blockquote>

<p>In order to use the 1.x version it's recommended <a href=""http://www.google.com/"" rel=""nofollow noreferrer"">here</a> to replace <code>import tensorflow as tf</code> for the following :<br/> <br/><code>import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()</code><br/><br/>It is safe to use this method, just adding these lines?, in my case it shows a warning:<br/><br/><code>WARNING:tensorflow:From /home/common/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term</code></p>
",0
59611000,Saving meta data/information in Keras model,"<p>Is it possible to save meta data/meta information in Keras model? My goal is to save input pre-processing parameters, train/test set used, class label maps etc. which I can use while loading model again.<br> I went through Keras documentation and did not find anything. I found similar <a href=""https://github.com/keras-team/keras/issues/6404"" rel=""noreferrer"">issue</a> on GitHub but it was closed two years back without any resolution.<br>
Currently I am saving all these information in separate file, and using this file while loading the model.
<br> Although probably not relevant but I am using <code>tf.keras</code> functional model and saving my model as <code>h5</code> file using <code>model.save()</code>.</p>
",0
59654496,How to understand TensorFlow source code for Imagenet preprocessing,"<p>I want to understand and replicate how does tensorflow.keras do the preprocessing of imagenet in the function <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet/preprocess_input?version=stable"" rel=""nofollow noreferrer"">tf.keras.applications.resnet.preprocess_input</a>, but I can not understand the <a href=""https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/applications/imagenet_utils.py"" rel=""nofollow noreferrer"">source code</a>:</p>

<pre class=""lang-py prettyprint-override""><code>@keras_export('keras.applications.imagenet_utils.preprocess_input')
@keras_modules_injection
def preprocess_input(*args, **kwargs):
  return imagenet_utils.preprocess_input(*args, **kwargs)
</code></pre>

<p>The definition seems to be recursive. Moreover, the documentation does not say anything about the function. How can I see what is this function really doing?</p>
",1
59709349,How does tf.dataset interact with keras.conv1D?,"<p>I'm using tf 1.15, i'm trying to make a regression task using a signal.</p>

<p>First of all i load my signals into the pipeline, i have several files, here i simulate the loading using a np.zeros to make the code usable by you.
Every file has this shape (?, 75000, 3), where ? is a random number of elements, 75000 is the number of samples in each element and 3 is the number of signals.</p>

<p>Using the tf.data i unpack them and i get a dataset who output signals with this shape (75000,), and i use them in my keras model.</p>

<p>Everything should be fine until i create the keras model, i copied my input pipeline because during my tests i got different errors using a generic tf.data.dataset or using the dataset built in this way.</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tensorflow as tf

# called in the dataset pipeline
def my_func(x):
    p = np.zeros([86, 75000, 3])
    x = p[:,:,0]
    y = p[:, :, 1]
    z = p[:, :, 2]
    return x, y, z

# called in the dataset pipeline
def load_sign(path):
    func = tf.compat.v1.numpy_function(my_func, [path], [tf.float64, tf.float64, tf.float64])
    return func

# Dataset pipeline
s = [1, 2]  # here i have the file paths, i simulate it with numbers
AUTOTUNE = tf.data.experimental.AUTOTUNE  
ds = tf.data.Dataset.from_tensor_slices(s)
# ds = ds.map(load_sign, num_parallel_calls=AUTOTUNE)
ds = ds.map(load_sign, num_parallel_calls=AUTOTUNE).unbatch()
itera = tf.data.make_one_shot_iterator(ds)
ABP, ECG, PLETH = itera.get_next() 

# Until there everything should be fine
# Here i create my convolutional network
signal = tf.keras.layers.Input(shape=(None,75000), dtype='float32')
x = tf.compat.v1.keras.layers.Conv1D(64, (1), strides=1, padding='same')(signal)
x = tf.keras.layers.Dense(75000)(x)
model = tf.keras.Model(inputs=signal, outputs=x, name='resnet18')

# And finally i try to insert my signal into model
logits = model(PLETH)
</code></pre>

<p>I get this error:</p>

<blockquote>
  <p>ValueError: Input 0 of layer conv1d is incompatible with the layer: its rank is undefined, but the layer requires a defined rank.</p>
</blockquote>

<p>Why? And how can i make it works?
Also the input size of my net should be this one according the documentation: </p>

<blockquote>
  <p>3D tensor with shape: (batch_size, steps, input_dim)</p>
</blockquote>

<p>What is the steps? In my case i assume it should be (batch_size, 1, 75000), right?</p>
",0
59727916,Gradient computation in Tensorflow,"<p>I am using Tensorflow v1.14 for creating networks and training them. Everything works fine and I don't have any problem with code. I use the function <code>tf.reduce_min()</code> in my loss function. For the gradients to flow, it is essential that the loss function is differentiable. But a <code>min</code> operator is not differentiable as such. This <a href=""https://datascience.stackexchange.com/a/21882/79728"">link</a>, gives the necessary explanation for the <code>tf.reduce_min()</code> function but without references. </p>

<p>In general there are functions in Tensorflow (<code>tf.cond</code>, <code>tf.where</code>, among many more) that are inherently not differentiable by their definition. I want to know how these are made differentiable by defining ""pseudo gradients"" and the proper references to documentation. Thanks.</p>
",1
59731667,Why does training using tf.GradientTape in tensorflow 2 have different behavior to training using fit API?,"<p>I am new to using tensorflow 2</p>

<p>I am familiar with using <code>keras</code> in tensorflow 1. And I usually use <code>fit</code> method API to train model. But recently in tensorflow 2, they introduced <strong>eager execution</strong>. So I implemented and compare a simple image classifier on CiFAR-10 dataset on both <code>fit</code> and <code>tf.GradientTape</code> and trained for 20 epochs each</p>

<p>After several runs, the results are as follow</p>

<ul>
<li>Model trained with <code>fit</code> API

<ul>
<li>Training dataset, loss is around 0.61-0.65 with accuracy of 76% - 80%</li>
<li>Validation dataset, loss is around 0.8 with accuracy of 72% - 75%</li>
</ul></li>
<li>Model trained with <code>tf.GradientTape</code>

<ul>
<li>Training dataset, loss is around 0.15-0.2 with accuracy of 91% - 94%</li>
<li>Validation dataset, loss is around 1.8-2 with accuracy of 64% - 67%</li>
</ul></li>
</ul>

<p>I am not sure why the model exhibits a different behavior. I think I might implement something wrong. I think it is weird that in <code>tf.GradientTape</code> the model start to overfit training dataset quicker</p>

<p>Here are some snippets</p>

<ol>
<li>Using <code>fit</code> API</li>
</ol>

<pre class=""lang-py prettyprint-override""><code>model = SimpleClassifier(10)
model.compile(
    optimizer=Adam(),
    loss=tf.keras.losses.CategoricalCrossentropy(),
    metrics=[tf.keras.metrics.CategoricalAccuracy()]
)
model.fit(X[:split_idx, :, :, :], y[:split_idx, :], batch_size=256, epochs=20, validation_data=(X[split_idx:, :, :, :], y[split_idx:, :]))
</code></pre>

<ol start=""2"">
<li>Using <code>tf.GradientTape</code></li>
</ol>

<pre class=""lang-py prettyprint-override""><code>with tf.GradientTape() as tape:
    y_pred = model(tf.stop_gradient(train_X))
    loss = loss_fn(train_y, y_pred)
    gradients = tape.gradient(loss, model.trainable_weights)
model.optimizer.apply_gradients(zip(gradients, model.trainable_weights))
</code></pre>

<p>The full code can be found <a href=""https://colab.research.google.com/drive/1dBOSSzBWNLK8pi7LN2if299-1FiY1q8f"" rel=""nofollow noreferrer"">here in Colab</a></p>

<p>References</p>

<ul>
<li><a href=""https://www.tensorflow.org/guide/effective_tf2"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/effective_tf2</a></li>
<li><a href=""https://www.tensorflow.org/api_docs/python/tf/GradientTape?version=stable"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/GradientTape?version=stable</a></li>
</ul>
",0
59743351,Tensorflow 2.0.0: AttributeError: 'TensorSliceDataset' object has no attribute 'as_numpy_iterator',"<p>I am testing tensorflow <code>tf.data.Dataset</code> method <code>as_numpy_iterator</code> using <code>tensorflow 2.0.0</code>. According to the official documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#as_numpy_iterator"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#as_numpy_iterator</a>, this function allows directly inspecting the content of a tensorflow dataset. But when I try the given example:</p>

<pre><code>dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3]) 
for element in dataset.as_numpy_iterator(): 
  print(element) 
</code></pre>

<p>There occurs an error: <code>AttributeError: 'TensorSliceDataset' object has no attribute 'as_numpy_iteractor'</code>. I am wondering if this method is just newly added, beyond the support of tensorflow 2.0.0. If so, is there an alternative to checking the dataset content as the <code>as_numpy_iterator()</code>?</p>
",1
59763351,How to compute gradients with tf.scatter_sub?,"<p>When implementing lambda-opt(an algorithm published on KDD'19) in tensorflow, I came across a problem to compute gradients with  <strong>tf.scatter_sub</strong>。</p>

<p>θ refers to an embedding matrix for docid.
The formulation is </p>

<p>θ(t+1)=θ(t) - α*(grad+2*λ*θ), </p>

<pre><code>delta = theta_grad_no_reg.values * lr + 2 * lr * cur_scale * cur_theta
next_theta_tensor = tf.scatter_sub(theta,theta_grad_no_reg.indices,delta)
</code></pre>

<p>then I use θ(t+1) for some computation. Finally, I want to compute gradients with respect to λ, not θ.</p>

<p>But the gradient is None.</p>

<p>I wrote a demo like this:</p>

<pre><code>import tensorflow as tf

w = tf.constant([[1.0], [2.0], [3.0]], dtype=tf.float32)
y = tf.constant([5.0], dtype=tf.float32)

# θ
emb_matrix = tf.get_variable(""embedding_name"", shape=(10, 3),
                    initializer=tf.random_normal_initializer(),dtype=tf.float32)
# get one line emb
cur_emb=tf.nn.embedding_lookup(emb_matrix,[0])
# The λ matrix
doc_lambda = tf.get_variable(name='docid_lambda', shape=(10, 3),
                             initializer=tf.random_normal_initializer(), dtype=tf.float32)
# get one line λ
cur_lambda=tf.nn.embedding_lookup(doc_lambda, [0])

# θ(t+1) Tensor(""ScatterSub:0"", shape=(10, 3), dtype=float32_ref)
next_emb_matrix=tf.scatter_sub(emb_matrix, [0], (cur_emb *cur_lambda)) 
# do some compute with θ(t+1) Tensor ,not Variable
next_cur_emb=tf.nn.embedding_lookup(next_emb_matrix,[0])

y_ = tf.matmul(next_cur_emb, w)
loss = tf.reduce_mean((y - y_) ** 2)
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
grad_var_list=optimizer.compute_gradients(loss)
print(grad_var_list)
# [(None, &lt;tf.Variable 'embedding_name:0' shape=(10, 3) dtype=float32_ref&gt;), (None, &lt;tf.Variable 'docid_lambda:0' shape=(10, 3) dtype=float32_ref&gt;)]
</code></pre>

<p>The gradient is None, too. It seems that <strong>tf.scatter_sub</strong> op doesn't provide gradient？</p>

<p>Thanks for your help！</p>

<p>If you have an interest in this algorithm, you can search for it, but it's not important about this question. </p>
",0
59772316,Multioutput model with custom losses containing dependencies with other outputs and other data,"<p>I have a model with multiple outputs,  the losses for each output can have dependencies with one of the other outputs, as well as some masks computed from the data. The overall loss of the model is a weighted sum over the losses.</p>

<p>My model is subclassing <code>tf.keras.Model</code> and I am trying to write clean code that I can use with <code>compile</code> and <code>fit</code>. I would like the weights of the losses to be given during the compile.</p>

<p>One way I have found addressing the loss dependencies issue (after reading some documentation and <a href=""https://stackoverflow.com/questions/50063613/what-is-the-purpose-of-the-add-loss-function-in-keras"">this answer</a>) is to feed the masks type of data as input of the model and, in the implementation of <code>call</code>, to add the loss of each output with <code>Model.add_loss</code>. Can someone confirm me this? How do I get <code>y_true</code> from there?</p>

<p>If this is a good solution, how do I specify that the overall model loss is a weighted sum of those losses during the <code>compile</code>, how can I access them?</p>

<p>Also would it be better to use <code>add_loss</code> on each layer in the implementation of the model's <code>call</code>? Same question, how do I access them during the <code>compile</code>?</p>

<p>If this was not a good solution, what is a good one?</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

class MyModel(tf.keras.Model):

    def __init__(self, base_trainable=False, feature_extractor=None, n=4, *kwargs):
        super(MyModel, self).__init__(*kwargs)
        if feature_extractor:
            self.feature_extractor = feature_extractor
        else:
            feature_extractor = tf.keras.applications.Resnet101(include_top=False,
                                                                weights='imagenet',
                                                                trainable=base_trainable)

        self.out1 = layers.Conv2D(n, kernel_size=(1,1), activation='sigmoid', name='out1')
        self.out2 = layers.Conv2D(n, kernel_size=(1,1), name='out2')
        self.out3 = layers.Conv2D(2*n, kernel_size=(1,1), 'out3')

    def call(self, inputs):
        img, mask1, mask2 = inputs
        x = self.feature_extractor(img)
        out1 = self.out1(x)
        out2 = self.out2(x)
        out3 = self.out3(x)
        # compute losses for each output? (but how do I access to each y_true?...)
        # ex:
        # 
        # model.add_loss(my_loss_for_out1(y1_true??
        #                                 out1,
        #                                 out2))
        # model.add_loss(my_loss_for_out1(y2_true??
        #                                 out2,
        #                                 mask1))
        # model.add_loss(my_loss_for_out1(y3_true??
        #                                 out3,
        #                                 mask2))


        return out1, out2, out3

model = MyModel()

model.compile(loss=???
              loss_weights=???)
</code></pre>

<p>Thank you</p>
",0
59872095,Why b comes out in front of tensorflow output?,"<p>I'm a Tensorflow beginner.
I've tried to print Hello world using below Tensorflow 1.15.0 code.</p>

<pre><code>import tensorflow as tf

h = tf.constant(""Hello"")
w = tf.constant("" World!"")
hw = h + w

with tf.Session() as sess:
    ans = sess.run(hw)

    print(ans)
</code></pre>

<p>When I run the code using jupyter notebook, <strong>b'Hello World!'</strong> came out.</p>

<p>What I expected is only 'Hello World!"". Why does the b come out in front of my output?</p>

<p>Many thank</p>
",0
59906819,What is the correct explanation for tf.dense?,"<p>I am using a tutorial to learn RNN. 
As shown in the image below, it has used tf.dense and given an explanation which I cannot understand. </p>

<p>(As a newbie to stackoverflow, I cannot insert images, hence the link)</p>

<p>The documentation at tensorflow.org also does not help much and it is so bad for a beginner like me that I've given a 1-star rating. It says the following (which does not make sense to me)</p>

<p><a href=""https://i.stack.imgur.com/cjFv5.png"" rel=""nofollow noreferrer"">The explanation at tensorflow.org</a></p>

<p>Can someone kindly explain it to me. Thank you</p>

<p><a href=""https://i.stack.imgur.com/tGIYM.png"" rel=""nofollow noreferrer"">Excerpt from the tutorial I am using to learn.</a></p>
",1
59908904,What is the difference between step size and learning rate in machine learning?,"<p>I am using TensorFlow to implement some basic ML code. I was wondering if anyone could give me a short explanation of the meaning of and difference between step size and learning rate in the following functions.</p>

<p>I used <em><a href=""https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/GradientDescentOptimizer"" rel=""nofollow noreferrer"">tf.train.GradientDescentOptimizer()</a></em> to set the parameter learning rate and <em><a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor#train"" rel=""nofollow noreferrer"">linear_regressor.train()</a></em> to set the number of steps. I've been looking through the documentation on tensorflow.org for these functions but I still do not have a complete grasp of the meaning of these parameters.</p>

<p>Thank you and let me know if there is any more info I can provide.</p>
",1
59946439,GPU tensorflow running slower than CPU tensorflow on laptop?,"<p>I recently installed the GPU tensorflow, CUDA and cuDNN on my laptop to train my models using my GPU using <a href=""https://towardsdatascience.com/how-to-use-tensorflow-on-the-gpu-of-your-laptop-with-ubuntu-18-04-554e1d5ea189"" rel=""nofollow noreferrer"">this</a> tutorial. My laptop is an Lenovo Ideapad 510 with Processor = i5-7th gen and GPU = GForce 940MX(4GB). Following the tutorial, I installed and configured all the required changes needed to use my GPU.</p>
<h1>results of training the mnist dataset on GPU</h1>
<p>Each epoc just took 6 seconds to compile 60,000 images. And on <code>nvidia-smi</code> table I could see my GPU memory usage was 19MiB. In the tutorial, his GPU memory usage was 777MiB.</p>
<p>Then I tried to run my own dataset and model which has 88000 images and runs for 10 epocs. the <code>nvidia-smi</code> for this training shows GPU usage as 19MiB. <code>tf.test.is_gpu_available()</code> is also returning FALSE.</p>
<h1>CNN MODEL</h1>
<pre><code>classifier = Sequential()


classifier.add(Conv2D(32, (3, 3), input_shape = (100, 100, 3), activation = 'relu'))
classifier.add(MaxPooling2D(pool_size = (2, 2)))

classifier.add(Conv2D(32, (3, 3), activation = 'relu'))
classifier.add(MaxPooling2D(pool_size = (2, 2)))

classifier.add(Flatten())

classifier.add(Dense(units = 128, activation = 'relu'))
classifier.add(Dense(units = 39, activation = 'softmax'))

classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])



from keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

test_datagen = ImageDataGenerator(rescale = 1./255)

training_set = train_datagen.flow_from_directory('train',
                                                 target_size = (100,100),
                                                 batch_size = 32,
                                                 class_mode = 'categorical')

test_set = test_datagen.flow_from_directory('test',
                                            target_size = (100, 100),
                                            batch_size = 32,
                                            class_mode = 'categorical')

classifier.fit_generator(training_set,
                         steps_per_epoch = 88534,
                         epochs = 10,
                         validation_data = test_set,
                         validation_steps = 1418)

classifier.save('/home/harish/Desktop/asl-alphabet/asl_pred.h5')
</code></pre>
<p>Why am I not able to train data faster than the usual CPU? How do I enable GPU for training?</p>
",0
59962253,How do I use hparams with estimators?,"<p>To log hparams <a href=""https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/hparams/api.py#L38"" rel=""noreferrer"">without using Keras</a>, I'm doing the following as suggested in the tf code <a href=""https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/hparams/api.py#L55"" rel=""noreferrer"">here</a>:</p>

<pre class=""lang-py prettyprint-override""><code>with tf.summary.create_file_writer(model_dir).as_default():
    hp_learning_rate = hp.HParam(""learning_rate"", hp.RealInterval(0.00001, 0.1))
    hp_distance_margin = hp.HParam(""distance_margin"", hp.RealInterval(0.1, 1.0))
    hparams_list = [
        hp_learning_rate,
        hp_distance_margin
    ]
    metrics_to_monitor = [
        hp.Metric(""metrics_standalone/auc"", group=""validation""),
        hp.Metric(""loss"", group=""train"", display_name=""training loss""),
    ]
    hp.hparams_config(hparams=hparams_list, metrics=metrics_to_monitor)
    hparams = {
        hp_learning_rate: params.learning_rate,
        hp_distance_margin: params.distance_margin,
    }
    hp.hparams(hparams)
</code></pre>

<p>Note that <code>params</code> is a dictionary object here that I'll pass to the estimator.</p>

<p>Then I train the estimator as usual,</p>

<pre><code>config = tf.estimator.RunConfig(model_dir=params.model_dir)
estimator = tf.estimator.Estimator(model_fn, params=params, config=config)
train_spec = tf.estimator.TrainSpec(...)
eval_spec = tf.estimator.EvalSpec(...)

tf.estimator.train_and_evaluate(estimator, train_spec=train_spec, eval_spec=eval_spec)
</code></pre>

<p>After training, when I launch tensorboard, I do have the hparams logged, but I do not see any metrics logged against them</p>

<p><a href=""https://i.stack.imgur.com/mYAYP.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mYAYP.png"" alt=""enter image description here""></a></p>

<p>I further confirmed that they show up in the <code>scalars</code> page with the same tag name for both train and validation i.e. <code>.</code> and <code>./eval</code>, but the hparams page doesn't see those logged tensors.</p>

<p><strong>How do I use hparams with estimators?</strong></p>

<hr>

<p>I'm using</p>

<pre><code>tensorboard              2.1.0
tensorflow               2.1.0
tensorflow-estimator     2.1.0
tensorflow-metadata      0.15.2
</code></pre>

<p>on <code>Python 3.7.5</code></p>

<hr>

<p><em>Attempt 1:</em></p>

<p>After some googling, I saw some older tf code where they passed <code>hparams</code> to <code>params</code> argument of Estimator, so just to make sure if tf2 logs those hparams by itself when given, I checked the <a href=""https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator"" rel=""noreferrer"">Estimator</a> docs and it says:</p>

<blockquote>
  <p>The <code>params</code> argument contains hyperparameters. It is passed to the
  <code>model_fn</code>, if the <code>model_fn</code> has a parameter named ""params"", and to the
  input functions in the same manner. <code>Estimator</code> only passes params
  along, it does not inspect it. The structure of <code>params</code> is therefore
  entirely up to the developer.</p>
</blockquote>

<p>So using hparams as params will not be useful.</p>

<hr>

<p><em>Attempt 2:</em></p>

<p>I doubt that since estimators use <code>tensorflow.python.summary</code>  instead of <code>tf.summary</code> which is the default in v2, tensors logged by v1 was probably not accessible and so, I also tried to use </p>

<pre><code>with tensorflow.python.summary.FileWriter(model_dir).as_default()
</code></pre>

<p>However that failed with <code>RuntimeError: tf.summary.FileWriter is not compatible with eager execution. Use tf.contrib.summary instead</code>.</p>

<p><em>Update</em>: I ran it with eager execution disabled. Now, even the hparam initial logging did not happen. There was no <code>hparams</code> tab in tensorboard as it failed with error </p>

<pre><code>E0129 13:03:07.656290 21584 hparams_plugin.py:104] HParams error: Can't find an HParams-plugin experiment data in the log directory. Note that it takes some time to scan the log directory; if you just started Tensorboard it could be that we haven't finished scanning it yet. Consider trying again in a few seconds.
</code></pre>

<p>Is there a way to make tensorboard read already logged metric tensors and link them with hparams?</p>
",0
59998335,Constantly update tf.cond based on bool value,"<p>I am using <code>tf.cond</code> for controlling the flow of the Tensorflow graph. I went through the documentation and was able to implement <code>tf.cond</code> based branching successfully. But my concern is that while the graph is being loaded the value of the <code>bool</code> variable is checked and the branching decision is made at the initialization step itself. Any further changes in the <code>bool</code> is not tracked. Following is the MWE that better describes the problem:</p>

<pre class=""lang-py prettyprint-override""><code>def funa():
    return tf.constant(32)

def funb():
    return tf.constant(25)

foo = True
x = tf.cond(tf.convert_to_tensor(foo), lambda: funa(), lambda: funb())
for i in range(20):
    global foo
    if i &gt; 10:
        foo = False
    print(sess.run(x))    
</code></pre>

<p>This prints only <code>32</code>s. </p>

<p>I tried with <code>eager_execution</code> too with the following code:</p>

<pre class=""lang-py prettyprint-override""><code>tf.enable_eager_execution()
def funa():
    return tf.constant(32)

def funb():
    return tf.constant(21)

foo = True
x = tf.cond(tf.convert_to_tensor(foo), lambda: funa(), lambda: funb())
for i in range(20):
    if i &gt; 10:
        foo = False
    print(x)
</code></pre>

<p>Still the same result.</p>

<p>So my question is how can I write code such that one part of the graph is chosen dynamically, based on the updates to the <code>bool</code> variable (if possible)? Thanks. I am using Tensorflow v1.14.</p>
",1
60013980,tf.nn.embedding_lookup_sparse 3D sparse tensor input,"<p>I have an embedding matrix and there is a 3D sparse tensor which is used to get the embedding output, after reading the docs of <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup_sparse"" rel=""nofollow noreferrer""><code>tf.nn.embedding_lookup_sparse</code></a> I found it only supports 2D sparse tensors,</p>

<blockquote>
  <p>sp_ids: N x M SparseTensor of int64 ids where N is typically batch size and M is arbitrary.</p>
</blockquote>

<p>My example code here</p>

<pre><code>import numpy as np
import tensorflow as tf
tf.enable_eager_execution()

# [feature number, embedding dim] 
w = tf.get_variable(""w"", [4, 4], initializer=tf.random_normal_initializer())

z = np.array(
     [
      [
        [0, 1, 2, 3],   # get the vector of row 0, 1, 2, 3 of the embedding matrix w and get the sum
        [2, 3]
      ],

      [
        [1, 3],
        [2]
      ],

      [
        [0, 1, 3],
        [1, 2]
      ]
     ])

sp = tf.SparseTensor(values=[0, 1, 2, 3, 2, 3, 1, 3, 2, 0, 1, 3, 1, 2],
                     indices=[[0,0,0],[0,0,1],[0,0,2],[0,0,3],[0,1,2],
                              [0,1,3],[1,0,1],[1,0,3],[1,1,2],[2,0,0],
                              [2,0,1],[2,0,3],[2,1,1],[2,1,2]],
                     dense_shape=[3, 2, 4])

tf.nn.embedding_lookup_sparse(w, sp, None, combiner='sum')
# the outputs
&lt;tf.Tensor: id=970, shape=(3, 4), dtype=float32, numpy=
array([[-5.8729677 , -1.3900641 ,  0.8126096 , -3.1223912 ],
       [-1.0788026 , -1.1324122 ,  0.34160078,  0.23714277],
       [-2.497394  , -2.7855003 ,  3.0201516 , -1.8009453 ]],
      dtype=float32)&gt;

print(w)
&lt;tf.Variable 'w:0' shape=(4, 4) dtype=float32, numpy=
array([[-2.5669768 , -0.38916406,  1.4039794 , -2.8173826 ],
       [ 1.1483854 , -1.2639242 ,  1.2745714 ,  0.7792944 ],
       [-1.3400027 , -0.46362385, -1.3652185 ,  0.27220532],
       [-0.8871854 ,  0.5951359 ,  0.43224794, -0.8143569 ]],
      dtype=float32)&gt;
</code></pre>

<p>But the expected output is a matrix with a dimension of <code>3x2x4</code>, not <code>3x4</code>. Does <code>tf.nn.embedding_lookup_sparse</code> support this operation?</p>
",1
60047705,Repeated use of GradientTape for multiple Jacobian calculations,"<p>I am attempting to compute the Jacobian of a TensorFlow neural network's outputs with respect to its inputs. This is easily achieved with the <code>tf.GradientTape.jacobian</code> method. The trivial example provided in the TensorFlow documentation is as follows:</p>

<pre><code>with tf.GradientTape() as g:
  x  = tf.constant([1.0, 2.0])
  g.watch(x)
  y = x * x
jacobian = g.jacobian(y, x)
</code></pre>

<p>This is fine if I want only want to compute the Jacobian of a single instance of the input tensor <code>x</code>. However, I need to repeatedly evaluate this Jacobian many, many times for various instances of <code>x</code>. For a non-trivial Jacobian calculation (e.g. for a deep convolutional neural network with non-linear activation functions), this is incredibly expensive to repeatedly rerun the GradientTape calculation and evaluate the <code>jacobian</code> method. I know from the <a href=""https://www.tensorflow.org/tutorials/customization/autodiff"" rel=""nofollow noreferrer"">TensorFlow documentation</a> that the gradients (and hence the Jacobian) are computed via automatic differentiation.  I have to imagine there is some internal storage of the analytical gradient of the network (computed by automatic differentiation) which is evaluated at the given inputs. </p>

<p>My question: am I correct in assuming that TensorFlow builds and stores (at least parts of) the analytical gradients needed to compute the Jacobian? And if so, is there a way to save this analytical gradient and re-evaluate the Jacobian with new inputs without having to reconstruct it via the GradientTape method?</p>

<p>A ""persistent"" GradientTape does not seem to solve this issue: it only allows for the repeated evaluation of a single GradientTape instance with respect to multiple internal arguments of the computation.</p>
",1
60064351,One-hot encoding using tf.data mixes up columns,"<h3>Minimum working examples</h3>

<p>Consider the following CSV file (<code>example.csv</code>)</p>

<pre><code>animal,size,weight,category
lion,large,200,mammal
ostrich,large,150,bird
sparrow,small,0.1,bird
whale,large,3000,mammal
bat,small,0.2,mammal
snake,small,1,reptile
condor,medium,12,bird
</code></pre>

<p>The goal is to convert all the categorical values into one-hot encodings. The <a href=""https://www.tensorflow.org/tutorials/load_data/csv"" rel=""nofollow noreferrer"">standard</a> way to do this in Tensorflow 2.0 is to use <code>tf.data</code>. Following that example, the code to deal with the dataset above is</p>

<pre><code>import collections
import tensorflow as tf

# Load the dataset.
dataset = tf.data.experimental.make_csv_dataset(
    'example.csv',
    batch_size=5,
    num_epochs=1,
    shuffle=False)

# Specify the vocabulary for each category.
categories = collections.OrderedDict()
categories['animal'] = ['lion', 'ostrich', 'sparrow', 'whale', 'bat', 'snake', 'condor']
categories['size'] = ['large', 'medium', 'small']
categories['category'] = ['mammal', 'reptile', 'bird']

# Define the categorical feature columns.
categorical_columns = []
for feature, vocab in categories.items():
  cat_col = tf.feature_column.categorical_column_with_vocabulary_list(
        key=feature, vocabulary_list=vocab)
  categorical_columns.append(tf.feature_column.indicator_column(cat_col))

# Retrieve the first batch and apply the one-hot encoding to it.
iterator = iter(dataset)
first_batch = next(iterator)
categorical_layer = tf.keras.layers.DenseFeatures(categorical_columns)

print(categorical_layer(first_batch).numpy())
</code></pre>

<h3>Question</h3>

<p>Running the code above, one gets </p>

<pre><code>[[1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1.]]
</code></pre>

<p>where it looks like the two last columns <code>size</code> and <code>category</code> have been flipped, despite the fact that <code>categories</code> is an <em>ordered</em> dictionary and the pre-existing order of the columns in the actual dataset. It's as if <code>tf.feature_column.categorical_column_with_vocabulary_list()</code> did some unwarranted alphabetical sorting of the columns. </p>

<p>What's the reason for the above. Is this really the best way to do one-hot encoding in the spirit of <code>tf.data</code>?</p>
",0
60104249,Missing modules and attributes for training in TensorFlow's Object Detection API,"<p>I'm currently attempting to train an object detection model. I'm following Gilbert Tanner's tutorial on YouTube. I am running TF version 1.9.0.</p>

<p>It seems as though I'm missing the necessary modules. When I run the following command:</p>

<pre><code>python model_main.py --logtostderr --model_dir=training/ --pipeline_config_path=traini
ng/faster_rcnn_inception_v2_pets.config
</code></pre>

<p>I get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""model_main.py"", line 26, in &lt;module&gt;
    from object_detection import model_lib
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\model_lib.py"", line 28, in &lt;module&gt;
    from object_detection import exporter as exporter_lib
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\exporter.py"", line 24, in &lt;module&gt;
    from object_detection.builders import model_builder
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\builders\model_builder.py"", line 35, in &lt;module&gt;
    from object_detection.models import faster_rcnn_inception_resnet_v2_feature_extractor as frcnn_inc_res
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\models\faster_rcnn_inception_resnet_v2_feature_extractor
.py"", line 30, in &lt;module&gt;
    from nets import inception_resnet_v2
  File ""C:\Users\Admin\Desktop\ObjectDetection\models\research\object_detection\nets\inception_resnet_v2.py"", line 375, in &lt;module&gt;
    batch_norm_updates_collections=tf.compat.v1.GraphKeys.UPDATE_OPS,
AttributeError: module 'tensorflow.compat' has no attribute 'v1'
</code></pre>

<p>For some reason, I've had to fix other problems with certain modules not being in the correct place (for instance, the nets module wasn't placed under the models/research/object_detection directory upon installation, it was instead placed under models/research/slim).</p>

<p>I'm not sure exactly how to fix this issue. I've tried bouncing between different 1.x versions of TensorFlow but each time I am met with similar errors, such as not having the 'v2' attribute. </p>

<p>I suspect I could be lacking a package that should be installed in my environment, but I'm not sure what it could be. I'm also unsure about why the necessary modules aren't properly installed. Here are all of the packages that are installed in my environment:</p>

<pre><code>Package Version Lastest Version
absl-py 0.9.0   0.8.1
astor   0.8.1   0.8.0
biwrap  0.1.6   
bleach  1.5.0   3.1.0
certifi 2019.11.28  2019.11.28
gast    0.3.3   0.3.2
grpcio  1.27.0  1.16.1
h5py    2.10.0  2.10.0
html5lib    0.9999999   1.0.1
keras-applications  1.0.8   1.0.8
keras-preprocessing 1.1.0   1.1.0
markdown    3.1.1   3.1.1
mock    3.0.5   3.0.5
numpy   1.18.1  1.18.1
object-detection    0.1 
pandas  1.0.0   1.0.0
pillow  7.0.0   7.0.0
pip 20.0.2  20.0.2
protobuf    3.11.3  3.11.2
pycocotools 2.0 
python  3.6.10  3.8.1
python-dateutil 2.8.1   2.8.1
pytz    2019.3  2019.3
setuptools  39.1.0  45.1.0
six 1.14.0  1.14.0
sqlite  3.31.1  3.31.1
tensorboard 1.9.0   2.0.0
tensorflow  1.9.0   2.0.0
tensorflow-estimator    1.13.0  2.0.0
tensorflow-plot 0.3.0   
tensorflow-tensorboard  1.5.1   
termcolor   1.1.0   1.1.0
vc  14.1    14.1
vs2015_runtime  14.16.27012 14.16.27012
werkzeug    0.16.1  0.16.1
wheel   0.34.2  0.34.2
wincertstore    0.2 0.2
</code></pre>

<p>Am I missing any necessary packages? Any help on this issue is appreciated. Please let me know if I have not included information that would be helpful.</p>

<p>EDIT:
Line 375 in C:\Users\Admin\Desktop\ObjectDetection\models\research\object_detection\nets\inception_resnet_v2.py is bolded below:</p>

<pre><code>def inception_resnet_v2_arg_scope(
    weight_decay=0.00004,
    batch_norm_decay=0.9997,
    batch_norm_epsilon=0.001,
    activation_fn=tf.nn.relu,
    **batch_norm_updates_collections=tf.compat.v1.GraphKeys.UPDATE_OPS**,
    batch_norm_scale=False):
</code></pre>

<p>Here is the link to the video I'm referring to. My problem is occurring when I run the command at 18:01.
<a href=""https://www.youtube.com/watch?v=HjiBbChYRDw"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=HjiBbChYRDw</a>
I realize the command I provided above is slightly different than the one shown in the video. However, in the written version of the tutorial, Gilbert Tanner has updated the command to the one I provided above.</p>

<p>Changing all references on tf.compat.v1.GraphKeys to tf.GraphKeys works, but more errors arise:</p>

<pre><code>AttributeError: module 'tensorflow.compat' has no attribute 'v2'
</code></pre>

<p>on this function signature:</p>

<pre><code>def global_pool(input_tensor, pool_op=tf.compat.v2.nn.avg_pool2d)
</code></pre>

<p>When I change it to this:</p>

<pre><code>def global_pool(input_tensor, pool_op=tf.nn.avg_pool2d)
</code></pre>

<p>I get this error:</p>

<pre><code>AttributeError: module 'tensorflow.nn' has no attribute 'avg_pool2d'
</code></pre>

<p>There is no documentation for avg_pool2d for TensorFlow 1.x and there is for TensorFlow 2.x, so I'm not sure why it's in this file if I have TensorFlow 1.9.</p>

<p>I notice tf.nn has attributes avg_pool and avg_pool3d, however, changing it to these causes a TypeError:</p>

<pre><code>Traceback (most recent call last):
  File ""model_main.py"", line 109, in &lt;module&gt;
    tf.app.run()
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""model_main.py"", line 105, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\estimator\training.py"", line 447, in train_and_evaluate
    return executor.run()
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\estimator\training.py"", line 531, in run
    return self.run_local()
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\estimator\training.py"", line 669, in run_local
    hooks=train_hooks)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 366, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1119, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1129, in _train_model_default
    input_fn, model_fn_lib.ModeKeys.TRAIN))
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 985, in _get_features_and_labels_from_input_fn
    result = self._call_input_fn(input_fn, mode)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1074, in _call_input_fn
    return input_fn(**kwargs)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\inputs.py"", line 504, in _train_input_fn
    params=params)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\inputs.py"", line 607, in train_input
    batch_size=params['batch_size'] if params else train_config.batch_size)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\builders\dataset_builder.py"", line 155, in build
    dataset = data_map_fn(process_fn, num_parallel_calls=num_parallel_calls)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 882, in map
    return ParallelMapDataset(self, map_func, num_parallel_calls)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1899, in __init__
    super(ParallelMapDataset, self).__init__(input_dataset, map_func)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1868, in __init__
    self._map_func.add_to_graph(ops.get_default_graph())
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\framework\function.py"", line 475, in add_to_graph
    self._create_definition_if_needed()
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\framework\function.py"", line 331, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\framework\function.py"", line 340, in _create_definition_if_needed_impl
    self._capture_by_value, self._caller_device)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\framework\function.py"", line 804, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1833, in tf_map_func
    ret = map_func(nested_args)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\builders\dataset_builder.py"", line 134, in process_fn
    processed_tensors = decoder.decode(value)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\data_decoders\tf_example_decoder.py"", line 388, in decod
e
    tensors = decoder.decode(serialized_example, items=keys)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\contrib\slim\python\slim\data\tfexample_decoder.py"", line 520, in decode
    outputs.append(handler.tensors_to_item(keys_to_tensors))
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\data_decoders\tf_example_decoder.py"", line 129, in tenso
rs_to_item
    item = self._handler.tensors_to_item(keys_to_tensors)
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\object_detection-0.1-py3.6.egg\object_detection\data_decoders\tf_example_decoder.py"", line 98, in tensor
s_to_item
    return tf.maximum(self._name_to_id_table.lookup(unmapped_tensor),
  File ""C:\Users\Admin\Anaconda3\envs\object_detection\lib\site-packages\tensorflow\python\ops\lookup_ops.py"", line 223, in lookup
    (self._key_dtype, keys.dtype))
TypeError: Signature mismatch. Keys must be dtype &lt;dtype: 'float32'&gt;, got &lt;dtype: 'string'&gt;.


</code></pre>

<p>Here is line 98 in tensors_to_item:</p>

<pre><code>    return tf.maximum(self._name_to_id_table.lookup(unmapped_tensor),
                      self._display_name_to_id_table.lookup(unmapped_tensor))
</code></pre>

<p>I'm not sure how to handle this issue and it seems like I shouldn't have changed the function signature. Is having to make this many changes to the modules normal?  </p>
",0
60145905,"Adversarial text generation with gradient clipping, still facing gradient explosion","<p>I am training an text generation model with Generative adversarial networks.
As a common method, I used <code>tf.clip_by_global_norm()</code> to avoid gradient problems. However, even I used gradient clipping, I am still facing gradient exploding problem with error caused by <code>tf.clip_by_global_norm()</code> function.
The document says: If <code>global_norm == infinity</code> then the entries in <code>t_list</code> are all set to <code>NaN</code> to signal that an error occurred.</p>

<p>I can hardly find the origin of problem with this situation since I believed that <code>tf.clip_by_global_norm</code> definitely avoid the gradient problem.</p>
",0
60182398,Creating a distance matrix in TensorFlow,"<p><strong>X</strong> is a Tensor(""stack:0"", shape=(10, 2), dtype=int32) that represents a matrix of coordinates like:</p>

<p>[[2, 1], [5, 5], [4, 1], [0, 0], [6, 1], [2, 4], [6, 3], [5, 2], [5, 0], [2, 2]]</p>

<p>I want to create a Euclidean Distance Matrix from <strong>X</strong> showing the distance between all coordinates pairs so I get a resulting Tensor with shape=(10, 10) like:</p>

<pre><code>[[0.000  2.000  5.000  4.123  1.414  1.414  6.082   2.000  4.123  4.000]

 [2.000  0.000  4.123  4.123  1.414  3.162  6.708   2.828  2.236  4.472]

 [5.000  4.123  0.000  2.000  3.605  5.000  4.472   3.605  3.162  3.000]

 [4.123  4.123  2.000  0.000  3.000  3.605  2.828   2.236  4.242  1.000]

 [1.414  1.414  3.605  3.000  0.000  2.000  5.385   1.414  3.000  3.162]

 [1.414  3.162  5.000  3.605  2.000  0.000  5.000   1.414  5.000  3.162]

 [6.082  6.708  4.472  2.828  5.385  5.000  0.000   4.123  7.071  2.236]

 [2.000  2.828  3.605  2.236  1.414  1.414  4.123   0.000  4.123  2.000]

 [4.123  2.236  3.162  4.242  3.000  5.000  7.071   4.123  0.000  5.000]

 [4.000  4.472  3.000  1.000  3.162  3.162  2.236   2.000  5.000  0.000]]
</code></pre>

<p>I tried to use  <strong>tf.norm</strong>  (<a href=""https://www.tensorflow.org/api_docs/python/tf/norm"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/norm</a>) but this function isn't working properly.
Any help would be appreciated.</p>
",0
60215970,What's the cleanest and most efficient way to pass two stereo images to a loss function in Keras?,"<p>First off, why am I using Keras? I'm trying to stay as high level as possible, which doesn't mean I'm scared of low-level Tensorflow; I just want to see how far I can go while keeping my code as simple and readable as possible.</p>

<p>I need my Keras model (custom-built using the Keras functional API) to read the left image from a stereo pair and minimize a loss function that needs to access both the right and left images. I want to store the data in a <code>tf.data.Dataset</code>.</p>

<p>What I tried:</p>

<ol>
<li>Reading the dataset as <code>(left image, right image)</code>, i.e. as tensors with shape <code>((W, H, 3), (W, H, 3))</code>, then use function closure: define a <code>keras_loss(left_images)</code> that returns a <code>loss(y_true, y_pred)</code>, with <code>y_true</code> being a <code>tf.Tensor</code> that holds the right image. The problem with this approach is that <code>left_images</code> is a <code>tf.data.Dataset</code> and Tensorflow complains (rightly so) that I'm trying to operate on a dataset instead of a tensor. </li>
<li><p>Reading the dataset as <code>(left image, (left image, right image))</code>, which should make <code>y_true</code> a <code>tf.Tensor</code> with shape <code>((W, H, 3), (W, H, 3))</code> that holds both the right and left images. The problem with this approach is that it...does not work and raises the following error:</p>

<pre><code>ValueError: Error when checking model target: the list of Numpy arrays 
that you are passing to your model is not the size the model expected. 
Expected to see 1 array(s), for inputs ['tf_op_layer_resize/ResizeBilinear'] 
but instead got the following list of 2 arrays: [&lt;tf.Tensor 'args_1:0' 
shape=(None, 512, 256, 3) dtype=float32&gt;, &lt;tf.Tensor 'args_2:0' 
shape=(None, 512, 256, 3) dtype=float32&gt;]...
</code></pre></li>
</ol>

<p>So, is there anything I did not consider? I read the documentation and found nothing about what gets considered as <code>y_pred</code> and what as <code>y_true</code>, nor about how to convert a dataset into a tensor smartly and without loading it all in memory. </p>

<p>My model is designed as such:</p>

<pre><code> def my_model(input_shape):
     width = input_shape[0]
     height = input_shape[1]
     inputs = tf.keras.Input(shape=input_shape)
     # &lt; a few more layers &gt;
     outputs = tf.image.resize(tf.nn.sigmoid(tf.slice(disp6, [0, 0, 0, 0], [-1, -1, -1, 2])), tf.Variable([width, height]))
     model = tf.keras.Model(inputs=inputs, outputs=outputs)
     return model
</code></pre>

<p>And my dataset is built as such (in case 2, while in case 1 only the function <code>read_stereo_pair_from_line()</code> changes):</p>

<pre><code>def read_img_from_file(file_name):
    img = tf.io.read_file(file_name)
    # convert the compressed string to a 3D uint8 tensor
    img = tf.image.decode_png(img, channels=3)
    # Use `convert_image_dtype` to convert to floats in the [0,1] range.
    img = tf.image.convert_image_dtype(img, tf.float32)
    # resize the image to the desired size.
    return tf.image.resize(img, [args.input_width, args.input_height])


def read_stereo_pair_from_line(line):
    split_line = tf.strings.split(line, ' ')
    return read_img_from_file(split_line[0]), (read_img_from_file(split_line[0]), read_img_from_file(split_line[1]))

# Dataset loading
list_ds = tf.data.TextLineDataset('test/files.txt')
images_ds = list_ds.map(lambda x: read_stereo_pair_from_line(x))
images_ds = images_ds.batch(1)
</code></pre>
",1
60287388,Error loading .npz files in tensorflow dataset,"<p>I'm trying to create a data pipeline in tensorflow, but my data is in .npz files.
Following the documentation at <a href=""https://www.tensorflow.org/guide/data#consuming_sets_of_files"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/data#consuming_sets_of_files</a>, for consuming sets of files, combined with using tf.py_function() for using numpy ops, I wrote a code, a subsection of which is:</p>

<pre><code>def load_data(filename):

    from preprocess import normalize

    # sess = tf.compat.v1.Session()
    # fln = sess.run(filename)
    print ('I AM TRYING TO LOAD : ', filename)
    mels = np.load(filename)['arr_0']
    mels = normalize(mels)

    return mels
</code></pre>

<p>I'm getting an error in np.load(filename), which goes like:</p>

<pre><code>I AM TRYING TO LOAD :  tf.Tensor(b'/media/prabhatk/Datasets/DCASE/features/augmentations=None features=mono fmax=22050 fmin=0 hop_length=1024 mel_htk=True n_fft=2048 n_mels=60 samplerate=44100/airport-lisbon-1000-40000-a.npz', shape=(), dtype=string)
2020-02-18 19:31:31.048435: W tensorflow/core/framework/op_kernel.cc:1610] Invalid argument: TypeError: expected str, bytes or os.PathLike object, not tensorflow.python.framework.ops.EagerTensor

</code></pre>

<p>Fixed this by using np.load(filename.numpy()), as stated by @jdehesa.
Now I run into some shape issues, although I've already reshaped the input in the load_data_wrapper() function.</p>

<p>Can anyone help me with this?</p>

<p>I am attaching the entire code and entire error message below.</p>

<p>CODE:</p>

<pre><code>import os
from os import path
import sys
import argparse

sys.path.insert(1,'../')
from helpers import locations
from helpers import metadata
from helpers import read_settings as Settings
from models import sbcnn

import numpy as np
import pandas as pd
import librosa as lb

from sklearn.preprocessing import LabelEncoder

import tensorflow as tf
# from tf.data.experimental import AUTOTUNE


def load_data(filename):

    from preprocess import normalize

    # sess = tf.compat.v1.Session()
    # fln = sess.run(filename)
    print ('I AM TRYING TO LOAD : ', filename)
    mels = np.load(filename)['arr_0']
    # mels = tf.io.read_file(filename)['arr_0']
    mels = normalize(mels)

    return mels


def load_data_wrapper(filename):

    # print ('INPUT TO LOAD DATA IS : ', filename)
    [mels,] = tf.py_function(
        load_data, [filename], [tf.float32]
        )
    mels.set_shape((1,60,431,1))

    # return mels
    return mels


def get_input_directory(input_dir, feature_settings):

    base_dir = input_dir if (input_dir != '') else locations.FEATURE_DIR_BASE
    assert path.exists(base_dir), 'Feature directory not found!!'

    settings = Settings.load_settings(feature_settings)
    feature_dir = Settings.settings_to_path(settings)
    feature_dir = path.join(base_dir,feature_dir)
    assert path.exists(feature_dir), 'Data not preprocessed according to given settings!!'

    return feature_dir


def construct_dataset(X, y):

    filepaths = tf.data.Dataset.from_tensor_slices(X)
    with tf.device('/device:CPU:*'):
        files = filepaths.map(load_data_wrapper)

    filelabels = tf.data.Dataset.from_tensor_slices(y)

    return tf.data.Dataset.zip((files, filelabels))


def parseArguments():
    parser = argparse.ArgumentParser()

    parser.add_argument('--training_metadata', type=str, default='', help='CSV file containing training file names and lables')
    parser.add_argument('--validation_metadata', type=str, default='', help='CSV file containing validation file names and lables')
    parser.add_argument('--input_dir', type=str, default='', help='Processed features directory')
    parser.add_argument('--feature_settings', type=str, default='', help='Load data with the given settings')

    args = parser.parse_args()

    return args


def main():
    arguments = parseArguments()

    input_dir = get_input_directory(arguments.input_dir, arguments.feature_settings)
    X_tr, y_tr = metadata.train(arguments.training_metadata, input_dir)
    X_val, y_val = metadata.validation(arguments.validation_metadata, input_dir)

    enc = LabelEncoder()
    enc.fit(y_tr)
    y_tr = enc.transform(y_tr)
    y_val = enc.transform(y_val)

    data_tr = construct_dataset(X_tr, y_tr)
    data_val = construct_dataset(X_val, y_val)

    model = sbcnn.build_model()
    model.compile(loss='categorical_crossentropy', 
        optimizer=tf.keras.optimizers.SGD(),
        metrics=['accuracy'])

    model.summary()

    model.fit(data_tr, epochs=10)


if __name__ == '__main__':
    main()
</code></pre>

<p>ERROR:</p>

<pre><code>2020-02-19 10:37:06.375134: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at conv_ops_fused_impl.h:693 : Invalid argument: input must be 4-dimensional[60,431]
2020-02-19 10:37:06.375184: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Invalid argument: input must be 4-dimensional[60,431]
     [[{{node sequential/conv2d/BiasAdd}}]]
      1/Unknown - 0s 389ms/stepTraceback (most recent call last):
  File ""pipeline.py"", line 112, in &lt;module&gt;
    main()
  File ""pipeline.py"", line 108, in main
    model.fit(data_tr, epochs=10)
  File ""/home/prabhatk/miniconda3/envs/DL/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 728, in fit
    use_multiprocessing=use_multiprocessing)
  File ""/home/prabhatk/miniconda3/envs/DL/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 324, in fit
    total_epochs=epochs)
  File ""/home/prabhatk/miniconda3/envs/DL/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 123, in run_one_epoch
    batch_outs = execution_function(iterator)
  File ""/home/prabhatk/miniconda3/envs/DL/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 86, in execution_function
    distributed_function(input_fn))
  File ""/home/prabhatk/miniconda3/envs/DL/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 457, in __call__
    result = self._call(*args, **kwds)
  File ""/home/prabhatk/miniconda3/envs/DL/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 520, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/home/prabhatk/miniconda3/envs/DL/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1823, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/home/prabhatk/miniconda3/envs/DL/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1141, in _filtered_call
    self.captured_inputs)
  File ""/home/prabhatk/miniconda3/envs/DL/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 1224, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager)
  File ""/home/prabhatk/miniconda3/envs/DL/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 511, in call
    ctx=ctx)
  File ""/home/prabhatk/miniconda3/envs/DL/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py"", line 67, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""&lt;string&gt;"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError:  input must be 4-dimensional[60,431]
     [[node sequential/conv2d/BiasAdd (defined at /home/prabhatk/miniconda3/envs/DL/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_801]

Function call stack:
distributed_function

</code></pre>
",0
60311184,how to loop over a tensor object until a condition met,"<p>I have a tensor like this:</p>

<pre><code>masked_bad_col = [[False  True  True False  True  True  True  True  True  True  True False]]
</code></pre>

<p>I want to loop through this tensor untill all elements get <code>True</code>.
So I have another function, which will update this tensor, lets call it <code>uniqueness</code>.</p>

<pre><code>def uniqueness():

   'blah blah blha'
   return tensor1, updated_masked_bad_col
</code></pre>

<p>I looked at the documentation and got to know that I can do that using <code>tf.while_loop</code>. Although, I could not find any example working on boolean stuff.
This is what I have done so far:</p>

<pre><code>tensor1, _ = tf.while_loop(masked_bad_col != True, uniqueness)
</code></pre>

<p>It is obviously incorrect, but don't know how to use each element of <code>masked_bad_col</code> as a condition to continue looping through <code>uniqueness</code> function.</p>

<p><strong>Update 1</strong>
This is the method I am trying to call in the loop:</p>

<pre><code>corpus = load_corpus('path_to_corpus/train.corpus')
topics = []
vocab, docs = corpus['vocab'], corpus['docs']
number_of_topics = 0
encoder_model = load_keras_model(
    'path_to_model/encoder_model',
    custom_objects={""KCompetitive"": KCompetitive})
weights = encoder_model.get_weights()[0]
for idx in range(encoder_model.output_shape[1]):
    token_idx = np.argsort(weights[:, idx])[::-1][:20]
    topics.append([(revdict(vocab)[x]) for x in token_idx])
    number_of_topics += 1

nparr = np.asarray(topics)
# print nparr.shape

unique, indices, count = np.unique(nparr, return_inverse=True, return_counts=True)

tensor1 = (np.sum(count[indices].reshape(nparr.shape), axis=1).reshape(1, nparr.shape[0]) / (
        number_of_topics * 20))

def uniqueness_score():
    corpus = load_corpus('path_to_corpus/train.corpus')
    topics = []
    vocab, docs = corpus['vocab'], corpus['docs']
    number_of_topics = 0
    encoder_model = load_keras_model(
        'path_to_model/encoder_model',
        custom_objects={""KCompetitive"": KCompetitive})
    weights = encoder_model.get_weights()[0]
    for idx in range(encoder_model.output_shape[1]):
        token_idx = np.argsort(weights[:, idx])[::-1][:20]
        topics.append([(revdict(vocab)[x]) for x in token_idx])
        number_of_topics += 1

    nparr = np.asarray(topics)

    unique, indices, count = np.unique(nparr, return_inverse=True, return_counts=True)

    tensor1 = (np.sum(count[indices].reshape(nparr.shape), axis=1).reshape(1, nparr.shape[0]) / (
            number_of_topics * 20))
    return tensor1
</code></pre>

<p>And this is the way I called this method in the <code>while_loop</code></p>

<pre><code>with tf.Session() as sess:

        tensor2, _ = tf.while_loop(
            # Loop condition (negated goal condition)
            lambda tensor1: ~tf.math.reduce_all(tensor1 &gt; tf.reduce_mean(tensor1)),
            # Loop body
            lambda tensor1: uniqueness_score(),
            # Loop variables
            [tensor1])
        # Returned loop value
        print(tensor2.eval())
</code></pre>
",1
60314717,How does shuffle and batch work in tf.data.dataset?,"<p>I'm working on a large dataset with around 10million datapoints so I've decided to use tf.data.dataset api for fetching dataset.</p>

<pre><code>train_dataset = tf.data.Dataset.from_tensor_slices((data))
train = train_dataset.shuffle(100000).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)
</code></pre>

<p>I've few doubts which isn't clear from tensorflow docs. I hope someone can address them.</p>

<p>How does the shuffle work in my case? Because I have 10 million datapoints should I shuffle all 10 million (or) will 100k be enough? Will it have any performance impact choosing a large shuffle? </p>

<p>Will the batch is considered only from shuffled dataset (or) the original dataset?</p>
",1
60384790,Difference - tf.gradients vs tf.keras.backend.gradients,"<p>Being new to Tensorflow, I am trying to understand the difference between underlying functionality of tf.gradients and tf.keras.backend.gradients.</p>

<p>The latter finds the gradient of input feature values w.r.t cost function. </p>

<p>But I couldn't get a clear idea on the former whether it computes the gradient over cost function or output probabilities (For example, consider the case of binary classification using a simple feed forward network. Output probability here is referred to the Sigmoid activation outcome of final layer with single neuron. Cost is given by Binary cross entropy)</p>

<p>I have referred the official documentation for tf.gradients, but it is short and vague (for me), and I did not get a clear picture - The documentation mentions it as just 'y' - is it cost or output probability? </p>

<p>Why I need the gradients? 
To implement a basic gradient based feature attribution. </p>
",1
60398554,"Should we apply repeat, batch shuffle to tf.data.Dataset when passing it to fit function?","<p>I still don't after having read documentation about <code>tf.keras.Model.fit</code> and <code>tf.data.Dataset</code>, when passing <code>tf.data.Dataset</code> to fit function, should I call <code>repeat</code> and <code>batch</code> on the dataset object or should I provide the <code>batch_size</code> and <code>epochs</code> arguments to fit instead? or both? Should I apply the same treatment to the validation set?</p>

<p>And while I'm here, can I <code>shuffle</code> the dataset before the <code>fit</code>? (seems like it's an obvious yes)
If so, before, after calling <code>Dataset.batch</code> and <code>Dataset.repeat</code> (if calling them)?</p>

<p><strong>Edit:</strong> When using <code>batch_size</code> argument, and without having called <code>Dataset.batch(batch_size)</code> previously, I am getting the following error:</p>

<pre><code>ValueError: The `batch_size` argument must not be specified for the given input type.
Received input: &lt;MapDataset shapes: ((&lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;), (&lt;unknown&gt;, &lt;unknown&gt;, &lt;unknown&gt;)), 
types: ((tf.float32, tf.float32, tf.float32, tf.float32), (tf.float32, tf.float32, tf.float32))&gt;, 
batch_size: 1
</code></pre>

<p>Thanks</p>
",1
60453533,Tensorflow what is the tf.contrib.nccl.allsum in new version?,"<p>It seems that from tensorflow 1.13, there is no api such as tf.contrib.nccl.allsum. However, in the Nvidia official GitHub <a href=""https://github.com/tkarras/progressive_growing_of_gans"" rel=""nofollow noreferrer"">https://github.com/tkarras/progressive_growing_of_gans</a>, which uses this old API to reduce sum from different gpu devices as the following. </p>

<pre><code># Sum gradients across devices.
            if len(devices) &gt; 1:
                with tf.name_scope('SumAcrossGPUs'), tf.device(None):
                    for var_idx, grad_shape in enumerate(self._grad_shapes):
                        g = [dev_grads[dev][var_idx][0] for dev in devices]
                        if np.prod(grad_shape): # nccl does not support zero-sized tensors
                            g = tf.contrib.nccl.all_sum(g)
                        for dev, gg in zip(devices, g):
                            dev_grads[dev][var_idx] = (gg, dev_grads[dev][var_idx][1])
</code></pre>

<p>I am not sure if there is similar api which can achieve the same collective operation cross different devices. I have checked the Tensorflow official website and it seems that programmers prefer to use <code>tf.distribute.MirroredStrategy</code> which hides the raw operation of <code>NCCL</code>. Thanks a lot.</p>
",1
60469970,How does tf.function compile a python function with autograph?,"<p>How does <code>tf.function</code> compile a python function operating on tensors into a graph, especially wrt autograph? The <a href=""https://www.tensorflow.org/api_docs/python/tf/function"" rel=""nofollow noreferrer"">docs</a> don't go into detail</p>

<blockquote>
  <p><code>tf.function</code> constructs a callable that executes a TensorFlow graph (<code>tf.Graph</code>) created by trace-compiling the TensorFlow operations in <code>func</code>, effectively executing <code>func</code> as a TensorFlow graph.</p>
</blockquote>

<p>Does it use the special methods called by conditionals (<code>__bool__</code>) and loops (<code>__iter__</code>) to 'trace' the function's implementation? For example</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

@tf.function
def op(t: tf.Tensor) -&gt; tf.Tensor:
    if tf.reduce_sum(t) == 0:
        for _ in t:
            ...
</code></pre>

<p>could use the fact that the <code>if</code> results in <code>Tensor.__bool__(...)</code> and <code>for _ in t</code> results in <code>Tensor.__iter__(...)</code></p>
",1
60516977,Difficulties in understanding higher order derivatives for tf.custom_gradient(),"<p>Based on the example as quoted in tensorflow's website here: <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/custom_gradient</a></p>

<pre><code>@tf.custom_gradient
def op_with_fused_backprop(x):
     y, x_grad = fused_op(x)

     def first_order_gradient(dy):
         @tf.custom_gradient
         def first_order_custom(unused_x):
             def second_order_and_transpose(ddy):
                 return second_order_for_x(...), gradient_wrt_dy(...)
             return x_grad, second_order_and_transpose
         return dy * first_order_custom(x)
     return y, first_order_gradient
</code></pre>

<p>There is a lack of details on why <code>second_order_and_transpose(ddy)</code> returns two objects. Based on the documentation of tf.custom_gradient, the <code>grad_fn</code> (<em>i.e.</em> <code>second_order_and_transpose()</code>) should return a list of Tensors which are the derivatives of dy w.r.t. <code>unused_x</code>. It is also not even clear why did they name it <code>unused_x</code>. Anyone has any idea on this example or in general create custom gradients for higher order derivatives?</p>
",1
60530304,Loading Custom Model with Tensorflow 2.1,"<p>I have made my own class subclassing <code>tf.keras.Model</code> and am trying to save and load a trained instance of it. I'm trying to follow <a href=""https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model"" rel=""noreferrer"">this</a> tutorial but every time I go to load the saved model I get the same error message: <code>TypeError: __init__() got an unexpected keyword argument 'reduction'</code>. I've tried adding that keyword argument to my class but it changes nothing. Any ideas?</p>
",0
60560719,TensorFlow Serving export signature without arguments,"<p>I would like to add extra signature to SavadModel, which will return business description and serve it with TensorFlow Serving.</p>

<pre><code>@tf.function
def info():
    return json.dumps({
       'name':  'My model',
       'description': 'This is model description.',
       'project': 'Product ABCD',
       'type': 'some_type',
       ...
})
</code></pre>

<p>As is written in TensorFlow Core manual <a href=""https://www.tensorflow.org/guide/saved_model#identifying_a_signature_to_export"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/saved_model#identifying_a_signature_to_export</a>, I can easily export signature which accepts arguments providing tf.TensorSpec. </p>

<p>Is it possible to export signature without arguments and call it on server?</p>

<hr>

<p>Added after @EricMcLachlan comments:</p>

<p>When I try to call a function without defined signature <strong>(input_signature=[])</strong> with a code like this:</p>

<pre><code>data = json.dumps({""signature_name"": ""info"", ""inputs"": None})

headers = {""content-type"": ""application/json""}
json_response = requests.post('http://localhost:8501/v1/models/my_model:predict', data=data, headers=headers)
</code></pre>

<p>I get next error in the response:</p>

<blockquote>
  <p>'_content': b'{ ""error"": ""Failed to get input map for signature: info"" }'</p>
</blockquote>
",0
60590333,Increasing each element of a tensor by the predecessor in Tensorflow 2.0,"<p>I'm new to <em>tensorflow 2.0</em>, and haven't done much except designing and training some artificial neural networks from boilerplate code. I'm trying to solve an <em>exercise for newcomers</em> into the new tensorflow. I created some code, but it doesn't work. Below is the <strong><em>problem definition</em></strong>:</p>

<hr>

<p>Assuming we have tensor <code>M</code> of rational numbers in shape of <code>(a, b, c)</code> and scalar <code>p ∈ (0, 1)</code> (memory factor), let’s create a function that will return tensor <code>N</code> in shape of <code>(a, b, c)</code>. Each element of <code>N</code> tensors moving along axis <em>c</em> should be increased by the value of predecessor multiplied by <code>p</code>.</p>

<p>Assuming we have tensor:</p>

<pre><code>T = [x1, x2, x3, x4]
</code></pre>

<p>in shape of <code>(1, 1, 4)</code>, we would like to get vector:</p>

<pre><code>[x1, x2+x1·p, x3+(x2+x1·p)·p, x4+(x3+(x2+x1·p)·p)*p] 
</code></pre>

<p>Solution should be created in <em>Tensorflow 2.0</em> and should be focused on delivering the shortest execution time on CPU. Created graph should allow to efficiently calculate derivative both on tensor <code>M</code> and value <code>p</code>.</p>

<hr>

<p>This is the <strong>code I created till now</strong>:</p>

<pre><code>import tensorflow as tf

@tf.function
def vectorize_predec(t, p):
    last_elem = 0
    result = []
    for el in t:
        result.append(el + (p * last_elem))
        last_elem = el + (p * last_elem)
    return result

p = tf.Variable(0.5, dtype='double')

m = tf.constant([[0, 1, 2, 3, 4],
          [1, 3, 5, 7, 10],
          [1, 1, 1, -1, 0]])

vectorize_predec(m, p)
</code></pre>

<p>But it throws a <code>TypeError</code>.</p>

<p>I looked around documentation, I've seen functions like <code>cumsum</code> and <code>polyeval</code>, but I'm not sure they fit my needs. To my understanding, I need to write my own customer function annotated with <code>@tf.function</code>. I'm also not sure how to handle 3-dimension tensors properly according to the problem definition (adding the predecessor should happen on the last (<em>""c""</em>) axis). </p>

<p>I've seen in documentation (here: <a href=""https://www.tensorflow.org/tutorials/customization/performance"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/customization/performance</a>) that there are ways to measure size of the produced graph. Although, I'm not sure how ""graph"" allows to efficiently calculate <em>derivative</em> both on tensor <code>M</code> and value <code>p</code>. ELI5 answers appreciated, or at least some materials I can read to educate myself better.</p>

<p>Thanks a lot! </p>
",1
60656499,View architechure of my neural net before attempting to train it,"<p>I which to play around with different architectures of neural networks made using TensorFlow 2. I.e. different numbers of layers, different layer types, different numbers of nodes in each layer etc etc. I am building my models with functions like <code>tf.keras.layers.Input()</code>, <code>tf.keras.layers.Dense()</code> and <code>tf.keras.layers.Conv2D</code> etc. I also wish to visualise the models I have built as a sanity check. Looking at the documentation of TensorBoard it appears that to view your model you need to employ callbacks in your training process (model.fit). This seems rather clumsy - surely there must be a way to visualise your network before attempting to start training it. Apart from anything, the training may crash precisely because your model architecture is flawed somehow! If this visualisation could be done via something other than TensorBoard that would be fine too.</p>

<p><strong>EDIT:</strong> It has been suggested that there may be an answer already => <a href=""https://stackoverflow.com/questions/48391075/is-it-possible-to-visualize-a-tensorflow-graph-without-a-training-op"">here</a> but it appears to involve the use of a session which I thought was no longer part of TensorFlow since version 2.</p>

<p><strong>EDIT:</strong> I just found the <code>summary()</code> function. That tells me really quite a lot about my network before attempting to train it.</p>
",0
60678015,Memory leak when using tf.browser.toPixels(),"<p>The following line is causing a memory leak .</p>

<p><code>await tf.browser.toPixels(val, temp);</code></p>

<p><code>val</code> is an array of dimensions <code>img_size * img_size</code> and <code>temp</code> is a canvas  . One tensor is not being disposed . Since it is an asynchronous call it cannot be put inside a <code>tf.tidy()</code> . Also according to official documentation <code>tf.browser.toPixels()</code> should return  a promise .So how can we dispose the tensor which is being formed ? (Size of tensor = img_size * img_size *4 )</p>

<p>Relevant Code : </p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>let val ;     //val is an array not a tensor
        tf.tidy(()=&gt;{
        let values = model.predict(batched).dataSync(); //batched is a tensor
        values = Array.from(values);
        //scale values
        values = values.map(x =&gt; x * 255)
        val = new Array();
        while (values.length &gt; 0) val.push(values.splice(0, image_size));
        });
        await tf.browser.toPixels(val, temp); //this line causes leak,commenting it fixes it
        finalRes.getContext('2d').drawImage(temp, dx, dy);</code></pre>
</div>
</div>
</p>
",0
60715505,Load Images Tensorflow using tf.Data using scalars throws exception,"<p>I am following the official guide on how to load your own image data with tf.Data at <a href=""https://www.tensorflow.org/tutorials/load_data/images"" rel=""nofollow noreferrer"">link</a>.</p>

<p>I am trying to use the tf.Data process.</p>

<p>This code:</p>

<pre class=""lang-py prettyprint-override""><code>def get_label(file_path):
  # convert the path to a list of path components
  parts = tf.strings.split(file_path, os.path.sep)
  # The second to last is the class-directory
  return parts[-2] == CLASS_NAMES
</code></pre>

<p>returns a rank 1 EagerTensor consisting of Boolean values. Which works when used in the following code:</p>

<pre class=""lang-py prettyprint-override""><code>def decode_img(img):
  # convert the compressed string to a 3D uint8 tensor
  img = tf.image.decode_jpeg(img, channels=3)
  # Use `convert_image_dtype` to convert to floats in the [0,1] range.
  img = tf.image.convert_image_dtype(img, tf.float32)
  # resize the image to the desired size.
  return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])

def process_path(file_path):
  label = get_label(file_path)
  # load the raw data from the file as a string
  img = tf.io.read_file(file_path)
  img = decode_img(img)
  return img, label

# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.
labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)
</code></pre>

<p>This code:</p>

<pre class=""lang-py prettyprint-override""><code>def get_label_nums(file_path):
  # convert the path to a list of path components
  parts = tf.strings.split(file_path, os.path.sep)
  # The second to last is the class-directory
  class_num = CLASS_NAMES.tolist().index(parts[-2])
  return class_num

def process_path_with_nums_as_labels(file_path):
  label = get_label_nums(file_path)
  # load the raw data from the file as a string
  img = tf.io.read_file(file_path)
  img = decode_img(img)
  return img, label

# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.
labeled_nums_ds = list_ds.map(process_path_with_nums_as_labels, num_parallel_calls=AUTOTUNE)
</code></pre>

<p>does not.</p>

<p>I have tried to use scaler numbers as that's what I am used to seeing but every time I try to process the data I get:
<code>OperatorNotAllowedInGraphError: using a</code>tf.Tensor<code>as a Python</code>bool<code>is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.</code></p>

<p>I have tried changing it to a <code>ndarray</code> regular tensor, decorated the function with <code>@tf.function</code> and I still can't get it to work. As far as I'm aware I am not using the tensor as a bool, only trying to use it as my label. Please tell me why I can use that tensor of bools and not scaler labels.</p>
",0
60744247,Augmentation of a tf.data.Dataset,"<p>Following <a href=""https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/data_augmentation.ipynb#scrollTo=zkLkgEudzKfc"" rel=""nofollow noreferrer"">this</a> guide here i stumbled across this:In order to augment a tf.data Dataset we manualy use the map function to map image transformations in each image of our original dataset:</p>
<pre><code>def convert(image, label):
  image = tf.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1]
  return image, label

def augment(image,label):
  image,label = convert(image, label)
  image = tf.image.convert_image_dtype(image, tf.float32) # Cast and normalize the image to [0,1]
  image = tf.image.resize_with_crop_or_pad(image, 34, 34) # Add 6 pixels of padding
  image = tf.image.random_crop(image, size=[28, 28, 1]) # Random crop back to 28x28
  image = tf.image.random_brightness(image, max_delta=0.5) # Random brightness

  return image,label

BATCH_SIZE = 64
# Only use a subset of the data so it's easier to overfit, for this tutorial
NUM_EXAMPLES = 2048

augmented_train_batches = (
    train_dataset
    # Only train on a subset, so you can quickly see the effect.
    .take(NUM_EXAMPLES)
    .cache()
    .shuffle(num_train_examples//4)
    # The augmentation is added here.
    .map(augment, num_parallel_calls=AUTOTUNE)
    .batch(BATCH_SIZE)
    .prefetch(AUTOTUNE)
) 
</code></pre>
<p>From what i can understand what this does is this:It takes the original <code>train_dataset</code> and creates a new <code>augmented_train_batches</code> dataset which has the same number of images  altered by maps transformations.After that what this does is feeding this dataset into <code>.fit</code> like this:</p>
<pre><code>model_with_aug.fit(augmented_train_batches, epochs=50, validation_data=validation_batches)
</code></pre>
<p>So what i can't seem to grasp is this:Aren't the data supposed to be altered after every epoch so that(according to documentation)our model won't see the same image more than once and moreover make our overfitting chances lower?</p>
<p>In this tutorial isn't <code>augmented_train_batches</code> just a slightly altered dataset which is fed over and over to our model?</p>
<p>Or is the augmentaion somehow being applied after each epoch in a way i can't understand?</p>
<p>P.S.I suppose augmentation(if done correctly) must alter the pre-transformed data in a same manner after every epoch and not keep applying transformations to the same altered dataset.</p>
",0
60775718,TensorFlow Lite model on Coral Dev Board not running on TPU,"<p>I have a TensorFlow Lite model and a Coral Dev Board, and I want to perform inference on the Dev Board's TPU.</p>

<p>When initialising the TensorFlow Lite interpreter in my Python inference script, I add ""libedgetpu.so.1"" as an experimental delegate, following the example in <a href=""https://github.com/google-coral/tflite/blob/master/python/examples/classification/classify_image.py"" rel=""nofollow noreferrer"">the Google Coral TFLite Python example</a> (linked to in the <a href=""https://coral.ai/docs/dev-board/get-started/#6-run-a-model-using-the-tensorflow-lite-api"" rel=""nofollow noreferrer"">getting started guide for the Coral Dev Board</a>), however inference is exactly the same speed as when I don't specify the TPU experimental delegate, so I'm assuming that inference is still running on the Dev Board's CPU. Inference time on the Dev Board (with and without the experimental delegate) is 32s; on my desktop PC, inference time for the same test-set is 10s if I run the TFLite model on CPU, and 1.3s if I run the same model in Keras before converting to TFLite (I assume this is faster than TFLite because it utilises multiple cores).</p>

<p>My question: How can I make inference run on the Dev Board's TPU instead of the CPU?</p>

<p>I wonder if this is something I need to specify while building the Keras model on my PC before converting to TFLite format (EG using a <code>with tf.device</code> context manager or something which makes the resulting TFLite model use the TPU), but I can't see anything about this in the <a href=""https://www.tensorflow.org/lite/convert/python_api"" rel=""nofollow noreferrer"">TensorFlow Lite Converter Python API documentation</a>.</p>

<p>The Dev Board is running Mendel version 2.0, Python version 3.5.3, tflite-runtime version 2.1.0.post1 (I know I should update the Mendel version, however I'm currently using a Windows PC, and it will be a pain to get access to a Linux machine, or to try and update the Dev Board from Windows using Putty, VirtualBox or WSL. If only Coral supported Windows, like Raspberry Pi does...).</p>

<p>Below is my inference script (I can also upload training script and model if necessary; dataset is MNIST, converted to NumPy float data as described in <a href=""https://gist.github.com/jakelevi1996/5c532463d59016f42d2bbdcfedd3372a"" rel=""nofollow noreferrer"">this Gist</a>):</p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
from time import perf_counter
try:
    # Try importing the small tflite_runtime module (this runs on the Dev Board)
    print(""Trying to import tensorflow lite runtime..."")
    from tflite_runtime.interpreter import Interpreter, load_delegate
    experimental_delegates=[load_delegate('libedgetpu.so.1.0')]
except ModuleNotFoundError:
    # Try importing the full tensorflow module (this runs on PC)
    try:
        print(""TFLite runtime not found; trying to import full tensorflow..."")
        import tensorflow as tf
        Interpreter = tf.lite.Interpreter
        experimental_delegates = None
    except ModuleNotFoundError:
        # Couldn't import either module
        raise RuntimeError(""Could not import Tensorflow or Tensorflow Lite"")

# Load data
mnist_file = np.load(""data/mnist.npz"")
x_test = mnist_file[""x_test""]
y_test = mnist_file[""y_test""]
x_test = x_test.astype(np.float32)

# Initialise the interpreter
tfl_filename = ""lstm_mnist_model_b10000.tflite""
interpreter = Interpreter(model_path=tfl_filename,
    experimental_delegates=experimental_delegates)
interpreter.allocate_tensors()

print(""Starting evaluation..."")
for _ in range(3):
    input_index = (interpreter.get_input_details()[0]['index'])
    output_index = (interpreter.get_output_details()[0]['index'])
    # Perform inference
    t0 = perf_counter()
    interpreter.set_tensor(input_index, x_test)
    interpreter.invoke()
    result = interpreter.get_tensor(output_index)
    t1 = perf_counter()
    # Print accuracy and speed
    num_correct = (result.argmax(axis=1) == y_test).sum()
    print(""Time taken (TFLite) = {:.4f} s"".format(t1 - t0))
    print('TensorFlow Lite Evaluation accuracy = {} %'.format(
        100 * num_correct / len(x_test)))
    # Reset interpreter state (I don't know why this should be necessary, but
    # accuracy suffers without it)
    interpreter.reset_all_variables()
</code></pre>
",0
60778828,higher order gradient through py_function,"<p>I wonder how to calculate higher order gradients through tf.py_function in tf2.0.  The following example (slightly modified from tensorflow doc) produces the correct dy_dx, and aa_x is None.  Thank you.</p>

<pre><code>import tensorflow as tf
import os

def huber(x, delta):
  if tf.abs(x) &lt;= delta:
    return x*x/ (2*delta)
  else:
    return tf.abs(x)-delta/2.0



x = tf.constant ([2.0 ] )         
z = tf.constant ([1.0 ] )

with tf.GradientTape (persistent=True) as g0:
  g0.watch(x)

  with tf.GradientTape (persistent=True) as g :
    g.watch (x)
    y = tf.py_function(func=huber, inp=[x, 3.] , Tout=tf.float32  )

  dy_dx = g.gradient(y, x)
  aa = tf.reduce_sum(dy_dx *z )

aa_x = g0.gradient (aa, x)
print (dy_dx)
print (aa_x)
</code></pre>
",0
60782077,How do you use tensorflow ctc_batch_cost function with keras?,"<p>I have been trying to implement a CTC loss function in keras for several days now.</p>
<p>Unfortunately, I have yet to find a simple way to do this that fits well with keras. I found tensorflow's <code>tf.keras.backend.ctc_batch_cost</code> function but there is not much documentation on it. I am confused about a few things. First, what are the <code>input_length</code> and <code>label_length</code> parameters? I am trying to make a handwriting recognition model and my images are 32x128, my RNN has 32 time steps, and my character list has a length of 80. I have tried to use 32 for both parameters and this gives me the error below.</p>
<p>Shouldn't the function already know the <code>input_length</code> and <code>label_length</code> from the shape of the first two parameters (<code>y_true</code> and <code>y_pred</code>)?</p>
<p>Secondly, do I need to encode my training data? Is this all done automatically?</p>
<p>I know tensorflow also has a function called <code>tf.keras.backend.ctc_decode</code>. Is this only used when making predictions?</p>
<pre><code>def ctc_cost(y_true, y_pred):
    return tf.keras.backend.ctc_batch_cost(
        y_true, y_pred, 32, 32)


model = tf.keras.Sequential([
    layers.Conv2D(32, 5, padding=&quot;SAME&quot;, input_shape=(32, 128, 1)),
    layers.BatchNormalization(),
    layers.Activation(&quot;relu&quot;),
    layers.MaxPool2D(2, 2),
    layers.Conv2D(64, 5, padding=&quot;SAME&quot;),
    layers.BatchNormalization(),
    layers.Activation(&quot;relu&quot;),
    layers.MaxPool2D(2, 2),
    layers.Conv2D(128, 3, padding=&quot;SAME&quot;),
    layers.BatchNormalization(),
    layers.Activation(&quot;relu&quot;),
    layers.MaxPool2D((1, 2), (1, 2)),
    layers.Conv2D(128, 3, padding=&quot;SAME&quot;),
    layers.BatchNormalization(),
    layers.Activation(&quot;relu&quot;),
    layers.MaxPool2D((1, 2), (1, 2)),
    layers.Conv2D(256, 3, padding=&quot;SAME&quot;),
    layers.BatchNormalization(),
    layers.Activation(&quot;relu&quot;),
    layers.MaxPool2D((1, 2), (1, 2)),
    layers.Reshape((32, 256)),
    layers.Bidirectional(layers.LSTM(256, return_sequences=True)),
    layers.Bidirectional(layers.LSTM(256, return_sequences=True)),
    layers.Reshape((-1, 32, 512)),
    layers.Conv2D(80, 1, padding=&quot;SAME&quot;),
    layers.Softmax(-1)
])

print(model.summary())

model.compile(tf.optimizers.RMSprop(0.001), ctc_cost)
</code></pre>
<p><strong>Error:</strong></p>
<p><em>tensorflow.python.framework.errors_impl.InvalidArgumentError: squeeze_dims[0] not in [0,0). for 'loss/softmax_loss/Squeeze' (op: 'Squeeze') with input shapes: []</em></p>
<p><strong>Model:</strong></p>
<pre><code>Layer (type)                 Output Shape              Param #
=================================================================
conv2d (Conv2D)              (None, 32, 128, 32)       832
batch_normalization (BatchNo (None, 32, 128, 32)       128
activation (Activation)      (None, 32, 128, 32)       0
max_pooling2d (MaxPooling2D) (None, 16, 64, 32)        0
conv2d_1 (Conv2D)            (None, 16, 64, 64)        51264
batch_normalization_1 (Batch (None, 16, 64, 64)        256
activation_1 (Activation)    (None, 16, 64, 64)        0
max_pooling2d_1 (MaxPooling2 (None, 8, 32, 64)         0
conv2d_2 (Conv2D)            (None, 8, 32, 128)        73856
batch_normalization_2 (Batch (None, 8, 32, 128)        512
activation_2 (Activation)    (None, 8, 32, 128)        0
max_pooling2d_2 (MaxPooling2 (None, 8, 16, 128)        0
conv2d_3 (Conv2D)            (None, 8, 16, 128)        147584
batch_normalization_3 (Batch (None, 8, 16, 128)        512
activation_3 (Activation)    (None, 8, 16, 128)        0
max_pooling2d_3 (MaxPooling2 (None, 8, 8, 128)         0
conv2d_4 (Conv2D)            (None, 8, 8, 256)         295168
batch_normalization_4 (Batch (None, 8, 8, 256)         1024
activation_4 (Activation)    (None, 8, 8, 256)         0
max_pooling2d_4 (MaxPooling2 (None, 8, 4, 256)         0
reshape (Reshape)            (None, 32, 256)           0
bidirectional (Bidirectional (None, 32, 512)           1050624
bidirectional_1 (Bidirection (None, 32, 512)           1574912
reshape_1 (Reshape)          (None, None, 32, 512)     0
conv2d_5 (Conv2D)            (None, None, 32, 80)      41040     
softmax (Softmax)            (None, None, 32, 80)      0
</code></pre>
<p><strong>Here is the tensorflow documentation I was referencing:</strong></p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/keras/backend/ctc_batch_cost"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/backend/ctc_batch_cost</a></p>
",1
60801403,NotFoundError: No registered 'PyFunc' OpKernel for 'CPU' devices compatible with node {{node PyFunc}} . Registered: <no registered kernels>,"<p>I am getting an error while trying to access data from a tf.data.Dataset object.
The dataset object is built from a generator. Any help will be appreciated.
I'm using TensorFlow 2 and trying to run the example from <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator</a></p>

<pre><code>def gen(): 
  for i in itertools.count(1): 
    yield (i, [1] * i) 

dataset = tf.data.Dataset.from_generator( 
     gen, 
     (tf.int64, tf.int64), 
     (tf.TensorShape([]), tf.TensorShape([None]))) 

list(dataset.take(3).as_numpy_iterator()) 
</code></pre>

<p>The  error is : </p>

<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in _next_internal(self)
    662         # Fast path for the case `self._structure` is not a nested structure.
--&gt; 663         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access
    664       except AttributeError:

AttributeError: 'tuple' object has no attribute '_from_compatible_tensor_list'

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py in execution_mode(mode)
   1896     ctx.executor = executor_new
-&gt; 1897     yield
   1898   finally:



...  
NotFoundError                             Traceback (most recent call last)
&lt;ipython-input-25-ac0e933e02b3&gt; in &lt;module&gt;
----&gt; 1 list(dataset.take(3).as_numpy_iterator())

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py in __next__(self)
   3640 
   3641   def __next__(self):
-&gt; 3642     return self.next()
   3643 
   3644 

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py in next(self)
   3637 
   3638   def next(self):
-&gt; 3639     return nest.map_structure(lambda x: x.numpy(), next(self._iterator))
   3640 
   3641   def __next__(self):

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in __next__(self)
    628 
    629   def __next__(self):  # For Python 3 compatibility
--&gt; 630     return self.next()
    631 
    632   def _next_internal(self):

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in next(self)
    672     """"""Returns a nested structure of `Tensor`s containing the next element.""""""
    673     try:
--&gt; 674       return self._next_internal()
    675     except errors.OutOfRangeError:
    676       raise StopIteration

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/iterator_ops.py in _next_internal(self)
    663         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access
    664       except AttributeError:
--&gt; 665         return structure.from_compatible_tensor_list(self._element_spec, ret)
    666 
    667   @property

/opt/conda/lib/python3.6/contextlib.py in __exit__(self, type, value, traceback)
     97                 value = type()
     98             try:
---&gt; 99                 self.gen.throw(type, value, traceback)
    100             except StopIteration as exc:
    101                 # Suppress StopIteration *unless* it's the same exception that

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/context.py in execution_mode(mode)
   1898   finally:
   1899     ctx.executor = executor_old
-&gt; 1900     executor_new.wait()
   1901 
   1902 

/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/executor.py in wait(self)
     65   def wait(self):
     66     """"""Waits for ops dispatched in this executor to finish.""""""
---&gt; 67     pywrap_tensorflow.TFE_ExecutorWaitForAllPendingNodes(self._handle)
     68 
     69   def clear_error(self):

NotFoundError: No registered 'PyFunc' OpKernel for 'CPU' devices compatible with node {{node PyFunc}}
    .  Registered:  &lt;no registered kernels&gt;
</code></pre>
",0
60816678,How does one log activations using `tf.keras.callbacks.TensorBoard`?,"<p>The documentation for <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard"" rel=""noreferrer""><code>tf.keras.callbacks.TensorBoard</code></a> states the tool can do it:</p>
<blockquote>
<p>This callback logs events for TensorBoard, including:</p>
<ul>
<li>Metrics summary plots</li>
<li>Training graph visualization</li>
<li><strong>Activation histograms</strong></li>
<li>Sampled profiling</li>
</ul>
</blockquote>
<p>Also later:</p>
<blockquote>
<p><code>histogram_freq</code>: frequency (in epochs) at which to compute <strong>activation</strong> and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations.</p>
</blockquote>
<p>However when using this parameter I don't see any activation summary written, only the weights themselves are written. Looking at the <a href=""https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/callbacks.py#L1712"" rel=""noreferrer"">source code</a> I don't see anything activation-related either.</p>
<p>So am I missing something? Is it possible to write activation summaries without custom code in TF2?</p>
",1
60872298,Using batch normalization during test time but not in inference mode,"<p>How to tell a <code>tf.keras</code> model <em>to not use</em> the Batch Normalization in <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization"" rel=""nofollow noreferrer"">inference mode</a> during prediction? </p>
",0
60883928,Quantization aware training in TensorFlow version 2 and BatchNorm folding,"<p>I'm wondering what the current available options are for simulating BatchNorm folding during quantization aware training in Tensorflow 2. Tensorflow 1 has the <code>tf.contrib.quantize.create_training_graph</code> function which inserts FakeQuantization layers into the graph and takes care of simulating batch normalization folding (according to this <a href=""https://arxiv.org/pdf/1806.08342.pdf"" rel=""noreferrer"">white paper</a>).</p>

<p>Tensorflow 2 has a <a href=""https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide.md"" rel=""noreferrer"">tutorial</a> on how to use quantization in their recently adopted <code>tf.keras</code> API, but they don't mention anything about batch normalization. I tried the following simple example with a BatchNorm layer:</p>

<pre><code>import tensorflow_model_optimization as tfmo

model = tf.keras.Sequential([
      l.Conv2D(32, 5, padding='same', activation='relu', input_shape=input_shape),
      l.MaxPooling2D((2, 2), (2, 2), padding='same'),
      l.Conv2D(64, 5, padding='same', activation='relu'),
      l.BatchNormalization(),    # BN!
      l.MaxPooling2D((2, 2), (2, 2), padding='same'),
      l.Flatten(),
      l.Dense(1024, activation='relu'),
      l.Dropout(0.4),
      l.Dense(num_classes),
      l.Softmax(),
])
model = tfmo.quantization.keras.quantize_model(model)
</code></pre>

<p>It however gives the following exception:</p>

<pre><code>RuntimeError: Layer batch_normalization:&lt;class 'tensorflow.python.keras.layers.normalization.BatchNormalization'&gt; is not supported. You can quantize this layer by passing a `tfmot.quantization.keras.QuantizeConfig` instance to the `quantize_annotate_layer` API.
</code></pre>

<p>which indicates that TF does not know what to do with it.</p>

<p>I also saw <a href=""https://stackoverflow.com/questions/52259343/quantize-a-keras-neural-network-model/57785739#57785739"">this related topic</a> where they apply <code>tf.contrib.quantize.create_training_graph</code> on a keras constructed model. They however don't use BatchNorm layers, so I'm not sure this will work.</p>

<p>So what are the options for using this BatchNorm folding feature in TF2? Can this be done from the keras API, or should I switch back to the TensorFlow 1 API and define a graph the old way?</p>
",0
60919434,String to one_hot tensor in Tensorflow,"<p>I have found in tensorflow doc the following function to compute and apply a vocabulary onto a string tensor but it was still using <code>tf.session</code> and I can't make it work with <code>tf.function</code>:</p>

<pre><code>import tensorflow as tf
import tensorflow_transform as tft


@tf.function(input_signature=(tf.TensorSpec(shape=[None], dtype=tf.string),))
def string_to_one_hot(labels):
    codes = tft.compute_and_apply_vocabulary(labels)
    return tf.one_hot(codes, depth=tf.cast(tf.reduce_max(codes), tf.int32))


test_labels = tf.constant(['a', 'b', 'a', 'c'])
test_one_hot = string_to_one_hot(test_labels)

&gt; tensorflow.python.framework.errors_impl.InvalidArgumentError:  You must feed a value for placeholder tensor 'compute_and_apply_vocabulary/vocabulary/Placeholder' with dtype string
     [[node compute_and_apply_vocabulary/vocabulary/Placeholder (defined at /Users/clementwalter/.pyenv/versions/keras_fsl/lib/python3.6/site-packages/tensorflow_transform/analyzer_nodes.py:102) ]] [Op:__inference_string_to_one_hot_52]

</code></pre>

<h2>EDIT</h2>

<p>I have been able to build such a function with direct use of the hash facilities. However I have had to use a hard-coded bucket_size/depth param. Any ideas?</p>

<pre><code>@tf.function(input_signature=(tf.TensorSpec(shape=[None], dtype=tf.string),))
def string_to_one_hot(labels):
    one_hot = tf.one_hot(tf.strings.to_hash_bucket_fast(labels, 1024), depth=1024)
    return tf.boolean_mask(one_hot, tf.reduce_sum(one_hot, axis=0) &gt; 0, axis=1)
</code></pre>
",1
61004723,Understanding param's of TensorFlow 2.x Attention layer,"<p>I'm reading the documentation on <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention"" rel=""nofollow noreferrer"">tf.keras.layers.Attention</a> in TensorFlow website. The documentation says the call argument takes <strong>Query Tensor, Value Tensor</strong>.
My question is what are these tensor. Are they,</p>

<ul>
<li>Query Tensor = lstm output</li>
<li>Value Tensor = lstm hidden state</li>
</ul>

<p>Please correct me if I'm wrong.</p>

<p>Thanks</p>
",1
61042977,Tensorflow image captioning / seq2seq decoder as model,"<p>I am using a model for sequence prediction starting from a latent representation of an encoded input, which forms the initial state of the decoder. It could be a feature vector from an image (for captioning) or the result of a seq2seq encoder. </p>

<p>My model is trained with teacher forcing and this goes quite fast. However inference is brutally slow because I do a stepwise sequence extension in the form of (pseudocode)</p>

<pre><code>sequence_terminated = False
sequence = np.array((0, output_features))
while not sequence_terminated:
   seq_output, seq_states = model.predict(seq_input)
   next_input, sequence_terminated = f(seq_output)
   sequence = np.concatenate(sequence, seq_output)
</code></pre>

<p>I have done a lot of optimization at this stage so I can predict sequences for hundreds of queries in parallel, but 1) running on CPU it scales linear at >32 or so sequences, and 2) running on GPU is actually slower than on CPU, presumedly because data has to be moved back and forth after every step and there is no profit off the GPU speed.</p>

<p>I'm additionally using a non-greedy sequence search that isn't Beam Search but can backtrack in a way similar to A*, more or less like this (pseudocode):</p>

<pre><code>sequence_terminated = False
sequence = np.array((0, output_features))
states = np.array((0, state_size))
pos = []
from_position = 0
remaining_sequences = 5
while remaining_sequences &gt; 0:
   seq_output, seq_states = model.predict(seq_input)
   sequence = np.concatenate(sequence, seq_output)
   states = np.concatenate(states, seq_states)
   pos.append(from_position)
   # Based on the outputs until now, find what sequence stub to continue:
   next_input, next_states, from_position, terminated = f(sequence, states)
   if terminated:
      remaining_sequences = remaining_sequences - 1

</code></pre>

<p>which gives top-n sequences backtracking from the last predicted position. Again this is more or less optimized on the CPU side of things for parallel prediction.</p>

<p>I think to get faster I need to run prediction completely on GPU without moving data back. But I don't get how to write this in TensorFlow. There is <code>tfa.seq2seq</code> (former <code>tf.contrib.seq2seq</code>) which has an infrastructure for decoders that presumably run efficiently as models, but I cannot find much documentation.</p>

<p>Note that my model (Keras functional API; it can also be used with <code>model()</code> instead of <code>model.predict()</code> or I can wire the output tensors somewhere else) is not simply 3 LSTM layers but has some inline feature engineering that is stateful, so it needs to be done in the model. <code>tfa.seq2seq.Decoder</code> seems to expect a single cell to wrap itself around?</p>

<p>Questions: 
1) Can I use the <code>tfa.seq2seq</code> decoder architecture for a blackbox model built and trained independently from the <code>tfa.seq2seq</code> architecture? If yes, where can I find info about that?
2) Are there any pointers on how to implement greedy and non-greedy sequence search directly on tensorflow, without falling back to python code like mine above? I understand I will probably have to give up my non-greedy approach and use just beam search, which will probably perform about the same.</p>
",0
61087933,Inequivalent output from tf.nn.conv2d and keras.layers.Conv2D,"<p>I've been reading the <em>Hands-On Machine Learning</em> textbook (2nd edition) by Aurélien Géron (<a href=""https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/"" rel=""nofollow noreferrer"">textbook publisher webpage here</a>). I've gotten into the content that applies CNNs to images. In the section titled <em>Tensorflow Implementation</em> of Chapter 14, they manually create filters that get passed to <code>tf.nn.conv2d</code> and applied to an image to produce a set of feature maps. After these manual filter examples, the book says:</p>
<blockquote>
<p>in a real CNN you would normally define filters as trainable variables ... Instead of manually creating the variables, use the <code>keras.layers.Conv2D</code> layer.</p>
</blockquote>
<p>The above quote implies to me that given identical inputs (and equivalent initializations), we should be able to derive identical outputs from <code>tf.nn.conv2d</code> and <code>keras.layers.Conv2D</code>. To validate this idea, I looked up whether the two functions were equivalent. According to <a href=""https://stackoverflow.com/questions/42785026/tf-nn-conv2d-vs-tf-layers-conv2d"">this previously answered SO post</a>, <strong>for convolution, the two functions are the same</strong>.</p>
<p>I set out to perform a simple test of their equivalence. I created a convolutional layer consisting of one feature map using a 7x7 filter (a.k.a: <em>convolutional kernel</em>) <strong>of all zeros</strong> that was implemented separately for <code>tf.nn.conv2d</code> and <code>keras.layers.Conv2D</code>. As expected, after summing all the pixel values in the difference of both images, this filter did cause the output images to have a value of zero for each pixel value. This difference of zero implies that the output images are identical.</p>
<p>I then decided to create the same 7x7 filter, but <strong>with all ones</strong> this time. Ideally, both functions should produce the same output, therefore the difference in the two output images should be zero. Unfortunately, when I check the difference in the output images (and sum the differences at each pixel), I get a nonzero sum value. Upon plotting the images and their difference, it is evident that they are not the same image (though they do look <em>very</em> similar at a glance).</p>
<p>After reading through the documentation for both functions, I believe that I am giving them equivalent inputs. <strong>What could I be doing/assuming incorrectly that is preventing both functions from producing identical outputs?</strong></p>
<p>I have attached my code and versionining information below for reference. The code uses the scikit-learn <code>china.jpg</code> sample image as input and <code>matplotlib.pyplot.imshow</code> to help in visualizing the output images and their difference.</p>
<blockquote>
<p>TF Version: 2.2.0-dev20200229</p>
<p>Keras Version: 2.3.1</p>
<p>Scikit-Learn Version: 0.22.1</p>
<p>Matplotlib Version: 3.1.3</p>
<p>Numpy Version: 1.18.1</p>
</blockquote>
<pre class=""lang-py prettyprint-override""><code>from sklearn.datasets import load_sample_image
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
import numpy as np

# Get the feature map as a result of tf.nn.conv2d
def featureMap1(batch):
    
    # Extract the channels
    batch_size, height, width, channels = batch.shape

    # Make a (7,7,3,1) filter set (one set of a 7x7 filter per channel)
    # of just ones. 
    filters = np.ones(shape=(7, 7, channels, 1), dtype=np.float32)

    # Run the conv2d with stride of 1 (i.e: in.shape = out.shape)
    # Generate one feature map for this conv layer
    fmaps = tf.nn.conv2d(batch, filters,
                         strides=1, padding='SAME',
                         data_format='NHWC')
    
    # Return the feature map
    return fmaps

# Get the feature map as a result of keras.layers.Conv2D
def featureMap2(batch):

    # Create the input layer with the shape of the images
    inputLayer = keras.layers.Input(shape=batch.shape[1:])
    
    # Create the convLayer which should apply the filter of all ones
    convLayer = keras.layers.Conv2D(filters=1, kernel_size=7,
                                    strides=1, padding='SAME',
                                    kernel_initializer='ones',
                                    data_format='channels_last',
                                    activation='linear')

    # Create the ouput layer
    outputLayer = convLayer(inputLayer)

    # Set up the model
    model = keras.Model(inputs=inputLayer,
                        outputs=outputLayer)

    # Perform a prediction, no model fitting or compiling
    fmaps = model.predict(batch)

    return fmaps 

def main():

    # Get the image and scale the RGB values to [0, 1]
    china = load_sample_image('china.jpg') / 255

    # Build a batch of just one image
    batch = np.array([china])

    # Get the feature maps and extract
    # the images within them
    img1 = featureMap1(batch)[0, :, :, 0]
    img2 = featureMap2(batch)[0, :, :, 0]

    # Calculate the difference in the images
    # Ideally, this should be all zeros...
    diffImage = np.abs(img1 - img2)

    # Add up all the pixels in the diffImage,
    # we expect a value of 0 if the images are
    # identical
    print('Differences value: ', diffImage.sum())

    # Plot the images as a set of 4
    figsize = 10
    f, axarr = plt.subplots(2, 2, figsize=(figsize,figsize))

    axarr[0,0].set_title('Original Image')
    axarr[0,0].imshow(batch[0], cmap='gray')

    axarr[1,0].set_title('Conv2D through tf.nn.conv2d')
    axarr[1,0].imshow(img1, cmap='gray')
    
    axarr[1,1].set_title('Conv2D through keras.layers.Conv2D')
    axarr[1,1].imshow(img2, cmap='gray')

    axarr[0,1].set_title('Diff')
    axarr[0,1].imshow(diffImage, cmap='gray')
    
    plt.show()
    
    return


main()
</code></pre>
",0
61102598,How to incorporate many features into a TensorFlow Probability Structural Time Series,"<p>I'm wondering how to train a Multivariate Bayesian Structural Time Series (BSTS) model that automatically performs feature selection on hundreds of input time series using Tensorflow Probability.</p>
<p>The <a href=""https://blog.tensorflow.org/2019/03/structural-time-series-modeling-in.html"" rel=""nofollow noreferrer"">TF-Probability BSTS blog post</a> shows how to include seasonal effects alongside a single input feature:</p>
<pre><code>  ...
  temp_effect = sts.LinearRegression(
      design_matrix=tf.reshape(temp - np.mean(temp),
                               (-1, 1)), name='temp_effect')
  ...
  model = sts.Sum([..., temp_effect,...],
                   observed_time_series=observed_time_series)
</code></pre>
<p>But what about when there are multiple input time series?</p>
<p>Reading through the <a href=""https://github.com/tensorflow/probability/blob/v0.9.0/tensorflow_probability/python/sts/regression.py"" rel=""nofollow noreferrer"">documentation</a> makes it seem that with many inputs the SparseLinearRegression would be preferrable, which makes sense, but how should I adapt my code?</p>
<p>The documentation for both LinearRegression and SparseLinearRegression method suggests using <code>design_matrix=tf.stack([series1, series2], axis=-1), weights_prior_scale=0.1)</code>, but since that's different from how TF-Probability's own blog post uses it I am unsure if that is the best way to go.</p>
<p>Should I be adding all (several hundred) input features inside the <code>design_matrix</code> of a single SparseLinearRegression, or should I be adding a separate LinearRegression for each feature and then use <code>sts.Sum()</code> to combine them all into the model? Though I would like the functionality of visualizing the impact of each feature, I am most interested in having the model automatically perform feature selection and generate weights for the remaining features which I can have access to.</p>
",1
61175291,Why is optimizer.minimize not working if we pass loss as tf.constant?,"<p>I simply have <code>train = optimizer.minimize(loss = tf.constant(4,dtype=""float32""))</code> Line of code that i change before everything is working. <br/></p>

<p>Why it is giving error ? Because documentation say it can be tensor <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam#minimize"" rel=""nofollow noreferrer"">Here is Docs</a> </p>

<pre><code>W = tf.Variable([0.5],tf.float32)
b = tf.Variable([0.1],tf.float32)
x = tf.placeholder(tf.float32)
y= tf.placeholder(tf.float32)
discounted_reward = tf.placeholder(tf.float32,shape=[4,], name=""discounted_reward"")
linear_model = W*x + b

squared_delta = tf.square(linear_model - y)
print(squared_delta)
loss = tf.reduce_sum(squared_delta*discounted_reward)
print(loss)
optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(loss = tf.constant(4,dtype=""float32""))
init = tf.global_variables_initializer()
sess = tf.Session()

sess.run(init)

for i in range(3):
    sess.run(train,{x:[1,2,3,4],y:[0,-1,-2,-3],discounted_reward:[1,2,3,4]})

print(sess.run([W,b]))
</code></pre>

<hr>

<p>I really need this thing to work. In this particular example we can have other ways to solve it but i need it to work as my actual code can do this only </p>

<p><hr/> Error is</p>

<pre><code>&gt; ValueError: No gradients provided for any variable, check your graph
&gt; for ops that do not support gradients, between variables
&gt; [""&lt;tf.Variable 'Variable:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_1:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_2:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_3:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_4:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_5:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_6:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_7:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_8:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_9:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_10:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_11:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_12:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_13:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_14:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_15:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_16:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_17:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_18:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_19:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_20:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_21:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_22:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_23:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_24:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_25:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_26:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_27:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_28:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_29:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_30:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_31:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_32:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_33:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_34:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_35:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_36:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_37:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_38:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_39:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_40:0' shape=(1,) dtype=float32_ref&gt;"",
&gt; ""&lt;tf.Variable 'Variable_41:0' shape=(1,) dtype=float32_ref&gt;""] and loss
&gt; Tensor(""Const_4:0"", shape=(), dtype=float32).
</code></pre>
",1
61200011,How do I add a dimension to class 'tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter' object in Python?,"<p>I have a image data set that I want to use to train a CNN. I have initialized a class ""tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter"" object that I understand is essentially an iterator that caches the train images in batches so that the entire data set need not be loaded at once.</p>

<p>I have received this error when trying to call model.fit():</p>

<pre><code>ValueError: Error when checking input: expected conv2d_input to have 4 dimensions, but got array with 
shape (None, 1)
</code></pre>

<p>I understand that I need to add a dimension to my model input. I want to add a channels dimension to my images. I have tried to use np.expand_dims() and tf.expand_dims() on my class ""tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter"" object but the former changes the object type and the latter is not supported for the class object. Any help is appreciated. Below is my model structure:</p>

<pre><code>model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))


model.summary()

model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10))
history = model.fit(train_data, epochs=10, validation_data=(val_data),steps_per_epoch=x, 
validation_steps=y)
</code></pre>

<p>I have been following the tutorial in the example listed here, <a href=""https://www.tensorflow.org/tutorials/load_data/images"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/load_data/images</a>, but have tried to create and load in my own data set.</p>

<p>Below is my tf pipeline: </p>

<pre><code>BATCH_SIZE = 32
IMG_HEIGHT = 224
IMG_WIDTH = 224
STEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)

data_dir = 'C:\\Users\\rtlum\\Documents\\DataSci_Projects\\PythonTensorFlowProjects\\google-images-download\\images'
list_ds = tf.data.Dataset.list_files(str(data_dir+""*.jpg""))  #Make dataset of file paths
class_names = ['sad', 'angry']

size = 0
for count in enumerate(list_ds):
    size += 1
val_data_size = size * .2

for f in list_ds.take(5):#test for correct file paths
  print(f.numpy())

def get_label(file_path):
  # convert the path to a list of path components
  parts = tf.strings.split(file_path, os.path.sep)
  # The second to last is the class-directory
  return parts[-2] == class_names

def decode_img(img):
  # convert the compressed string to a 3D uint8 tensor
  img = tf.image.decode_jpeg(img, channels=3)
  # Use `convert_image_dtype` to convert to floats in the [0,1] range.
  img = tf.image.convert_image_dtype(img, tf.float32)
  # resize the image to the desired size.
  return tf.image.resize(img, [64, 64])

def process_path(file_path):
  label = get_label(file_path)
  # load the raw data from the file as a string
  img = tf.io.read_file(file_path)
  img = decode_img(img)
  return img, label

# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.
labeled_ds = list_ds.map(process_path)

for image, label in labeled_ds.take(1):
  print(""Image shape: "", image.numpy().shape)
  print(""Label: "", label.numpy())

shuffle_buffer_size=1000

def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):
  # This is a small dataset, only load it once, and keep it in memory.
  # use `.cache(filename)` to cache preprocessing work for datasets that don't
  # fit in memory.
  if cache:
    if isinstance(cache, str):
      ds = ds.cache(cache)
    else:
      ds = ds.cache()
  ds = ds.shuffle(buffer_size=shuffle_buffer_size)
  # Repeat forever
  ds = ds.repeat()
  ds = ds.batch(BATCH_SIZE)
  # `prefetch` lets the dataset fetch batches in the background while the model
  # is training.
  ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
  return ds

ds = prepare_for_training(list_ds)


val_data = ds.take(int(val_data_size))
train_data = ds.skip(int(val_data_size))
</code></pre>
",0
61250353,tf.keras.callbacks.ModelCheckpoint vs tf.train.Checkpoint,"<p>I am kinda new to TensorFlow world but have written some programs in Keras. Since TensorFlow 2 is officially similar to Keras, I am quite confused about what is the difference between tf.keras.callbacks.ModelCheckpoint and tf.train.Checkpoint. If anybody can shed light on this, I would appreciate it. </p>
",0
61273445,Tensorflow MapDataset iterator fails,"<p>I am trying to implement the method suggested by the tensorflow documentation over here (<a href=""https://www.tensorflow.org/tutorials/load_data/images"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/load_data/images</a>) to load images from local directory as a tensorflow dataset. Especially I am interested in loading using tf.data as a tf.data.Dataset object as it is suggested that the performance is better that way. I pretty much took the exact code from the documentation page and also made sure that the tensorflow version matches to the one in the documentation</p>

<p>The problem happens when I try to iterate over the MapDataset object using take().</p>

<pre><code>import os
import sys
import pathlib

import IPython.display as display
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf

AUTOTUNE = tf.data.experimental.AUTOTUNE

BATCH_SIZE = 32
IMG_HEIGHT = 224
IMG_WIDTH = 224
STEPS_PER_EPOCH = np.ceil(3670/BATCH_SIZE)
CLASS_NAMES = None

#https://www.tensorflow.org/tutorials/load_data/images

def get_label(file_path):
    # convert the path to a list of path components
    #parts = tf.strings.split(file_path, result_type = 'RaggedTensor')
    parts = tf.strings.split(file_path)

    # The second to last is the class-directory
    return parts[-2] == CLASS_NAMES

def decode_img(img):
    # convert the compressed string to a 3D uint8 tensor
    img = tf.image.decode_jpeg(img, channels=3)

    # Use `convert_image_dtype` to convert to floats in the [0,1] range.
    img = tf.image.convert_image_dtype(img, tf.float32)

    # resize the image to the desired size.
    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])

def process_path(file_path):
    label = get_label(file_path)

    # load the raw data from the file as a string
    img = tf.io.read_file(file_path)
    img = decode_img(img)

    return img, label

def test():

    data_dir = tf.keras.utils.get_file(origin='https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
                                         fname='flower_photos', untar=True)

    data_dir = pathlib.Path(data_dir)

    global CLASS_NAMES
    CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != ""LICENSE.txt""])

    list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))

    labeled_ds = list_ds.map(process_path)
    print('type: ', type(labeled_ds))

    for image, label in labeled_ds.take(1):
        print(""Image shape: "", image.numpy().shape)
        print(""Label: "", label.numpy())

def main():
    test()  

if __name__ == '__main__':
    main()

</code></pre>

<p>I get the following error and have no idea how to go about resolving this</p>

<pre><code>2020-04-17 09:47:53.816123: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at strided_slice_op.cc:108 : Invalid argument: slice index -1 of dimension 0 out of bounds.
2020-04-17 09:47:53.820082: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at iterator_ops.cc:941 : Invalid argument: slice index -1 of dimension 0 out of bounds.
         [[{{node strided_slice}}]]
Traceback (most recent call last):
  File ""C:\Users\VVJ3281\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow_core\python\eager\context.py"", line 1897, in execution_mode
    yield
  File ""C:\Users\VVJ3281\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py"", line 659, in _next_internal
    output_shapes=self._flat_output_shapes)
  File ""C:\Users\VVJ3281\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow_core\python\ops\gen_dataset_ops.py"", line 2478, in iterator_get_next_sync
    _ops.raise_from_not_ok_status(e, name)
  File ""C:\Users\VVJ3281\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 6606, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""&lt;string&gt;"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: slice index -1 of dimension 0 out of bounds.
         [[{{node strided_slice}}]] [Op:IteratorGetNextSync]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "".\img_sub_model.py"", line 150, in &lt;module&gt;
    main()
  File "".\img_sub_model.py"", line 145, in main
    test()
  File "".\img_sub_model.py"", line 136, in test
    for image, label in labeled_ds.take(1):
  File ""C:\Users\VVJ3281\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py"", line 630, in __next__
    return self.next()
  File ""C:\Users\VVJ3281\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py"", line 674, in next
    return self._next_internal()
  File ""C:\Users\VVJ3281\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow_core\python\data\ops\iterator_ops.py"", line 665, in _next_internal
    return structure.from_compatible_tensor_list(self._element_spec, ret)
  File ""C:\Users\VVJ3281\AppData\Local\Continuum\anaconda3\lib\contextlib.py"", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File ""C:\Users\VVJ3281\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow_core\python\eager\context.py"", line 1900, in execution_mode
    executor_new.wait()
  File ""C:\Users\VVJ3281\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow_core\python\eager\executor.py"", line 67, in wait
    pywrap_tensorflow.TFE_ExecutorWaitForAllPendingNodes(self._handle)
tensorflow.python.framework.errors_impl.InvalidArgumentError: slice index -1 of dimension 0 out of bounds.
         [[{{node strided_slice}}]]
</code></pre>

<p>By some random coincidence I found that when CLASS_NAMES is set to None, the code runs and the lebel object of labeled_ds has a value 'False'</p>

<p>See output below</p>

<pre><code>type:  &lt;class 'tensorflow.python.data.ops.dataset_ops.MapDataset'&gt;
Image shape:  (224, 224, 3)
Label:  False
</code></pre>
",1
61348582,Tensorflow 2 timeseries_dataset_from_array input vs target batch shapes difference,"<p>The new <code>tf.keras.preprocessing.timeseries_dataset_from_array</code> function is used to create sliding minibatch windows over the sequential data, for example for tasks involving rnn networks.</p>

<p>According to the docs it returns a minibatch of inputs and targets. However, the target minibatch this function returns does not have a <code>sequence_length</code> (timesteps) dimension. For example.</p>

<pre class=""lang-py prettyprint-override""><code>data = timeseries_dataset_from_array(
            data=tokens,
            targets=targets,
            sequence_length=25,
            batch_size=32,
        )

for minbatch in data:
    inputs, targets = minbatch
    assert(inputs.shape[1] == targets.shape[1]) # error
</code></pre>

<p>The <code>inputs</code> have <code>[32, 25, 1]</code> shape in case you already just have word indices there and <code>targets</code> confusingly have <code>[32, 1]</code> shape. </p>

<p>So, my question is how am I supposed to map a tensor of inputs with a window of 25 units to a target tensor with a window of 0 units?</p>

<p>How I always train sequence models is by feeding the input tensor of <code>[32, 25, 1]</code> which is then projected into <code>[32, 25, 100]</code> and then you feed the target tensor to the network of size <code>[32, 25, 1]</code> to your loss function or if you have multi-class problem a target vector of <code>[32, 25, num_of_classes]</code>. </p>

<p>That is why I am confused by the shape of the target tensor from <code>timeseries_dataset_from_array</code> and the intuition behind it. </p>
",0
61355289,When will tf.print ACTUALLY WORK as expected (i.e. print the values of tensors and variables)?,"<p>First of all, I am using TensorFlow 2.0 and I only care about this version or higher (and I am already caring too much for such a piece of software that only produces headaches).</p>

<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/print"" rel=""nofollow noreferrer"">TensorFlow documentation</a> of <code>tf.print</code> says </p>

<blockquote>
  <p>Print the specified inputs.</p>
</blockquote>

<p>and then</p>

<blockquote>
  <p>A TensorFlow operator that prints the specified inputs to a desired output stream or logging level. The inputs may be </p>
  
  <ul>
  <li><strong>dense</strong> or </li>
  <li><strong>sparse Tensors</strong>, </li>
  <li><strong>primitive python objects</strong>, </li>
  <li><strong>data structures that contain tensors</strong>, and </li>
  <li><strong>printable Python objects</strong>. </li>
  </ul>
  
  <p>Printed tensors will recursively show the first and last elements of each dimension to summarize. </p>
</blockquote>

<p>This is all very nice, but I still don't get where <code>tf.print</code> will ACTUALLY WORK (i.e. print the VALUES of variables and tensors) in my code. Of course, needless to say, I couldn't care less about the symbolic representations of tensors, variables or whatever. Whenever I try to use <code>tf.print</code>, I want to see the VALUES (real numbers, vectors or matrices). </p>

<p>I've tried to use <code>tf.print</code> in multiple cases and in multiple places, e.g. </p>

<ul>
<li><p>in a method that is called from the <code>__init__</code> method of a custom layer that is called during model building (so before compiling the model) in order to print the value of a tensor (at least, this is what the <code>type(my_var)</code> returns, i.e. it returns <code>&lt;class 'tensorflow.python.framework.ops.Tensor'&gt;</code>), but nothing is printed. If I try to add <code>@tf.function</code> (I still don't get the usage of this function!), nothing changes. According to the documentation above <code>tf.print</code> is supposed to print tensors, my variable is a tensor and TensorFlow decides to ignore my call, and then one wonders why did I decide to use TF? Why? </p>

<p>Also, I am using TF 2.0 and, even if I don't use the decorator <code>@tf.function</code>, <code>print(tf.executing_eagerly())</code> prints False, which is really what I was expecting.</p></li>
<li><p>in a custom loss function, where a similar behaviour happens (i.e. sometimes something is printed, sometimes it is not, sometimes I try to add the decorator <code>@tf.function</code> to the custom loss function and see if something changes, but nothing changes, or maybe yes).</p></li>
</ul>

<p>Ok, so, as you can see, I have no idea where <code>tf.print</code> will do what I want, i.e. I want to see the values of tensors. If something is a tensor, it must have a value. Similarly for variables.</p>

<p>So, when will <code>tf.print</code> ACTUALLY PRINT THE VALUES OF TENSORS? </p>

<p>I am looking for answers that say e.g., ""<code>tf.print</code> will NEVER work"" or ""it will only work if you are dreaming"". Apart from the jokes and sarcasm, I am really looking for answers that tell me exactly in which places of my code or which stages of developing a model with TF <code>tf.print</code>  will actually do what it is supposed to do. Please, don't tell me that <code>tf.print</code> will work when the input is a tensor!! </p>
",1
61355474,Why does tf.executing_eagerly() return False in TensorFlow 2?,"<p>Let me explain my set up. I am using TensorFlow 2.1, the Keras version shipped with TF, and TensorFlow Probability 0.9.</p>

<p>I have a function <code>get_model</code> that creates (with the functional API) and returns a model using Keras and custom layers. In the <code>__init__</code> method of these custom layers <code>A</code>, I call a method <code>A.m</code>, which executes the statement <code>print(tf.executing_eagerly())</code>, but it returns <code>False</code>. Why?</p>

<p>To be more precise, this is roughly my setup</p>

<pre><code>def get_model():
    inp = Input(...)
    x = A(...)(inp) 
    x = A(...)(x)
    ...
    model = Model(inp, out)
    model.compile(...)
    return model

class A(tfp.layers.DenseFlipout): # TensorFlow Probability
    def __init__(...):
        self.m()

    def m(self): 
        print(tf.executing_eagerly()) # Prints False
</code></pre>

<p>The documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/executing_eagerly"" rel=""nofollow noreferrer""><code>tf.executing_eagerly</code></a> says</p>

<blockquote>
  <p>Eager execution is enabled by default and this API returns True in most of cases. However, this API might return False in the following use cases.</p>
  
  <ul>
  <li>Executing inside <code>tf.function</code>, unless under <code>tf.init_scope</code> or <code>tf.config.experimental_run_functions_eagerly(True)</code> is previously called.</li>
  <li>Executing inside a transformation function for <code>tf.dataset</code>.</li>
  <li><code>tf.compat.v1.disable_eager_execution()</code> is called.</li>
  </ul>
</blockquote>

<p>But these cases are not my case, so <code>tf.executing_eagerly()</code> should return <code>True</code> in my case, but no. Why?</p>

<p>Here's a simple complete example (in TF 2.1) that illustrates the problem.</p>

<pre><code>import tensorflow as tf


class MyLayer(tf.keras.layers.Layer):
    def call(self, inputs):
        tf.print(""tf.executing_eagerly() ="", tf.executing_eagerly())
        return inputs


def get_model():
    inp = tf.keras.layers.Input(shape=(1,))
    out = MyLayer(8)(inp)
    model = tf.keras.Model(inputs=inp, outputs=out)
    model.summary()
    return model


def train():
    model = get_model()
    model.compile(optimizer=""adam"", loss=""mae"")
    x_train = [2, 3, 4, 1, 2, 6]
    y_train = [1, 0, 1, 0, 1, 1]
    model.fit(x_train, y_train)


if __name__ == '__main__':
    train()
</code></pre>

<p>This example prints <code>tf.executing_eagerly() = False</code>.</p>

<p>See <a href=""https://github.com/tensorflow/tensorflow/issues/38775"" rel=""nofollow noreferrer"">the related Github issue</a>.</p>
",1
61368708,How to implement and understand Pre-processing and Data augmentation with tensorflow_datasets (tfds)?,"<p>I'm learning segmentation and data augmentation based in this <a href=""https://www.tensorflow.org/tutorials/images/segmentation"" rel=""nofollow noreferrer"">TF 2.0 tutorial</a> that uses <strong>Oxford-IIIT Pets</strong>.</p>

<p>For pre-processing/data augmentation they provide a set of functions into a specific pipeline:</p>

<pre><code># Import dataset
dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)

def normalize(input_image, input_mask):
  input_image = tf.cast(input_image, tf.float32) / 255.0
  input_mask -= 1
  return input_image, input_mask

@tf.function
def load_image_train(datapoint):
  input_image = tf.image.resize(datapoint['image'], (128, 128))
  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))

  if tf.random.uniform(()) &gt; 0.5:
    input_image = tf.image.flip_left_right(input_image)
    input_mask = tf.image.flip_left_right(input_mask)

  input_image, input_mask = normalize(input_image, input_mask)

  return input_image, input_mask

TRAIN_LENGTH = info.splits['train'].num_examples
BATCH_SIZE = 64
BUFFER_SIZE = 1000
STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE

train = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)

train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()
train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
</code></pre>

<p>This code brought me several doubts given the tf syntax. To prevent me from just doing a ctrl C ctrl V and actually understanding how tensorflow works, I would like to ask some questions:</p>

<p>1) In <code>normalize</code> function, the line <code>tf.cast(input_image, tf.float32) / 255.0</code> can be changed by <code>tf.image.convert_image_dtype(input_image, tf.float32)</code>?</p>

<p>2) In <code>normalize</code> function it's possible to change my segmentation_mask values in <code>tf.tensor</code> format without changing to a <code>numpy</code>? What I desire to do is to only work with two possible masks (0 and 1) and not with (0, 1 and 2). Using numpy I made something like this:</p>

<pre><code>segmentation_mask_numpy = segmentation_mask.numpy()
segmentation_mask_numpy[(segmentation_mask_numpy == 2) | (segmentation_mask_numpy == 3)] = 0
</code></pre>

<p>It's possible to do this without a numpy transformation?</p>

<p>3) In <code>load_image_train</code> function they say that this function is doing data augmentation, <strong>but</strong> how? In my perspective they are changing the original image with a flip given a random number and not providing another image to the dataset based in the original image. So, the function goal is to change a image and not add to my dataset an aug_image keeping the original? If I'm correct how can I change this function to give an aug_image and keep my original image in the dataset?</p>

<p>4) In others questions such as <a href=""https://stackoverflow.com/questions/55141076/how-to-apply-data-augmentation-in-tensorflow-2-0-after-tfds-load"">How to apply data augmentation in TensorFlow 2.0 after tfds.load()</a> and <a href=""https://stackoverflow.com/questions/55421290/tensorflow-2-0-keras-how-to-write-image-summaries-for-tensorboard/55754700#55754700"">TensorFlow 2.0 Keras: How to write image summaries for TensorBoard</a> they used a lot of <code>.map()</code> sequential calls or <code>.map().map().cache().batch().repeat()</code>. My question is: there is this necessity? Exist a more simple way to do this? I tried to read tf documentation, but without success.</p>

<p>5) You recommed to work with <code>ImageDataGenerator</code> from keras as presented <a href=""https://stackoverflow.com/questions/59648804/how-can-i-combine-imagedatagenerator-with-tensorflow-datasets-in-tf2"">here</a> or this tf approach is better?</p>
",1
61428918,tensorflow2: keras: model.fit() callbacks and eager mode,"<p>I am running Tensorflow 2.1 with keras API. I am following the following coding style:</p>

<pre><code>    model = tf.keras.Sequential()
    ...
    model.fit(..., callbacks=callbacks)
</code></pre>

<p>Now, I would like to save some intermediate layer tensor value as image summary (as a sample what is happening at n-th training step). In order to do this, I've implemented my own callback class. I've also learned how <code>keras.callbacks.TensorBoard</code> is implemented, since it can save layer weights as image summaries.
I do the following in my <code>on_epoch_end</code>:</p>

<pre><code>tensor = self.model.get_layer(layer_name).output

with context.eager_mode():
    with ops.init_scope():
        tensor = tf.keras.backend.get_value(tensor)
    tf.summary.image(layer_name, tensor, step=step, max_outputs=1)
</code></pre>

<p>Unfortunately, I am still getting issue related to eager/graph modes:</p>

<pre><code>    tensor = tf.keras.backend.get_value(tensor)
  File ""/home/matwey/lab/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py"", line 3241, in get_value
    return x.numpy()
AttributeError: 'Tensor' object has no attribute 'numpy'
</code></pre>

<p>Unfortunately, there is a little to no documentation on how to correctly combine keras callbacks and <code>tf.summary.image</code>. How could I overcome this issue?</p>

<p><strong>upd:</strong> tf_nightly-2.2.0.dev20200427 has the same behaviour.</p>
",1
61480051,How to make Google Cloud AI Platform detect `tf.summary.scalar` calls during training?,"<p>(Note: I have also asked this question <a href=""https://github.com/GoogleCloudPlatform/cloudml-hypertune/issues/4"" rel=""noreferrer"">here</a>)</p>

<h3>Problem</h3>

<p>I have been trying to get Google Cloud's AI platform to display the accuracy of a Keras model, trained on the AI platform. I configured the hyperparameter tuning with <code>hptuning_config.yaml</code> and it works. However I can't get AI platform to pick up <code>tf.summary.scalar</code> calls during training.</p>

<h3>Documentation</h3>

<p>I have been following the following documentation pages:</p>

<p><strong>1. <a href=""https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview"" rel=""noreferrer"">Overview of hyperparameter tuning</a></strong></p>

<p><strong>2. <a href=""https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning"" rel=""noreferrer"">Using hyperparameter tuning</a></strong></p>

<p>According to <strong>[1]</strong>:</p>

<blockquote>
  <p><strong>How AI Platform Training gets your metric</strong>
  You may notice that there are no instructions in this documentation for passing your hyperparameter metric to the AI Platform Training training service. That's because the service monitors TensorFlow summary events generated by your training application and retrieves the metric.""</p>
</blockquote>

<p>And according to <strong>[2]</strong>, one way of generating such a Tensorflow summary event is by creating a callback class as so:</p>

<pre><code>class MyMetricCallback(tf.keras.callbacks.Callback):

    def on_epoch_end(self, epoch, logs=None):
        tf.summary.scalar('metric1', logs['RootMeanSquaredError'], epoch)
</code></pre>

<h3>My code</h3>

<p>So in my code I included:</p>

<pre><code># hptuning_config.yaml

trainingInput:
  hyperparameters:
    goal: MAXIMIZE
    maxTrials: 4
    maxParallelTrials: 2
    hyperparameterMetricTag: val_accuracy
    params:
    - parameterName: learning_rate
      type: DOUBLE
      minValue: 0.001
      maxValue: 0.01
      scaleType: UNIT_LOG_SCALE
</code></pre>

<pre><code># model.py

class MetricCallback(tf.keras.callbacks.Callback):

    def on_epoch_end(self, epoch, logs):
        tf.summary.scalar('val_accuracy', logs['val_accuracy'], epoch)
</code></pre>

<p>I even tried</p>

<pre><code># model.py

class MetricCallback(tf.keras.callbacks.Callback):
    def __init__(self, logdir):
        self.writer = tf.summary.create_file_writer(logdir)

    def on_epoch_end(self, epoch, logs):
        with writer.as_default():
            tf.summary.scalar('val_accuracy', logs['val_accuracy'], epoch)
</code></pre>

<p>Which successfully saved the 'val_accuracy' metric to Google storage (I can also see this with TensorBoard). But this does not get picked up by the AI platform, despite the claim made in <strong>[1]</strong>.</p>

<h3>Partial solution:</h3>

<p>Using the <a href=""https://github.com/GoogleCloudPlatform/cloudml-hypertune"" rel=""noreferrer"">Cloud ML Hypertune</a> package, I created the following class:</p>

<pre><code># model.py

class MetricCallback(tf.keras.callbacks.Callback):
    def __init__(self):
        self.hpt = hypertune.HyperTune()

    def on_epoch_end(self, epoch, logs):
        self.hpt.report_hyperparameter_tuning_metric(
            hyperparameter_metric_tag='val_accuracy',
            metric_value=logs['val_accuracy'],
            global_step=epoch
        )
</code></pre>

<p>which works! But I don't see how, since it all it seems to do is write to a file on the AI platform <strong>worker</strong> at <code>/tmp/hypertune/*</code>. There is nothing in the Google Cloud documentation that explains how this is getting picked up by the AI platform...</p>

<p>Am I missing something in order to get <code>tf.summary.scalar</code> events to be displayed?</p>
",0
61488761,Can't understand the loss functions for the GAN model used in the tensorflow documentation,"<p>I can't understand the loss function in GAN model in tensorflow documentation. Why use <code>tf.ones_like()</code> for <strong>real_loss</strong> and <code>tf.zeros_like()</code> for <strong>fake outputs</strong>??</p>

<pre><code>def discriminator_loss(real_output,fake_output):
  real_loss = cross_entropy(tf.ones_like(real_output),real_output)
  fake_loss = cross_entropy(tf.zeros_like(fake_output),fake_output)
  total_loss = real_loss + fake_loss
  return total_loss
</code></pre>
",1
61522019,Is it still necessary to implement `compute_output_shape()` when defining a custom tf.keras Layer?,"<p>I have implemented a custom <code>Layer</code> in <code>tf.keras</code>, using TensorFlow 2.1.0.</p>

<p>In the past, when using the stand-alone Keras, it was important to define the <code>compute_output_shape(input_shape)</code> method in any custom layer so that the computational graph could be created. </p>

<p>Now, having moved to TF2, I found out that even if I remove that method from my custom implementation the layer still works as expected. Apparently, it works both in eager and graph mode.
This is an example of what I mean: </p>

<pre class=""lang-py prettyprint-override""><code>from tensorflow.keras.layers import Layer, Input
from tensorflow.keras.models import Sequential
import numpy as np


class MyLayer(Layer):
    def call(self, inputs):
        return inputs[:, :-1]  # Do something that changes the shape


m = Sequential([MyLayer(), MyLayer()])
m.predict(np.ones((10, 3)))  # This would not have worked in the past
</code></pre>

<p>Is it safe to say that <code>compute_output_shape()</code> is not necessary anymore? Am I missing something important?</p>

<p>In the documentation there's no explicit mention of removing <code>compute_output_shape()</code>, although none of the examples implements it explicitly. </p>

<p>Thanks</p>
",1
61522378,tf.keras: Handling iteration over variable-length sequences in custom Model without custom training method,"<p>I'm trying to implement a simple recurrent neural network architecture with attention for a sequence to sequence task using <code>tf.keras</code> in Tensorflow 2.1.
I've followed the <a href=""https://www.tensorflow.org/tutorials/text/nmt_with_attention"" rel=""nofollow noreferrer"">Tensorflow tutorial</a> on this mostly and so far, it is working.</p>

<p>However, the guide uses a self-written training function where I would like to use the various <code>tf.keras.Model</code> functions like <code>fit</code>, so I tried to adjust the code to allow me to use those.
The problematic point is the following:
In the decoder part, due to the attention architecture, one has to manually iterate over timesteps, do the attention computation and feed the result to the next step.
My decoder model looks something like this:</p>

<pre class=""lang-py prettyprint-override""><code>class MyDecoderModel(tf.keras.Model):
    def __init__(self, **kwargs):
        ...

    def call(self, x):
        seq_in, state_in = x
        batch_size, seq_len = seq_in.shape
        for step in range(seq_len):
            do_stuff()
</code></pre>

<p>Most importantly, the sequence length of the input must be defined and not be <code>None</code>. But I want to work with variable sequence lengths.
As far as I understand, this makes the use of the Keras functional API, that I would normally use, impossible with this Model, since writing something like</p>

<pre class=""lang-py prettyprint-override""><code>x = tf.keras.Input(shape=(None,))
x = MyDecoderModel()(x)
</code></pre>

<p>would feed the <code>None</code> into the call.
So I decided to work only with Model subclassing, writing a main model like</p>

<pre class=""lang-py prettyprint-override""><code>class MyMainModel(tf.keras.Model):
    ...
    def call(self, x):
        enc_in, dec_in = x
        state = self.my_encoder(enc_in)
        dec_out = self.my_decoder((dec_in, state))
        return dec_out
</code></pre>

<p>This seems to work in principle, calling this main model with input data succesfully produces results.
However, when using the <code>fit()</code> method on this model, things happen that I don't understand:
First, the model is called with unknown batch size, but specified sequence length.
Subsequently, it is only called with specified batch size, but unknown sequence length.
Hence <code>None</code> is passed and the iteration fails.</p>

<p>Now why does this happen? During training, the sequence length of the batch should definitely be known, why is it not passed to my model?
Is there any chance of solving this without a custom training method?
I'd be very happy if someone could explain what's going on there...</p>
",0
61526556,Serializing a tensor and writing to tfrecord from within a graph,"<p>I would like to write tensorflow example records to a TFRecordWriter from inside an AutoGraph generated graph.</p>

<p>The documentation for tensorflow 2.0 states the following:</p>

<blockquote>
  <p>The simplest way to handle non-scalar features is to use tf.serialize_tensor to convert tensors to binary-strings. Strings are scalars in tensorflow.</p>
</blockquote>

<p>However, <code>tf.io.serialize_tensor</code> returns a tensor of byte-string. Creating an Example proto requires a bytes list, not a tensor. </p>

<p>How do I write a tf.train.Example to a tf record from inside a graph?</p>

<p>Code to reproduce:</p>

<pre><code>%tensorflow_version 2.x
import tensorflow as tf

@tf.function
def example_write():
  writer = tf.io.TFRecordWriter(""test.tfr"")
  x = tf.constant([[0, 1], [2, 3]])
  x = tf.io.serialize_tensor(x)
  feature = {
      ""data"": tf.train.Features(
        bytes_list=tf.train.BytesList(value=[x]))
  }
  ex = tf.train.Example(features=tf.train.Features(
      feature=feature))
  writer.write(ex.SerializeToString())

example_write()
</code></pre>

<p>and the error</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-6-df8a97eb17c9&gt; in &lt;module&gt;()
     12   writer.write(ex.SerializeToString())
     13 
---&gt; 14 example_write()

8 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966           except Exception as e:  # pylint:disable=broad-except
    967             if hasattr(e, ""ag_error_metadata""):
--&gt; 968               raise e.ag_error_metadata.to_exception(e)
    969             else:
    970               raise

TypeError: in user code:

    &lt;ipython-input-6-df8a97eb17c9&gt;:6 example_write  *
        feature = {

    TypeError: &lt;tf.Tensor 'SerializeTensor:0' shape=() dtype=string&gt; has type Tensor, but expected one of: bytes
</code></pre>
",1
61581952,Object Detection Performance Issues Using Tensorflow 2.1.0 and Tensorflow Hub,"<p>Running through some of the object detection documentation and examples found online utilizing the OpenImagesV4 Data Model I am experiencing less than favorable performance on the processing speed of the detection events. The code I am using is as follows and is a stripped down version of the detection so I can understand the performance metrics. The Camera Stream Processes Fine without using any detection, Once detection is implemented it slows the feed down by roughly 20 seconds or so. I have seen this done in TF1.14 using the old object detection with tf.graph() functions with near zero-delay on a different model so my question is really where can more performance be made for the feed stream or where are my hang-ups at with this stripped down version. This is using the gpu for processing but only seeing spikes at ~6%. My original thought was to introduce threading on the Detection process but I am not sure how to go about doing that or if it is necessary</p>

<p><strong>Software</strong></p>

<ul>
<li>Tensorflow version (2.1.0)</li>
<li>Cuda 10.1</li>
<li>cudnn 7</li>
</ul>

<p><strong>Hardware</strong></p>

<ul>
<li>CPU: Intel i7-4820K</li>
<li>GPU: Geforce GTX 1660 (6GB)</li>
<li>Memory: 16GB</li>
</ul>

<pre><code>import cv2
import time
import gc
from datetime import datetime
import tensorflow as tf
import tensorflow_hub as hub

low_res_vid_source = ""http://192.168.1.85:14238/videostream.cgi?loginuse=####&amp;loginpas=######""
hi_res_vid_source = ""rtsp://####:####@192.168.1.85:10554/tcp/av0_0""
cap = cv2.VideoCapture(low_res_vid_source)

#Low Res (640): Hi Res (1280)
width = cap.get(3)

#Low Res (480): Hi Res (720)
height = cap.get(4)

print(""Dimensions: Width: "", width, ""Height: "", height)
#Remote Loading
#module_handle = ""https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1""

#Local Loading
module_handle = ""C://Users//Isaiah//tf2//Tutorial Sets//Expert//HubCache//ddd04e3eaa283f2b3ae566e084863074d12b403a""
detector = hub.load(module_handle).signatures['default']

def LoadStream():
   ret, frame = cap.read()
   image_resize_val = (1280, 720)
   frame = cv2.resize(frame, image_resize_val)

   ## Average Calculation Time of Conversion Of Pixel Normalization = 0.018950 Seconds
   frame = frame / 255

   ## Average Calculation Time of Conversion Of Image Data Type      = 0.001999 Seconds
   converted_img = tf.image.convert_image_dtype(frame, tf.float32)[tf.newaxis, ...]

   ## Average Calculation Time of Loading Results From Detector      = 1.7 Seconds
   time_start = time.time()
   results = detector(converted_img)
   time_end = time.time()
   print(""Detection Took: "", time_end - time_start)
   cv2.imshow('camera feed', frame)


while True:
   LoadStream()

   if cv2.waitKey(1) &amp; 0xFF == ord('q'):
      cv2.destroyAllWindows()
      break
</code></pre>

<p>Output From the Conda Environment for this code is as follows and nothing seems to be really sticking out</p>

<pre><code>(tf2-gpu) C:\Users\Isaiah\tf2\Tutorial Sets\Expert\Camera_Feed&gt;python Camera_Feed_Raw.py
2020-05-03 16:52:36.567941: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
Dimensions: Width:  640.0 Height:  360.0
2020-05-03 16:54:52.037826: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-05-03 16:54:52.253465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
pciBusID: 0000:03:00.0 name: GeForce GTX 1660 computeCapability: 7.5
coreClock: 1.815GHz coreCount: 22 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.86GiB/s
2020-05-03 16:54:52.260714: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-05-03 16:54:52.272442: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-05-03 16:54:52.282134: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-05-03 16:54:52.287729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-05-03 16:54:52.300130: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-05-03 16:54:52.307647: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-05-03 16:54:52.326362: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-05-03 16:54:52.331006: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-05-03 16:54:52.334046: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX
2020-05-03 16:54:52.626783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties:
pciBusID: 0000:03:00.0 name: GeForce GTX 1660 computeCapability: 7.5
coreClock: 1.815GHz coreCount: 22 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.86GiB/s
2020-05-03 16:54:52.633826: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-05-03 16:54:52.638740: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-05-03 16:54:52.642777: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-05-03 16:54:52.647763: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-05-03 16:54:52.651710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-05-03 16:54:52.656789: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-05-03 16:54:52.660852: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-05-03 16:54:52.667018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-05-03 16:54:53.626966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-03 16:54:53.630823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0
2020-05-03 16:54:53.633295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N
2020-05-03 16:54:53.638096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4630 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1660, pci bus id: 0000:03:00.0, compute capability: 7.5)
2020-05-03 16:57:25.429470: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-05-03 16:57:26.697611: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-05-03 16:57:29.627538: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation. This message will be only logged once.
Detection Took:  58.80091857910156
Detection Took:  1.747373104095459
Detection Took:  1.7253808975219727
Detection Took:  1.736377477645874
Detection Took:  1.7273805141448975
Detection Took:  1.7343783378601074
Detection Took:  1.742375373840332
Detection Took:  1.7413759231567383
Detection Took:  1.7293803691864014
Detection Took:  1.7283804416656494
Detection Took:  1.7403762340545654
Detection Took:  1.7323787212371826
Detection Took:  1.7373778820037842
Detection Took:  1.7323782444000244
</code></pre>
",0
61645365,How are function metrics aggregated over batches in tensorflow model validation?,"<p>In tensorflow <code>tf.keras.Model.compile</code>, you can pass a <code>lambda y_true, y_pred: val</code> function as a metric (though, it seems not documented), but I asked my self : ""How does it aggregate it over the batches"" ?</p>

<p>I searched the documentation, but I've found nowhere how it is done ?</p>

<p>By the way, I don't even know if it is an undefined behavior to do so and one should instead subclass the Metric class ? ( or at least provide the required methods).</p>

<p>Also, is it pertinent to pass a loss as a metric (and in this case, same question : how is it aggregated over the batches ? )</p>
",1
61656822,"Tensorflow 2.0 Hugging Face Transformers, TFBertForSequenceClassification, Unexpected Output Dimensions in Inference","<p><strong>Summary:</strong></p>

<p>I want to fine-tune BERT for sentence classification on a custom dataset. I have followed some examples I have found, like <a href=""https://stackoverflow.com/questions/59978959/how-to-use-hugging-face-transformers-library-in-tensorflow-for-text-classificati"">this one</a>, which was very helpful. I have also looked at <a href=""https://gist.github.com/papapabi/124c6ac406e6bbd1f28df732e953ac6d"" rel=""nofollow noreferrer"">this gist</a>. </p>

<p><strong>The problem I have is that when running inference for some samples, the output has other dimensions than I would expect.</strong></p>

<p>When I run inference for 23 samples, I get a tuple with a numpy array of dimensions (1472, 42), where 42 is the number of classes. I would expect dimensions (23, 42).</p>

<p><strong>Code and Other Details:</strong></p>

<p>I run the inference on the trained model using Keras like this:</p>

<pre><code>preds = model.predict(features)
</code></pre>

<p>Where <em>features</em> is tokenized and converted to a Dataset:</p>

<pre><code>for sample, ground_truth in tests:
    test_examples.append(InputExample(text=sample, category_index=ground_truth))

features = convert_examples_to_tf_dataset(test_examples, tokenizer)
</code></pre>

<p>Where <code>sample</code> can be e.g. <code>""A test sentence I want classified""</code> and <code>ground_truth</code> can be e.g. <code>12</code> which is the encoded label. Because I do inference, what I supply as ground truth shouldn't matter of course.</p>

<p>The <code>convert_examples_to_tf_dataset</code>-function looks as follows (which I found in <a href=""https://gist.github.com/papapabi/124c6ac406e6bbd1f28df732e953ac6d"" rel=""nofollow noreferrer"">this gist</a>):</p>

<pre><code>def convert_examples_to_tf_dataset(
    examples: List[Tuple[str, int]],
    tokenizer,
    max_length=64,
):
    """"""
    Loads data into a tf.data.Dataset for finetuning a given model.

    Args:
        examples: List of tuples representing the examples to be fed
        tokenizer: Instance of a tokenizer that will tokenize the examples
        max_length: Maximum string length

    Returns:
        a ``tf.data.Dataset`` containing the condensed features of the provided sentences
    """"""
    features = [] # -&gt; will hold InputFeatures to be converted later

    for e in examples:
        # Documentation is really strong for this method, so please take a look at it
        input_dict = tokenizer.encode_plus(
            e.text,
            add_special_tokens=True,
            max_length=max_length, # truncates if len(s) &gt; max_length
            return_token_type_ids=True,
            return_attention_mask=True,
            pad_to_max_length=True, # pads to the right by default
        )

        # input ids = token indices in the tokenizer's internal dict
        # token_type_ids = binary mask identifying different sequences in the model
        # attention_mask = binary mask indicating the positions of padded tokens so the model does not attend to them

        input_ids, token_type_ids, attention_mask = (input_dict[""input_ids""],
            input_dict[""token_type_ids""], input_dict['attention_mask'])

        features.append(
            InputFeatures(
                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.category_index
            )
        )

    def gen():
        for f in features:
            yield (
                {
                    ""input_ids"": f.input_ids,
                    ""attention_mask"": f.attention_mask,
                    ""token_type_ids"": f.token_type_ids,
                },
                f.label,
            )

    return tf.data.Dataset.from_generator(
        gen,
        ({""input_ids"": tf.int32, ""attention_mask"": tf.int32, ""token_type_ids"": tf.int32}, tf.int64),
        (
            {
                ""input_ids"": tf.TensorShape([None]),
                ""attention_mask"": tf.TensorShape([None]),
                ""token_type_ids"": tf.TensorShape([None]),
            },
            tf.TensorShape([]),
        ),
    )

with tf.device('/cpu:0'):
    train_data = convert_examples_to_tf_dataset(train_examples, tokenizer)
    train_data = train_data.shuffle(buffer_size=len(train_examples), reshuffle_each_iteration=True) \
                           .batch(BATCH_SIZE) \
                           .repeat(-1)

    val_data = convert_examples_to_tf_dataset(val_examples, tokenizer)
    val_data = val_data.shuffle(buffer_size=len(val_examples), reshuffle_each_iteration=True) \
                           .batch(BATCH_SIZE) \
                           .repeat(-1)
</code></pre>

<p>It works as I would expect and running <code>print(list(features.as_numpy_iterator())[1])</code> yields the following:</p>

<pre><code>({'input_ids': array([  101, 11639, 19962, 23288, 13264, 35372, 10410,   102,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0,     0,     0,     0,     0,     0,     0,     0,     0,
           0], dtype=int32), 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
      dtype=int32), 'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
      dtype=int32)}, 6705)
</code></pre>

<p>So far everything looks like I would expect. And it seems like the tokenizer is working as it should; 3 arrays of length 64 (which corresponds to my set max-length), and a label as an integer.</p>

<p>The model has been trained as follows:</p>

<pre><code>config = BertConfig.from_pretrained(
    'bert-base-multilingual-cased',
    num_labels=len(label_encoder.classes_),
    output_hidden_states=False,
    output_attentions=False
)
model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', config=config)

# train_data is then a tf.data.Dataset we can pass to model.fit()
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-05, epsilon=1e-08)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')
model.compile(optimizer=optimizer,
              loss=loss,
              metrics=[metric])

model.summary()

history = model.fit(train_data,
                    epochs=EPOCHS,
                    steps_per_epoch=train_steps,
                    validation_data=val_data,
                    validation_steps=val_steps,
                    shuffle=True,
                    )
</code></pre>

<p><strong>Results</strong></p>

<p>The problem now is that when running a prediction <code>preds = model.predict(features)</code>, the output dimensions does not correspond to what the <a href=""https://huggingface.co/transformers/model_doc/bert.html#tfbertforsequenceclassification"" rel=""nofollow noreferrer"">documentation</a> says: <code>logits (Numpy array or tf.Tensor of shape (batch_size, config.num_labels)):</code>. <strong>What I get is a tuple containing a numpy array with dimensions: (1472,42).</strong></p>

<p>42 makes sense as this is my number of classes. I sent 23 samples for the test, and 23 x 64 = 1472. 64 is my max sentence length, so it kind of sounds familiar. Is this output incorrect? How can I convert this output to an actual class prediction for each input sample? I get 1472 predictions when I would expect 23.</p>

<p>Please let me know if I can provide more details that could help solve this.</p>
",0
61661160,Comparison of tf.keras.preprocessing.text.Tokenizer() and tfds.features.text.Tokenizer(),"<p>As some background, I've been looking more and more into NLP and text-processing lately. I am much more familiar with Computer Vision. I understand the idea of Tokenization completely. </p>

<p>My confusion stems from the various implementations of the <strong><code>Tokenizer</code></strong> class that can be found within the <strong><code>Tensorflow</code></strong> ecosystem.</p>

<p>There is a <strong><code>Tokenizer</code></strong> class found within <strong><code>Tensorflow Datasets</code></strong> (<strong><code>tfds</code></strong>) as well as one found within <strong><code>Tensorflow</code></strong> proper: <strong><code>tfds.features.text.Tokenizer()</code></strong> &amp; <strong><code>tf.keras.preprocessing.text.Tokenizer()</code></strong> respectively.</p>

<p>I looked into the source code (linked below) but was unable to glean any useful insights</p>

<ul>
<li><a href=""https://github.com/tensorflow/datasets/blob/1705802f695a6d985d21469b3ce9d836629c7e1d/tensorflow_datasets/core/features/text/text_encoder.py#L357"" rel=""nofollow noreferrer""><strong><code>tfds</code></strong> implementation</a></li>
<li><a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/preprocessing/text.py"" rel=""nofollow noreferrer""><strong><code>tf</code></strong> implementation... line <strong><code>18</code></strong> links to the next link</a></li>
<li><a href=""https://github.com/tensorflow/tensorflow/blob/75d1fdd15ded19724c975dfd02fe08b47f5fef11/tensorflow/python/summary/summary.py#L234"" rel=""nofollow noreferrer"">text data summarization function</a></li>
</ul>

<hr>

<p><strong>The tl;dr question here is: Which library do you use for what? And what are the benefits of one library over the other?</strong></p>

<hr>

<p><strong>NOTE</strong> </p>

<p>I was following along with the <strong><a href=""https://www.coursera.org/learn/natural-language-processing-tensorflow/home/welcome"" rel=""nofollow noreferrer"">Tensorflow In Practice Specialization</a></strong> as well as this <strong><a href=""https://www.tensorflow.org/tutorials/load_data/text"" rel=""nofollow noreferrer"">tutorial</a></strong>. The TF in Practice Specialization uses the <strong><code>tf.Keras.preprocessing.text.Tokenizer()</code></strong> implementation and the text loading tutorial uses <strong><code>tfds.features.text.Tokenizer()</code></strong></p>
",0
61694639,Object detction api of Tensorflow does not work,"<p>I have been trying to use the object detection api of tensorflow. After alot of trouble I have installed all the necessary modules for the same. Then when I run the ""<strong>object_detection_tutorial</strong>"" from the <strong>models/research/object_detection</strong> I come up with the error as in the detection section as follows:</p>

<p><strong>Detection</strong></p>

<p><em>Load an object detection model:</em></p>

<pre><code>model_name = 'ssd_mobilenet_v1_coco_2017_11_17'
detection_model = load_model(model_name)
</code></pre>

<p><em>tensorflow:Saver not created because there are no variables in the graph to restore</em></p>

<pre><code>print(detection_model.inputs)
</code></pre>

<p><em>tf.Tensor 'image_tensor:0' shape=(?, ?, ?, 3) dtype=uint8</em></p>

<p>And again I get the error in below line as:</p>

<pre><code>for image_path in TEST_IMAGE_PATHS:
  show_inference(detection_model, image_path)
</code></pre>

<p><strong>error:</strong></p>

<p>* in run_inference_for_single_image(model, image)</p>

<pre><code>     12   # Convert to numpy arrays, and take index [0] to remove the batch dimension.
     13   # We're only interested in the first num_detections.
---&gt; 14   num_detections = int(output_dict.pop('num_detections'))
     15   output_dict = {key:value[0, :num_detections].numpy() 
     16                  for key,value in output_dict.items()}
</code></pre>

<p>TypeError: int() argument must be a string, a bytes-like object or a number, not 'Tensor'*</p>

<p>Please any help why am I getting this error. I am new to tensorflow just want to run this tutorial successfully. I have installed ""Tensorflow 2.2.0"" and all the modules. I also did the same with Tensorflow lower version but same issue.</p>
",0
61717694,Embed trainable bijector into Keras model,"<p>I am trying to implement normalizing flows embedded in a Keras model. In all examples I can find, such as the documentation of <a href=""https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/MaskedAutoregressiveFlow"" rel=""nofollow noreferrer"">MAF</a>, the bijectors which constitute the normalizing flows are embedded into a <code>TransformedDistribution</code> and exposed directly for training etc.</p>

<p>I am trying to embed this TransformedDistribution in a keras Model to match the architecture of other models I have which are inheriting from keras Model. </p>

<p>Unfortunately all my attempts (see code) so far fail at transferring the trainable variables inside the transformed distribution to the keras Model.</p>

<p>I have tried to make the bijector inherit from <code>tf.keras.layers.Layer</code>, which did not change anything.</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import tensorflow_probability as tfp

tfd = tfp.distributions
tfb = tfp.bijectors


class Flow(tfb.Bijector, tf.Module):
    """"""
    tf.Module to register trainable_variables
    """"""

    def __init__(self, d, init_sigma=0.1, **kwargs):
        super(Flow, self).__init__(
            dtype=tf.float32,
            forward_min_event_ndims=0,
            inverse_min_event_ndims=0,
            **kwargs
        )
        # Shape of the flow goes from Rd to Rd
        self.d = d
        # Weights/Variables initializer
        self.init_sigma = init_sigma
        w_init = tf.random_normal_initializer(stddev=self.init_sigma)
        # Variables
        self.u = tf.Variable(
            w_init(shape=[1, self.d], dtype=tf.float32),
            dtype=tf.float32,
            name='u',
            trainable=True,
        )

    def _forward(self, x):
        return x

    def _inverse(self, y):
        return y


class Flows(tf.keras.Model):

    def __init__(self, d=2, shape=(100, 2), n_flows=10, ):
        super(Flows, self).__init__()
        # Parameters
        self.d = d
        self.shape = shape
        self.n_flows = n_flows
        # Base distribution - MF = Multivariate normal diag
        base_distribution = tfd.MultivariateNormalDiag(
            loc=tf.zeros(shape=shape, dtype=tf.float32)
        )
        # Flows as chain of bijector
        flows = []
        for n in range(n_flows):
            flows.append(Flow(self.d, name=f""flow_{n + 1}""))
        bijector = tfb.Chain(list(reversed(flows)))
        self.flow = tfd.TransformedDistribution(
            distribution=base_distribution,
            bijector=bijector
        )

    def call(self, *inputs):
        return self.flow.bijector.forward(*inputs)

    def log_prob(self, *inputs):
        return self.flow.log_prob(*inputs)

    def sample(self, num):
        return self.flow.sample(num)


q = Flows()
# Call to instantiate variables
q(tf.zeros(q.shape))
# Prints no trainable params
print(q.summary())
# Prints expected trainable params
print(q.flow.trainable_variables)
</code></pre>

<p>Any idea if this is even possible? Thanks!</p>
",0
61720708,How do you save a Tensorflow dataset to a file?,"<p>There are at least two more questions like this on SO but not a single one has been answered.</p>

<p>I have a dataset of the form:</p>

<pre><code>&lt;TensorSliceDataset shapes: ((512,), (512,), (512,), ()), types: (tf.int32, tf.int32, tf.int32, tf.int32)&gt;
</code></pre>

<p>and another of the form:</p>

<pre><code>&lt;BatchDataset shapes: ((None, 512), (None, 512), (None, 512), (None,)), types: (tf.int32, tf.int32, tf.int32, tf.int32)&gt;
</code></pre>

<p>I have looked and looked but I can't find the code to save these datasets to files that can be loaded later. The closest I got was <a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/TFRecordWriter"" rel=""noreferrer"">this page in the TensorFlow docs</a>, which suggests serializing the tensors using <code>tf.io.serialize_tensor</code> and then writing them to a file using <code>tf.data.experimental.TFRecordWriter</code>.</p>

<p>However, when I tried this using the code:</p>

<pre><code>dataset.map(tf.io.serialize_tensor)
writer = tf.data.experimental.TFRecordWriter('mydata.tfrecord')
writer.write(dataset)
</code></pre>

<p>I get an error on the first line:</p>

<blockquote>
  <p>TypeError: serialize_tensor() takes from 1 to 2 positional arguments but 4 were given</p>
</blockquote>

<p>How can I modify the above (or do something else) to accomplish my goal?</p>
",1
61730614,Metric system behavior in TensorFlow and Keras,"<p>I am trying to understand how the metrics system works in TensorFlow and Keras.</p>

<p>I have a sequential model containing dense layers, and the only metric is <code>""accuracy""</code> (as a string):</p>

<pre class=""lang-py prettyprint-override""><code>model.compile(
    # [...]
    metrics=[""accuracy""],
    loss=""binary_crossentropy"")
</code></pre>

<p>The average values associated with that accuracy were between 90% and 100%.</p>

<p>However, I tried to change my metrics after seing all the available classes on the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics"" rel=""nofollow noreferrer"">TensorFlow documentation</a> and I replaced <code>""accuracy""</code> with its Python-class equivalent, <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy"" rel=""nofollow noreferrer""><code>tf.keras.metrics.Accuracy</code></a>:</p>

<pre class=""lang-py prettyprint-override""><code>model.compile(
    # [...]
    metrics=[tf.keras.metrics.Accuracy()],
    loss=""binary_crossentropy"")
</code></pre>

<p>No modification was made to the dataset nor the model (apart from the metrics), and yet the values changed: The accuracy is now between 40% and 60%.</p>

<p>I then tried to pass an instance of <a href=""https://www.tensorflow.org/api_docs/python/tf/compat/v1/metrics/accuracy"" rel=""nofollow noreferrer""><code>tf.metrics.Accuracy</code></a>, but the results were the same as before with <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy"" rel=""nofollow noreferrer""><code>tf.keras.metrics.Accuracy</code></a>.</p>

<p>My question is:</p>

<p><strong>Aren't <code>""accuracy""</code> and <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy"" rel=""nofollow noreferrer""><code>tf.keras.metrics.Accuracy</code></a> supposed to represent the same metric?</strong></p>
",1
61743921,can we build object detection model using Tensorflow or it is only possible with the help f tf.keras,"<p>Is there any way to build object detection model using Tensorflow without any help of tf.keras module?</p>

<p>From Tensorflow documentation I'm  not able to find any example which helps to create model without Keras.</p>
",1
61767803,Tensorflow 1.x to Tensorflow 2.1.0,"<p>I am trying to update code written in Tensorflow 1.x to code in Tensorflow 2.1.0. I have been converting codes using Tensorflow 2.1.0 documentation, and I had no problems until this code.</p>

<pre><code>loss = tf.losses.softmax_cross_entropy(one_hot_labels, logits)
</code></pre>

<p>Above code is Tensorflow 1.x version, and I think, according to Tensorflow 2.1.0 documentation, the properly updated code is </p>

<pre><code>loss = tf.nn.softmax_cross_entropy_with_logits(one_hot_labels, logits)
</code></pre>

<p>Then, when I run</p>

<pre><code>return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)
</code></pre>

<p>I get the following error.</p>

<pre><code>Loss must be scalar, given: Tensor(""softmax_cross_entropy_with_logits/Reshape_2:0"", shape=(512,), dtype=float32)**
</code></pre>

<p>So, I am guessing in Tensorflow 1.x version, the loss was passed as 'tensor' to tf.estimator.EstimatorSpec, but in Tensorflow 2.1.0, the loss has to be passed as <code>scalar</code> to <code>tf.estimator.EstimatorSpec</code>? Loss (the way it is defined here) in both Tensorflow 1.x and 2.1.0 is tensor if I remember it correctly.</p>

<p>So, does anyone know how to convert tensor to scalar (which I don't think will be sufficient nor efficient in building the CNN model) or better yet, how to solve this dilemma?</p>

<p>Or did I convert the original code the wrong way?</p>

<p>I would very much appreciate if compat.v1. is not used unless absolutely necessary (i.e. no other way to use the code in Tensorflow 2.1.0 than compat.v1.)</p>
",1
61786707,What effects should tensorflow.compat.v1.disable_v2_behavior() have on training using the Keras API?,"<p>I have a CNN that trains, on a few hundred thousand examples, to a validation accuracy of ~95% after one epoch. It's straight forward code, using Keras to define a network using the Sequential API. Originally I prepared and used this model on TF 1.3. When I port it over to TF 2.1, replacing the keras calls with tensorflow.keras, it gets to ~60% quickly and gets stuck there (seemingly for many epochs), and the training loss always seems to converge to the same value.</p>

<p>If I add in <code>tf.disable_v2_behavior()</code> at the top of the script, it trains similarly to before.</p>

<p>The documentation states simply that ""It switches all global behaviors that are different between TensorFlow 1.x and 2.x to behave as intended for 1.x"". Hidden behind the Keras API, I haven't found a clear answer to what this really means in practice. Why should I expect a VGG-like CNN, defined using Keras and trained with <code>model.fit()</code>, to work well without v2 behaviour but to fail so consistently with?</p>

<p>Edit: <code>disable_eager_execution()</code> produces the same result, with improved performance.</p>
",1
61799546,How to custom losses by subclass tf.keras.losses.Loss class in Tensorflow2.x,"<p>When I read the <a href=""https://www.tensorflow.org/guide/keras/train_and_evaluate#custom_losses"" rel=""noreferrer"">guides</a> in the websites of Tensorflow , I find two ways to custom losses. The first one is to define a loss function,just like:</p>

<pre class=""lang-py prettyprint-override""><code>def basic_loss_function(y_true, y_pred):
    return tf.math.reduce_mean(tf.abs(y_true - y_pred))
</code></pre>

<p>And for the sake of simplicity, we assume the batch size is also 1, so the shape of <code>y_true</code> and <code>y_pred</code> are both (1, c), where c is the number of classes. So in this method, we give two vectors <code>y_true</code> and <code>y_pred</code>, and return a value(scala).</p>

<p>Then, the second method is to subclass <code>tf.keras.losses.Loss</code> class, and the code in guide is: </p>

<pre class=""lang-py prettyprint-override""><code>class WeightedBinaryCrossEntropy(keras.losses.Loss):
    """"""
    Args:
      pos_weight: Scalar to affect the positive labels of the loss function.
      weight: Scalar to affect the entirety of the loss function.
      from_logits: Whether to compute loss from logits or the probability.
      reduction: Type of tf.keras.losses.Reduction to apply to loss.
      name: Name of the loss function.
    """"""
    def __init__(self, pos_weight, weight, from_logits=False,
                 reduction=keras.losses.Reduction.AUTO,
                 name='weighted_binary_crossentropy'):
        super().__init__(reduction=reduction, name=name)
        self.pos_weight = pos_weight
        self.weight = weight
        self.from_logits = from_logits

    def call(self, y_true, y_pred):
        ce = tf.losses.binary_crossentropy(
            y_true, y_pred, from_logits=self.from_logits)[:,None]
        ce = self.weight * (ce*(1-y_true) + self.pos_weight*ce*(y_true))
        return ce
</code></pre>

<p>In the call method, as usual, we give two vectors <code>y_true</code> and <code>y_pred</code>, but I notice that it return <code>ce</code>, which is a VECTOR with shape (1, c) !!!</p>

<p>So is there any problem in the above toy example ? Or Tensorflow2.x has some magic behind that ?</p>
",0
61810094,Abysmal tf.GradientTape performance compared to tf.gradients() for computing jacobians,"<p><strong>SOLUTION BELOW:</strong></p>

<p><strong>Scenario:</strong></p>

<p>I am trying to compute the jacobian of a user defined function many, many times in a loop. I am able to do this with TF 2's GradientTape as well as the older session based tf.gradients() method. The problem is that GradientTape is terribly slow (100x slower) than tf.gradients(). It has features i'd like to use (bath_jacobian, hessian support, etc), but if it's 100x slower then i can't use it.</p>

<p><strong>The Question:</strong></p>

<p>It's not clear to me if i'm simply misusing GradientTape, or if it will always be slower because it has to re-differentiate the provided function every time its called (my suspicion). I'm asking for tips to fix my use of GradientTape or a confirmation that it will always be fundamentally slower than tf.gradients by orders of magnitude.</p>

<p><strong>Related Questions:</strong></p>

<ul>
<li><a href=""https://stackoverflow.com/questions/60047705/repeated-use-of-gradienttape-for-multiple-jacobian-calculations"">Repeated use of GradientTape for multiple Jacobian calculations</a> - same scenario, unanswered</li>
<li><a href=""https://stackoverflow.com/questions/58854129/does-gradienttape-need-to-re-differentiate-each-evaluation-of-a-derivative"">Does `GradientTape` need to re-differentiate each evaluation of a derivative?</a> - same scenario, unanswered</li>
<li><a href=""https://stackoverflow.com/questions/58612362/using-one-gradienttape-with-global-context"">using one GradientTape with global context</a> - loosely related, having trouble applyng that solution to my scenario</li>
</ul>

<p><strong>Fully contained minimum example to compare GradientTape and tf.gradients():</strong></p>

<pre><code>import tensorflow as tf
from tensorflow.python.framework.ops import disable_eager_execution
import numpy as np
# from tensorflow.python.ops.parallel_for.gradients import jacobian, batch_jacobian
import timeit


class FunctionCaller(object):
    def __init__(self, func, nX, dtype=tf.float64, useSessions=True):

        if useSessions:
            disable_eager_execution()

        self.func = func
        self.nX = nX
        self.useSessions = useSessions
        self.dtype = dtype
        self.sess = tf.compat.v1.Session() if useSessions else None

        if not useSessions:
            return

        #
        # we are in session mode, so build the graph and take the batch-jacobian of the function's outputs
        #
        xTensor = tf.compat.v1.placeholder(dtype, shape=[None, nX])

        # add function to graph and guarantee its output shape
        func_tensor = tf.reshape(func(xTensor), [-1, nX])

        # take the gradient for each output, one at a time, and stack the results back together
        each_output = tf.unstack(func_tensor, nX, axis=1)

        jac_x = tf.stack([tf.gradients(output, xTensor, unconnected_gradients='zero')[0]
                          for output in each_output], axis=1)

        # record these tensors so we can use them later with session.run()
        self.xTensor = xTensor
        self.func_tensor = func_tensor
        self.jac_func_tensor = jac_x

    def jac(self, x_i):
        if self.useSessions:
            return self.sess.run(self.jac_func_tensor, {self.xTensor: x_i})
        else:
            return self._useGradientTape(x_i)

    # THIS FUNCTION IS SUPER INEFFICIENT.
    def _useGradientTape(self, x_i):
        with tf.GradientTape(persistent=True) as g:
            xTensor = tf.Variable(x_i, dtype=self.dtype)  # is this my problem??? i recreate x every time?
            y = tf.reshape(self.func(xTensor), [-1, self.nX])
        jac_x_at_i = g.batch_jacobian(y, xTensor)
        # del g
        return jac_x_at_i.numpy()

    def __del__(self):
        if self.sess is not None:
            self.sess.close()


def main():
    @tf.function
    def Xdot(x_i):
        x_0, x_1, x_2 = tf.split(x_i, 3, axis=1)
        return tf.concat([x_2 * tf.sin(x_2), x_2 * tf.cos(x_2), x_2], axis=1)

    nT = 20
    nX = 3

    # create some trash data
    x_i = np.arange(nT*nX).reshape([-1, nX])

    nTrials = 100

    # try the eager version first
    caller_eager = FunctionCaller(Xdot, nX, useSessions=False)
    start_time = timeit.default_timer()
    for _ in range(nTrials):
        jac_eager = caller_eager.jac(x_i)
    elapsed = timeit.default_timer() - start_time
    print(""eager code took {} sec: {} sec/trial"".format(elapsed, elapsed/nTrials))

    # now try the sessions version
    caller_sessions = FunctionCaller(Xdot, nX, useSessions=True)
    start_time = timeit.default_timer()
    caller_sessions.jac(x_i)  # call it once to do its graph building stuff?
    for _ in range(nTrials):
        jac_session = caller_sessions.jac(x_i)
    elapsed = timeit.default_timer() - start_time
    print(""session code took {} sec: {} sec/trial"".format(elapsed, elapsed/nTrials))

    residual = np.max(np.abs(jac_eager - jac_session))
    print('residual between eager and session trials is {}'.format(residual))

if __name__ == ""__main__"":
    main()

</code></pre>

<h1><strong>EDIT - SOLUTION:</strong></h1>

<p>xdurch0 pointed out below that I should wrap _useGradientTape() in a @tf.function - something I was unsuccessful with before for other reasons. Once I did that, I had to move xTensor's definition outside the @tf.function wrapper by making it a member variable and using tf.assign(). </p>

<p>With all this done, I find that GradientTape (for this simple example) is now on the same order of magnitude as tf.gradints. When running enough trials (~1E5), it's twice as fast as tf.gradients. <em>awesome!</em></p>

<pre><code>import tensorflow as tf
from tensorflow.python.framework.ops import disable_eager_execution
import numpy as np
import timeit


class FunctionCaller(object):
    def __init__(self, func, nT, nX, dtype=tf.float64, useSessions=True):

        if useSessions:
            disable_eager_execution()

        self.func = func
        self.nX = nX
        self.useSessions = useSessions
        self.dtype = dtype
        self.sess = tf.compat.v1.Session() if useSessions else None

        if not useSessions:
            #  you should be able to create without an initial value, but tf is demanding one
            #  despite what the docs say. bug?
            #  tf.Variable(initial_value=None, shape=[None, nX], validate_shape=False, dtype=self.dtype)
            self.xTensor = tf.Variable([[0]*nX]*nT, dtype=self.dtype)  # x needs to be properly sized once
            return

        #
        # we are in session mode, so build the graph and take the batch-jacobian of the function's outputs
        #
        xTensor = tf.compat.v1.placeholder(dtype, shape=[None, nX])

        # add function to graph and guarantee its output shape
        func_tensor = tf.reshape(func(xTensor), [-1, nX])

        # take the gradient for each output, one at a time, and stack the results back together
        each_output = tf.unstack(func_tensor, nX, axis=1)

        jac_x = tf.stack([tf.gradients(output, xTensor, unconnected_gradients='zero')[0]
                          for output in each_output], axis=1)

        # record these tensors so we can use them later with session.run()
        self.xTensor = xTensor
        self.func_tensor = func_tensor
        self.jac_func_tensor = jac_x

    def jac(self, x_i):
        if self.useSessions:
            return self.sess.run(self.jac_func_tensor, {self.xTensor: x_i})
        else:
            return self._useGradientTape(x_i).numpy()

    @tf.function  # THIS IS CRUCIAL
    def _useGradientTape(self, x_i):
        with tf.GradientTape(persistent=True) as g:
            self.xTensor.assign(x_i)  # you need to create the variable once outside the graph
            y = tf.reshape(self.func(self.xTensor), [-1, self.nX])
        jac_x_at_i = g.batch_jacobian(y, self.xTensor)
        # del g
        return jac_x_at_i

    def __del__(self):
        if self.sess is not None:
            self.sess.close()


def main():
    @tf.function
    def Xdot(x_i):
        x_0, x_1, x_2 = tf.split(x_i, 3, axis=1)
        return tf.concat([x_2 * tf.sin(x_2), x_2 * tf.cos(x_2), x_2], axis=1)

    nT = 20
    nX = 3

    # create some trash data
    x_i = np.random.random([nT, nX])

    nTrials = 1000  # i find that nTrials&lt;=1E3, eager is slower, it's faster for &gt;=1E4, it's TWICE as fast for &gt;=1E5

    # try the eager version first
    caller_eager = FunctionCaller(Xdot, nT, nX, useSessions=False)
    start_time = timeit.default_timer()
    for _ in range(nTrials):
        jac_eager = caller_eager.jac(x_i)
    elapsed = timeit.default_timer() - start_time
    print(""eager code took {} sec: {} sec/trial"".format(elapsed, elapsed/nTrials))

    # now try the sessions version
    caller_sessions = FunctionCaller(Xdot, nT, nX, useSessions=True)
    start_time = timeit.default_timer()
    for _ in range(nTrials):
        jac_session = caller_sessions.jac(x_i)
    elapsed = timeit.default_timer() - start_time
    print(""session code took {} sec: {} sec/trial"".format(elapsed, elapsed/nTrials))

    residual = np.max(np.abs(jac_eager - jac_session))
    print('residual between eager and session trials is {}'.format(residual))

if __name__ == ""__main__"":
    main()

</code></pre>
",0
61884176,Understanding tf.name_scope,"<p>I am trying to understand tf.name_scope. The documentation mentions the following:</p>

<p>""This context manager pushes a name scope, which will make the name of all operations added within it have a prefix.</p>

<p>For example, to define a new Python op called my_op:</p>

<pre><code>def my_op(a, b, c, name=None):
  with tf.name_scope(""MyOp"") as scope:
    a = tf.convert_to_tensor(a, name=""a"")
    b = tf.convert_to_tensor(b, name=""b"")
    c = tf.convert_to_tensor(c, name=""c"")
    # Define some computation that uses `a`, `b`, and `c`.
    return foo_op(..., name=scope)
</code></pre>

<p>When executed, the Tensors a, b, c, will have names MyOp/a, MyOp/b, and MyOp/c.""</p>

<p>My understanding is that the with block does not introduce a new local scope in Python. Under normal situation, the tensor variable a will also refer to the local parameter a of function my_op.   How is the name prefixing with ""MyOp/"" implemented using Python context? In the source code link for tf.name_scope (<a href=""https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/framework/ops.py#L6423-L6442"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/framework/ops.py#L6423-L6442</a>) there is an invocation of </p>

<pre><code>ctx = context.context()
</code></pre>

<p>but I could not find the semantics of context.context(). Most context manager discussion talk about <strong>enter</strong> and <strong>exit</strong>, but no mention of variable renaming with some prefix. Is this some introspective mechanism in Python that allows the manipulation of Python variable scopes? Many thanks for any insights.</p>
",0
61885570,Reading a tfrecord: DecodeError: Error parsing message,"<p>I am using colab to run a <a href=""https://colab.research.google.com/github/tensorflow/ranking/blob/master/tensorflow_ranking/examples/handling_sparse_features.ipynb"" rel=""nofollow noreferrer"">tutorial</a> on tensorflow ranking. It uses wget to fetch the tfrecord:</p>

<pre><code>!wget -O ""/tmp/train.tfrecords"" ""http://ciir.cs.umass.edu/downloads/Antique/tf-ranking/ELWC/train.tfrecords""
</code></pre>

<p>I am using this code to try to look at the structure of the tfrecord:</p>

<pre><code>for example in tf.compat.v1.python_io.tf_record_iterator(""/tmp/train.tfrecords""):
    print(tf.train.Example.FromString(example))
    break
</code></pre>

<p>And I am getting:</p>

<pre><code>DecodeError: Error parsing message
</code></pre>

<p>How to generally look at the structure of tfrecords instead?</p>

<p>A second question: Where to find documentation on classes like <code>tf.train.Example</code>? I just find this <a href=""https://www.tensorflow.org/api_docs/python/tf/train/Example"" rel=""nofollow noreferrer"">empty page</a>.</p>
",1
61946263,How to understand the first argument of the Keras Conv2D layer?,"<p>I'm following the <a href=""https://tensorflow.google.cn/tutorials/quickstart/advanced"" rel=""nofollow noreferrer"">TensorFlow 2 quickstart for experts</a> guide and trying to understand the first argument of making an instance of <code>Conv2D</code>. </p>

<pre><code>filters: Integer, the dimensionality of the output space
    (i.e. the number of output filters in the convolution).
</code></pre>

<p>As the guide uses the same <code>32</code> for the <code>batch</code> size and <code>filters</code>, is there a specific reason to choose <code>32</code>, and should both of these parameters always match each other?</p>

<h3>Relevant code:</h3>

<pre><code>train_ds = tf.data.Dataset.from_tensor_slices(
    (x_train, y_train)).shuffle(10000).batch(32)
</code></pre>

<p>... ...</p>

<pre><code>self.conv1 = Conv2D(32, 3, activation='relu')
</code></pre>
",0
61946509,tf.estimator input_fn and eager mode,"<p>I tried to use <code>numpy</code> inside <code>cnn_model.evaluate()</code>, but it gave <code>AttributeError: 'Tensor' object has no attribute 'numpy'</code>. I used <code>numpy</code> to calculate accuracy and mean squared error using <code>tf.keras.metrics.Accuracy()</code> and <code>tf.keras.metrics.MeanSquaredError()</code> inside <code>cnn_model.evaluate()</code></p>

<p>I googled it, and in tensorflow documentation, it said </p>

<p><strong>""Calling methods of Estimator will work while eager execution is enabled. However, the model_fn and input_fn is not executed eagerly, Estimator will switch to graph mode before calling all user-provided functions (incl. hooks), so their code has to be compatible with graph mode execution.""</strong>  </p>

<p>So, I was wondering how I can update the current tf 1.x code to tf 2.1.0 code, while also using above information.</p>

<p>My current code is:</p>

<pre><code>eval_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(
    x={""x"": np.array(train_inputs, dtype=np.float32)},
    y=np.array(train_labels, dtype=np.float32),
    #y=np.array(train_labels),
    batch_size=1,
    num_epochs=1,
    shuffle=False)

eval_results = CNN.evaluate(input_fn=eval_input_fn)
</code></pre>

<p>What I have tried so far is add <code>tf.compat.v1.enable_eager_execution()</code> to the 1) beginning of the code after all the imports, 2) next line right after importing tf, 3) line right before declaring eval_input_fn, 4) line right before calling eval_results, 5) inside CNN model definition. It all failed to turn on the eager mode.</p>

<p>One other option that I found was remove @tf.function decorator, but I have no idea what that means and how to pass input_fn if @tf.function is removed.</p>
",0
61959517,How to train Keras model with multiple inputs in Tensorflow 2.2?,"<p>I'd like to train a Keras model with two inputs (one text input and some numerical features), but I struggle to get it working. I've setup a model as described in the <a href=""https://www.tensorflow.org/guide/keras/functional#models_with_multiple_inputs_and_outputs"" rel=""nofollow noreferrer"">Tensorflow documentation about models with multiple inputs</a>:</p>

<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from tensorflow.keras import Input, Model, models, layers


def build_model():
    input1 = Input(shape=(50,), dtype=tf.int32, name='x1')
    input2 = Input(shape=(1,), dtype=tf.float32, name='x2')
    y1 = layers.Embedding(1000, 10, input_length=50)(input1)
    y1 = layers.Flatten()(y1)
    y = layers.Concatenate(axis=1)([y1, input2])
    y = layers.Dense(1)(y)
    return Model(inputs=[input1, input2], outputs=y)
</code></pre>

<p>Building that model works fine too:</p>

<pre class=""lang-py prettyprint-override""><code>model = build_model()
model.compile(loss='mse')
model.summary()
</code></pre>

<p>You can find the output of <code>summary()</code> in <a href=""https://gist.github.com/hohl/8befac56b7137b1c54ca73735f1058f9"" rel=""nofollow noreferrer"">this gist</a>.</p>

<p>Then some (dummy) data is needed to get fit onto the model:</p>

<pre class=""lang-py prettyprint-override""><code>def make_dummy_data():
    X1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([100, 50], maxval=1000, dtype=tf.int32))
    X2 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([100, 1], dtype=tf.float32))
    X = tf.data.Dataset.zip((X1, X2)).map(lambda x1, x2: {'x1': x1, 'x2': x2})
    y_true = tf.data.Dataset.from_tensor_slices(tf.random.uniform([100, 1], dtype=tf.float32))
    return X, y_true


X, y_true = make_dummy_data()
Xy = tf.data.Dataset.zip((X, y_true))
model.fit(Xy, batch_size=32)
</code></pre>

<p>...but now <code>fit()</code> fails with an incomprehensible error message (see <a href=""https://gist.github.com/hohl/93ad258d01229e3505fe857c73501102"" rel=""nofollow noreferrer"">full message here</a>), which starts with a (probably relevant) warning:</p>

<pre><code>WARNING:tensorflow:Model was constructed with shape (None, 50) for input Tensor(""x1:0"", shape=(None, 50), dtype=int32), but it was called on an input with incompatible shape (50, 1).
</code></pre>

<p>Huh, where did that extra dimension of size 1 come from? And, how do I get rid of it?</p>

<p>One more thing: further simplification of this dummy model by removing the <code>Embedding</code>-layer does suddenly make the model run.</p>

<p>If you want to play around with the above sample, I prepared <a href=""https://colab.research.google.com/drive/1PaCe0vdhfcbQgEWkfG_-Ys3ezccpIT1e?usp=sharing"" rel=""nofollow noreferrer"">a notebook on Google Colab for it</a>. Any help appreciated.</p>
",1
61973237,Parallel execution of TF ops in eager code?,"<p>Let's say I have this code (<a href=""https://www.tensorflow.org/guide/function"" rel=""nofollow noreferrer"">via</a>):</p>

<pre><code>tf.compat.v1.disable_eager_execution()

@tf.function
def f(x):
  while tf.reduce_sum(x) &gt; 1:
    tf.print(x)
    tf.print(""hello"")
    x = tf.tanh(x)
  return x

with tf.compat.v1.Session() as session:
  session.run(f(tf.random.uniform([5])))
</code></pre>

<p>As I understand (<a href=""https://stackoverflow.com/questions/61964090/running-defun-in-graph-mode"">via</a>), that code works even with <code>disable_eager_execution</code>, also the code inside <code>f</code> uses eager-mode.</p>

<p>In graph mode, all TF operations run in parallel (when their inputs or control dependencies are available).</p>

<p>Will the TF operations in eager mode always run sequentially? In this code example, the <code>tf.print(""hello"")</code> could potentially be executed independently. Although I don't know if there are any implicit control dependencies (part of the loop; but does it depend on <code>tf.print(x)</code>?). Maybe <code>tf.print</code> is also an exception here.</p>

<p>Are there ways to run TF operations in parallel in eager mode?</p>
",0
61986166,How can I save a Tensorflow 2.2.0 model with a custom training loop?,"<p>I am struggling to save a tf.keras model to easily load and be able to use it. I have used the tf.keras.Model subclass method to construct a MLP model with a custom loss function, as you can see below:</p>

<pre><code>class MyModel(tf.keras.Model):
    def __init__(self):
        super(MyModel, self).__init__()
        self.dense1 = Dense(400, activation='relu', kernel_initializer=initializers.glorot_uniform(), input_dim=5)
        self.dense2 = Dense(400, activation='relu', kernel_initializer=initializers.glorot_uniform())
        self.dense3 = Dense(400, activation='relu', kernel_initializer=initializers.glorot_uniform())
        self.dense4 = Dense(400, activation='relu', kernel_initializer=initializers.glorot_uniform())
        self.dense_out = Dense(1, activation='relu', kernel_initializer=initializers.glorot_uniform())

    @tf.function(input_signature=[tf.TensorSpec(shape=(None, 5), dtype=tf.float32, name='inputs')])   #CHECK tf.saved_model.save docs!
    def call(self, inputs, **kwargs):
        x = self.dense1(inputs)
        x = self.dense2(x)
        x = self.dense3(x)
        x = self.dense4(x)
        return self.dense_out(x)

    def get_loss(self, X, Y):
        with tf.GradientTape() as tape:
            tape.watch(tf.convert_to_tensor(X))
            Y_pred = self.call(X)
        return tf.reduce_mean(tf.math.square(Y_pred-Y)) + tf.reduce_mean(tf.maximum(0, tape.gradient(Y_pred, X)[:, 2]))

    def get_grad_and_loss(self, X, Y):
        with tf.GradientTape() as tape:
            tape.watch(tf.convert_to_tensor(X))
            L = self.get_loss(X, Y)
        g = tape.gradient(L, self.trainable_weights)
        return g, L
</code></pre>

<p>I then make an instance of the model and proceed with a standard training loop:</p>

<pre><code>model = MyModel()
epochs = 5
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25)
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch)
val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))
val_dataset = val_dataset.shuffle(buffer_size=1024).batch(batch)
val_acc_metric = tf.keras.metrics.MeanAbsoluteError()


## TRAINING LOOP
losses = []
for epoch in range(epochs):
    print(f'############ START OF EPOCH {epoch + 1} ################')
    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):
        grads, L = model.get_grad_and_loss(x_batch_train, y_batch_train)
        losses.append(float(L))
        optimizer.apply_gradients(zip(grads, model.trainable_weights))

        if step % 100 == 0:
            print('Training loss (for one batch) at step %s: %s' % (step, float(L)))
            print(f'Seen so far: {(step+1)*batch} samples')

    # Run a validation loop at the end of each epoch.
    for val_step, (x_batch_val, y_batch_val) in enumerate(val_dataset):
        val_logits = model.call(x_batch_val)
        # Update val metrics
        val_acc_metric(y_batch_val, val_logits)
    val_acc = val_acc_metric.result()
    val_acc_metric.reset_states()
    print(f'Validation acc: {val_acc}')
</code></pre>

<p>I have tried to follow the steps outlined <a href=""https://stackoverflow.com/questions/60930158/tensorflow-saving-subclass-model-which-has-multiple-arguments-to-call-method"">here</a>. I call the model on a random input in order to trigger model.build internally, and then I attempt to save the model using the following:</p>

<pre><code>model.save('mymodel', signatures=model.call.get_concrete_function([tf.TensorSpec(shape=(None, 5), dtype=tf.float32, name='inputs')]))
</code></pre>

<p>I then get this following error:</p>

<pre><code>Traceback (most recent call last):
File ""&lt;input&gt;"", line 1, in &lt;module&gt;
File ""/Users/Maximocravero/opt/miniconda3/envs/finance_research/lib/python3.8/site- 
packages/tensorflow/python/eager/def_function.py"", line 959, in get_concrete_function
concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
File ""/Users/Maximocravero/opt/miniconda3/envs/finance_research/lib/python3.8/site- 
packages/tensorflow/python/eager/def_function.py"", line 871, in 
_get_concrete_function_garbage_collected
return self._stateless_fn._get_concrete_function_garbage_collected(  # pylint: 
disable=protected-access
File ""/Users/Maximocravero/opt/miniconda3/envs/finance_research/lib/python3.8/site- 
packages/tensorflow/python/eager/function.py"", line 2480, in 
_get_concrete_function_garbage_collected
raise ValueError(""Structure of Python function inputs does not match ""
ValueError: Structure of Python function inputs does not match input_signature.
</code></pre>

<p>I don't understand this issue as I specify the same TensorSpec as in the tf.function above the model.call attribute. I attempted this without including the tf.function above the model call, which leads to an error relating to the input dimensions having to be set. I am able to address this by calling the model on an arbitrary input, which does allow me to save the model but I have to compile it prior to using it and I get the following warning:</p>

<pre><code>WARNING:tensorflow:From 
/Users/Maximocravero/opt/miniconda3/envs/finance_research/lib/python3.8/site- 
packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling 
BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with 
constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
</code></pre>

<p>My question is whether or not I am completely missing something or if it is usual to have to compile loaded custom models? I am running <a href=""https://github.com/tensorflow/tensorflow/releases"" rel=""noreferrer"">TensorFlow 2.2.0</a> with Python 3.8.2, and based on the <a href=""https://www.tensorflow.org/guide/keras/save_and_serialize#whole-model_saving_loading"" rel=""noreferrer"">documentation</a> for saving models this should really be quite simple. I am new to TensorFlow so it may well be that it's a silly mistake, but ultimately it's still a basic model with 5 inputs and a single output. Any help would be greatly appreciated. </p>
",0
61988657,Why does tensorflow.rank always return shape with null value,"<p>Being a beginner to TensorFlow <strong>I couldn't get why does tensorflow.rank always return shape with null value?</strong></p>

<p><strong>This is what I am working on:</strong></p>

<pre><code>import tensorflow as tf
%tensorflow_version 2.x

list_2d = [[1,2,3,4],
             [5,6,7,8],
             [9,10,11,12]
]
tensor_2d = tf.Variable(list_2d)

print(tensor_2d.shape)
print(tf.rank(tensor_2d))
</code></pre>

<p><strong>and the output is</strong> </p>

<pre><code>(3, 4)
tf.Tensor(2, shape=(), dtype=int32)
</code></pre>

<p>So <strong>my question is what is this <code>shape=()</code> from <code>tf.rank</code> output</strong>?</p>

<p>I couldn't get much from here - <a href=""https://www.tensorflow.org/api_docs/python/tf/rank"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/rank</a></p>
",1
61993100,shared_name for TensorFlow operations,"<p>Many TensorFlow operations have <code>shared_name</code> as an optional argument.
For example <code>make_initializable_iterator</code> (<a href=""https://stackoverflow.com/questions/55612210/how-to-use-shared-name-on-initializable-iterator"">related question</a>), most (all?) TF resources (variables, <code>TensorArray</code>, ...), <code>ConditionalAccumulator</code>, <code>_MutableDenseHashTable</code>, <code>FIFOQueue</code> (<a href=""https://github.com/tensorflow/tensorflow/issues/17050"" rel=""nofollow noreferrer"">related issue</a>), etc.</p>

<p>In the documentation, it often says sth like this:</p>

<blockquote>
  <p>shared_name: If non-empty, this table will be shared under the given name across multiple sessions.</p>
</blockquote>

<p>But how does that work? How do I actually share that resource / tensor / op (or what actually exactly?) across multiple sessions?</p>

<p>Would that be multiple sessions in the same process? Or multiple sessions across multiple processes/machines (remotely)?</p>

<p>Would it share the same memory (only possible if within the same process, or at least same host, by using shared memory)? Or how else would it synchronize the state?</p>

<p>And is <code>Graph.container</code> related to that? From that doc:</p>

<blockquote>
  <p>Stateful operations, such as variables and queues, can maintain their states on devices so that they can be shared by multiple processes.</p>
</blockquote>

<p>How does the sharing across multiple processes work?</p>

<p>And is <a href=""https://www.tensorflow.org/api_docs/python/tf/distribute"" rel=""nofollow noreferrer"">distributed TensorFlow (<code>tf.distribute</code>)</a> related to that? How?</p>

<p>Or <code>remote_call</code>? (See also <a href=""https://stackoverflow.com/questions/61988958/tf-python-remote-call-vs-tf-device-when-do-you-need-remote-call"">this question</a>.)</p>
",1
61994285,TensorFlow: Error using weighted_categorical_column,"<p>I work on a binary classification problem containing a field STREET. In a first step I used the Tokenization to the get a word list (frequency of how often one word appears in the different datasets). Then I used this information to create two columns in my Dataframe describing the word and how often it was used:</p>

<pre><code>def buildWeightList(indexes, tokenizer):
    weights = []
    for index in indexes:
        if index == 0:
            weights.append(0)
        else:
            weights.append(tokenizer.index_docs.get(index))
    return weights
</code></pre>

<pre><code>street_tokenized = ts.texts_to_sequences(data['STREETPRO'])
data['STREETPRO'] = tf.keras.preprocessing.sequence.pad_sequences(street_tokenized, maxlen=1)
data['STREETFREQ']  = buildWeightList(data['STREETPRO'], ts)
</code></pre>

<p>After I converted the Dataframe to a TensorFlow Dataset I have used the following code to add it to my future columns:</p>

<pre><code>vocabulary_list = np.arange(0, street_num_words + 1, 1).tolist()
street_voc = tf.feature_column.categorical_column_with_vocabulary_list(
    key='STREETPRO', vocabulary_list=vocabulary_list, dtype=tf.dtypes.int64)

weighted_street = tf.feature_column.weighted_categorical_column(categorical_column=street_voc, weight_feature_key='STREETFREQ', dtype=tf.dtypes.int64)
street_one_hot = feature_column.indicator_column(weighted_street)

feature_columns.append(street_one_hot)
</code></pre>

<p>As you can see I used the function tf.feature_column.weighted_categorical_column. Unfortunately I get the following error when I try to train my model:</p>

<pre><code>InvalidArgumentError:  indices and values rows (indexing dimension) must match. (indices = 5, values = 1)
     [[node sequential/dense_features_2/STREETPRO_weighted_by_STREETFREQ_indicator/SparseMerge/SparseReorder (defined at &lt;ipython-input-40-964101dd1dc8&gt;:3) ]] [Op:__inference_train_function_986]
</code></pre>

<p>Furthermore I get the following warning:</p>

<pre><code>WARNING:tensorflow:From ...\feature_column\feature_column_v2.py:4366: sparse_merge (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
</code></pre>

<p>Now I have two questions:</p>

<p><strong>First</strong>: does it make sense to use this function for my described problem? Unfortunately, I couldn’t find a detailed description how this function works (only this short documentations: <a href=""https://www.tensorflow.org/api_docs/python/tf/feature_column/weighted_categorical_column"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/feature_column/weighted_categorical_column</a>) </p>

<p><strong>Second</strong>: How can I fix the described error?</p>
",1
62000090,How does tflite fuses Relu into conv layers?,"<p>When converting TF model to tflite model (or in other words - quantize a model using ""post-training quantization""), the Relu layers disappears from the graph. This is explained in the <a href=""https://www.tensorflow.org/lite/guide/ops_compatibility"" rel=""nofollow noreferrer"">documentation</a>: ""operations that can be simply removed from the graph (tf.identity), replaced by tensors (tf.placeholder), or fused into more complex operations (tf.nn.bias_add).""</p>

<p>My question is - how can a Relu layer be fused into a prior layer? (What is the math beyond this ""fusion""? Is this a specific procedure to quantized models, or this can be done also in the original floating-point model?)</p>
",1
62019212,Tensorflow input pipeline using text,"<p>in the last weeks I tried to get the input pipeline running with tf.records under tensorflow (tf 2.0.1). From a CSV sentences are loaded and a record is generated:</p>

<pre><code>import tensorflow as tf
import pathlib
import sys
import csv

PATH_PARENT = str(pathlib.Path(__file__).parent.absolute())
if PATH_PARENT.endswith('models'):
    PATH_PARENT = PATH_PARENT[:-len('models')]

PATH_PARENT = PATH_PARENT.replace(""\\"", '/')

sys.path.append(PATH_PARENT)

def create_tf_example(features, label):
    tf_example = tf.train.Example(features=tf.train.Features(feature={
        'Sentence': tf.train.Feature(bytes_list=tf.train.BytesList(value=[features.encode('utf-8')])),
        'Class': tf.train.Feature(bytes_list=tf.train.BytesList(value=[label.encode('utf-8')])),
    }))
    return tf_example

intent_load_list = [""training_data_intent_Music_controler_0.csv""]   # Example: musik,&lt;slot_0&gt;,play,&lt;slot_music_controle&gt;

for load_intent in intent_load_list:
    start = 14
    end = load_intent.rfind(""_"")
    label = load_intent[start : end]
    print(""loading intent "" + label)
    csv_data = []

    with open(PATH_PARENT + ""models/"" + load_intent, 'r') as csv_file:
            csv_reader = csv.reader(csv_file)
            for row in csv_reader:
                clean_output = ''
                for word in row:
                    if '&lt;' not in word:
                        clean_output = clean_output + word + ' '
                csv_data.append(clean_output)

    with tf.io.TFRecordWriter(PATH_PARENT + ""models/dataset.tfrecords"") as writer:
        for row in csv_data:
            features = row

            example = create_tf_example(features, label)
            writer.write(example.SerializeToString())        
    writer.close()
</code></pre>

<p>This works so far. This record should be loaded later, edited with a tf-hub model and then trained with the record. However, I either get an IndexEror or it fails completely:</p>

<pre><code>import numpy as np
import tensorflow as tf
from tensorflow import keras
import tensorflow_hub as hub
import sys
import pathlib

PATH_PARENT = str(pathlib.Path(__file__).parent.absolute())
if PATH_PARENT.endswith('models'):
    PATH_PARENT = PATH_PARENT[:-len('models')]

PATH_PARENT = PATH_PARENT.replace(""\\"", '/')

sys.path.append(PATH_PARENT)

embed = hub.load(""https://tfhub.dev/google/nnlm-de-dim50-with-normalization/2"")

dataset = tf.data.TFRecordDataset(filenames = [PATH_PARENT + ""models/dataset.tfrecords""])

def prepare_for_training(ds, shuffle_buffer_size=1024, batch_size=2):
    ds = ds.map(lambda x: embed([x]))

    ds = ds.shuffle(buffer_size=shuffle_buffer_size).batch(batch_size)

    print(ds)

    return ds


def convert_data(data):
    data_np = embed(data)
    data_list = data_np.tolist()
    return data_list


batch_size = 64
n_intents = 2

train_ds = prepare_for_training(dataset, batch_size=batch_size)

build_model = keras.Sequential()
build_model.add(keras.layers.Input(shape=(None, 50)))
build_model.add(keras.layers.Dense(50, activation='relu'))
build_model.add(keras.layers.Dropout(0.2))
build_model.add(keras.layers.Dense(20, activation='relu'))
build_model.add(keras.layers.Dropout(0.2))
build_model.add(keras.layers.Dense(n_intents, activation='softmax'))

build_model.summary()
build_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

build_model.fit(train_ds, epochs=5)     # IndexError - list index out of range
print(""done"")
</code></pre>

<p>Has anyone perhaps tried something similar or an idea? Unfortunately the documentation didn't help much. Thanks in advance</p>
",1
62055783,Tf.keras model.predict() returns class probabilities that are higher than 1?,"<p>I am trying to call model.predict() in tf.keras on a CNN to predict the class for a single image. For some reason, the class probabilities are coming back higher than 1 which is nonsensical. I am unsure why this is occurring. Below is how  I train my CNN:</p>

<pre><code>class_names = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']
model = models.Sequential()
model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1), kernel_regularizer=tf.keras.regularizers.l1(0.01)))
model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Dropout(0.5))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Dropout(0.5))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))


model.summary()

model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(7))


#model.summary()
model.compile(optimizer='adam',loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),metrics=['accuracy'])
lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=3) #monitors the validation loss for signs of a plateau and then alter the learning rate by the specified factor if a plateau is detected

early_stopper = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=6, mode='auto')  #This will monitor and stop the model training if it is not further converging

checkpointer = tf.keras.callbacks.ModelCheckpoint('C:\\Users\\rtlum\\Documents\\DataSci_Projects\\PythonTensorFlowProjects\\Datasets\\FER2013_Model_Weights\\Model\\weights.hd5', monitor='val_loss', verbose=1, save_best_only=True) #This allows checkpoints to be saved each epoch just in case the model stops training

epochs = 100
batch_size = 64
learning_rate = 0.001

model.fit(
          train_data,
          train_labels,
          epochs = epochs,
          batch_size = batch_size,
          validation_split = 0.2,
          shuffle = True,
          callbacks=[lr_reducer, checkpointer, early_stopper]
          )
</code></pre>

<p>Below is how I call model.predict() and pass in  a single image to predict:</p>

<pre><code>    model = tf.keras.models.load_model('Model\\weights.hd5')
    img = Image.open(test_image).convert('L')
    img = img.resize([48, 48])
    image_data = np.asarray(img, dtype=np.uint8)
    #image_data = np.resize(img,3072)
    image_data = image_data / 255
    image_data_test = image_data.reshape((1, 48, 48, 1))
    class_names = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']
    x = model.predict(image_data_test)
    app.logger.info(x)
    image_pred = np.argmax(x)
    y = round(x[0][np.argmax(x)], 2)
    confidence = y * 100
    print(class_names[image_pred], confidence)
</code></pre>

<p>And finally, below is the class probabilities I receive from model.predict():</p>

<pre><code>&gt;&gt;&gt; x = model.predict(image_data_test)
&gt;&gt;&gt; x
array([[ 1.0593076 , -3.5140653 ,  0.7505076 ,  2.1341033 ,  0.02394461,
        -0.08749148,  0.6640976 ]], dtype=float32)
</code></pre>
",0
62197100,deep learning early stopping callback strange behavior in a loop,"<p>I'm testing sequential models accuracy in a loop including an early stopping callback.</p>

<p>My environment is</p>

<pre><code>tensorflow                2.1.0           gpu_py37h7a4bb67_0
tensorflow-base           2.1.0           gpu_py37h6c5654b_0
tensorflow-estimator      2.0.1                    pypi_0    pypi
tensorflow-gpu            2.0.0                    pypi_0    pypi
</code></pre>

<p>I'm running my tests in a jupyter notebook.</p>

<p>Python version is 3.7.7</p>

<pre><code>for x in range:
   create_model()
   history=model.fit(..callback=[es_val_loss]..)
</code></pre>

<p>my callback is</p>

<pre><code>es_val_loss = EarlyStopping(monitor='val_loss', 
                   mode='auto', 
                   verbose=0, 
                   patience=PATIENCE,
                   min_delta=ES_VAL_LOSS_MIN_DELTA,
                   baseline=None,
                   restore_best_weights=True)
</code></pre>

<p>and my create_model() is</p>

<pre><code>def create_model( ... ):
    ## Create our model
    model = Sequential()

    # 1st layer
    model.add(Dense())
    model.add(Dropout())
    # other layers
    model.add(Dense())
    model.add(Dropout())

    model.add(Dense())
    model.add(Dropout())

    # output layer: dim=1, activation sigmoid
    model.add(Dense( ))
    myoptimizer = optimizers.Adam(lr=learning_rate)
    # Compile the model
    model.compile(loss='binary_crossentropy',   # since we are predicting 0/1
             optimizer=myoptimizer,
             metrics=['accuracy'])
    return model
</code></pre>

<p>With the first model, the callback response starts from inf for val_loss</p>

<pre><code>Epoch 00001: val_loss improved from inf to ... saving model to ...
</code></pre>

<p>For the other models in the loop, my callback retains the result of the previous model and doesn't start from inf but with the previous model best minimal val_loss value.</p>

<pre><code>Epoch 00001: val_loss improved from ... to ..., saving model to ...
</code></pre>

<p>how can I solve this behavior so that each model iteration starts with its own cleared values for the callback?</p>

<p>I tried with setting <code>model.reset_states()</code>and/or <code>tf.keras.backend.clear_session()</code> in the loop without success.</p>

<p><strong>Edit</strong></p>

<p>After further investigations, I found that the messages sent by the callback are misleading.
If I put <code>verbose=0</code> in the callback arguments and <code>verbose=2</code> in the <code>**model.fit**</code> arguments, I can see that the callback is working as expected.</p>
",0
62211822,TF2 Keras - Feature Engineering in Keras saved model via Tensorflow Serving,"<p>The Tensorflow 2 documentation for preprocessing / feature engineering over a Keras model seems to be quite confusing and isn't very friendly.</p>

<p>Currently I have a simple Keras N-layer model with TF feature columns feeding as dense layer. For training I have CSV files read using <code>tf.dataset</code> API and I have written a feature engineering function that creates new features using <code>dataset.map</code> function.</p>

<pre><code>def feature_engg_features(features):
  #Add new features
  features['nodlgrbyvpatd'] = features['NODLGR'] / features['VPATD']

  return(features)
</code></pre>

<p>I can save the model easily using <code>tf.keras.models.save_model</code> method. However I am having trouble figuring out how to attach the <code>feature_engineering</code> steps in the serving function.</p>

<p><strong>Requirement</strong>: Now I want to take the same feature engineering function above and attach it to my <code>serving function</code> so that in JSON input via <code>tensorflow_model_server</code> the same feature engineering steps are applied. I know about the lambda Layer option in Keras but I want to do this via <code>saved_model</code> method but there are a lot of difficulties here.</p>

<p>For Example, below code gives error:</p>

<pre><code>def feature_engg_features(features):
  #Add new features
  features['nodlgrbyvpatd'] = features['NODLGR'] / features['VPATD']
  return(features)

@tf.function
def serving(data):
    data = tf.map_fn(feature_engg_features, data, dtype=tf.float32)

    # Predict
    predictions = m_(data)

version = ""1""
tf.keras.models.save_model(
    m_,
    ""./exported_model/"" + version,
    overwrite=True,
    include_optimizer=True,
    save_format=None,
    signatures=serving,
    options=None
)
</code></pre>

<p>Error:</p>

<pre><code>Only `tf.functions` with an input signature or concrete functions can be used as a signature.
</code></pre>

<p>The above error is because I have not provided InputSignature of my Keras model but I am not able to understand that I have 13 input fields, what is expected as input signature.</p>

<p>So I wanted to know if anyone knows the shortest way of solving this out. This is a very basic requirement and Tensorflow seems to have kept this quite complicated for Keras Tensorflow model serving.</p>

<p>GIST: <a href=""https://colab.research.google.com/gist/rafiqhasan/6abe93ac454e942317005febef59a459/copy-of-dl-e2e-structured-mixed-data-tf-2-keras-estimator.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/gist/rafiqhasan/6abe93ac454e942317005febef59a459/copy-of-dl-e2e-structured-mixed-data-tf-2-keras-estimator.ipynb</a></p>

<p><strong>EDIT:</strong>
I fixed it, so TensorSpec has to be generated and passed for each feature and also model( ) has to be called in serving function.</p>

<pre><code>@tf.function
def serving(WERKS, DIFGRIRD, SCENARIO, TOTIRQTY, VSTATU, EKGRP, TOTGRQTY, VPATD, EKORG, NODLGR, DIFGRIRV, NODLIR, KTOKK):
    ##Feature engineering
    nodlgrbyvpatd = tf.cast(NODLGR / VPATD, tf.float32)

    payload = {
        'WERKS': WERKS,
        'DIFGRIRD': DIFGRIRD,
        'SCENARIO': SCENARIO,
        'TOTIRQTY': TOTIRQTY,
        'VSTATU': VSTATU,
        'EKGRP': EKGRP,
        'TOTGRQTY': TOTGRQTY,
        'VPATD': VPATD,
        'EKORG': EKORG,
        'NODLGR': NODLGR,
        'DIFGRIRV': DIFGRIRV,
        'NODLIR': NODLIR,
        'KTOKK': KTOKK,
        'nodlgrbyvpatd': nodlgrbyvpatd,        
    }

    ## Predict
    ##IF THERE IS AN ERROR IN NUMBER OF PARAMS PASSED HERE OR DATA TYPE THEN IT GIVES ERROR, ""COULDN'T COMPUTE OUTPUT TENSOR""
    predictions = m_(payload)
    return predictions

serving = serving.get_concrete_function(WERKS=tf.TensorSpec([None,], dtype= tf.string, name='WERKS'), 
                                        DIFGRIRD=tf.TensorSpec([None,], name='DIFGRIRD'),
                                        SCENARIO=tf.TensorSpec([None,], dtype= tf.string, name='SCENARIO'), 
                                        TOTIRQTY=tf.TensorSpec([None,], name='TOTIRQTY'),
                                        VSTATU=tf.TensorSpec([None,], dtype= tf.string, name='VSTATU'), 
                                        EKGRP=tf.TensorSpec([None,], dtype= tf.string, name='EKGRP'),
                                        TOTGRQTY=tf.TensorSpec([None,], name='TOTGRQTY'), 
                                        VPATD=tf.TensorSpec([None,], name='VPATD'),
                                        EKORG=tf.TensorSpec([None,], dtype= tf.string, name='EKORG'), 
                                        NODLGR=tf.TensorSpec([None,], name='NODLGR'),
                                        DIFGRIRV=tf.TensorSpec([None,], name='DIFGRIRV'),
                                        NODLIR=tf.TensorSpec([None,], name='NODLIR'),
                                        KTOKK=tf.TensorSpec([None,], dtype= tf.string, name='KTOKK')
                                        )

version = ""1""
tf.saved_model.save(
    m_,
    ""./exported_model/"" + version,
    signatures=serving
)
</code></pre>
",1
62223016,Single Prediction Image doesn't need to be rescaled?,"<p>I followed a tutorial to make my first Convolutional Neural Network using Keras and I have a small question regarding the rescaling step.</p>

<p>So when we are importing the training set and test set, we create an instance of  the <code>tf.keras.preprocessing.image.ImageDataGenerator</code> class and use it as:</p>

<pre class=""lang-py prettyprint-override""><code>train_datagen = ImageDataGenerator(rescale=1/255)
</code></pre>

<p>Along with some other augmentation parameters. My understanding is that we use the <code>rescale</code> parameter to normalize the pixel values of the images imported.</p>

<p>But when we load up a single image to run through the CNN, we write something like (code from keras docs):</p>

<pre class=""lang-py prettyprint-override""><code>image = tf.keras.preprocessing.image.load_img(image_path)
input_arr = keras.preprocessing.image.img_to_array(image)
input_arr = np.array([input_arr])  # Convert single image to a batch.
predictions = model.predict(input_arr)
</code></pre>

<p>My question is, I cannot see the single input image being <code>rescaled</code> anywhere. Is it being done implicitly, or is there no need to actually perform rescaling? If the latter, then why is it so?</p>

<p>Thanks!</p>
",1
62236460,How to set bounds and constraints on Tensorflow Variables (tf.Variable),"<p>I am using Tensorflow to minimize a function. The function takes about 10 parameters. Every single parameter has bounds, e.g. a minimum and a maximum value the parameter is allowed to take. For example, the parameter x1 needs to be between 1 and 10.</p>

<p>I also have a pair of parameters that need to have the following constraint x2 > x3. In other words, x2 must always be bigger than x3. (In addition to this, x2 and x3 also have bounds, similarly to the example of x1 above.)</p>

<p>I know that tf.Variable has a ""constraint"" argument, however I can't really find any examples or documentation on how to use this to achieve the bounds and constraints as mentioned above.</p>

<p>Thank you!</p>
",1
62237432,In which cases we use the attribute trainable_variables over trainable_weights and vice-versa of a tf.keras.Model in TF2?,"<p>I was studying how to do transfer learning in TF 2 and I saw that at <a href=""https://www.tensorflow.org/tutorials/images/transfer_learning#compile_the_model"" rel=""nofollow noreferrer"">this tutorial from Tensorflow</a> they use the attribute <code>trainable_variables</code> to reference the trainable variables of a model but in this <a href=""https://keras.io/guides/transfer_learning/"" rel=""nofollow noreferrer"">other tutorial from the keras documentation</a> they use the attribute <code>trainable_weights</code> of a <code>tf.keras.Model</code>.</p>

<p>I checked both attributes with a simple model, and they give me the same result. </p>

<pre><code>import tensorflow as tf
print(tf.__version__)

inputs = tf.keras.layers.Input(shape=[64, 64, 3])

x = tf.keras.layers.Conv2D(128, kernel_size=3, strides=2)(inputs)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)

model = tf.keras.Model(inputs=inputs, outputs=x)

print(""\nTrainable weights"")
vars_model = [var.name for var in model.trainable_weights]
print(*vars_model, sep=""\n"")

print(""\nTrainable variables"")
vars_model = [var.name for var in model.trainable_variables]
print(*vars_model, sep=""\n"")
</code></pre>

<p>Output: </p>

<pre><code>2.2.0

Trainable weights
conv2d/kernel:0
conv2d/bias:0
batch_normalization/gamma:0
batch_normalization/beta:0

Trainable variables
conv2d/kernel:0
conv2d/bias:0
batch_normalization/gamma:0
batch_normalization/beta:0
</code></pre>

<p>I checked this <a href=""https://stackoverflow.com/q/49020732"">other issue</a> and tried to follow the definition of both attributes: <code>trainable_variables</code> seems to be <a href=""https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/base_layer.py#L1971"" rel=""nofollow noreferrer"">here</a> and <code>trainable_weights</code> seems to be <a href=""https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/base_layer.py#L1072"" rel=""nofollow noreferrer"">here</a> and <a href=""https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/network.py#L566"" rel=""nofollow noreferrer"">here</a>, since <code>td.keras.Model</code> also inherits from <code>network.Network</code>. The former seems to be returning the <code>trainable_weights</code> variable. But, I am not sure that this happens in ""all"" cases.</p>

<p>So, I am wondering in which cases we use <code>trainable_variables</code> over <code>trainable_weights</code> and vice-versa? and why?</p>
",0
62244261,What does InvalidArgumentError in tensorflow 2 mean?,"<p>I am new tensorflow. I am trying to implement Linear Regression with custom training, following this <a href=""https://www.tensorflow.org/tutorials/customization/custom_training"" rel=""nofollow noreferrer"">tutorial</a>. </p>

<p>But when I try to compute <code>W*x + b</code>
I am getting this error </p>

<pre><code>tf.add(tf.matmul(W,x),b)
</code></pre>

<blockquote>
  <p>InvalidArgumentError: cannot compute Add as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:Add]</p>
</blockquote>

<p>I initialized W and b</p>

<p><code>W = tf.Variable(np.random.rand(1,9))</code>  </p>

<p><code>b = tf.Variable([1],dtype = tf.float32)</code></p>

<p><code>x = tf.Variable(np.random.rand(9,100))</code></p>

<p>But when I changed the initialisation of b to</p>

<p><code>b = tf.Variable(np.random.rand(1))</code></p>

<p>I did not get any error. What is the reason for this?</p>
",0
62249084,What is the numpy equivalent of TensorFlow Xavier initializer for CNN?,"<p>I would like to re-create the Xavier initialization in NumPy (using basic functions) in the same way that TensorFlow2 does for CNN. 
Here is how I learned to do Xavier initialization in NumPy:</p>

<pre><code># weights.shape = (2,2)
np.random.seed(0)
nodes_in = 2*2
weights = np.random.rand(2,2) * np.sqrt(1/nodes_in)

&gt;&gt;&gt;array([[0.27440675, 0.35759468],
          [0.30138169, 0.27244159]])
</code></pre>

<p>This is the way I learned Xavier initialization for the logistic regression model. It seems that for Convolution Neural Network it should be different but I don't know how.</p>

<pre><code>initializer = tf.initializers.GlorotUniform(seed=0)
tf.Variable(initializer(shape=[2,2],dtype=tf.float32))

&gt;&gt;&gt;&lt;tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=
   array([[-0.7078647 ,  0.50461936],
          [ 0.73500216,  0.6633029 ]], dtype=float32)&gt;
</code></pre>

<p>I'm confused by the TensorFlow <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform"" rel=""noreferrer"">documentation</a> when they explain the ""fan_in"" and ""fan_out"". I'm guessing this is where the problem is. Can somebody dumb it down for me, please? </p>

<p>Much appreciate it!</p>

<p><em>[UPDATE]:</em></p>

<p>When I follow the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform"" rel=""noreferrer"">tf.keras.initializers.GlorotUniform</a> documentation I still don't come to the same results:</p>

<pre><code># weights.shape = (2,2)
np.random.seed(0)
fan_in = 2*2
fan_out = 2*2
limit = np.sqrt(6/(fan_in + fan_out))
np.random.uniform(-limit,limit,size=(2,2))
&gt;&gt;&gt;array([[0.08454747, 0.37271892],
          [0.17799139, 0.07773995]])
</code></pre>
",1
62284095,What are the parameters to tf.GradientTape()'s __exit__ function?,"<p>According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/GradientTape"" rel=""nofollow noreferrer"">documentation</a> for <code>tf.GradientTape</code>, its <code>__exit__()</code> method takes three positional arguments: <code>typ, value, traceback</code>.</p>

<p><strong>What exactly are these parameters?</strong> </p>

<p>How does the <code>with</code> statement infer them? </p>

<p>What values should I give them in the code below (where I'm <em>not</em> using a <code>with</code> statement):</p>

<pre><code>x = tf.Variable(5)

gt = tf.GradientTape()
gt.__enter__()
y = x ** 2
gt.__exit__(typ = __, value = __, traceback = __)
</code></pre>
",1
62304650,How to convert `tf.contrib.lookup.index_table_from_file` into Tensorflow v2,"<p>I'm new to Tensorflow and working on code from v1 Tensorflow, but <code>tf.contrib</code> module is no longer supported in Tensorflow and I'm facing trouble finding their substitute in V2.<br/>
I disabled V2 with this code :</p>

<pre><code>import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
</code></pre>

<p>But still it is not working for <code>tf.contrib.lookup.index_table_from_file</code> and I'm getting an error 
<code>AttributeError: module 'tensorflow_core.compat.v1' has no attribute 'contrib'</code>. I tried looking in their official documentation <a href=""https://www.tensorflow.org/guide/migrate"" rel=""nofollow noreferrer"">Here</a> but couldn't find one.</p>

<p>What is the substitute for  <code>tf.contrib.lookup.index_table_from_file</code> function in V2? </p>
",1
62318212,Is it possible to create an Estimator with arbitrarily many input Tensors for Predict SignatureDef without placeholders in TensorFlow 2.X?,"<p><strong>The Problem</strong></p>

<p>I am converting my Tensorflow 1.14 estimator to TensorFlow 2.1. My current workflow involves training my tensorflow model on gcloud's ai-platform <a href=""https://cloud.google.com/ai-platform/training/docs/training-jobs"" rel=""nofollow noreferrer"">(training on gcloud)</a> and using their model service to deploy my model for online predictions <a href=""https://cloud.google.com/ai-platform/prediction/docs/deploying-models"" rel=""nofollow noreferrer"">(model service)</a>. </p>

<p>The issue when upgrading to TensorFlow 2 is that they have done away with placeholders, which is affecting my <code>serving_input_fn</code> and how I export my estimator model. With tensorflow 2, if I export a model without the use of placeholders, my model's ""predict"" <code>SignatureDef</code> only has a single ""examples"" tensor whereas previously it had many inputs named appropriately through my <code>serving_input_fn</code>. </p>

<p>The previous set up for my estimator was as follows: </p>

<pre><code>def serving_input_fn():

    inputs = {
        'feature1': tf.compat.v1.placeholder(shape=None, dtype=tf.string),
        'feature2': tf.compat.v1.placeholder(shape=None, dtype=tf.string),
        'feature3': tf.compat.v1.placeholder(shape=None, dtype=tf.string),
        ...
    }

    return tf.estimator.export.ServingInputReceiver(features=split_features, receiver_tensors=inputs)

exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)

eval_spec = tf.estimator.EvalSpec(
    input_fn=lambda: input_eval_fn(args.test_dir),
    exporters=[exporter],
    start_delay_secs=10,
    throttle_secs=0)

...

tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
</code></pre>

<p>And this has worked fine in the past, it has allowed me to have a multi-input ""predict"" SignatureDef where I can send a json of the inputs to ai-platforms model service and get predictions back. But since I am trying to not rely on the <code>tf.compat.v1</code> library, I want to avoid using placeholders. </p>

<p><strong>What I've tried</strong></p>

<p>Following the documentation linked <a href=""https://www.tensorflow.org/guide/saved_model#savedmodels_from_estimators"" rel=""nofollow noreferrer"">here</a> I've replaced my serving_input_fn with the <code>tf.estimator.export.build_parsing_serving_input_receiver_fn</code> method: </p>

<pre><code>feature_columns = ... # list of feature columns 
serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
  tf.feature_column.make_parse_example_spec(feature_columns))
</code></pre>

<p>However, this gives me the following ""predict"" SignatureDef:</p>

<pre><code>signature_def['predict']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['examples'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: input_example_tensor:0
</code></pre>

<p>whereas before my ""predict"" SignatureDef was as follows:</p>

<pre><code>signature_def['predict']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['feature1'] tensor_info:
        dtype: DT_STRING
        shape: unknown_rank
        name: Placeholder:0
    inputs['feature2'] tensor_info:
        dtype: DT_STRING
        shape: unknown_rank
        name: Placeholder_1:0
    inputs['feature3'] tensor_info:
        dtype: DT_STRING
        shape: unknown_rank
        name: Placeholder_2:0
</code></pre>

<p>I've also tried using the <code>tf.estimator.export.build_raw_serving_input_receiver_fn</code>, but my understanding is that this method requires actual Tensors in order to be used instead of a feature spec. Unless I use placeholders, I don't really understand where to grab these serving Tensors from.</p>

<p><strong>So my main questions are:</strong></p>

<ul>
<li>Is it possible to create a multi-input ""predict"" signature def from an estimator model without using placeholders in Tensorflow 2? </li>
<li>If it is not possible, how am I supposed to provide the instances to gcloud predictions service for the ""examples"" tensor in the ""predict"" signature def? </li>
</ul>

<p>Thanks! </p>
",0
62349329,Distributed training using MirrorStrategy in tensorflow 2.2 with custom training loop not working - getting stuck when updating gradients,"<p>I'm using tf.distribute.Strategy to train a model, based on unet, with MirrorStrategy over two (or more) gpus. Below is my code for the custom train loop I use for the forward and backward passes of the network. For some reason, the logits, loss and gradients of the first batch of the first epoch are calculated but then it gets stuck at optimizer.apply_gradients(zip(gradients, model.trainable_variables). I can't for the life of me what the problem is so any help would be much appreciated.</p>

<pre><code>import os
import glob
import numpy as np
import tensorflow as tf
from tensorflow.keras.utils import Progbar
import tensorflow.keras.backend as K
from tensorflow.keras import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation, Dense, BatchNormalization, Dropout
from tensorflow.keras.layers import UpSampling2D, concatenate
from evaluation import diceCoef

tf.config.experimental_run_functions_eagerly(True)


class Train():
    def __init__(self, model, lossFunc, optimizer, strategy, epochs, batchSize):
        self.epochs = epochs
        self.batchSize = batchSize
        self.strategy = strategy
        #self.lossFunc = lossFunc
        self.lossFunc = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)
        self.optimizer = optimizer
        self.model = model
        self.history = {'trainloss': [], 'trainmetric':[], 'valmetric': []}


    def computeLoss(self, yPred, yTrue):

        #loss = tf.reduce_sum(self.lossFunc(yPred, yTrue)) * (1./self.batchSize)
        loss = self.lossFunc(yPred, yTrue)
        loss = loss * (1. / self.strategy.num_replicas_in_sync)
        #print(loss)

        return loss


    @tf.function
    def trainStep(self, x, y, i):
        #x = batch[0]
        #y = batch[1]
        x = tf.cast(x, tf.float32)
        y = tf.cast(y, tf.float32) 
        #print(self.model.trainable_variables)
        with tf.GradientTape() as tape:
            logits = self.model(x, training=True)
            logits = tf.cast(logits, tf.float32) 
            loss = self.computeLoss(logits, y)
            #loss = self.lossFunc(logits, y)
            #print('loss', loss)
        gradients = tape.gradient(loss, self.model.trainable_variables)
        print(len(gradients))
        print(len(self.model.trainable_variables))
        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
        return loss, logits


    @tf.function
    def validStep(self, x, y):
        logits = self.model(x, training=False)
        loss = self.lossFunc(y, logits)

        return loss, logits,


    @tf.function
    def distributedTrainEpoch(self, dataset, trainSteps):

        totalDice = 0
        totalLoss = 0
        #prog = Progbar(trainSteps-1)

        for i, batch in enumerate(dataset):
            x = batch[0]
            #y = tf.expand_dims(batch[1], axis=-1)
            y = batch[1]
            batchLoss, logits = self.strategy.run(self.trainStep, args=(x,y,i))
            print('batchloss', batchLoss)
            #pred = (logits.numpy() &gt; 0.5).astype('int16').astype(np.float16)
            #batchDice = self.strategy.run(diceCoef, args=(pred, y))
            totalLoss += self.strategy.reduce(tf.distribute.ReduceOp.SUM, batchLoss, axis=None)
            #totalDice += self.strategy.reduce(tf.distribute.ReduceOp.SUM, batchDice, axis=None)
            #prog.update(i)

        return totalLoss, totalDice


    @tf.function
    def distributedValidEpoch(self, dataset):
        totalLoss = 0
        totalDice = 0
        for d in dataset:
            x = d[0]
            y = tf.expand_dims(d[1], axis=-1)
            loss, logits = self.strategy.run(self.validStep, args=(x, y))
            pred = (logits.numpy() &gt; 0.5).astype('int16').astype(np.float16)
            dice = self.strategy.run(diceCoef, args=(pred, y))
            totalLoss += self.strategy.reduce(tf.distribute.ReduceOp.SUM, loss, axis=None)
            totalDice += self.strategy.reduce(tf.distribute.ReduceOp.SUM, dice, axis=None)

        return totalLoss, totalDice


    def forward(self, trainDataset, validDataset, trainSteps, validSteps):

        for e in range(self.epochs):

            tf.print('Epoch: {}/{}...'.format(e+1, self.epochs), end="""")

            trainLoss, trainDice = self.distributedTrainEpoch(trainDataset, trainSteps)
            avgTrainDice = trainDice.numpy()[0] / trainSteps
            avgTrainLoss = trainLoss.numpy() / trainSteps
            print('train', avgTrainDice)
            print('loss', avgTrainLoss)

            tf.print(' Epoch: {}/{},  loss - {:.2f}, dice - {:.2f}'.format(e+1,
                   self.epochs, avgTrainLoss, avgTrainDice), end="""")

            valLoss, valDice = self.distributedValidEpoch(validDataset)

            avgValidDice = valDice.numpy()[0] / validSteps
            avgValidLoss = valLoss.numpy() / validSteps


            self.history['trainmetric'].append(avgTrainDice)
            self.history['trainloss'].append(avgTrainLoss)
            self.history['valmetric'].append(avgValidDice)
            self.history['valmetric'].append(avgValidLoss)

            tf.print('  val_loss - {:.3f}, val_dice - {:.3f}'.format(avgValidLoss, avgValidDice))

        return self.model, history
</code></pre>

<p>This is the part of the code from another script that sets up the strategy scope, builds the model and calls the train class.</p>

<pre><code> with strategy.scope():

        if model == 'fcn8':
            print('Model: {}'.format(model))
            with tf.device('/cpu:0'):
                if api == 'functional':
                    fcn = FCN()
                    model = fcn.getFCN8()
                elif api=='subclass':
                    model = FCN()

        elif model == 'unet':
            print('Model: {}'.format(model))
            with tf.device('/cpu:0'):
                if api=='functional':
                    unetModel = unet2.UnetFunc()
                    model = unetModel.unet()
                elif api=='subclass':
                    model = unetsc.UnetSC(filters=filters)
                    model.build((1, imgDims, imgDims, 3))

        elif model == 'unetmini':
            print('Model: {}'.format(model))
            with tf.device('/cpu:0'):
                if api == 'functional':
                    unetminiModel = UnetMini(filters=filters)
                    model = unetminiModel.unetmini()
                elif api=='subclass':
                    model = UnetMini(filters)

        elif model == 'resunet':
            print('Model: {}'.format(model))
            with tf.device('/cpu:0'):
                if api=='functional':
                    resunetModel =  ResUnet(filters)
                    model = resunetModel.ResUnetFunc()
                elif api=='subclass':
                    model = ResunetSc(filters)

        elif model == 'resunet-a':
            print('Model: {}'.format(model))
            with tf.device('/cpu:0'):
                if api=='functional':
                    resunetModel =  ResUnetA(filters)
                    model = resunetModel.ResUnetAFunc()
                elif api=='subclass':
                    model = ResunetASc(filters)

        elif model == 'attention':
            print('Model: {}'.format(model))
            with tf.device('/cpu:0'):
                if api == 'functional':
                    attenModel = AttenUnetFunc(filters)
                    model = attenModel.attenUnet()
                elif api=='subclass':
                    model = AttenUnetSC(filters)
        else:
            raise ValueError('No model requested, please update config file')

#        print('trainable variables', str(model.trainable_variables))

        trainer = train.Train(model, loss, optimizer, strategy, epoch, batchSize)

        trainDistDataset = strategy.experimental_distribute_dataset(trainDataset)
        validDistDataset = strategy.experimental_distribute_dataset(validDataset)

        model, history = trainer.forward(trainDistDataset, validDistDataset, trainSteps, validSteps)
</code></pre>

<p>And the subclassed unet model as follows:</p>

<pre><code>class UnetSC(Model):
    def __init__(self, filters=[16,32,64,128, 256], finalActivation='sigmoid', activation='relu', 
                    nOutput=1, kSize=(3,3), pSize=(2,2), dropout=0, normalize=True, padding='same', dtype='float32'):
        super(UnetSC, self).__init__(dtype=dtype)

        self.normalize = normalize
        self.conve1_1 = Conv2D(filters[0], kSize, activation='relu', padding='same', name='greg')
        self.batchnorm1 = BatchNormalization(name='greggggggg')
        self.conve1_2 = Conv2D(filters[0], kSize, activation='relu', padding='same')
        self.batchnorm2 = BatchNormalization()
        self.pool1 = MaxPooling2D((2, 2))

        self.conve2_1 = Conv2D(filters[1], kSize, activation='relu', padding='same')
        self.batchnorm3 = BatchNormalization()
        self.conve2_2 = Conv2D(filters[1], kSize, activation='relu', padding='same')
        self.batchnorm4 = BatchNormalization()
        self.pool2 = MaxPooling2D((2, 2))

        self.conve3_1 = Conv2D(filters[2], kSize, activation='relu', padding='same')
        self.batchnorm5 = BatchNormalization()
        self.conve3_2 = Conv2D(filters[2], kSize, activation='relu', padding='same')
        self.batchnorm6 = BatchNormalization()
        self.pool3 = MaxPooling2D((2, 2))

        self.conve4_1 = Conv2D(filters[3], kSize, activation='relu', padding='same')
        self.batchnorm7 = BatchNormalization()
        self.conve4_2 = Conv2D(filters[3], kSize, activation='relu', padding='same', name='finalencoder')
        self.batchnorm8 = BatchNormalization()
        self.pool4 = MaxPooling2D((2, 2))

        self.convb_1 = Conv2D(filters[4], kSize, activation='relu', padding='same')
        self.batchnorm9 = BatchNormalization()
        self.convb_2 = Conv2D(filters[4], kSize, activation='relu', padding='same')
        self.batchnorm10 = BatchNormalization()

        self.upsampling1 = UpSampling2D((2, 2))
        self.conc1 = Concatenate()
        self.convd1_1 = Conv2D(filters[3], kSize, activation='relu', padding='same')
        self.batchnorm11 = BatchNormalization()
        self.convd1_2 = Conv2D(filters[3], kSize, activation='relu', padding='same')
        self.batchnorm12 = BatchNormalization()

        self.upsampling2 = UpSampling2D((2, 2))
        self.conc2 = Concatenate()
        self.convd2_1 = Conv2D(filters[2], kSize, activation='relu', padding='same')
        self.batchnorm13 = BatchNormalization()
        self.convd2_2 = Conv2D(filters[2], kSize, activation='relu', padding='same')
        self.batchnorm14 = BatchNormalization()

        self.upsampling3 = UpSampling2D((2, 2))
        self.conc3 = Concatenate()
        self.convd3_1 = Conv2D(filters[1], kSize, activation='relu', padding='same')
        self.batchnorm15 = BatchNormalization()
        self.convd3_2 = Conv2D(filters[1], kSize, activation='relu', padding='same')
        self.batchnorm16 = BatchNormalization()

        self.upsampling4 = UpSampling2D((2, 2))
        self.conc4 = Concatenate()
        self.convd4_1 = Conv2D(filters[0], kSize, activation='relu', padding='same')
        self.batchnorm17 = BatchNormalization()
        self.convd4_2 = Conv2D(filters[0], kSize, activation='relu', padding='same')
        self.batchnorm18 = BatchNormalization()

        self.final = Conv2D(nOutput, kernel_size=(1, 1), strides=(1, 1), activation=finalActivation)


    def call(self, x, training=True):

        e1 = self.conve1_1(x)
        e1 = self.batchnorm1(e1)
        e1 = self.conve1_2(e1)
        e1 = self.batchnorm2(e1)
        p1 = self.pool1(e1)

        e2 = self.conve2_1(p1)
        e2 = self.batchnorm3(e2)
        e2 = self.conve2_2(e2)
        e2 = self.batchnorm4(e2)
        p2 = self.pool2(e2)

        e3 = self.conve3_1(p2)
        e3 = self.batchnorm5(e3)
        e3 = self.conve3_2(e3)
        e3 = self.batchnorm6(e3)
        p3 = self.pool3(e3)

        e4 = self.conve4_1(p3)
        e4 = self.batchnorm7(e4)
        e4 = self.conve4_2(e4)
        e4 = self.batchnorm8(e4)
        p4 = self.pool4(e4)

        b = self.convb_1(p4)
        b = self.batchnorm9(b)
        b = self.convb_2(b)
        b = self.batchnorm10(b)

        d1 = self.upsampling1(b)
        d1 = self.conc1([e4, d1])
        d1 = self.convd1_1(d1)
        d1 = self.batchnorm11(d1)
        d1 = self.convd1_2(d1)
        d1 = self.batchnorm12(d1)

        d2 = self.upsampling2(d1)
        d2 = self.conc2([e3, d2])
        d2 = self.convd2_1(d2)
        d2 = self.batchnorm13(d2)
        d2 = self.convd2_2(d2)
        d2 = self.batchnorm14(d2)

        d3 = self.upsampling3(d2)
        d3 = self.conc3([e2, d3])
        d3 = self.convd3_1(d3)
        d3 = self.batchnorm15(d3)
        d3 = self.convd3_2(d3)
        d3 = self.batchnorm16(d3)

        d4 = self.upsampling4(d3)
        d4 = self.conc4([e1, d4])
        d4 = self.convd4_1(d4)
        d4 = self.batchnorm17(d4)
        d4 = self.convd4_2(d4)
        d4 = self.batchnorm18(d4)

        x = self.final(d4)

        return x


u = UnetSC()
u = u.build((1, 256,256,3))

The error output trace 

Using TensorFlow backend.

Now executing following model: unet_32_adam_diceloss_FR_0_2.5x_germ_32
2020-06-12 18:14:00.672680: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-06-12 18:14:00.815119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:3f:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-06-12 18:14:00.816539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: 
pciBusID: 0000:40:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-06-12 18:14:00.817342: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-06-12 18:14:00.820640: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-06-12 18:14:00.823040: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-06-12 18:14:00.823833: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-06-12 18:14:00.826794: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-06-12 18:14:00.829026: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-06-12 18:14:00.834643: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-06-12 18:14:00.839962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1
2020-06-12 18:14:00.840532: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-06-12 18:14:00.855173: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz
2020-06-12 18:14:00.857769: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x58fdc10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-06-12 18:14:00.857804: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-06-12 18:14:01.277928: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x59680f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-06-12 18:14:01.278008: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-16GB, Compute Capability 7.0
2020-06-12 18:14:01.278031: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-PCIE-16GB, Compute Capability 7.0
2020-06-12 18:14:01.284602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:3f:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-06-12 18:14:01.291638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: 
pciBusID: 0000:40:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s
2020-06-12 18:14:01.291808: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-06-12 18:14:01.291883: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-06-12 18:14:01.291935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-06-12 18:14:01.291988: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-06-12 18:14:01.292039: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-06-12 18:14:01.292086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-06-12 18:14:01.292151: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-06-12 18:14:01.304148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1
2020-06-12 18:14:01.304295: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-06-12 18:14:01.312107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-06-12 18:14:01.312143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 
2020-06-12 18:14:01.312164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y 
2020-06-12 18:14:01.312180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N 
2020-06-12 18:14:01.318105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14864 MB memory) -&gt; physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:3f:00.0, compute capability: 7.0)
2020-06-12 18:14:01.320434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 14864 MB memory) -&gt; physical GPU (device: 1, name: Tesla V100-PCIE-16GB, pci bus id: 0000:40:00.0, compute capability: 7.0)

Epoch: 1/40...WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.
2020-06-12 18:14:16.135798: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-06-12 18:14:18.493751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10


74
74

74
74
</code></pre>

<p>Then it just sticks here. Please help!</p>
",0
62398282,Poor Performance of Tensorflow 2 Keras Model with Variable-Length Training Data,"<p>I'm using Tensorflow 2.2.0-gpu, and I have a simple Keras model that's composed of a few dense layers and a linear output (reference the code below).  I'm training the model on variable-length samples, and when I run the code I get warnings about tf.function retracing.  From what I've read, function tracing is expensive, and consequently the performance is poor.  Here's the code, which takes about <strong>330 seconds</strong> to run on my machine.</p>

<pre><code>#import tensorflow as tf
#tf.compat.v1.disable_eager_execution()

import numpy as np
import timeit
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import optimizers

def main():
  state_input = keras.Input((2,))
  hidden1     = layers.Dense(units = 64, activation = ""relu"")(state_input)
  hidden2     = layers.Dense(units = 128, activation = ""relu"")(hidden1)
  hidden3     = layers.Dense(units = 128, activation = ""relu"")(hidden2)
  output      = layers.Dense(units = 2, activation = ""linear"")(hidden3)

  model = keras.Model(inputs = state_input, outputs = output)
  opt   = optimizers.Adam(lr = 1e-4)

  model.compile(optimizer = opt, loss = ""mean_squared_error"")

  np.random.seed(0)

  def train():
    for i in range(2000):
      print(i)

      num_samples = np.random.randint(int(1e4), int(1e5))
      x = np.random.rand(num_samples, 2)
      y = np.random.rand(num_samples, 2)

      model.train_on_batch(x, y)

  print(timeit.timeit(train, number=1))

if __name__ == ""__main__"":
  main()
</code></pre>

<p>If I disable eager execution using <code>tf.compat.v1.disable_eager_execution()</code> (line 2 in the code), then the same code runs in about <strong>30 seconds</strong>.  This is similar to the performance I was seeing under Tensorflow 1.</p>

<p>Is there a way I can change my model such that I get similar performance to that attained with eager execution diabled?  Namely, can the model be changed such that the function retracing isn't incurred on each call?</p>

<p>For reference, this is the warning that's generated when <code>train_on_batch</code> is called:</p>

<pre><code>WARNING:tensorflow:10 out of the last 11 calls to &lt;function Model.make_train_function.&lt;locals&gt;.train_function at 0x7f68f3724158&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.
</code></pre>
",0
62405592,Tensorflow 2 Metrics produce wrong results with 2 GPUs,"<p>I took this piece of code from tensorflow documentation about distributed training with custom loop <a href=""https://www.tensorflow.org/tutorials/distribute/custom_training"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/distribute/custom_training</a> and I just fixed it to work with the tf.keras.metrics.AUC and run it with 2 GPUS (2 Nvidia V100 from a DGX machine). </p>

<pre><code># Import TensorFlow
import tensorflow as tf

# Helper libraries
import numpy as np


print(tf.__version__)


fashion_mnist = tf.keras.datasets.fashion_mnist
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

# Adding a dimension to the array -&gt; new shape == (28, 28, 1)
# We are doing this because the first layer in our model is a convolutional
# layer and it requires a 4D input (batch_size, height, width, channels).
# batch_size dimension will be added later on.
train_images = train_images[..., None]
test_images = test_images[..., None]

# One hot
train_labels = tf.keras.utils.to_categorical(train_labels, 10)
test_labels = tf.keras.utils.to_categorical(test_labels, 10)

# Getting the images in [0, 1] range.
train_images = train_images / np.float32(255)
test_images = test_images / np.float32(255)

# If the list of devices is not specified in the
# `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.
GPUS = [0, 1]
devices = [""/gpu:"" + str(gpu_id) for gpu_id in GPUS]
strategy = tf.distribute.MirroredStrategy(devices=devices)

print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))


BUFFER_SIZE = len(train_images)

BATCH_SIZE_PER_REPLICA = 64
GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync

EPOCHS = 10


train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)
test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)

train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)
test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)


def create_model():
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu'),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Conv2D(64, 3, activation='relu'),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10, activation='softmax')
    ])

  return model


with strategy.scope():
  # Set reduction to `none` so we can do the reduction afterwards and divide by
  # global batch size.
  loss_object = tf.keras.losses.CategoricalCrossentropy(
      from_logits=True,
      reduction=tf.keras.losses.Reduction.NONE)
  def compute_loss(labels, predictions):
    per_example_loss = loss_object(labels, predictions)
    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)


with strategy.scope():
  test_loss = tf.keras.metrics.Mean(name='test_loss')

  train_accuracy = tf.keras.metrics.CategoricalAccuracy(
      name='train_accuracy')
  test_accuracy = tf.keras.metrics.CategoricalAccuracy(
      name='test_accuracy')
  train_auc = tf.keras.metrics.AUC(name='train_auc')
  test_auc = tf.keras.metrics.AUC(name='test_auc')


# model, optimizer, and checkpoint must be created under `strategy.scope`.
with strategy.scope():
  model = create_model()

  optimizer = tf.keras.optimizers.Adam()


def train_step(inputs):
  images, labels = inputs

  with tf.GradientTape() as tape:
    predictions = model(images, training=True)
    loss = compute_loss(labels, predictions)

  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

  train_accuracy(labels, predictions)
  train_auc(labels, predictions)
  return loss

def test_step(inputs):
  images, labels = inputs

  predictions = model(images, training=False)
  t_loss = loss_object(labels, predictions)

  test_loss.update_state(t_loss)
  test_accuracy(labels, predictions)
  test_auc(labels, predictions)


# `run` replicates the provided computation and runs it
# with the distributed input.
@tf.function
def distributed_train_step(dataset_inputs):
  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))
  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,
                         axis=None)

@tf.function
def distributed_test_step(dataset_inputs):
  return strategy.run(test_step, args=(dataset_inputs,))


for epoch in range(EPOCHS):
  # TRAIN LOOP
  total_loss = 0.0
  num_batches = 0
  for x in train_dist_dataset:
    total_loss += distributed_train_step(x)
    num_batches += 1
  train_loss = total_loss / num_batches

  # TEST LOOP
  for x in test_dist_dataset:
    distributed_test_step(x)

  template = (""Epoch {}, Loss: {}, Accuracy: {}, AUC: {},""
              ""Test Loss: {}, Test Accuracy: {}, Test AUC: {}"")
  print (template.format(epoch+1,
                         train_loss, train_accuracy.result()*100, train_auc.result()*100,
                         test_loss.result(), test_accuracy.result()*100, test_auc.result()*100))

  test_loss.reset_states()
  train_accuracy.reset_states()
  test_accuracy.reset_states()
  train_auc.reset_states()
  test_auc.reset_states()
</code></pre>

<p>The problem is that AUC's evaluation is definitely wrong cause it exceeds its range (should be from 0-100) and i get theese results by running the above code for one time:</p>

<pre><code>Epoch 1, Loss: 1.8061423301696777, Accuracy: 66.00833892822266, AUC: 321.8688659667969,Test Loss: 1.742477536201477, Test Accuracy: 72.0999984741211, Test AUC: 331.33709716796875
Epoch 2, Loss: 1.7129968404769897, Accuracy: 74.9816665649414, AUC: 337.37017822265625,Test Loss: 1.7084736824035645, Test Accuracy: 75.52999877929688, Test AUC: 337.1878967285156
Epoch 3, Loss: 1.643971562385559, Accuracy: 81.83333587646484, AUC: 355.96209716796875,Test Loss: 1.6072628498077393, Test Accuracy: 85.3499984741211, Test AUC: 370.603759765625
Epoch 4, Loss: 1.5887378454208374, Accuracy: 87.27833557128906, AUC: 373.6204528808594,Test Loss: 1.5906082391738892, Test Accuracy: 87.13999938964844, Test AUC: 371.9998474121094
Epoch 5, Loss: 1.581775426864624, Accuracy: 88.0, AUC: 373.9468994140625,Test Loss: 1.5964380502700806, Test Accuracy: 86.68000030517578, Test AUC: 371.0227355957031
Epoch 6, Loss: 1.5764907598495483, Accuracy: 88.49166870117188, AUC: 375.2404479980469,Test Loss: 1.5832056999206543, Test Accuracy: 87.94000244140625, Test AUC: 373.41998291015625
Epoch 7, Loss: 1.5698528289794922, Accuracy: 89.19166564941406, AUC: 376.473876953125,Test Loss: 1.5770654678344727, Test Accuracy: 88.58000183105469, Test AUC: 375.5516662597656
Epoch 8, Loss: 1.564456820487976, Accuracy: 89.71833801269531, AUC: 377.8564758300781,Test Loss: 1.5792100429534912, Test Accuracy: 88.27000427246094, Test AUC: 373.1791687011719
Epoch 9, Loss: 1.5612279176712036, Accuracy: 90.02000427246094, AUC: 377.9949645996094,Test Loss: 1.5729509592056274, Test Accuracy: 88.9800033569336, Test AUC: 375.5257263183594
Epoch 10, Loss: 1.5562015771865845, Accuracy: 90.54000091552734, AUC: 378.9789123535156,Test Loss: 1.56815767288208, Test Accuracy: 89.3499984741211, Test AUC: 375.8636474609375
</code></pre>

<p>Accuracy is ok but it seems that it's the only one metric that behaves nice. I tried other metrics too but they are not evaluated correctly. It seems that the problems come when using more than one GPU, cause when I run this code with one GPU it produce the right results. </p>
",0
62467822,Implement ConvND in Tensorflow,"<p>So I need a ND convolutional layer that also supports complex numbers. So I decided to code it myself. </p>

<p>I tested this code on numpy alone and it worked. Tested with several channels, 2D and 1D and complex. However, I have problems when I do it on TF.</p>

<p>This is my code so far:</p>

<pre><code>def call(self, inputs):
    with tf.name_scope(""ComplexConvolution_"" + str(self.layer_number)) as scope:
        inputs = self._verify_inputs(inputs)            # Check inputs are of expected shape and format
        inputs = self.apply_padding(inputs)             # Add zeros if needed
        output_np = np.zeros(                           # I use np because tf does not support the assigment
            (inputs.shape[0],) +                        # Per each image
            self.output_size,                           # Image out size
            dtype=self.input_dtype                      # To support complex numbers
        )
        img_index = 0
        for image in inputs:
            for filter_index in range(self.filters):
                for i in range(int(np.prod(self.output_size[:-1]))):  # for each element in the output
                    index = np.unravel_index(i, self.output_size[:-1])
                    start_index = tuple([a * b for a, b in zip(index, self.stride_shape)])
                    end_index = tuple([a+b for a, b in zip(start_index, self.kernel_shape)])
                    # set_trace()
                    sector_slice = tuple(
                        [slice(start_index[ind], end_index[ind]) for ind in range(len(start_index))]
                    )
                    sector = image[sector_slice]
                    new_value = tf.reduce_sum(sector * self.kernels[filter_index]) + self.bias[filter_index]
                    # I use Tied Bias https://datascience.stackexchange.com/a/37748/75968
                    output_np[img_index][index][filter_index] = new_value  # The complicated line
                    img_index += 1
        output = apply_activation(self.activation, output_np)
    return output
</code></pre>

<p><code>input_size</code> is a tuple of shape (dim1, dim2, ..., dim3, channels). An 2D rgb conv for example will be (32, 32, 3) and <code>inputs</code> will have shape (None, 32, 32, 3).</p>

<p>The output size is calculated from an equation I found in this paper: <a href=""https://arxiv.org/abs/1603.07285"" rel=""nofollow noreferrer"">A guide to convolution arithmetic for deep learning</a></p>

<pre><code>out_list = []
for i in range(len(self.input_size) - 1):   # -1 because the number of input channels is irrelevant
    out_list.append(int(np.floor((self.input_size[i] + 2 * self.padding_shape[i] - self.kernel_shape[i]) / self.stride_shape[i]) + 1))
out_list.append(self.filters)
</code></pre>

<p>Basically, I use <code>np.zeros</code> because if I use <code>tf.zeros</code> I cannot assign the <code>new_value</code> and I get:
<code>TypeError: 'Tensor' object does not support item assignment</code></p>

<p>However, in this current state I am getting:<br>
<code>NotImplementedError: Cannot convert a symbolic Tensor (placeholder_1:0) to a numpy array.</code></p>

<p>On that same assignment. I don't see an easy fix, I think I should change the strategy of the code completely.</p>
",0
62469769,Custom layer uses function with @tf.custom_gradient throws error: decorator currently supports arguments only when eager execution is enabled,"<p>python == 3.7.6
Tensorflow = 2.0.0</p>

<p>I've created a custom layer which calls a function decorated with @tf.custom_gradient, very similar to the situation described by the answer under the following question:
<a href=""https://stackoverflow.com/questions/56657993/how-to-create-a-keras-layer-with-a-custom-gradient-in-tf2-0"">How to create a keras layer with a custom gradient in TF2.0?</a></p>

<p>But I keep getting this error at runtime: <code>ValueError: The custom_gradient decorator currently supports keywords arguments only when eager execution is enabled.</code></p>

<p>The function itself runs just fine when called in the main function. So to be explicit:</p>

<p><code>output = custom_function(input)</code> runs just fine with eager execution, but </p>

<pre><code>input_tensor = Input(shape=(1, input_length))
output_layer = Custom_Layer(...)(input_tensor)
model = Model([input_tensor], [output_layer])
model.compile(optimizer='rmsprop', run_eagerly=True, loss='mae', metrics=['accuracy'])
model.summary()
</code></pre>

<p>doesn't work at all. I've tried every combination of turning on and off eager execution, including inside the compile command as shown and inside the call function of the layer and at the top of the script using <code>tf.config.experimental_run_functions_eagerly(True)</code>, but nothing stops the error from appearing.</p>

<p>I cannot seem to find any documentation related to this occurrence.</p>
",1
62506281,How to get logits from Tensorflow pertained model?,"<p>I'm following the tutorial of Tensorflow/models/research/object_detection
My model loaded like this, same as tutorial</p>
<p><code>model = tf.saved_model.load(str(model_dir))</code></p>
<p>However, the output put of the model is like this:</p>
<p><code>{'num_detections': tf.float32, 'detection_scores': tf.float32, 'detection_classes': tf.float32, 'detection_boxes': tf.float32}</code></p>
<p>I want to run some postprocessing on the original logits, how can I get them?</p>
<p>Link to tutorial: <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/object_detection_tutorial.ipynb"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/object_detection_tutorial.ipynb</a></p>
",0
62582614,Error while importing img_to_array function: AttributeError: module 'tensorflow' has no attribute 'name_scope',"<p>So I am new to Keras and Tensorflow. I'm trying to import the <code>img_to_array</code> function and for some reason it is not importing it. I read the documentation it says <code>from PIL import Image</code> and after that use <code>tf.keras.preprocessing.image.img_to_array(img)</code> and that doesn't work either. I am using anaconda.</p>
<p>Here's the code:</p>
<pre><code>from keras.preprocessing.image import img_to_array
</code></pre>
<p>And here's the error:</p>
<pre><code>AttributeError: module 'tensorflow' has no attribute 'name_scope'
</code></pre>
",1
62611459,How to support mixed precision in custom Tensorflow layers?,"<p><strong>When developing my own custom layers for <code>tf.keras</code>: how am I supposed to support mixed precision?</strong></p>
<p>The <a href=""https://www.tensorflow.org/guide/mixed_precision"" rel=""nofollow noreferrer"">documentation of mixed precision</a> - a feature which is currently marked as experimental in Tensorflow 2.2 - only explains how to use it from a consumers perspective with predefined layers such as the <code>tf.keras.layers.Dense</code> one.</p>
<p>I already tried to guess it myself and found two - maybe relevant - details:</p>
<ul>
<li><p>The <code>dtype</code> property stays as <code>float32</code> by default when using 16-bit mixed precision.</p>
</li>
<li><p>There is a <code>mixed_precision.get_layer_policy(layer)</code> method (see <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/get_layer_policy"" rel=""nofollow noreferrer"">docs</a>) and a <code>mixed_precision.global_policy()</code> method (see <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/global_policy"" rel=""nofollow noreferrer"">docs</a>) which could be used to retrieve the configured <code>compute_dtype</code> and <code>variable_dtype</code>.</p>
</li>
</ul>
<p>Am I supposed to use the above <code>get_layer_policy</code>-method and just cast my variables into <code>compute_dtype</code> within the <code>call(...)</code> method of my layer? (And pass <code>variable_dtype</code> in my layers <code>build(...)</code> method to <code>add_weight(...)</code> when creating variables?)</p>
<p>For example, here is naive sample implementation of a standard dense neuron layer:</p>
<pre class=""lang-py prettyprint-override""><code>  def call(self, input):
    policy = mixed_precision.get_layer_policy(self)
    bias = tf.cast(self._bias, policy.compute_dtype)
    weights = tf.cast(self._weights, policy.compute_dtype)
    y = tf.nn.bias_add(tf.matmul(input, weights), bias)
    outputs = self._activation(y)
    return outputs
</code></pre>
<p>Sure, nobody would implement such basic stuff themselves, that one is just for demonstration. But, would this be the way the Tensorflow team expects us to implement the <code>call(...)</code> methods of our custom layers?</p>
",1
62668240,Keep getting ValueError while training on tf.data.datasets and include validation_data,"<p>Running the <code>model.fit(...)</code> method on my dataset. I keep getting errors like this:</p>
<pre><code>ValueError: in user code:
... 
ValueError: Could not find matching function to call loaded from the SavedModel. Got:
      Positional arguments (4 total):
        * Tensor(&quot;inputs:0&quot;, shape=(160, 160, 3), dtype=float32)
        * False
        * False
        * 0.99
      Keyword arguments: {}
</code></pre>
<p>However, I only get this error when I include <code>validatin_data</code> in my <code>model.fit(...)</code> call.
Any clarification about this would help. I do not mind sharing my Google Colab notebook to show how I construct my training and validation data with <code>tf.data.Dataset</code>.</p>
<p>My code can be found here, on <a href=""https://colab.research.google.com/drive/1l_aaV_pSSrGwslyUXy0BF1VTnKzRZuNs?usp=sharing"" rel=""nofollow noreferrer"">Google Colab</a>.</p>
",0
62670041,batch_size in tf model.fit() vs. batch_size in tf.data.Dataset,"<p>I have a large dataset that can fit in host memory. However, when I use tf.keras to train the model, it yields GPU out-of-memory problem. Then I look into tf.data.Dataset and want to use its batch() method to batch the training dataset so that it can execute the model.fit() in GPU. According to its documentation, an example is as follows:</p>
<pre><code>train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))
test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels))

BATCH_SIZE = 64
SHUFFLE_BUFFER_SIZE = 100

train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
test_dataset = test_dataset.batch(BATCH_SIZE)
</code></pre>
<p>Is the BATCH_SIZE in dataset.from_tensor_slices().batch() the same as the batch_size in the tf.keras modelt.fit()?</p>
<p>How should I choose BATCH_SIZE so that GPU has sufficient data to run efficiently and yet its memory is not overflown?</p>
",1
62752605,Loss function in tf.nn.sampled_softmax_loss,"<p>I have a question regarding Tensorflow:</p>
<p>Which loss function is used in <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss"" rel=""nofollow noreferrer""><code>tf.nn.sampled_softmax_loss</code></a>?</p>
<p>I believe it's <em><strong>cross-entropy</strong></em>, but it is not written on the official website. Can anyone confirm my guess?</p>
",1
62770686,Test the .trt file using tensorflow,"<p>Below <strong>output_saved_model_dir</strong> in this directory i am having trt file named <strong>final_model_gender_classification_gpu0_int8.trt</strong></p>
<pre><code>output_saved_model_dir='/home/cocoslabs/Downloads/age_gender_trt'
saved_model_loaded = tf.saved_model.load(output_saved_model_dir, tags=[tag_constants.SERVING])
</code></pre>
<p>When I run the above script it showing error as follows:</p>
<pre><code>File &quot;test.py&quot;, line 7, in &lt;module&gt;
    saved_model_loaded = tf.saved_model.load(output_saved_model_dir, tags=[tag_constants.SERVING])
  File &quot;/home/cocoslabs/deepstream_docker/venv/lib/python3.6/site-packages/tensorflow_core/python/saved_model/load.py&quot;, line 528, in load
    return load_internal(export_dir, tags)
  File &quot;/home/cocoslabs/deepstream_docker/venv/lib/python3.6/site-packages/tensorflow_core/python/saved_model/load.py&quot;, line 537, in load_internal
    saved_model_proto = loader_impl.parse_saved_model(export_dir)
  File &quot;/home/cocoslabs/deepstream_docker/venv/lib/python3.6/site-packages/tensorflow_core/python/saved_model/loader_impl.py&quot;, line 83, in parse_saved_model
    constants.SAVED_MODEL_FILENAME_PB))
OSError: SavedModel file does not exist at: /home/cocoslabs/Downloads/age_gender_trt/{saved_model.pbtxt|saved_model.pb}
</code></pre>
<p>From the above error what I understand is tf.saved_model.load() accept only .pb or .pbtxt files. Is it right ? But as per this link <a href=""https://stackoverflow.com/questions/60750727/load-and-run-test-a-trt-model/62769009#62769009"">Load and run test a .trt model</a> what they said is tf.saved_model.load() function will accept .trt file. Help me to rectify this error. Thank You.</p>
",0
62799237,TensorFlow: How to feed a dataset that doesn't fit into memory?,"<p><a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer""><code>DataSet</code> documentation</a> claims that it <code>Represents a potentially large set of elements.</code> as well as <code>Iteration happens in a streaming fashion, so the full dataset does not need to fit into memory.</code>.</p>
<p>I spent several hours in the <a href=""https://www.tensorflow.org/guide/data"" rel=""nofollow noreferrer"">official docs</a> trying to find out how to feed a large dataset in a streaming fashion. No success. All examples use either <code>from_tensor_slices</code> or generator, both methods are not recommended by TensorFlow themselves because of <code>2GB limit for the tf.GraphDef protocol buffer</code> and <code> it has limited portability and scalability. It must run in the same python process that created the generator, and is still subject to the Python GIL.</code>.</p>
<p>The only documented way I found to make it work in a streaming fashion is <a href=""https://www.tensorflow.org/tutorials/load_data/tfrecord"" rel=""nofollow noreferrer"">to use <code>TFRecord</code></a>. It allows to stream file by file. The only huge problem: my feature input shape is <code>(200000, 2)</code> (all float32). To flatten and convert it to <code>FloatList</code> is not an option because I feed it to Conv1D with 2 parallel sequences.</p>
<p>It feels like either I overlooked something, or it's just poorly documented, or <code>tf.Dataset</code> doesn't do what it claims to do.</p>
<p>Is there a way to make DataSet work in a streaming fashion for a large set of elements?</p>
",1
62818943,Tensorflow pretrained models input channel range,"<p>I came across this <a href=""https://www.tensorflow.org/tutorials/images/transfer_learning"" rel=""nofollow noreferrer"">example</a> which implements a pretrained model. It says:</p>
<blockquote>
<p>Format the Data</p>
<p>Use the tf.image module to format the images for the task.</p>
<p>Resize the images to a fixed input size, and rescale the input
channels to a range of [-1,1]</p>
</blockquote>
<pre><code>IMG_SIZE = 160 # All images will be resized to 160x160

def format_example(image, label):
  image = tf.cast(image, tf.float32)
  image = (image/127.5) - 1
  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
  return image, label
</code></pre>
<p>I was wondering about this. What I understand is that <code>image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))</code> resizes the images (which can have any size) to one consistent size. I understand that <code>image = (image/127.5) - 1</code> does not change the actual size of the images, but changes the values (pixels) (which are between 0 to 255) to a range of [-1,1]. In other examples I saw normalization/standardization being done to a range of [0,1], so rescaling by 1.0/255. I do not understand when I have to use which. If I use my own model, it is up to me to scale to a range of [-1,1] or [0,1]? However, when I use a pretrained model I need to know what is required. I googled the mobilenetv2 model, but could not find any documentation telling me that the required input channel is [-1,1]. In this <a href=""https://github.com/tensorflow/tensorflow/issues/35336#issuecomment-568572711"" rel=""nofollow noreferrer"">comment</a> it says all pretrained tensorflow models require an input channel of [-1,1]. Is that true? Especially, is that true that all models in the <a href=""https://tfhub.dev/"" rel=""nofollow noreferrer"">tensorflow hub</a> (if about images) require a range of [-1,1]?</p>
<p>Finally, how do I find out what the required range is for a pretrained model? I would not have figured out the [-1,1] in case of MobileNetv2 by my own. On the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNetV2"" rel=""nofollow noreferrer"">tensorflow MobileNetv2</a> page I could not find this information.</p>
<p>Furthermore: Is there a way to basically have this done automatically? So that I use a function and it automatically checks the pretrained tensorflow dataset (which has an object storing that information) and applies it (assuming 0-255 is my input)? I think <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v2/preprocess_input"" rel=""nofollow noreferrer"">tf.keras.applications.mobilenet_v2.preprocess_input</a> is doing something else (I am not really understanding what it does)? And it is also just for mobilenetv2.</p>
",0
62863208,Loading PNG image to,"<p>I'm looking for an efficient method to load PNG files as a TensorFlow 2.x date set. There are multiple answers here which all are using Tensorflow 1.x syntax. I also checked the <a href=""https://www.tensorflow.org/tutorials/load_data/images"" rel=""nofollow noreferrer"">official Tensorflow tutorial</a> but they are using <code>tf.keras.preprocessing.image_dataset_from_directory</code> API which is only available in tf-nightly.</p>
<p>Is there any API for Tensorflow 2.x capable of loading images from a folder, assigning the subfolder as the label, and return a TensorFlow dataset?</p>
",0
62911595,AssertionError: assert t.graph == cond_graph when generating heatmap for LocallyConnected2D layer with keras,"<p>I'm trying to create a heatmap of the last <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/LocallyConnected2D"" rel=""nofollow noreferrer"">LocallyConnected2D</a> layer (from tf.keras.layers, so no own implementation) of a publicly available <a href=""https://github.com/swghosh/DeepFace/blob/master/deepface/deepface.py"" rel=""nofollow noreferrer"">implementation</a> of <a href=""https://research.fb.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/"" rel=""nofollow noreferrer"">deepface</a> (face recogntion/verification model) in Keras.</p>
<p>The heatmap I am  generating with this example <a href=""https://keras.io/examples/vision/grad_cam/"" rel=""nofollow noreferrer"">implementation</a> from the keras website. The example works for me, but when I try to obtain the heatmap of the deepface model, I get an error which I don't get.</p>
<pre><code>---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
&lt;ipython-input-12-7ff17cbeefd4&gt; in &lt;module&gt;
----&gt; 1 heatmap = make_gradcam_heatmap(img, model, last_conv_layer_name, classifier_layer_names)
      2 
      3 plt.matshow(heatmap)
      4 plt.show()

&lt;ipython-input-10-3b61c4411dd2&gt; in make_gradcam_heatmap(img_array, model, last_conv_layer_name, classifier_layer_names)
     30     # This is the gradient of the top predicted class with regard to
     31     # the output feature map of the last conv layer
---&gt; 32     grads = tape.gradient(top_class_channel, last_conv_layer_output)
     33 
     34     # This is a vector where each entry is the mean intensity of the gradient

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\eager\backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)
   1046         output_gradients=output_gradients,
   1047         sources_raw=flat_sources_raw,
-&gt; 1048         unconnected_gradients=unconnected_gradients)
   1049 
   1050     if not self._persistent:

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\eager\imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)
     75       output_gradients,
     76       sources_raw,
---&gt; 77       compat.as_str(unconnected_gradients.value))

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\eager\backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)
    155       gradient_name_scope = &quot;gradient_tape/&quot;
    156     with ops.name_scope(gradient_name_scope):
--&gt; 157       return grad_fn(mock_op, *out_grads)
    158   else:
    159     return grad_fn(mock_op, *out_grads)

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\ops\cond_v2.py in _IfGrad(op, *grads)
    169   # Resolve references to forward graph tensors in grad graphs and ensure
    170   # they are in-scope, i.e., belong to one of outer graphs of the grad graph.
--&gt; 171   true_grad_inputs = _resolve_grad_inputs(true_graph, true_grad_graph)
    172   false_grad_inputs = _resolve_grad_inputs(false_graph, false_grad_graph)
    173 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\ops\cond_v2.py in _resolve_grad_inputs(cond_graph, grad_graph)
    410     # `cond_graph`.
    411     if t.graph != grad_graph.outer_graph:
--&gt; 412       assert t.graph == cond_graph
    413       # `internal_captures` are not treated as intermediates and hence not added
    414       # to If op outputs. So we get the outer tensor corresponding to those

AssertionError: 
</code></pre>
<p>Does anyone maybe know what this error means and how I can solve it?
Thank you for your time!</p>
<p>Python version: 3.7.7 <br />
Keras version: 2.3.1 <br />
Tensorflow version: 2.2.0</p>
",0
62956096,Is it possible to extract trained class names from tflite model?,"<p>I have tried to search everywhere, tried everything in <code>tflite_interpreter = tf.lite.Interpreter(model_path='model.tflite')</code>, read tflite documentation but I cannot find the method to extract the class names from the model.</p>
<p>Is it possible?</p>
",1
62962147,TensorFlow - Fashion MNIST Steps Per Epoch,"<p>I'm working with the Kera's Fashion MNIST dataset. When I fit my model, I noticed to complete one epoch it would have to go through 1500 steps.</p>
<pre><code>history = model.fit(x_train, y_train, epochs=30, validation_split=0.2)

Epoch 3/30
1500/1500 [==============================] - 3s 2ms/step - loss: 0.4494 - sparse_categorical_accuracy: 0.8438 - val_loss: 0.4691 - val_sparse_categorical_accuracy: 0.8308
Epoch 4/30
964/1500 [==================&gt;...........] - ETA: 0s - loss: 0.4294 - sparse_categorical_accuracy: 0.8504
</code></pre>
<p>I was looking at the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">docs</a> for the fit function, but couldn't understand why the default steps were set to 1500. I understand when the <code>steps_per_epoch</code> is <code>None</code> the behavior is dependent on the data type of the dataset, but how can I check if the data type is a tensor or tf.data?</p>
",1
62988423,Resetting graph in Tensorflow 2,"<p>In my use case, I have some time series data where at each time <code>t</code>, I train a new model over a rolling window. In tensorflow 1, I had to do the following otherwise models will accumulate in the default graph and essentially leak memory.</p>
<pre><code>import tensorflow as tf
import keras.backend as K
...
tf.reset_default_graph()
K.clear_session()
</code></pre>
<p>In tensorflow 2, I've found equivalent functions <a href=""https://www.tensorflow.org/api_docs/python/tf/compat/v1/reset_default_graph"" rel=""nofollow noreferrer"">tf.compat.v1.reset_default_graph()</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session"" rel=""nofollow noreferrer"">tf.keras.backend.clear_session()</a>. However, from the <a href=""https://www.tensorflow.org/guide/migrate"" rel=""nofollow noreferrer"">documentation</a>, TF2 ties graph variables to python variables so theoretically if a python variable is destroyed, the graph variable should also be destroyed. Is this interpretation correct? I've tried putting model creation code in a loop, whilst memory usage still grows, it wasn't the sort of explosion I've witnessed in TF1.</p>
",1
62994289,Saving TFrecords with TPU,"<p>I'm trying to use <code>tf.data.experimental.TFRecordWriter</code> to save dataset on Google cloud bucket using TPU. The code from the example in documentation works:</p>
<pre><code>dataset = tf.data.Dataset.range(3)
dataset = dataset.map(tf.io.serialize_tensor)
writer = tf.data.experimental.TFRecordWriter(&quot;gs://oleg-zyablov/test.tfrec&quot;)
writer.write(dataset)
</code></pre>
<p>But I have dataset of tuples (string, int64), where first is jpg-encoded image and second is label. When I pass it to writer.write() method, it says: 'tuple' object has no attribute 'is_compatible_with'.</p>
<p>I guess I have to pack image and label into tf.train.Example to make it work. I use the following code:</p>
<pre><code>def serialize(image, class_idx):
  tfrecord = tf.train.Example(features = tf.train.Features(feature = {
    'image': tf.train.Feature(bytes_list = tf.train.BytesList(value = [image.numpy()])),
    'class': tf.train.Feature(int64_list = tf.train.Int64List(value = [class_idx.numpy()]))
  }))
  return tfrecord.SerializeToString()

#saving_pipeline is a dataset of (string, int64) tuples
saving_pipeline_serialized = saving_pipeline.map(serialize)

writer = tf.data.experimental.TFRecordWriter(&quot;gs://oleg-zyablov/car-classification/train_tfrecords/test.tfrecord&quot;)
writer.write(saving_pipeline_serialized)
</code></pre>
<p>But I get the following error:</p>
<pre><code>'Tensor' object has no attribute 'numpy'
</code></pre>
<p>Although I didn't turn off eager mode and this code <code>tf.constant([], dtype = float).numpy()</code> works. Maybe TPU works not in eager mode? Ok, I changed .numpy() to .eval() in the code above. Then I get the foloowing error:</p>
<pre><code>Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`
</code></pre>
<p>What session does TPU use and how do I specify it? When I run the code below:</p>
<pre><code>with tf.compat.v1.Session():
  saving_pipeline_serialized = saving_pipeline.map(serialize)
</code></pre>
<p>I get an error:</p>
<pre><code>Cannot use the default session to evaluate tensor: the tensor's graph is different from the session's graph. Pass an explicit session to `eval(session=sess)`.
</code></pre>
<p>But I don't know how to get the current graph and pass it to tf.compat.v1.Session(). When I go another way and type:</p>
<pre><code>image.eval(session = tf.compat.v1.get_default_session())
</code></pre>
<p>It says:</p>
<pre><code>Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`
</code></pre>
<p>Is it possible to use .eval() on TPU? Or how do I perform my task another way?</p>
",1
63004540,How to pad 1 dimensinal vector in tensorflow? Getting InvalidArgumentError: paddings must be a matrix with 2 columns with tf.pad,"<p>I am trying to use tf.pad. Here is my attempt to pad the tensor to length 20, with values 10.</p>
<pre><code>tf.pad(tf.constant([1, 2, 3, 45]), paddings=20, constant_values=10)
</code></pre>
<p>I get this error message</p>
<pre><code>InvalidArgumentError: paddings must be a matrix with 2 columns: [2,1] [Op:PadV2]
</code></pre>
<p>I am looking at the documentation</p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/pad"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/pad</a></p>
<blockquote>
<p>paddings is an integer tensor with shape [n, 2], where n is the rank of tensor. For each dimension D of input, paddings[D, 0] indicates how many values to add before the contents of tensor in that dimension, and paddings[D, 1] indicates how many values to add after the contents of tensor in that dimension</p>
</blockquote>
<p>But I am unable to figure out how to shape the pad value</p>
",1
63014556,"Cannot convert a list of ""strings"" to a tf.Dataset.from_tensor_slicer() - ValueError: Can't convert non-rectangular Python sequence to Tensor","<p>I have the following data:</p>
<pre class=""lang-py prettyprint-override""><code>partial_x_train_features = [
    [b'south pago pago victor mclaglen jon hall frances farmer olympe bradna gene lockhart douglass dumbrille francis ford ben welden abner biberman pedro cordoba rudy robles bobby stone nellie duran james flavin nina campana alfred e green treasure hunt adventure adventure'],
    [b'easy virtue jessica biel ben barnes kristin scott thomas colin firth kimberley nixon katherine parkinson kris marshall christian brassington charlotte riley jim mcmanus pip torrens jeremy hooton joanna bacon maggie hickey georgie glen stephan elliott young englishman marry glamorous american brings home meet parent arrive like blast future blow entrenched british stuffiness window comedy romance'],
    [b'fragments antonin gregori derangere anouk grinberg aurelien recoing niels arestrup yann collette laure duthilleul david assaraf pascal demolon jean baptiste iera richard sammel vincent crouzet fred epaud pascal elso nicolas giraud michael abiteboul gabriel le bomin psychiatrist probe mind traumatized soldier attempt unlock secret drove gentle deeply disturbed world war veteran edge insanity drama war'],
    [b'milka film taboos milka elokuva tabuista irma huntus leena suomu matti turunen eikka lehtonen esa niemela sirkka metsasaari tauno lehtihalmes ulla tapaninen toivo tuomainen hellin auvinen salmi rauni mollberg small finnish lapland community milka innocent year old girl live mother miss dead father prays god love haymaking employ drama'],
    [b'sleeping car david naughton judie aronson kevin mccarthy jeff conaway dani minnick ernestine mercer john carl buechler gary brockette steve lundquist billy stevenson michael scott bicknell david coburn nicole hansen tiffany million robert ruth douglas curtis jason david naughton move abandon train car resurrect vicious ghost landlady dead husband mister near fatal encounter comedy horror']]

partial_x_train_plot = [[b'treasure hunt adventure'],
                        [b'young englishman marry glamorous american brings home meet parent arrive like blast future blow entrenched british stuffiness window'],
                        [b'psychiatrist probe mind traumatized soldier attempt unlock secret drove gentle deeply disturbed world war veteran edge insanity'],
                        [b'small finnish lapland community milka innocent year old girl live mother miss dead father prays god love haymaking employ'],
                        [b'jason david naughton move abandon train car resurrect vicious ghost landlady dead husband mister near fatal encounter']]

partial_x_train_actors_array = [[b'victor mclaglen', b'jon hall', b'frances farmer',
                                 b'olympe bradna', b'gene lockhart', b'douglass dumbrille',
                                 b'francis ford', b'ben welden', b'abner biberman',
                                 b'pedro de cordoba', b'rudy robles', b'bobby stone',
                                 b'nellie duran', b'james flavin', b'nina campana'],
                                [b'jessica biel', b'ben barnes', b'kristin scott thomas',
                                 b'colin firth', b'kimberley nixon', b'katherine parkinson',
                                 b'kris marshall', b'christian brassington', b'charlotte riley',
                                 b'jim mcmanus', b'pip torrens', b'jeremy hooton', b'joanna bacon',
                                 b'maggie hickey', b'georgie glen'],
                                [b'gregori derangere', b'anouk grinberg', b'aurelien recoing',
                                 b'niels arestrup', b'yann collette', b'laure duthilleul',
                                 b'david assaraf', b'pascal demolon', b'jean-baptiste iera',
                                 b'richard sammel', b'vincent crouzet', b'fred epaud',
                                 b'pascal elso', b'nicolas giraud', b'michael abiteboul'],
                                [b'irma huntus', b'leena suomu', b'matti turunen',
                                 b'eikka lehtonen', b'esa niemela', b'sirkka metsasaari',
                                 b'tauno lehtihalmes', b'ulla tapaninen', b'toivo tuomainen',
                                 b'hellin auvinen-salmi'],
                                [b'david naughton', b'judie aronson', b'kevin mccarthy',
                                 b'jeff conaway', b'dani minnick', b'ernestine mercer',
                                 b'john carl buechler', b'gary brockette', b'steve lundquist',
                                 b'billy stevenson', b'michael scott-bicknell', b'david coburn',
                                 b'nicole hansen', b'tiffany million', b'robert ruth']]

partial_x_train_reviews = [
    [b'edward small take director alfred e green cast crew uncommonly attractive brilliant assemblage south sea majority curiously undersung piece location far stylize date goldwyn hurricane admittedly riddle cliche formula package visual technical excellence scarcely matter scene stop heart chiseled adonis jon hall porcelain idol frances farmer outline profile s steam background volcano romantic closeup level defies comparison edward small film typically string frame individual work art say outdid do workhorse composer edward ward song score year prior work universal stun phantom opera'],
    [b'jessica biel probably best know virtuous good girl preacher kid mary camden heaven get tackle classic noel coward role early play easy virtue american interloper english aristocratic family unsettle family matriarch kristin scott thomas noel coward write upper class twit pretension wit keep come kind adopt way adopt oscar wilde george bernard shaw kid grow poverty way talent entertain upper class take coward heart felt modern progressive generally term social trend whittakers easy virtue kind aristocrat anybody like hang party invite noel entertain amelia earhart aviation jessica biel character auto race young widow detroit area course area motor car auto race fresh win monte carlo win young ben barnes heir whittaker estates lot land debt barnes bring biel home family mortify classless american way sense recognize class distinction thing get rid title nobility aristocrats story scott thomas dominate family try desperately estate husband colin firth serve world war horror do probably horror trench war slaughter fact class distinction tend melt combat biel kind like wife rule whittaker roost scandal past threatens disrupt barnes biel marriage form crux story turn fact end really viewer figure eventually happen second film adaption easy virtue silent film direct young alfred hitchcock easy virtue actually premier america london star great american stage actress jane cowl guess coward figure american heroine best american theatergoer british one version easy virtue direct flawlessly stephen elliot fine use period music noel coward cole porter end credit really mock upper class coward tradition play going gets tough tough going believe elliott try say class especially one right stuff course obligatory fox hunt upper class indulge oscar wilde say unspeakable uneatable chance younger generation expose noel coward worth see'],
    [b'saw night eurocine event movie european country show day european city hear le bomin barely hear derangere la chambre des officiers fortunately surprise discover great talent unknown large audience derangere absolutely astonish play character antonin verset victim post wwi trauma live trouble scene endure month war cast excellent great work cinematography offer really nice shot great landscape stun face edit really subtile bit memory make sense story minute movie show real chill ww archive action flick like sensitive psychologic movie really think absolutely recommend les fragments d antonin let le bomin'],
    [b'rauni mollberg earth sinful song favorite foreign film establish director major talent film festival circuit get amazing followup milka base work novelist timo mukka till worthy major dvd exposure unlike kaurismaki bros follow double handedly create tongue cheek deadpan finnish film style fan world mollberg commit naturalistic approach film overflow nature life lust earthiness find scandi cinema mainly work famous talent swede vilgot sjoman curious yellow fame director film tabu title imply mollberg effort quite effective sidestep fully treat screen theme incest making adult character father figure real blood relate daddy applies usual merely step father gimmick use countless time american movie incest work matti turunen kristus perkele translate christ devil really common law step dad underage milka beautiful offbeat fashion young girl portray shot irma huntus bring screen sexiness bergman harriet andersson decade earlier create international success summer monika sawdust tinsel imagine actress milka role shame do pursue act career afterward completing strong line leena suomu earth mother type confines act narrow emotional range prove solid rock crucial role bookended spectacularly beautiful shot birch wood winter virtually black white visually color presence milka film quickly develop nature theme presence strange click beak bird talisman early scene milka handyman turunen frolicking naked lake emerge oh natural sex play year old milka man result tastefully shoot intimacy imply ejaculation set trouble come religious aspect remote farm community heavily stress especially enjoy motif spiritual guidance cantor malmstrom quality anti stereotypical play eikka lehtonen instead rigid cruel turn care milka illegitimate baby bear strong romance turunen stud continue service mom woman neighborhood present utterly natural viewer position watch ethnographic exercise moralistic tale powerful technique milka frequently speak directly camera viewer forceful monologue bear crisp sound record sound nature include rain constant motif make milka engross experience view film subtitle knowledge finnish lapp recall best silent era classic direction strong convey dramatic content theme way transcend language kudos mollberg talented cinematographer job work remain obscurity ripe rediscovery'],
    [b'wonder horror film write woody allen wannabe come like check imaginatively direct typical enjoyable haunt place premise solid makeup effect good job major flaw dialogue overload cheeky wisecrack witticisms sample want scary shopping ex wife hit mark deliver inappropriate moment hero battle evil ghost']]

partial_y_train = [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                   [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],
                   [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0],
                   [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                   [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]  # multilabel classification
</code></pre>
<p>And I want to transform them into <code>Tensor</code>s with the <code>tf.Dataset.from_tensor_slices()</code> method like below:</p>
<pre class=""lang-py prettyprint-override""><code>partial_x_train_features_tensor=tf.data.Dataset.from_tensor_slices((partial_x_train_features, partial_y_train))
partial_x_train_plot_tensor=tf.data.Dataset.from_tensor_slices((partial_x_train_plot, partial_y_train))
partial_x_train_reviews_tensor=tf.data.Dataset.from_tensor_slices((partial_x_train_reviews, partial_y_train))
partial_x_train_actors_array=tf.data.Dataset.from_tensor_slices((partial_x_train_actors_array, partial_y_train))
</code></pre>
<p>But I get the following error:</p>
<pre><code>ValueError: Can't convert non-rectangular Python sequence to Tensor
</code></pre>
<p>I know that actors are not equally sized arrays but searching on a couple of similar questions (i.e. <a href=""https://stackoverflow.com/questions/56304986/valueerror-cant-convert-non-rectangular-python-sequence-to-tensor"">question1</a>, <a href=""https://stackoverflow.com/questions/61334069/converting-a-list-of-unequally-shaped-arrays-to-tensorflow-2-dataset-valueerror"">question2</a>) couldn't resolve my problem.</p>
<p>Please also follow my <a href=""https://colab.research.google.com/drive/1oOZuQAZbGuZeZ1V3ErtzHipUa0UdyZSf?usp=sharing"" rel=""nofollow noreferrer"">colab notebook</a> if you want to replicate the issue and please write in the comments if I missed any duplicate question.</p>
",0
63020800,"Understanding Tensorflow Object-Detection API, kwargs for Checkpoint class, what is `_base_tower_layers_for_heads`?","<p>Currently, I've been learning how to use Object-Detection API from Tensorflow. I follow a quick start tutorial for training with custom data with <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb"" rel=""nofollow noreferrer"">this notebook</a> as suggested by them. In the effort to understanding each line of the code, I stumbled upon this snippet code in the &quot;Create Model and Restore Weight&quot; part.</p>
<pre><code>fake_box_predictor = tf.compat.v2.train.Checkpoint(
    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,
    # _prediction_heads=detection_model._box_predictor._prediction_heads,
    #    (i.e., the classification head that we *will not* restore)
    _box_prediction_head=detection_model._box_predictor._box_prediction_head,
    )
</code></pre>
<p>I don't really understand what are the keyword arguments that are available for the <code>Checkpoint</code> class in that particular snippet code. My question is; is there any documentation out there that shows the list of the keyword arguments? or at least explain what are <code>_base_tower_layers_for_heads</code> and<code>_box_prediction_head</code>?</p>
<p>I've read the <code>tf.train.Checkpoint</code> <a href=""https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint"" rel=""nofollow noreferrer"">documentation</a>. It says that we can provide <code>models</code> or <code>optimizers</code> for the constructor's keyword argument. I am already familiar with this class to restore the weights to my model, however, I find it is alien to see <code>_base_tower_layers_for_heads</code> or <code>_box_prediction_head</code> for the keyword argument.</p>
<p>I do know about 'heads' and different types of 'heads' in the object detection architecture and their relation to transfer learning, what I don't understand is in the context of their data structure. How do I know, these keyword arguments exist? and is there any other else? I would really appreciate it if somebody could give me insights or at least tell me where can I find documentation that I can read to understand it more.</p>
",1
63064796,how to manage batches for model.provide_groundtruth,"<p>I'm trying to use TensorFlow 2 Object Detection API with a custom dataset for multi classes to train an SSD, I took as base the example provide by the documentation: <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb</a>
My current problem is when I start the fine tuning:</p>
<blockquote>
<p>InvalidArgumentError: The first dimension of paddings must be the rank
of inputs[2,2] [6] [Op:Pad]</p>
</blockquote>
<p>That seems to be related with the section of model.provide_groundtruth on train_step_fn, as I mention I took my data from a TensorFlow record, I mapped this to a dataset and divide it into batches using padded_batches(tf.data.TFRecordDataset) seems that this is the correct to feed the training with the image but now my problem is the groundtruth because this now is also converted to batches [batch_size,num_detections,coordinate_bbox], is this the problem? any idea on how to fix this issue.
Thanks</p>
<p>P.S. I tried to used the version of modified the pipeline.config file and run the model_main_tf2.py as was in the past with TensorFlow 1 but this method is buggy.</p>
",0
63122813,Can not set Momentum of optimizer,"<p>I am using the SGD optimizer and want to set the momentum after initialization similar to learning rate scheduling by using <code>tf.keras.backend.set_value(optimizer.momentum, momentumValue)</code>: <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_value"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_value</a></p>
<p>However, all I get is following error:</p>
<pre><code>AttributeError: 'LossScaleOptimizer' object has no attribute 'momentum'
</code></pre>
<p>Is there any way to set the momentum?
This is important for implementing the 1Cycle Policy as the momentum should also cycle, but I can´t believe there is no way to adjust the momentum in keras after initialization.</p>
",0
63140320,How to use sequence/generator on tf.data.Dataset object to fit partial data into memory?,"<p>I am doing image classification with Keras on Google Colab. I load images with the tf.keras.preprocessing.image_dataset_from_directory() function (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory</a>) which returns a tf.data.Dataset object:</p>
<pre><code>train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset=&quot;training&quot;,
  seed=1234,
  image_size=(img_height, img_width),
  batch_size=batch_size,
  label_mode=&quot;categorical&quot;)
</code></pre>
<p>I found that when the data contains thousands of images, model.fit() will use all memory after training a number of batches (I am using Google Colab and can see RAM usage grow during the first epoch).
Then I try to use Keras Sequence, which is a suggested solution of loading partial data into RAM (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence</a>):</p>
<pre><code>  class DatasetGenerator(tf.keras.utils.Sequence):
      def __init__(self, dataset):
          self.dataset = dataset

      def __len__(self):
          return tf.data.experimental.cardinality(self.dataset).numpy()

      def __getitem__(self, idx):
          return list(self.dataset.as_numpy_iterator())[idx]
</code></pre>
<p>And I train the model with:</p>
<p><code>history = model.fit(DatasetGenerator(train_ds), ...)</code></p>
<p>The problem is that <strong>getitem</strong>() must return a batch of data with index. However, the list() function I use has to put the whole dataset into RAM and thus hit memory limit when a DatasetGenerator object instantiates (tf.data.Dataset object does not support indexing with []).</p>
<p>My questions:</p>
<ol>
<li>Is there any way to implement <strong>getitem</strong>() (get a specific batch from the dataset object) without putting the whole object into memory?</li>
<li>If item 1 is not possible, is there any workaround?</li>
</ol>
<p>Thanks in advance!</p>
",0
63146831,What is the analytic interpretation for Tensorflow custom gradient?,"<p>In the official <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer"">tf.custom_gradient</a> documentation it shows how to define custom gradients for <code>log(1 + exp(x))</code></p>
<pre class=""lang-py prettyprint-override""><code>@tf.custom_gradient
def log1pexp(x):
  e = tf.exp(x)
  def grad(dy):
    return dy * (1 - 1 / (1 + e))
  return tf.math.log(1 + e), grad
</code></pre>
<p>When <code>y = log(1 + exp(x))</code>, analytically the derivative comes out to be <code>dy/dx = (1 - 1 / (1 + exp(x)))</code>.</p>
<p>However in the code <code>def grad</code> says its <code>dy * (1 - 1 / (1 + exp(x)))</code>.
<code>dy/dx = dy * (1 - 1 / (1 + exp(x)))</code> is not a valid equation. While <code>dx = dy * (1 - 1 / (1 + exp(x)))</code> is wrong as it should be the reciprocal.</p>
<p>What does the <code>grad</code> function equate to?</p>
",1
63158314,Tensorflow 2.3.0 - Warning: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version,"<p>I've just updated to TF-2.3. In a model using <code>tf.data.Dataset.from_tensor_slices</code> as data source, I get the folowing warning:</p>
<pre><code>WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Iterator.get_next_as_optional()` instead.
</code></pre>
<p>I didn't find the instructions on the documentation on how to use the updated methods.</p>
<pre><code>train_dataset = tf.data.Dataset.from_tensor_slices(
    (
        {&quot;input_1&quot;: x1_train, 
         &quot;input_2&quot;: x2_train}, 
        {&quot;output&quot;: y_train},
    )
)

train_batches = train_dataset.batch(GLOBAL_BATCH_SIZE)
</code></pre>
<p>Training:</p>
<pre><code>history = model.fit(
    x = train_batches,
    epochs=30,
    verbose = 1,
)
</code></pre>
<p>Thanks in advance.</p>
",1
63359268,TensorFlow 2: Re-saving a SavedModel?,"<p>I am trying to load a model saved in <a href=""https://www.tensorflow.org/guide/saved_model"" rel=""nofollow noreferrer"">SavedModel</a> format, and then apply some calculations on top of it and re-save the whole pipeline. A minimal code is the following (from <a href=""https://www.kaggle.com/camaskew/host-baseline-example"" rel=""nofollow noreferrer"">this Kaggle kernel</a>):</p>
<pre class=""lang-python prettyprint-override""><code># Load the model. After that, the `delg_model` will be of the type
# `tensorflow.python.training.tracking.tracking.AutoTrackable`
delg_model = tf.saved_model.load('path/to/saved/model/dir')

# we don't need the whole model, so we prune it. After that, the
# `global_feature_extraction_fn` will be of the type
# `tensorflow.python.eager.wrap_function.WrappedFunction`
delg_input_tensor_names = ['input_image:0', 'input_scales:0']
global_feature_extraction_fn = delg_model.prune(
    delg_input_tensor_names, ['global_descriptors:0'])
</code></pre>
<blockquote>
<p><strong>Question:</strong> Now, I want to save the <code>global_feature_extraction_fn</code>, with some other TF ops to post-process the output, in the same SavedModel format. What is the correct way to do that?</p>
</blockquote>
<hr />
<h2>What I have tried</h2>
<p>I have tried to follow the TensorFlow documentation of <a href=""https://www.tensorflow.org/guide/saved_model#saving_a_custom_model"" rel=""nofollow noreferrer"">saving custom models to SavedModel format</a> and define a <code>tf.Module</code>:</p>
<pre class=""lang-python prettyprint-override""><code>class DelgModule(tf.Module):
    def __init__(self):
        super().__init__()

    @tf.function(input_signature=[
        tf.TensorSpec(shape=[None, None, 3], name='input_image')
    ])
    def call(self, input_tensor):
        # custom function on top of the model's output
        embedding = tf.nn.l2_normalize(
            global_feature_extraction_fn(
                input_tensor,                         # input_image
                tf.convert_to_tensor([0.7, 1.0, 1.4]) # input_scales
            )[0],
            axis=1, name='l2_normalized_output')
        return output_tensors = {
            'global_descriptor': embedding
        }

delg_module = DelgModule()
</code></pre>
<p>Then I ran it on the test image to build the <code>tf.function</code> and make sure that it works correctly (produces the right output). But when I tried to save it as follows:</p>
<pre class=""lang-python prettyprint-override""><code>tf.saved_model.save(
    delg_module, export_dir='./delg_resaved',
    signatures={
        'serving_default': delg_module.call
    })
</code></pre>
<p>the resulting model was incorrect. The original one weighted 90 MB while the one in <code>delg_resaved</code> weights only 800 KB. I also tried doing <code>tf.saved_model.load</code> <strong>inside</strong> the <code>DelgModule.call</code> function, so that the graph creation and variables loading is done entirely inside that <code>tf.function</code>, but the results remained the same.</p>
",0
63383594,How does Tensorflow build() work from tf.keras.layers.Layer,"<p>I was wondering if anyone knew how the <code>build()</code> function works from the <code>tf.keras.layers.Layer</code> class under the hood. According to the <a href=""https://www.tensorflow.org/tutorials/customization/custom_layers"" rel=""noreferrer"">documentation</a>:</p>
<blockquote>
<p><em>build is called when you know the shapes of the input tensors and can
do the rest of the initialization</em></p>
</blockquote>
<p>so to me it seems like the class is behaving similar to this:</p>
<pre><code>class MyDenseLayer:
  def __init__(self, num_outputs):
    self.num_outputs = num_outputs

  def build(self, input_shape):
    self.kernel = self.add_weight(&quot;kernel&quot;,
                                  shape=[int(input_shape[-1]), self.num_outputs])

  def __call__(self, input):
    self.build(input.shape) ## build is called here when input shape is known
    return tf.matmul(input, self.kernel)
</code></pre>
<p>I can't imagine <code>build()</code> would be called for ever <code>__call__</code>, but it is the only place where the input is passed in. Does anyone know how exactly this works under the hood?</p>
",1
63390396,Customise train_step in model.fit() Tensorflow Keras - InvalidArgumentError: Operation 'while' has no attr named '_XlaCompile',"<p>I am trying to implement a custom multi-input and -output model which uses a learning algorithm as proposed in <a href=""https://arxiv.org/pdf/1801.07593.pdf"" rel=""nofollow noreferrer"">this</a> paper. The model itself works fine without the custom learning algorithm which I use as a baseline. The problem I encounter is that the code got stuck in the train_step function in the DebiasModel class at code line:</p>
<pre><code>mc_pred = self.main_classifier([xu, xs], training=True)
</code></pre>
<p>It did not return an error. After running for an hour, I interrupted the kernel and it returns the error message saying:</p>
<pre><code>InvalidArgumentError: Operation 'while' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

InvalidArgumentError: Operation 'gradients/while_grad/Placeholder_28' has no attr named '_read_only_resource_inputs'.
</code></pre>
<p>I am not sure what the issue is and I have tried to use persistent=True in tf.GradientTape as well instead of declaring two gradientTapes in single watch. But, exactly the same error occurs.</p>
<p>Does anyone have any idea what this issue is? And how it can be solved?</p>
<p>I am using Tensorflow V2.3.0 and Keras V2.4.0</p>
<p><strong>Source Code</strong></p>
<pre><code>class model_components:

  def mitigation_expert():
    inputs = Input(shape=(300,), dtype=tf.int32, name=&quot;me_input&quot;)
    x = Embedding(num_tokens, 300, weights=[embedding_matrix], input_length=max_length, trainable=False, name=&quot;me_embedding&quot;)(inputs)
    x = LSTM(300, return_sequences=False, name=&quot;me_lstm&quot;)(x)

    model = Model(inputs, x)

    return model

  def control_expert():
    inputs = Input(shape=(22,), dtype=tf.int32, name=&quot;ce_input&quot;)
    y = Dense(19, activation='relu', name=&quot;ce_hidden&quot;)(inputs)

    model = Model(inputs, y)

    return model

  def main_classifier():
    # Expert components
    me = model_components.mitigation_expert()
    ce = model_components.control_expert()

    # Main classifier
    ensemble = concatenate([me.output, ce.output], name=&quot;pred_ensemble&quot;)
    pred_output = Dense(319, activation=&quot;relu&quot;, name=&quot;pred_hidden&quot;)(ensemble)
    pred_output = Dense(3, activation=&quot;softmax&quot;, name=&quot;pred_output&quot;)(pred_output)

    model = Model(inputs=[me.input, ce.input], outputs=pred_output, name=&quot;main_classifier&quot;)

    return model
  
  def adversary_classifier():
    # Mitigation Expert component
    me = model_components.mitigation_expert()

    # Adversary classifier
    adv_output = Dense(300, activation='relu', name=&quot;adv_hidden&quot;)(me.output)
    adv_output = Dense(1, activation='sigmoid', name=&quot;adv_output&quot;)(adv_output)

    model = Model(inputs=me.input, outputs=adv_output, name=&quot;adversary_classifier&quot;)

    return model
</code></pre>
<pre><code>def tf_normalize(x):
  return x / (tf.norm(x) + np.finfo(np.float32).tiny)

class DebiasModel(keras.Model):
    def __init__(self, main_classifier, adversary_classifier):
        super(DebiasModel, self).__init__()
        self.main_classifier = main_classifier
        self.adversary_classifier = adversary_classifier

    def compile(self, mc_optimizer, adv_optimizer, mc_loss, adv_loss, debias_param):
        super(DebiasModel, self).compile()
        self.mc_optimizer = mc_optimizer
        self.adv_optimizer = adv_optimizer
        self.mc_loss = mc_loss
        self.adv_loss = adv_loss
        self.debias_param = debias_param

    def train_step(self, data):
      # Unpack data from model.fit()
      x, y, sample_weight = data

      # Unpack input and output features
      xu, xs = x
      y_mc = y['pred_output']
      z_adv = y['adv_output']

      # Unpack sample_weights
      mainClass_weights = sample_weight[&quot;pred_output&quot;]
      protectClass_weights = sample_weight[&quot;adv_output&quot;]

      # Generate prediction and compute loss for Main_Classifier
      with tf.GradientTape() as mc_tape, tf.GradientTape() as me_mc_tape:
        mc_pred = self.main_classifier([xu, xs], training=True)
        mc_loss = self.mc_loss(y_mc, mc_pred, sample_weight=mainClass_weights)
      
      # Compute and Apply Gradients for CE &amp; Main Classifier
      mc_trainable_vars = self.main_classifier.trainable_weights[3:]
      mc_grads = mc_tape.gradient(mc_loss, mc_trainable_vars)
      self.mc_optimizer.apply_gradients(zip(mc_grads, mc_trainable_vars))

      # Generate prediction and compute loss for Adversary_Classifier
      with tf.GradientTape() as adv_tape, tf.GradientTape() as me_adv_tape:
        adv_pred = self.adversary_classifier(xu)
        adv_loss = self.adv_loss(z_adv, adv_pred, sample_weight=protectClass_weights)
      
      # Compute and Apply Gradients for CE &amp; Main Classifier
      adv_trainable_vars = self.adversary_classifier.trainable_weights[3:]
      adv_grads = adv_tape.gradient(adv_loss, adv_trainable_vars)
      self.adv_optimizer.apply_gradients(zip(adv_grads, adv_trainable_vars))

      # Compute and Apply Gradients to debias ME
      me_adv_debias_trainable_vars = self.adversary_classifier.trainable_weights[:3]
      adv_debias_grads = me_adv_tape.gradient(adv_loss, me_adv_debias_trainable_vars)
      adv_debias_dict = tf.lookup.StaticHashTable(
          tf.lookup.KeyValueTensorInitializer(me_adv_debias_trainable_vars, adv_debias_grads), 0)
      
      me_mc_debias_trainable_vars = self.main_classifier.trainable_weights[:3]
      mc_debias_grads = me_mc_tape.gradient(mc_loss, me_mc_debias_trainable_vars)

      me_grads = []

      for g, v in zip(mc_debias_grads, me_mc_debias_trainable_vars):
        unit_adv = tf_normalize(adv_debias_dict.lookup(v))
        g -= tf.math.reduce_sum(g * unit_adv) * unit_adv
        g -= self.debias_param * adv_debias_dict.lookup(v)
        me_grads.append(zip(g, v))
      
      self.mc_optimizer.apply_gradients(me_grads)
      
      return {&quot;pred_loss&quot;: mc_loss, &quot;adv_loss&quot;: adv_loss}
</code></pre>
<pre><code>model = DebiasModel(model_components.main_classifier(),
                    model_components.adversary_classifier())

model.compile(mc_optimizer=tf.keras.optimizers.Adam(),
              adv_optimizer=tf.keras.optimizers.Adam(),
              mc_loss=tf.keras.losses.CategoricalCrossentropy(),
              adv_loss=tf.keras.losses.BinaryCrossentropy(),
              debias_param=1)

epoch = 5
sample_weights = {
    &quot;pred_output&quot;: mainClass_weight,
    &quot;adv_output&quot;: protectClass_weight,}

model.fit(x=[xu_train, xs_train],
          y={&quot;pred_output&quot;: y_train, &quot;adv_output&quot;: z_train},
          validation_data=([xu_val, xs_val], {&quot;pred_output&quot;: y_val, &quot;adv_output&quot;: z_val}),
          sample_weight=sample_weights, epochs=epoch, batch_size=256, verbose=1)
</code></pre>
<p><strong>Error Traceback</strong></p>
<pre><code>---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)
   2485       with c_api_util.tf_buffer() as buf:
-&gt; 2486         pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)
   2487         data = pywrap_tf_session.TF_GetBuffer(buf)

InvalidArgumentError: Operation 'while' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
51 frames
ValueError: Operation 'while' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
InvalidArgumentError: Operation 'gradients/while_grad/Placeholder_28' has no attr named '_read_only_resource_inputs'.
</code></pre>
<p>Note: I have not added the full traceback, but if needed I can provide it. Many thanks in advance!</p>
",0
63399368,Error with exporting TF2.2.0 model with tf.lookup.StaticHashTable for Serving,"<p>I'm using <code>StaticHashTable</code> as in one Lambda layer after the output layer of my tf.keras model. It's quite simple actually: I've a text classification models and I'm adding a simple lambda layer that takes the <code>model.output</code> and convert the model_id to more general labels. I can save this version of model with model.save(... as H5 format..) without any issue, and can load it back and use it without any problem.</p>
<p>Issue is, when I try to export my TF2.2.0 model for TF-Serving, I can't find how I can export it. Here is what I can do with TF1.X or with <code>TF2.X + tf.compat.v1.disable_eager_execution()</code></p>
<pre class=""lang-py prettyprint-override""><code>tf.compat.v1.disable_eager_execution()
version = 1
name = 'tmp_model'
export_path = f'/opt/tf_serving/{name}/{version}'
builder = saved_model_builder.SavedModelBuilder(export_path)

model_signature = tf.compat.v1.saved_model.predict_signature_def(
    inputs={
        'input': model.input
    }, 
    outputs={
        'output': model.output
    }
)

with tf.compat.v1.keras.backend.get_session() as sess:
    builder.add_meta_graph_and_variables(
        sess=sess,
        tags=[tf.compat.v1.saved_model.tag_constants.SERVING],
        signature_def_map={
            'predict': model_signature
        },
        # For initializing Hashtables
        main_op=tf.compat.v1.tables_initializer()
    )
    builder.save()
</code></pre>
<p>This will save my models with TF1.X format for serving and I can use it without any issue. Things is, I'm using LSTM layer and I want to use my model on GPU. By the documentation, if I disable the eager mode, I can't use the GPU-version of LSTM with TF2.2. And without going through above mentioned code, I can't save my model for serving wrt TF2.2 standard and StaticHashTables.</p>
<p>Here is how I'm trying to export my TF2.2 model which is using StaticHashTables in final layer; and which is giving error as below:</p>
<pre class=""lang-py prettyprint-override""><code>class MyModule(tf.Module):

    def __init__(self, model):
        super(MyModule, self).__init__()
        self.model = model
    
    @tf.function(input_signature=[tf.TensorSpec(shape=(None, 16), dtype=tf.int32, name='input')])
    def predict(self, input):
        result = self.model(input)
        return {&quot;output&quot;: result}

version = 1
name = 'tmp_model'
export_path = f'/opt/tf_serving/{name}/{version}'

module = MyModule(model)
tf.saved_model.save(module, export_path, signatures={&quot;predict&quot;: module.predict.get_concrete_function()})
</code></pre>
<p><strong>Error:</strong></p>
<pre class=""lang-sh prettyprint-override""><code>AssertionError: Tried to export a function which references untracked object Tensor(&quot;2907:0&quot;, shape=(), dtype=resource).
TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.
</code></pre>
<p>Any suggestion or am I missing anything on exporting TF2.2 model which is using the <code>StaticHashTables</code> in final Lambda layer for TensorFlow Serving?</p>
<p>More info here: <a href=""https://github.com/tensorflow/serving/issues/1719"" rel=""noreferrer"">https://github.com/tensorflow/serving/issues/1719</a></p>
<p>Thanks!</p>
",0
63482945,How does tf.nn.dilation2d compute gradient and learn its filters,"<p>I want to understand how the tf.nn.dilation2d is working and how the &quot;filters&quot;, which refers to a structural element are learned.</p>
<p>The official documentation is available here : <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/dilation2d"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/dilation2d</a></p>
<p>The documentation didn't reference a scientific paper, and I found a lot of different idea in scientific literature, about morphological filters in deep learnign. Here just some examples :</p>
<ul>
<li><a href=""https://arxiv.org/abs/1906.01751"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1906.01751</a></li>
<li><a href=""https://arxiv.org/abs/1909.01532"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1909.01532</a></li>
<li><a href=""https://doi.org/10.1109/TNNLS.2018.2890334"" rel=""nofollow noreferrer"">https://doi.org/10.1109/TNNLS.2018.2890334</a></li>
<li><a href=""https://doi.org/10.1142/S0218001419540247"" rel=""nofollow noreferrer"">https://doi.org/10.1142/S0218001419540247</a></li>
</ul>
<p>I searched into the code (<a href=""https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/ops/nn_ops.py#L327-L392"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/ops/nn_ops.py#L327-L392</a>), but the tf.nn.dilation2d only call gen_nn_ops.dilation2d</p>
<pre><code>@tf_export(&quot;nn.dilation2d&quot;, v1=[])
@dispatch.add_dispatch_support
def dilation2d_v2(input, filters, strides, padding, data_format, dilations, name=None):
  if data_format != &quot;NHWC&quot;:
    raise ValueError(&quot;Data formats other than NHWC are not yet supported&quot;)
  return gen_nn_ops.dilation2d(input=input,
                               filter=filters,
                               strides=strides,
                               rates=dilations,
                               padding=padding,
                               name=name)
</code></pre>
<p>I searched into gen_nn_ops.py (which I found inside my python lib folder, probably because it's generated from somewhere else) but I didn't understand what the code is doing.</p>
<pre><code>def dilation2d(input, filter, strides, rates, padding, name=None):
  r&quot;&quot;&quot;Computes the grayscale dilation of 4-D `input` and 3-D `filter` tensors.

  The `input` tensor has shape `[batch, in_height, in_width, depth]` and the
  `filter` tensor has shape `[filter_height, filter_width, depth]`, i.e., each
  input channel is processed independently of the others with its own structuring
  function. The `output` tensor has shape
  `[batch, out_height, out_width, depth]`. The spatial dimensions of the output
  tensor depend on the `padding` algorithm. We currently only support the default
  &quot;NHWC&quot; `data_format`.

  In detail, the grayscale morphological 2-D dilation is the max-sum correlation
  (for consistency with `conv2d`, we use unmirrored filters):

      output[b, y, x, c] =
         max_{dy, dx} input[b,
                            strides[1] * y + rates[1] * dy,
                            strides[2] * x + rates[2] * dx,
                            c] +
                      filter[dy, dx, c]

  Max-pooling is a special case when the filter has size equal to the pooling
  kernel size and contains all zeros.

  Note on duality: The dilation of `input` by the `filter` is equal to the
  negation of the erosion of `-input` by the reflected `filter`.

  Args:
    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
      4-D with shape `[batch, in_height, in_width, depth]`.
    filter: A `Tensor`. Must have the same type as `input`.
      3-D with shape `[filter_height, filter_width, depth]`.
    strides: A list of `ints` that has length `&gt;= 4`.
      The stride of the sliding window for each dimension of the input
      tensor. Must be: `[1, stride_height, stride_width, 1]`.
    rates: A list of `ints` that has length `&gt;= 4`.
      The input stride for atrous morphological dilation. Must be:
      `[1, rate_height, rate_width, 1]`.
    padding: A `string` from: `&quot;SAME&quot;, &quot;VALID&quot;`.
      The type of padding algorithm to use.
    name: A name for the operation (optional).

  Returns:
    A `Tensor`. Has the same type as `input`.
  &quot;&quot;&quot;
  _ctx = _context._context or _context.context()
  tld = _ctx._thread_local_data
  if tld.is_eager:
    try:
      _result = pywrap_tfe.TFE_Py_FastPathExecute(
        _ctx._context_handle, tld.device_name, &quot;Dilation2D&quot;, name,
        tld.op_callbacks, input, filter, &quot;strides&quot;, strides, &quot;rates&quot;, rates,
        &quot;padding&quot;, padding)
      return _result
    except _core._FallbackException:
      try:
        return dilation2d_eager_fallback(
            input, filter, strides=strides, rates=rates, padding=padding,
            name=name, ctx=_ctx)
      except _core._SymbolicException:
        pass  # Add nodes to the TensorFlow graph.
    except _core._NotOkStatusException as e:
      _ops.raise_from_not_ok_status(e, name)
  # Add nodes to the TensorFlow graph.
  if not isinstance(strides, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'strides' argument to &quot;
        &quot;'dilation2d' Op, not %r.&quot; % strides)
  strides = [_execute.make_int(_i, &quot;strides&quot;) for _i in strides]
  if not isinstance(rates, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'rates' argument to &quot;
        &quot;'dilation2d' Op, not %r.&quot; % rates)
  rates = [_execute.make_int(_i, &quot;rates&quot;) for _i in rates]
  padding = _execute.make_str(padding, &quot;padding&quot;)
  _, _, _op, _outputs = _op_def_library._apply_op_helper(
        &quot;Dilation2D&quot;, input=input, filter=filter, strides=strides,
                      rates=rates, padding=padding, name=name)
  _result = _outputs[:]
  if _execute.must_record_gradient():
    _attrs = (&quot;T&quot;, _op._get_attr_type(&quot;T&quot;), &quot;strides&quot;,
              _op.get_attr(&quot;strides&quot;), &quot;rates&quot;, _op.get_attr(&quot;rates&quot;),
              &quot;padding&quot;, _op.get_attr(&quot;padding&quot;))
    _inputs_flat = _op.inputs
    _execute.record_gradient(
        &quot;Dilation2D&quot;, _inputs_flat, _attrs, _result)
  _result, = _result
  return _result

Dilation2D = tf_export(&quot;raw_ops.Dilation2D&quot;)(_ops.to_raw_op(dilation2d))


def dilation2d_eager_fallback(input, filter, strides, rates, padding, name, ctx):
  if not isinstance(strides, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'strides' argument to &quot;
        &quot;'dilation2d' Op, not %r.&quot; % strides)
  strides = [_execute.make_int(_i, &quot;strides&quot;) for _i in strides]
  if not isinstance(rates, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'rates' argument to &quot;
        &quot;'dilation2d' Op, not %r.&quot; % rates)
  rates = [_execute.make_int(_i, &quot;rates&quot;) for _i in rates]
  padding = _execute.make_str(padding, &quot;padding&quot;)
  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, filter], ctx)
  (input, filter) = _inputs_T
  _inputs_flat = [input, filter]
  _attrs = (&quot;T&quot;, _attr_T, &quot;strides&quot;, strides, &quot;rates&quot;, rates, &quot;padding&quot;,
  padding)
  _result = _execute.execute(b&quot;Dilation2D&quot;, 1, inputs=_inputs_flat,
                             attrs=_attrs, ctx=ctx, name=name)
  if _execute.must_record_gradient():
    _execute.record_gradient(
        &quot;Dilation2D&quot;, _inputs_flat, _attrs, _result)
  _result, = _result
  return _result


def dilation2d_backprop_filter(input, filter, out_backprop, strides, rates, padding, name=None):
  r&quot;&quot;&quot;Computes the gradient of morphological 2-D dilation with respect to the filter.

  Args:
    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
      4-D with shape `[batch, in_height, in_width, depth]`.
    filter: A `Tensor`. Must have the same type as `input`.
      3-D with shape `[filter_height, filter_width, depth]`.
    out_backprop: A `Tensor`. Must have the same type as `input`.
      4-D with shape `[batch, out_height, out_width, depth]`.
    strides: A list of `ints` that has length `&gt;= 4`.
      1-D of length 4. The stride of the sliding window for each dimension of
      the input tensor. Must be: `[1, stride_height, stride_width, 1]`.
    rates: A list of `ints` that has length `&gt;= 4`.
      1-D of length 4. The input stride for atrous morphological dilation.
      Must be: `[1, rate_height, rate_width, 1]`.
    padding: A `string` from: `&quot;SAME&quot;, &quot;VALID&quot;`.
      The type of padding algorithm to use.
    name: A name for the operation (optional).

  Returns:
    A `Tensor`. Has the same type as `input`.
  &quot;&quot;&quot;
  _ctx = _context._context or _context.context()
  tld = _ctx._thread_local_data
  if tld.is_eager:
    try:
      _result = pywrap_tfe.TFE_Py_FastPathExecute(
        _ctx._context_handle, tld.device_name, &quot;Dilation2DBackpropFilter&quot;,
        name, tld.op_callbacks, input, filter, out_backprop, &quot;strides&quot;,
        strides, &quot;rates&quot;, rates, &quot;padding&quot;, padding)
      return _result
    except _core._FallbackException:
      try:
        return dilation2d_backprop_filter_eager_fallback(
            input, filter, out_backprop, strides=strides, rates=rates,
            padding=padding, name=name, ctx=_ctx)
      except _core._SymbolicException:
        pass  # Add nodes to the TensorFlow graph.
    except _core._NotOkStatusException as e:
      _ops.raise_from_not_ok_status(e, name)
  # Add nodes to the TensorFlow graph.
  if not isinstance(strides, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'strides' argument to &quot;
        &quot;'dilation2d_backprop_filter' Op, not %r.&quot; % strides)
  strides = [_execute.make_int(_i, &quot;strides&quot;) for _i in strides]
  if not isinstance(rates, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'rates' argument to &quot;
        &quot;'dilation2d_backprop_filter' Op, not %r.&quot; % rates)
  rates = [_execute.make_int(_i, &quot;rates&quot;) for _i in rates]
  padding = _execute.make_str(padding, &quot;padding&quot;)
  _, _, _op, _outputs = _op_def_library._apply_op_helper(
        &quot;Dilation2DBackpropFilter&quot;, input=input, filter=filter,
                                    out_backprop=out_backprop,
                                    strides=strides, rates=rates,
                                    padding=padding, name=name)
  _result = _outputs[:]
  if _execute.must_record_gradient():
    _attrs = (&quot;T&quot;, _op._get_attr_type(&quot;T&quot;), &quot;strides&quot;,
              _op.get_attr(&quot;strides&quot;), &quot;rates&quot;, _op.get_attr(&quot;rates&quot;),
              &quot;padding&quot;, _op.get_attr(&quot;padding&quot;))
    _inputs_flat = _op.inputs
    _execute.record_gradient(
        &quot;Dilation2DBackpropFilter&quot;, _inputs_flat, _attrs, _result)
  _result, = _result
  return _result

Dilation2DBackpropFilter = tf_export(&quot;raw_ops.Dilation2DBackpropFilter&quot;)(_ops.to_raw_op(dilation2d_backprop_filter))


def dilation2d_backprop_filter_eager_fallback(input, filter, out_backprop, strides, rates, padding, name, ctx):
  if not isinstance(strides, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'strides' argument to &quot;
        &quot;'dilation2d_backprop_filter' Op, not %r.&quot; % strides)
  strides = [_execute.make_int(_i, &quot;strides&quot;) for _i in strides]
  if not isinstance(rates, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'rates' argument to &quot;
        &quot;'dilation2d_backprop_filter' Op, not %r.&quot; % rates)
  rates = [_execute.make_int(_i, &quot;rates&quot;) for _i in rates]
  padding = _execute.make_str(padding, &quot;padding&quot;)
  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, filter, out_backprop], ctx)
  (input, filter, out_backprop) = _inputs_T
  _inputs_flat = [input, filter, out_backprop]
  _attrs = (&quot;T&quot;, _attr_T, &quot;strides&quot;, strides, &quot;rates&quot;, rates, &quot;padding&quot;,
  padding)
  _result = _execute.execute(b&quot;Dilation2DBackpropFilter&quot;, 1,
                             inputs=_inputs_flat, attrs=_attrs, ctx=ctx,
                             name=name)
  if _execute.must_record_gradient():
    _execute.record_gradient(
        &quot;Dilation2DBackpropFilter&quot;, _inputs_flat, _attrs, _result)
  _result, = _result
  return _result


def dilation2d_backprop_input(input, filter, out_backprop, strides, rates, padding, name=None):
  r&quot;&quot;&quot;Computes the gradient of morphological 2-D dilation with respect to the input.

  Args:
    input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.
      4-D with shape `[batch, in_height, in_width, depth]`.
    filter: A `Tensor`. Must have the same type as `input`.
      3-D with shape `[filter_height, filter_width, depth]`.
    out_backprop: A `Tensor`. Must have the same type as `input`.
      4-D with shape `[batch, out_height, out_width, depth]`.
    strides: A list of `ints` that has length `&gt;= 4`.
      1-D of length 4. The stride of the sliding window for each dimension of
      the input tensor. Must be: `[1, stride_height, stride_width, 1]`.
    rates: A list of `ints` that has length `&gt;= 4`.
      1-D of length 4. The input stride for atrous morphological dilation.
      Must be: `[1, rate_height, rate_width, 1]`.
    padding: A `string` from: `&quot;SAME&quot;, &quot;VALID&quot;`.
      The type of padding algorithm to use.
    name: A name for the operation (optional).

  Returns:
    A `Tensor`. Has the same type as `input`.
  &quot;&quot;&quot;
  _ctx = _context._context or _context.context()
  tld = _ctx._thread_local_data
  if tld.is_eager:
    try:
      _result = pywrap_tfe.TFE_Py_FastPathExecute(
        _ctx._context_handle, tld.device_name, &quot;Dilation2DBackpropInput&quot;,
        name, tld.op_callbacks, input, filter, out_backprop, &quot;strides&quot;,
        strides, &quot;rates&quot;, rates, &quot;padding&quot;, padding)
      return _result
    except _core._FallbackException:
      try:
        return dilation2d_backprop_input_eager_fallback(
            input, filter, out_backprop, strides=strides, rates=rates,
            padding=padding, name=name, ctx=_ctx)
      except _core._SymbolicException:
        pass  # Add nodes to the TensorFlow graph.
    except _core._NotOkStatusException as e:
      _ops.raise_from_not_ok_status(e, name)
  # Add nodes to the TensorFlow graph.
  if not isinstance(strides, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'strides' argument to &quot;
        &quot;'dilation2d_backprop_input' Op, not %r.&quot; % strides)
  strides = [_execute.make_int(_i, &quot;strides&quot;) for _i in strides]
  if not isinstance(rates, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'rates' argument to &quot;
        &quot;'dilation2d_backprop_input' Op, not %r.&quot; % rates)
  rates = [_execute.make_int(_i, &quot;rates&quot;) for _i in rates]
  padding = _execute.make_str(padding, &quot;padding&quot;)
  _, _, _op, _outputs = _op_def_library._apply_op_helper(
        &quot;Dilation2DBackpropInput&quot;, input=input, filter=filter,
                                   out_backprop=out_backprop, strides=strides,
                                   rates=rates, padding=padding, name=name)
  _result = _outputs[:]
  if _execute.must_record_gradient():
    _attrs = (&quot;T&quot;, _op._get_attr_type(&quot;T&quot;), &quot;strides&quot;,
              _op.get_attr(&quot;strides&quot;), &quot;rates&quot;, _op.get_attr(&quot;rates&quot;),
              &quot;padding&quot;, _op.get_attr(&quot;padding&quot;))
    _inputs_flat = _op.inputs
    _execute.record_gradient(
        &quot;Dilation2DBackpropInput&quot;, _inputs_flat, _attrs, _result)
  _result, = _result
  return _result

Dilation2DBackpropInput = tf_export(&quot;raw_ops.Dilation2DBackpropInput&quot;)(_ops.to_raw_op(dilation2d_backprop_input))


def dilation2d_backprop_input_eager_fallback(input, filter, out_backprop, strides, rates, padding, name, ctx):
  if not isinstance(strides, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'strides' argument to &quot;
        &quot;'dilation2d_backprop_input' Op, not %r.&quot; % strides)
  strides = [_execute.make_int(_i, &quot;strides&quot;) for _i in strides]
  if not isinstance(rates, (list, tuple)):
    raise TypeError(
        &quot;Expected list for 'rates' argument to &quot;
        &quot;'dilation2d_backprop_input' Op, not %r.&quot; % rates)
  rates = [_execute.make_int(_i, &quot;rates&quot;) for _i in rates]
  padding = _execute.make_str(padding, &quot;padding&quot;)
  _attr_T, _inputs_T = _execute.args_to_matching_eager([input, filter, out_backprop], ctx)
  (input, filter, out_backprop) = _inputs_T
  _inputs_flat = [input, filter, out_backprop]
  _attrs = (&quot;T&quot;, _attr_T, &quot;strides&quot;, strides, &quot;rates&quot;, rates, &quot;padding&quot;,
  padding)
  _result = _execute.execute(b&quot;Dilation2DBackpropInput&quot;, 1,
                             inputs=_inputs_flat, attrs=_attrs, ctx=ctx,
                             name=name)
  if _execute.must_record_gradient():
    _execute.record_gradient(
        &quot;Dilation2DBackpropInput&quot;, _inputs_flat, _attrs, _result)
  _result, = _result
  return _result
</code></pre>
<p>Thank you for your time.</p>
",1
63485162,Tensorflow load image dataset with image labels,"<p>I'm trying to find an easy way to get my data into a TensorFlow dataset without having to load it before and process it as a NumPy array.</p>
<p>In this case, I'm working on a segmentation model and my data is structured so one directory has the training data images and a different directory has the 'masks', essentially also images.</p>
<p>With <code>tf.keras.preprocessing.image_dataset_from_directory</code>, I can load an image dataset and it will either set the labels as the name of the directory or let me set them myself through a function argument but it won't let me set an argument so it takes labels from a different directory. I'm reading through the documentation but I don't see an easy way to load these types of datasets where the labels are images on their own.</p>
",1
63514487,TFLiteConverter Segmentation Fault when running integer quantization,"<p>I'm using tensorflow==1.15.3 and I'm hitting a segmentation fault attempting <a href=""https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only"" rel=""nofollow noreferrer"">int8 post-training quantization</a>. The documentation for the 1.15 version of the TFLiteConverter can be found <a href=""https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/lite/TFLiteConverter"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I found <a href=""https://github.com/tensorflow/tensorflow/issues/29829"" rel=""nofollow noreferrer"">a similar issue on github</a>, but their solution to provide <code>--add_postprocessing_op=true</code> has not solved the segmentation fault.</p>
<p>I've debugged it using PDB and found exactly where it crashes. It never reaches my <code>representative_dataset</code> function. It faults when running <code>CreateWrapperCPPFromBuffer(model_content)</code>:</p>
<pre><code>&gt; .../python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py(51)__init__()
-&gt; .CreateWrapperCPPFromBuffer(model_content))
(Pdb) s
Fatal Python error: Segmentation fault

Current thread 0x00007ff40ee9f740 (most recent call first):
  File &quot;.../python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py&quot;, line 51 in __init__
  File &quot;.../python3.6/site-packages/tensorflow_core/lite/python/lite.py&quot;, line 236 in _calibrate_quantize_model
  File &quot;.../python3.6/site-packages/tensorflow_core/lite/python/lite.py&quot;, line 993 in convert
  File &quot;.../convert_model_to_tflite_int8.py&quot;, line 97 in &lt;module&gt;
  File &quot;&lt;string&gt;&quot;, line 1 in &lt;module&gt;
  File &quot;/usr/lib/python3.6/bdb.py&quot;, line 434 in run
  File &quot;/usr/lib/python3.6/pdb.py&quot;, line 1548 in _runscript
  File &quot;/usr/lib/python3.6/pdb.py&quot;, line 1667 in main
  File &quot;/usr/lib/python3.6/pdb.py&quot;, line 1694 in &lt;module&gt;
  File &quot;/usr/lib/python3.6/runpy.py&quot;, line 85 in _run_code
  File &quot;/usr/lib/python3.6/runpy.py&quot;, line 193 in _run_module_as_main
[1]    17668 segmentation fault (core dumped)  python -m pdb convert_model_to_tflite_int8.py  --add_postprocessing_op=true
</code></pre>
<p>Here is my conversion code:</p>
<pre class=""lang-py prettyprint-override""><code>converter = tf.lite.TFLiteConverter.from_frozen_graph(
  graph_def_file=pb_model_path,
  input_arrays=[&quot;device_0/input_node_name:1&quot;],
  output_arrays=[&quot;device_0/output_node_name&quot;],
  input_shapes={&quot;device_0/input_node_name:1&quot;: [100, 16384]}
)
converter.allow_custom_ops = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

def test():
  pdb.set_trace()
  print(' ! ! ! representative_dataset_gen ! ! ! ')
  zeros = np.zeros(shape=(1, 100, 16384), dtype='int8')
  ds = tf.data.Dataset.from_tensor_slices((zeros)).batch(1)
  for input_value in ds.take(1):
    yield [input_value]
converter.representative_dataset = test

pdb.set_trace()
tflite_model = converter.convert()

tflite_model_size = open(model_name, 'wb').write(tflite_model)
print('TFLite Model is %d bytes' % tflite_model_size)
</code></pre>
<p>FWIW my model conversion works for <code>tf.float16</code> (not using <code>representative_dataset</code> there, though).</p>
",0
63557158,"How to compute statistics (sum, mean, variance, etc.) over an entire dataset in Tensorflow","<p>Computing mean, total, etc. of each feature in a dataset seems quite trivial in <code>Pandas</code> and <code>Numpy</code>, but I couldn't find any similarly easy functions/operations for <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""noreferrer""><code>tf.data.Dataset</code></a>. Actually I found <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#reduce"" rel=""noreferrer""><code>tf.data.Dataset.reduce</code></a> which allows me to compute running <code>sum</code>, but it's not that easy for other operation (<code>min</code>, <code>max</code>, <code>std</code>, etc.)
<br>
<br>So, my question is, is there a simple way to compute statistics for <code>tf.data.Dataset</code>? Moreover, is there a way to standardize/normalize (an entire, i.e. not in batch) <code>tf.data.Dataset</code>, especially if not using <code>tf.data.Dataset.reduce</code>?</p>
",0
63575474,How do I transform relative bounding boxes after cropping?,"<p>I need to use bounding boxes in this format, according to <a href=""https://www.tensorflow.org/api_docs/python/tf/image/draw_bounding_boxes"" rel=""nofollow noreferrer""><code>tf.image.draw_bounding_boxes</code></a>:</p>
<pre><code>[y_min, x_min, y_max, x_max]
</code></pre>
<p>I have this image of shape <code>(720, 1280, 3)</code> (full size image <a href=""https://i.stack.imgur.com/HiJCV.jpg"" rel=""nofollow noreferrer"">here</a>):</p>
<p><a href=""https://i.stack.imgur.com/OPGtv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OPGtv.png"" alt=""enter image description here"" /></a></p>
<p>And I have bounding boxes:</p>
<pre><code>array([[0.134722 , 0.425    , 0.327778 , 0.5      ],
       [0.294444 , 0.3140625, 0.444444 , 0.4124995],
       [0.473611 , 0.294531 , 0.636111 , 0.417969 ],
       [0.5972225, 0.392968 , 0.7986115, 0.497656 ],
       [0.5416665, 0.486719 , 0.6999995, 0.645313 ],
       [0.3986115, 0.425    , 0.5319445, 0.546094 ],
       [0.3499995, 0.5726565, 0.5041665, 0.6710935],
       [0.215278 , 0.507031 , 0.383334 , 0.590625 ]])
</code></pre>
<p>It works with the rectangle image:</p>
<pre><code>image = plt.imread(myimage) 
x = tf.image.draw_bounding_boxes(image[None, ...], 
                             np.array(bboxes)[None, ...], 
                             [[255., 0., 0.] for i in range(len(bboxes))])
plt.imshow(x[0, ...].numpy().astype(np.uint8))
plt.show()
</code></pre>
<p><a href=""https://i.stack.imgur.com/xduiR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xduiR.png"" alt=""enter image description here"" /></a></p>
<p>But it's all failing when I crop my picture to a square:</p>
<pre><code>image = plt.imread(myimage)[:, 280:-280]
x = tf.image.draw_bounding_boxes(image[None, ...], 
                             np.array(bboxes)[None, ...], 
                             [[255., 0., 0.] for i in range(len(bboxes))])
plt.imshow(x[0, ...].numpy().astype(np.uint8))
plt.show()
</code></pre>
<p><a href=""https://i.stack.imgur.com/joAsw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/joAsw.png"" alt=""enter image description here"" /></a></p>
<p>How can I have the right bounding boxes after cropping to a square? I'm looking to have a 640x640 square, then I will resize to 320x320.</p>
<p>**Edit: ** This worked:</p>
<pre><code> h, w, c = image.shape
    h_pad = 640 - h
    w_pad = 640 - w
    image = np.pad(image, [[h_pad//2, h_pad//2], 
                           [w_pad//2, w_pad//2], [0, 0]], mode='edge')
    train_images_np.append(image) 
    
    annotations = []
    labels_temp = []
    with open(annot_path) as txtfile:
        reader = csv.reader(txtfile, delimiter=',')
        for row in reader:
            annotations.append(list([(h*(2*float(row[0])-1)/640+1)/2, 
                                     (w*(2*float(row[1])-1)/640+1)/2, 
                                     (h*(2*float(row[2])-1)/640+1)/2, 
                                     (w*(2*float(row[3])-1)/640+1)/2
                                    ]))
</code></pre>
",0
63632809,How to fail during training on nan loss?,"<p>I try to find how to fail training after loss becomes nan</p>
<p>I have found the old post about how to do this for Tensorflow 1
<a href=""https://stackoverflow.com/questions/42647403/tf-train-nantensorhookloss-fail-on-nan-loss-false-will-still-raise-exception"">tf.train.NanTensorHook(loss, fail_on_nan_loss=False) will still raise exception with TF1.0</a></p>
<p>But I am working currently with Tensorflow 2 and I have not found in documentation any metioning about how to do it with second version ...</p>
<p>Any help will be appreciated ...</p>
",1
63639345,tf.data.experimental.save VS TFRecords,"<p>I have notice that the method <a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/save"" rel=""nofollow noreferrer""><code>tf.data.experimental.save</code></a> (added in r2.3) allows to save a <code>tf.data.Dataset</code> to file in just one line of code, which seems extremely convenient. Are there still some benefits in serializing a <code>tf.data.Dataset</code> and writing it into a TFRecord ourselves, or is this save function supposed to replace this process?</p>
",0
63650393,How to create model_config file for TFX Serving?,"<p>I have a bunch of custom defined model instances inheriting from  <code>tf.keras.layers.Layer</code> that I save. I would like to serve them with TFX Serving, which requires me to have a <code>model_config</code> file.</p>
<p>I am wondering how to create this according to the book. Right now I have the following code which I believe is more about my own bricolage than what I am supposed to do...</p>
<pre><code>model_server_config = model_server_config_pb2.ModelServerConfig()

    #Create a config to add to the list of served models
    config_list = model_server_config_pb2.ModelConfigList()       
        
    for i in range(0,len(trainable_unit_name)): # add models one by one to the model config.    
        model_name  = name[i]
        base_path = &quot;/models/{}&quot;.format(name[i])
        
        one_config = config_list.config.add()
        
        one_config.name           = model_name
        one_config.base_path      = base_path
        one_config.model_platform =&quot;tensorflow&quot;
        
    model_server_config.model_config_list.MergeFrom(config_list)
    with open(C.CONF_FILEPATH, 'w+') as f:        
        f.write(&quot;model_config_list {&quot; + config_list.__str__() + &quot;}&quot;) #manually wrap it around &quot;model_config_list { ..&quot; because this is the required format by TFX Serving.
</code></pre>
",0
63799945,How to broadcast with distributed TensorFlow,"<p>I want to implement broadcast some values from chief to all workers with distributed TensorFlow like MPI's bcast:
<a href=""https://mpi4py.readthedocs.io/en/stable/tutorial.html#collective-communication"" rel=""nofollow noreferrer"">https://mpi4py.readthedocs.io/en/stable/tutorial.html#collective-communication</a></p>
<p>I guess <a href=""https://github.com/tensorflow/tensorflow/blob/eed8f6c0946cd229d9b749295dd02045271b066e/tensorflow/python/ops/collective_ops.py#L113-L120"" rel=""nofollow noreferrer"">broadcast_send</a> or <a href=""https://www.tensorflow.org/api_docs/python/tf/raw_ops/CollectiveBcastSend?hl=en"" rel=""nofollow noreferrer"">tf.raw_ops.CollectiveBcastSend</a> is the operation, but I cloud not found any examples on TensorFlow official document.</p>
<p>Is there a good example to use such the row level distributed operations?</p>
",1
63821290,"ValueError: Value tf.Tensor.. shape=(), dtype=float64) has insufficient rank for batching.?","<p>I'm trying to take a dataframe and convert them to tensors to train a model in keras.</p>
<p>I think it's being triggered when I am converting my Y label to a tensor:</p>
<pre><code>  X_train = df_train1.drop(variableToPredict, axis=1)
  y_train = df_train1[variableToPredict].copy()


X_train=tf.data.Dataset.from_tensor_slices(dict(X_train))
  y_train=tf.data.Dataset.from_tensor_slices(dict(y_train))
</code></pre>
<p>I'm getting the following error when casting y_train to tensor from slices:</p>
<pre><code>ValueError: Value tf.Tensor(0.10559591064345274, shape=(), dtype=float64) has insufficient rank for batching.
</code></pre>
<p>In the tutorials this seems to work but I think those tutorials are doing multiclass classifications whereas I'm doing a regression so y_train is a series not multiple columns.</p>
<p>Any suggestions of what I can do?</p>
",0
63825081,Convert a tensorflow detection model using coremltools,"<p>I have created a detection model using this tutorial.
<a href=""https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#"" rel=""nofollow noreferrer"">https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#</a></p>
<p>Can I convert it using coreml tools ?</p>
<p>When I use tf.saved_model.load I get a</p>
<pre><code>&lt;tensorflow.python.saved_model.load.Loader._recreate_base_user_object.&lt;locals&gt;._UserObject object at 0x000001C86252F3D0&gt;.
</code></pre>
<p>And thus coremltools return this error.</p>
<blockquote>
<p>NotImplementedError: Expected model format: [SavedModel |
[concrete_function] | tf.keras.Model | .h5], got
&lt;tensorflow.python.saved_model.load.Loader._recreate_base_user_object.._UserObject
object at 0x000001C86252F3D0&gt;</p>
</blockquote>
<p>What is this object ?</p>
<p><strong>Thank you !</strong></p>
<p>code sample :</p>
<pre><code>new_model = tf.saved_model.load(&quot;my-model/saved_model&quot;)
image_input = ct.ImageType(shape=(1, 224, 224, 3,),
                          bias=[-1,-1,-1], scale=1/127)
core_ml_model = ct.convert( new_model, inputs=[image_input], source=&quot;tensorflow&quot;)
core_ml_model.save(&quot;SSDMobileNetV2.mlmodel&quot;)
</code></pre>
<p>ps : I also tried
<code>core_ml_model = ct.convert(&quot;my-model/saved_model&quot;)</code>
Which give</p>
<blockquote>
<p>TypeError: 'Var' object is not subscriptable</p>
</blockquote>
",0
63851431,How to Augment the Training Set using the tf.keras.utils.Sequence API?,"<p>TensorFlow documentation have the following example that can illustrate how to create a batch generator to feed a training set in batches to a model when the training set is too large to fit in memory:</p>
<pre class=""lang-py prettyprint-override""><code>from skimage.io import imread
from skimage.transform import resize
import tensorflow as tf
import numpy as np
import math

# Here, `x_set` is list of path to the images
# and `y_set` are the associated classes.

class CIFAR10Sequence(tf.keras.utils.Sequence):

    def __init__(self, x_set, y_set, batch_size):
        self.x, self.y = x_set, y_set
        self.batch_size = batch_size

    def __len__(self):
        return math.ceil(len(self.x) / self.batch_size)

    def __getitem__(self, idx):
        batch_x = self.x[idx * self.batch_size:(idx + 1) *
        self.batch_size]
        batch_y = self.y[idx * self.batch_size:(idx + 1) *
        self.batch_size]

        return np.array([
            resize(imread(file_name), (200, 200))
               for file_name in batch_x]), np.array(batch_y)
</code></pre>
<p>My intention is to further increase the diversity of the training set by rotating each image 3x by 90º. In each Epoch of the training process, the model would first be fed with the &quot;0º training set&quot; and next with the 90º, 180º and 270º rotating sets, respectively.</p>
<p>How can I modify the previous piece of code to perform this operation inside the <code>CIFAR10Sequence()</code> data generator?</p>
<p>Please don't use <code>tf.keras.preprocessing.image.ImageDataGenerator()</code> so that the answer does not lose its generality for another type of similar problems that are of a different nature.</p>
<p>NB: The idea would be to create the new data &quot;in real time&quot; as the model is fed instead of creating (in advance) and storing on disk a new and augmented training set bigger than the original one to be used later (also in batches) during the training process of the model.</p>
<p>Thx in advance</p>
",1
63869134,Converting TensorFlow tensor into Numpy array,"<h1>Problem Description</h1>
<p>I am trying to write a custom loss function in TensorFlow 2.3.0. To calculate the loss, I need the <code>y_pred</code> parameter to be converted to a numpy array. However, I can't find a way to convert it from <code>&lt;class 'tensorflow.python.framework.ops.Tensor'&gt;</code> to numpy array, even though there seem to TensorFlow functions to do so.</p>
<h1>Code Example</h1>
<pre><code>def custom_loss(y_true, y_pred):
    print(type(y_pred))
    npa = y_pred.make_ndarray()
    ...
    

if __name__ == '__main__':
    ...
    model.compile(loss=custom_loss, optimizer=&quot;adam&quot;)
    model.fit(x=train_data, y=train_data, epochs=10)
</code></pre>
<p>gives the error message: <code>AttributeError: 'Tensor' object has no attribute 'make_ndarray</code>
after printing the type of the <code>y_pred</code> parameter: <code>&lt;class 'tensorflow.python.framework.ops.Tensor'&gt;</code></p>
<h1>What I have tried so far</h1>
<p>Looking for a solution I found this seems to be a common issue and there a couple of suggestions, but they did not work for me so far:</p>
<p><strong>1. &quot; ... so just call .numpy() on the Tensor object.&quot;: <a href=""https://www.thetopsites.net/article/51302819.shtml"" rel=""noreferrer"">How can I convert a tensor into a numpy array in TensorFlow?</a></strong></p>
<p>so I tried:</p>
<pre><code>def custom_loss(y_true, y_pred):
    npa = y_pred.numpy()
    ...
</code></pre>
<p>giving me <code>AttributeError: 'Tensor' object has no attribute 'numpy'</code></p>
<p><strong>2. &quot;Use tensorflow.Tensor.eval() to convert a tensor to an array&quot;: <a href=""https://www.kite.com/python/answers/how-to-convert-a-tensorflow-tensor-to-a-numpy-array-in-python"" rel=""noreferrer"">How to convert a TensorFlow tensor to a NumPy array in Python</a></strong></p>
<p>so I tried:</p>
<pre><code>def custom_loss(y_true, y_pred):
    npa = y_pred.eval(session=tf.compat.v1.Session())
    ...
</code></pre>
<p>giving me one of the longest trace of error messages I ever have seen with the core being:</p>
<pre><code>InvalidArgumentError: 2 root error(s) found.
      (0) Invalid argument: You must feed a value for placeholder tensor 'functional_1/conv2d_2/BiasAdd/ReadVariableOp/resource' with dtype resource
         [[node functional_1/conv2d_2/BiasAdd/ReadVariableOp/resource (defined at main.py:303) ]]
         [[functional_1/cropping2d/strided_slice/_1]]
      (1) Invalid argument: You must feed a value for placeholder tensor 'functional_1/conv2d_2/BiasAdd/ReadVariableOp/resource' with dtype resource
         [[node functional_1/conv2d_2/BiasAdd/ReadVariableOp/resource (defined at main.py:303) ]]
</code></pre>
<p>also having to call TensorFlow Compatibility Functions from Version 1.x does not feel very future-proof, so I do not like this approach too much anyhow.</p>
<p><strong>3. Looking at the TensorFlow Docs there seemed to be the function I needed just waiting: <a href=""https://www.tensorflow.org/api_docs/python/tf/make_ndarray"" rel=""noreferrer"">tf.make_ndarray</a> Create a numpy ndarray from a tensor.</strong></p>
<p>so I tried:</p>
<pre><code>def custom_loss(y_true, y_pred):
    npa = tf.make_ndarray(y_pred)
    ...
</code></pre>
<p>giving me <code>AttributeError: 'Tensor' object has no attribute 'tensor_shape'</code></p>
<p>Looking at the example in the TF documentation they use this on a proto_tensor, so I tried converting to a proto first:</p>
<pre><code>def custom_loss(y_true, y_pred):
    proto_tensor = tf.make_tensor_proto(y_pred)
    npa = tf.make_ndarray(proto_tensor)
    ...
</code></pre>
<p>but already the <code>tf.make_tensor_proto(y_pred)</code> raises the error: <code>TypeError: Expected any non-tensor type, got a tensor instead.</code></p>
<p>Also trying to make a const tensor first gives the same error:</p>
<pre><code>def custom_loss(y_true, y_pred):
    a = tf.constant(y_pred)
    proto_tensor = tf.make_tensor_proto(a)
    npa = tf.make_ndarray(proto_tensor)
    ...
</code></pre>
<p>There are many more posts around this but it seems they are all coming back to these three basic ideas. Looking forward to your suggestions!</p>
",1
63878497,How to Update Trainable Variables on My Custom Optimizer Using Tensorflow 2,"<p>I'm a newbie studying Convolution Neural Network nowadays.</p>
<p>So, I have been implementing AlexNet referring a paper titled &quot;ImageNet Classification with Deep Convolution Neural Networks&quot;, which uses Tensorflow 2.3 in anaconda environment.
However, I got frustrated while I'm implementing a custom optimizer.</p>
<p>My problem : I have to modify the optimizer according to AlexNet paper. I can't find out the reference How to update the variable that uses TensorflowV2, I had googling though. There is only &quot;tf.assign()&quot; uses, It is not supported in Tensorflow V2, but I'm also afraid of the compatibility between V1 and V2 if I'm going to use this function.</p>
<p>I just know that I have to customize &quot;_resource_apply_dense&quot; function for adapting my update rule. Then, I loaded some hyperparameter in there. but I don't know how to update the hyperparameter.</p>
<p>(tf.Variable() can use as a python variable, so I suppose that it is the same as tf.Variable()....?)</p>
<p>Thank you all readers in advanced <em>^_^</em></p>
<p>Here's the code.</p>
<pre><code>- update rule in AlexNet
 v_(i+1)= momentum * v_i- 0.0005 * learning_rate * w_i - learning_rate * gradient
 w_(i+1) = w_i + v_(i+1)

# where
# w_i = weight
# v_i = velocity
</code></pre>
<pre class=""lang-py prettyprint-override""><code>from tensorflow.python.framework import ops
from tensorflow.python.keras.optimizer_v2 import optimizer_v2
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import resource_variable_ops
from tensorflow.python.training import training_ops
from tensorflow.python.util.tf_export import keras_export
import tensorflow as tf

class AlexSGD(optimizer_v2.OptimizerV2):

    _HAS_AGGREGATE_GRAD = True

    def __init__(self,
                learning_rate=0.01,
                weight_decay=0.0005,
                momentum=0.9,
                name=&quot;AlexSGD&quot;,
                **kwargs):
        super(AlexSGD, self).__init__(name, **kwargs)
        self._set_hyper(&quot;learning_rate&quot;, kwargs.get(&quot;lr&quot;, learning_rate))
        self._set_hyper(&quot;decay&quot;, self._initial_decay)
        
        self._is_first = True
        self._set_hyper(&quot;vx&quot;, 0)
        self._set_hyper(&quot;pg&quot;, 0)
        self._set_hyper(&quot;pv&quot;, 0)
        self._weight_decay = False
        if isinstance(weight_decay, ops.Tensor) or callable(weight_decay) or 
            weight_decay &gt; 0:
        self._weight_decay = True
        if isinstance(weight_decay, (int, float)) and (weight_decay &lt; 0 or 
            weight_decay &gt; 1):
        raise ValueError(&quot;`weight_decay` must be between [0, 1].&quot;)
        self._set_hyper(&quot;weight_decay&quot;, weight_decay)

        self._momentum = False
        if isinstance(momentum, ops.Tensor) or callable(momentum) or momentum &gt; 0:
        self._momentum = True
        if isinstance(momentum, (int, float)) and (momentum &lt; 0 or momentum &gt; 1):
        raise ValueError(&quot;`momentum` must be between [0, 1].&quot;)
        self._set_hyper(&quot;momentum&quot;, momentum)

    def _create_slots(self, var_list):
        if self._momentum:
        for var in var_list:
            self.add_slot(var, &quot;momentum&quot;)
        if self._weight_decay:
        for var in var_list:
            self.add_slot(var, &quot;weight_decay&quot;)
        for var in var_list:
        self.add_slot(var, 'pv') # previous variable i.e. weight or bias    
        for var in var_list:
        self.add_slot(var, 'pg') # previous gradient
        for var in var_list:
        self.add_slot(var, 'vx') # update velocity

    def _prepare_local(self, var_device, var_dtype, apply_state):
        super(AlexSGD, self)._prepare_local(var_device, var_dtype, apply_state)
        apply_state[(var_device, var_dtype)][&quot;momentum&quot;] = array_ops.identity(
            self._get_hyper(&quot;momentum&quot;, var_dtype))
        apply_state[(var_device, var_dtype)][&quot;weight_decay&quot;] = array_ops.identity(
            self._get_hyper(&quot;weight_decay&quot;, var_dtype))
        apply_state[(var_device, var_dtype)][&quot;vx&quot;] = array_ops.identity(
            self._get_hyper(&quot;vx&quot;, var_dtype))
        apply_state[(var_device, var_dtype)][&quot;pv&quot;] = array_ops.identity(
            self._get_hyper(&quot;pv&quot;, var_dtype))
        apply_state[(var_device, var_dtype)][&quot;pg&quot;] = array_ops.identity(
            self._get_hyper(&quot;pg&quot;, var_dtype))

    # main function
    @tf.function
    def _resource_apply_dense(self, grad, var, apply_state=None):
        var_device, var_dtype = var.device, var.dtype.base_dtype
        coefficients = ((apply_state or {}).get((var_device, var_dtype))
                        or self._fallback_apply_state(var_device, var_dtype))
        momentum_var = self.get_slot(var, &quot;momentum&quot;)
        weight_decay_var = self.get_slot(var, &quot;weight_decay&quot;)
        vx_var = self.get_slot(var, &quot;vx&quot;)
        pv_var = self.get_slot(var, &quot;pv&quot;)
        pg_var = self.get_slot(var, &quot;pg&quot;)
        lr_t = self._decayed_lr(var_dtype)

        # update rule in AlexNet
        # v_(i+1) = momentum * v_i - 0.0005 * lr * w_i - lr * grad
        # w_(i+1) = w_i + v_(i+1)
        # where
        # w_i = var
        # vx, v_i = velocity (Feel like I need to set this variable as a slot) 
        # lr = learning_rate
        # grad = gradient
        
        # I'm confused why pv, pg variables are declared... 
        # does it replace by var &amp; grad ?  (pv, pg refer from blog)
        # pv = previous var
        # pg = previous gradient

        if self._is_first:
        self._is_first = False
        vx_var = grad
        new_var = var + vx_var
        else:
        vx_var = momentum_var * vx_var - weight_decay_var*lr_t*pv_var- 
        lr_t*pg_var
        new_var = var + vx_var

        print(&quot;grad:&quot;,grad)
        print(&quot;var:&quot;,var)
        print(&quot;vx_var:&quot;,vx_var)
        print(&quot;new_var:&quot;,new_var)
        print(&quot;pv_var:&quot;,pv_var)
        print(&quot;pg_var:&quot;,pg_var)
        
        # TODO: I got stuck how update the variables because tf.assign() function 
        #       is deprecated in Tensorflow V2 
        pg_var = grad
        pv_var = var

        if var == new_var:
        var = new_var
        
        # TODO: In order to update variables, I can't find the equivalent 
        #        &quot;tf.assign&quot; method in TF V2
        # pg_var.assign(grad)
        # vx_var.assign(vx_var)
        # var.assign(new_var)
        
        
        
        &quot;&quot;&quot;
        # TODO: I referred the below code from Tensorflow official document, and I 
        #       realized the training_ops module is in c++ library, So I thought I 
        #       can't modify it ( Cuz I need to modify an update function of 
        #       velocity 

        # return training_ops.resource_apply_keras_momentum(
        #     var.handle,
        #     momentum_var.handle,
        #     coefficients[&quot;lr_t&quot;],
        #     grad,
        #     coefficients[&quot;momentum&quot;],
        #     use_locking=self._use_locking,
        #     use_nesterov=self.nesterov)


        # if self._momentum :
        #  momentum_var = self.get_slot(var, &quot;momentum&quot;)
        #   return training_ops.resource_apply_keras_momentum(
        #     var.handle,
        #     momentum_var.handle,
        #     coefficients[&quot;lr_t&quot;],
        #     grad,
        #     coefficients[&quot;momentum&quot;],
        #     use_locking=self._use_locking,
        #     use_nesterov=self.nesterov)
        # else:
        #   return training_ops.resource_apply_gradient_descent(
        #       var.handle, coefficients[&quot;lr_t&quot;], grad, 
                use_locking=self._use_locking)
        &quot;&quot;&quot;


    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):
        raise NotImplementedError

    def get_config(self):
        config = super(AlexSGD, self).get_config()
        config.update({
            &quot;learning_rate&quot;: self._serialize_hyperparameter(&quot;learning_rate&quot;),
            &quot;decay&quot;: self._serialize_hyperparameter(&quot;decay&quot;),
            &quot;weight_decay&quot;: self._serialize_hyperparameter(&quot;weight_decay&quot;),
            &quot;momentum&quot;: self._serialize_hyperparameter(&quot;momentum&quot;),
        })
        return config
</code></pre>
<p>Reference: <a href=""https://www.kdnuggets.com/2018/01/custom-optimizer-tensorflow.html"" rel=""nofollow noreferrer"">https://www.kdnuggets.com/2018/01/custom-optimizer-tensorflow.html</a></p>
<p>This is another Reference, it also uses tf.assign()</p>
<pre class=""lang-py prettyprint-override""><code>from tensorflow.python.ops import control_flow_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.ops import state_ops
from tensorflow.python.framework import ops
from tensorflow.python.training import optimizer
import tensorflow as tf

class AlexOptimizer(optimizer.Optimizer):
    def __init__(self, learning_rate=&quot;learning_rate&quot;,alpha=&quot;alpha&quot;,beta=&quot;beta&quot;, #weight_decay=&quot;weight_decay&quot;, use_locking=False, name=&quot;AlexOptimizer&quot;):
        super(AlexOptimizer, self).__init__(use_locking, name)
        self._lr = learning_rate
        self._wd = weight_decay
        self._alpha = alpha
        self._beta = beta
        # Tensor versions of the constructor arguments, created in _prepare().
        self._lr_t = None
        self._wd_t = None
        self._alpha_t = None
        self._beta_t = None

    def _prepare(self):
        self._lr_t = ops.convert_to_tensor(self._lr, name=&quot;learning_rate&quot;)
        self._wd_t = ops.convert_to_tensor(self._wd, name=&quot;weight_decay&quot;)
        self._alpha_t = ops.convert_to_tensor(self._beta, name=&quot;alpha_t&quot;)
        self._beta_t = ops.convert_to_tensor(self._beta, name=&quot;beta_t&quot;)

    def _create_slots(self, var_list):
        # Create slots for the first and second moments.
        for v in var_list:
            self._zeros_slot(v, &quot;m&quot;, self._name)

    def _apply_dense(self, grad, var):
        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)
        wd_t = math_ops.cast(self._wd_t, var.dtype.base_dtype)
        alpha_t = math_ops.cast(self._alpha_t, var.dtype.base_dtype)
        beta_t = math_ops.cast(self._beta_t, var.dtype.base_dtype)

        eps = 1e-7 #cap for moving average
        m = self.get_slot(var, &quot;m&quot;)
        m_t = m.assign(tf.maximum(beta_t * m + eps, tf.abs(grad)))

        var_update = state_ops.assign_sub(var, lr_t*grad*tf.exp( tf.log(alpha_t)*tf.sign(grad)*tf.sign(m_t))) 
        # Update 'ref' by subtracting value
        # Create an op that groups multiple operations.
        # When this op finishes, all ops in input have finished
        return control_flow_ops.group(*[var_update, m_t])
    def _apply_sparse(self, grad, var):
        raise NotImplementedError(&quot;Sparse gradient updates are not supported.&quot;)
</code></pre>
<p>I would like to modify the code using OptimizerV2, What should I update the variables?</p>
<p>(p.s.) is it right the &quot;@tf.function&quot; usage above &quot;def _resource_apply_dense()&quot;?</p>
<p>on the other hand, my model is continuously shuffling on and on during training ㅠ_ㅠ (this code in a procedure of dataset preprocessing (tf.data.datsets.shuffle()) even it doesn't exist in while loop.......... (sorry for not posting this code... so never mind....)</p>
",0
63889769,Tensorflow Input Shapes Incompatible,"<p>Trying to build a Tensorflow model where my data has 70 features. Here is the setup of my first layer:</p>
<p><code>tf.keras.layers.Dense(units=50, activation='relu', input_shape=(None,70)),</code></p>
<p>Setting the input shape to <code>(None,70)</code> seemed best to me as I am using a feed forward neural network where each &quot;row&quot; of data is unique. I am using a batch size (for now) of size 10. Should my input shape change to <code>(10,70)</code>??</p>
<p>I have tried with the original <code>(None, 70)</code> and gotten the error:</p>
<pre><code>WARNING:tensorflow:Model was constructed with shape (None, None, 70) for input Tensor(&quot;dense_33_input:0&quot;, shape=(None, None, 70), dtype=float32), but it was called on an input with incompatible shape (10, 70).

TypeError: Input 'y' of 'Mul' Op has type float64 that does not match type float32 of argument 'x'.
</code></pre>
<p>Quite confused on exactly what is going wrong with the <code>input_shape</code> as <code>(None, 70)</code> seems to fit the best. Any help is much appreciated.</p>
<p>Edit: Wanted to add a reproducible example for more context. Sorry for the length. This is a reproduction of [this example][1] to better suit my current data (non-image).</p>
<p><strong>Variational Autoencoder Model</strong></p>
<pre><code>class VAE(tf.keras.Model):
    
    def __init__(self, latent_dim):
        super(VAE, self).__init__()
        self.latent_dim = latent_dim
        self.encoder = tf.keras.Sequential(
        [
            tf.keras.layers.Dense(units=50, activation='relu', input_shape=(70,)),
            tf.keras.layers.Dense(latent_dim + latent_dim), #No activation
        ])
        
        self.decoder = tf.keras.Sequential(
        [
            tf.keras.layers.Dense(units=50, activation='relu', input_shape=(latent_dim,)),
            tf.keras.layers.Dense(units=70),
        ])
        
    @tf.function
    def sample(self, eps=None):
        if eps is None:
            eps = tf.random.normal(shape=(100, self.latent_dim))
        return self.decode(eps, apply_sigmoid=True)

    def encode(self, x):
        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)
        return mean, logvar

    def reparameterize(self, mean, logvar):
        eps = tf.random.normal(shape=mean.shape)
        return eps * tf.exp(logvar * .5) + mean

    def decode(self, z, apply_sigmoid=False):
        logits = self.decoder(z)
        if apply_sigmoid:
            probs = tf.sigmoid(logits)
            return probs
        return logits


  [1]: https://www.tensorflow.org/tutorials/generative/cvae
</code></pre>
<p><strong>Optimizer &amp; Loss Funx</strong></p>
<pre><code>optimizer = tf.keras.optimizers.Adam(1e-4)

def log_normal_pdf(sample, mean, logvar, raxis=1):
    log2pi = tf.math.log(2. * np.pi)
    return tf.reduce_sum(
        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis)

def compute_loss(model, x):
    mean, logvar = model.encode(x)
    z = model.reparameterize(mean, logvar)
    x_logit = model.decode(z)
    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)
    logpx_z = -tf.reduce_sum(cross_ent, axis=[1])
    logpz = log_normal_pdf(z, 0, 0)
    logqz_x = log_normal_pdf(z, mean, logvar)
    return -tf.reduce_mean(logpx_z + logpz - logqz_x)

@tf.function
def train_step(model, x, optimizer):
    with tf.GradientTape() as tape:
        loss = compute_loss(model, x)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
</code></pre>
<p><strong>Train</strong></p>
<pre><code>X = tf.random.uniform((100,70))
y = tf.random.uniform((100,))

ds_train = tf.data.Dataset.from_tensor_slices((X, y))

tf.random.set_seed(1)

train = ds_train.shuffle(buffer_size=len(X))
train = train.batch(batch_size=10, drop_remainder=False)

epochs = 5
latent_dim = 2

model = VAE(2)

for epoch in range(1, epochs+1):
    start_time = time.time()
    for i, (train_x, train_y) in enumerate(train):
        train_step(model, train_x, optimizer)
    end_time = time.time()
    
    loss = tf.keras.metrics.Mean()
    for i, (test_x, test_y) in enumerate(ds_test):
        loss(compute_loss(model, test_x))
    elbo = -loss.result()
    display.clear_output(wait=False)
    print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'
         .format(epoch, elbo, end_time - start_time))
</code></pre>
",0
63919438,TensorFlow keras model fit() parameters steps_per_epoch and epochs behavior on train set,"<p>I'm using a tf.data dataset containing my training data consisting of (lets say) 100k images.
I'm also using a tf.data dataset containing my validation set.
Since an epoch of all 100k images takes quite long (in my case approximately one hour) before I get any feedback on performance on the validation set, I set the <code>steps_per_epoch</code> parameter in tf.keras.Model <code>fit()</code> to <code>10000</code>.
Using a batch size of 1 this results into having 10 validation scores when reaching 100k of images.
In order to complete one epoch of 100k images of my entire training dataset, I set the <code>epochs</code> parameter to <code>10</code></p>
<p>However, I'm not sure if using <code>steps_per_epoch</code> and <code>epochs</code> this way has any other consequences. Is it correct to use these parameters in order to get more frequent feedback on performance?
And also a more specific question, does it use all 100k images or does it use the same first 10k images of my training set at every 'epoch'?
I already dug into the <a href=""https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">TensorFlow docs</a> and read several different stack overflow questions, but I couldn't find anything conclusive to answer my own question. Hope you can help!</p>
<p>Tensorflow version I'm using is 2.2.0.</p>
",1
63953040,How to use a TFRecord file for batch prediction on GCP AI Platform?,"<p>TL;DR
How does Google Cloud AI Platform unpack <code>TFRecord</code> files when doing batch predictions?</p>
<p>I have deployed a trained Keras model to Google Cloud AI Platform, but I'm having trouble with the file format for batch predictions. For training I'm using a <code>tf.data.TFRecordDataset</code> to read a list of <code>TFRecord</code> like the following which all works fine.</p>
<pre><code>def unpack_tfrecord(record):
    parsed = tf.io.parse_example(record, {
        'chunk': tf.io.FixedLenFeature([128, 2, 3], tf.float32),  # Input
        'class': tf.io.FixedLenFeature([2], tf.int64),            # One-hot classification (binary)
    })

    return (parsed['chunk'], parsed['class'])

files = [str(p) for p in training_chunks_path.glob('*.tfrecord')]
dataset = tf.data.TFRecordDataset(files).batch(32).map(unpack_tfrecord)
model.fit(x=dataset, epochs=train_epochs)
tf.saved_model.save(model, model_save_path)
</code></pre>
<p>I upload the saved model to Cloud Storage and create a new model in AI Platform. AI Platform documentation states that &quot;Batch with gcloud tool [supports] Text file with JSON instance strings or TFRecord file (may be compressed)&quot; (<a href=""https://cloud.google.com/ai-platform/prediction/docs/overview#prediction_input_data"" rel=""nofollow noreferrer"">https://cloud.google.com/ai-platform/prediction/docs/overview#prediction_input_data</a>). But when I provide a TFRecord file i get the error:</p>
<pre><code>(&quot;'utf-8' codec can't decode byte 0xa4 in position 1: invalid start byte&quot;, 8)
</code></pre>
<p>My TFRecord file contains a bunch of Protobuf encoded <code>tf.train.Example</code>. I'm not providing the <code>unpack_tfrecord</code> function to AI Platform, so I guess it makes sense that it can not unpack it properly, but I have node idea where to go from here. I'm not interested in using the JSON format as the data is too large.</p>
",0
63958039,How do I interpret tf.keras.Model.predict() output?,"<p>I am having trouble finding the documentation I need on this. To summarize the issue, I have trained a tf.keras model using two classes of images, labeled as '0' or '1'. I now want to use this model to predict whether new images are a '0' or '1'. My question is as follows: <code>model.predict()</code> returns a number between 1 and 0, but I can't seem to find what exactly this is. Is it correct to say that this is it's prediction (ie, closer to 1 means the image is likely a 1, and closer to 0 means the image is likely a 0)? Or is there something else going on here. I have included the code, and some output, below. In this case, is <code>pred</code> the probability the image is a 1, and <code>1 - pred</code> the probability the image is a 0?</p>
<p>Thanks for any and all help.</p>
<pre><code>for img_path in test_filenames:
   img = tf.keras.preprocessing.image.load_img(img_path, target_size=(IMAGE_SIZE,IMAGE_SIZE))
   img_array = tf.keras.preprocessing.image.img_to_array(img)
   img_array = tf.expand_dims(img_array, 0)
   pred = model.predict(img_array)
   print(pred)
</code></pre>
<p>Returns</p>
<pre><code>[[0.8361757]]
[[0.26765466]]
[[0.2722953]]
[[0.81938094]]
[[0.24995388]]
[[0.45974937]]
</code></pre>
",0
63958998,Tensorflow/AI Cloud Platform: HyperTune trials failed to report the hyperparameter tuning metric,"<p>I'm using the <code>tf.estimator</code> API with TensorFlow 2.1 on Google AI Platform to build a DNN Regressor. To use AI Platform Training hyperparameter tuning, I followed <a href=""https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning"" rel=""nofollow noreferrer"">Google's docs</a>.
I used the following configuration parameters:</p>
<p>config.yaml:</p>
<pre><code>trainingInput:
    scaleTier: BASIC
    hyperparameters:
        goal: MINIMIZE
        maxTrials: 2
        maxParallelTrials: 2
        hyperparameterMetricTag: rmse
        enableTrialEarlyStopping: True
        params:
        - parameterName: batch_size
          type: DISCRETE
          discreteValues:
          - 100
          - 200
          - 300
        - parameterName: lr
          type: DOUBLE
          minValue: 0.0001
          maxValue: 0.1
          scaleType: UNIT_LOG_SCALE 
</code></pre>
<p>And to add the metric to my summary, I used the following code for my DNNRegressor:</p>
<pre><code>def rmse(labels, predictions):
    pred_values = predictions['predictions']
    rmse = tf.keras.metrics.RootMeanSquaredError(name='root_mean_squared_error')
    rmse.update_state(labels, pred_values)
    return {'rmse': rmse}

def train_and_evaluate(hparams):
    ...
    estimator = tf.estimator.DNNRegressor(
                       model_dir = output_dir,
                       feature_columns = get_cols(),
                       hidden_units = [max(2, int(FIRST_LAYER_SIZE * SCALE_FACTOR ** i))
                        for i in range(NUM_LAYERS)],
                       optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
                       config = run_config)
    estimator = tf.estimator.add_metrics(estimator, rmse)
</code></pre>
<p>According to Google's documentation, the <code>add_metric</code> function creates a new estimator with the metric specified, which is then used as the hyperparameter metric. However, the AI Platform Training service doesn't recognise this metric:
<a href=""https://i.stack.imgur.com/Hwzq1.png"" rel=""nofollow noreferrer"">Job details on AI Platform</a></p>
<p>On running the code locally, the rmse metric does get outputted in the logs.
So, <strong>how do I make the metric available to the Training job on AI Platform using Estimators?</strong></p>
<p>Additionally, there is an option of reporting the metrics through the <code>cloudml-hypertune</code> Python package. But it requires the value of the metric as one of the input arguments. <strong>How do I extract the metric from <code>tf.estimator.train_and_evaluate</code> function</strong> (since that's the function I use to train/evaluate my estimator) to input into the <code>report_hyperparameter_tuning_metric</code> function?</p>
<pre><code>hpt = hypertune.HyperTune()
hpt.report_hyperparameter_tuning_metric(
    hyperparameter_metric_tag='rmse',
    metric_value=??,
    global_step=1000
)
</code></pre>
<p>ETA: <a href=""https://i.stack.imgur.com/jSfPw.png"" rel=""nofollow noreferrer"">Logs show no error</a>. It says that the job completed successfully even though it fails.</p>
",0
64081367,Slicing a tensor with a tensor of indices and tf.gather,"<p>I am trying to slice a tensor with a indices tensor. For this purpose I am trying to use <code>tf.gather</code>.
However, I am having a hard time understanding the <a href=""https://www.tensorflow.org/api_docs/python/tf/gather"" rel=""nofollow noreferrer"">documentation</a> and don't get it to work as I would expect it to:</p>
<p>I have two tensors. An <code>activations</code> tensor with a shape of <code>[1,240,4]</code> and an <code>ids</code> tensor with the shape <code>[1,1,120]</code>. I want to slice the second dimension of the <code>activations</code> tensor with the indices provided in the third dimension of the <code>ids</code> tensor:</p>
<pre><code>downsampled_activations = tf.gather(activations, ids, axis=1)
</code></pre>
<p>I have given it the <code>axis=1</code> option since that is the axis in the <code>activations</code> tensor I want to slice.</p>
<p>However, this does not render the expected result and only gives me the following error:</p>
<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[0,0,1] = 1 is not in [0, 1)
</code></pre>
<p>I have tried various combinations of the <code>axis</code> and <code>batch_dims</code> options, but to no avail so far and the documentation doesn't really help me on my path. Anybody care to explain the parameters in more detail or on the example above would be very helpful!</p>
<p><strong>Edit:</strong>
The IDs are precomputed before runtime and come in through an input pipeline as such:</p>
<pre><code>features = tf.io.parse_single_example(
            serialized_example,
            features={ 'featureIDs': tf.io.FixedLenFeature([], tf.string)}
</code></pre>
<p>They are then reshaped into the previous format:</p>
<pre><code>feature_ids_raw = tf.decode_raw(features['featureIDs'], tf.int32)
feature_ids_shape = tf.stack([batch_size, (num_neighbours * 4)])
feature_ids = tf.reshape(feature_ids_raw, feature_ids_shape)
feature_ids = tf.expand_dims(feature_ids, 0)
</code></pre>
<p>Afterwards they have the previously mentioned shape (<code>batch_size = 1</code> and <code>num_neighbours = 30</code> -&gt; <code>[1,1,120]</code>) and I want to use them to slice the <code>activations</code> tensor.</p>
<p><strong>Edit2:</strong> I would like the output to be <code>[1,120,4]</code>. (So I would like to gather the entries along the second dimension of the <code>activations</code> tensor in accordance with the IDs stored in my <code>ids</code> tensor.)</p>
",1
64129397,How to multiply tensors with different shapes/dimensions?,"<p>I have a convolutional autoencoder model. While an autoencoder typically focuses on reconstructing the input without using any label information, I want to use the class label to perform class conditional scaling/shifting after convolutions. I am curious if utilizing the label in this way might help produce better reconstructions.</p>
<pre><code>num_filters = 32
input_img = layers.Input(shape=(28, 28, 1)) # input image
label = layers.Input(shape=(10,)) # label

# separate scale value for each of the filter dimensions
scale = layers.Dense(num_filters, activation=None)(label) 
# conv_0 produces something of shape (None,14,14,32)
conv_0 = layers.Conv2D(num_filters, (3, 3), strides=2, activation=None, padding='same')(input_img) 

# TODO: Need help here. Multiply conv_0 by scale along each of the filter dimensions. 
# This still outputs something of shape (None,14,14,32)
# Essentially each 14x14x1 has it's own scalar multiplier 
</code></pre>
<p>In the example above, the output of the convolutional layer is (14,14,32) and the scale layer is of shape (32,). I want the convolutional output to be multiplied by the corresponding scale value along each filter dimension. For example, if these were numpy arrays I could do something like <code>conv_0[:, :, i] * scale[i]</code> for i in range(32).</p>
<p>I looked at <code>tf.keras.layers.Multiply</code> which can be found <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Multiply"" rel=""nofollow noreferrer"">here</a>, but based on the documentation I believe that takes in tensors of the same size as input. How do I work around this?</p>
",0
64162361,How to iterate on keras Dataset and edit content,"<p>I am working on this movie classification problem
<a href=""https://www.tensorflow.org/tutorials/keras/text_classification"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/keras/text_classification</a></p>
<p>In this example text files(12500 files with movie revies) are read and a batched dataset is prepared like below</p>
<pre class=""lang-py prettyprint-override""><code>raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(
    'aclImdb/train', 
    batch_size=batch_size, 
    validation_split=0.2, 
    subset='training', 
    seed=seed)
</code></pre>
<p>at the time of standardization</p>
<pre><code>def custom_standardization(input_data):
  lowercase = tf.strings.lower(input_data)
  stripped_html = tf.strings.regex_replace(lowercase, '&lt;br /&gt;', ' ')
#I WANT TO REMOVE STOP WORDS HERE, CAN I DO
  return tf.strings.regex_replace(stripped_html,'[%s]' % re.escape(string.punctuation),'')
</code></pre>
<p>Problem: I understand that I have got training dataset with labels in variable '<strong>raw_train_ds</strong>'. Now I want to iterate over this dataset and remove stop words from the movie review text and store back to same variable, I tried to do it in function 'custom_standardization' but it gives type error,</p>
<p>I also tried to use <code>tf.strings.as_strings</code> but it returns error
<em>InvalidArgumentError: Value for attr 'T' of string is not in the list of allowed values: int8, int16, int32, int64</em></p>
<p>can someone please help on it OR simply please help how to remove stopwords from the batch dataset</p>
",0
64312153,tf.newaxis operation in TensorFlow,"<p><code>x_train = x_train[..., tf.newaxis].astype(&quot;float32&quot;)</code></p>
<p><code>x_test = x_test[..., tf.newaxis].astype(&quot;float32&quot;)</code></p>
<p>Can someone please explain how <code>tf.newaxis</code> works ?</p>
<p>I found a brief mention in the documentation</p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/strided_slice"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/strided_slice</a></p>
<p>but I could not properly understand.</p>
",1
64313451,Shall we set experimental_aggregate_gradients to False or True when manually accumulating gradients in Tensorflow?,"<p>I am trying to perform a manual gradient accumulation in <strong>Tensorflow 2.2.0</strong> using <strong>Python 3.8.5</strong>.</p>
<p>I have a piece of code where I gather a series of gradients( of total number of grads_total_number), in a list: <code>grads_list</code> and I do the accumulation using following code:</p>
<pre><code>avg_accum_grads=[]    
for grad_ind in range(grads_total_number):
        avg_accum_grads.append(tf.reduce_mean(grads_list[grad_ind], axis=0))
</code></pre>
<p>I then intend to apply these grads via my optimizer to my model:</p>
<pre><code>myopt.apply_gradients(zip(avg_accum_grads, model.trainable_variables),experimental_aggregate_gradients=True)
</code></pre>
<p>, where my optimizer is Adam defined as <code>tf.keras.optimizers.Adam</code></p>
<p>However, due to the documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer"" rel=""nofollow noreferrer"">here</a>, I am confused if I have to set <code>experimental_aggregate_gradients</code> to False. I could not clearly understand, here that I have done the accumulation manually, if I let it be true, it continues to accumulate?</p>
<p>Any help is very appreciated.</p>
",1
64315183,Dealing with highly imbalanced datasets using Tensorflow Dataset and Keras Tuner,"<p>I have a highly imbalanced dataset (3% Yes, 87% No) of textual documents, containing a title and abstract feature. I have transformed these documents into <code>tf.data.Dataset</code> entities with padded batches. Now, I am trying to train this dataset using Deep Learning. With <code>model.fit()</code> in TensorFlow, you have the <code>class_weights</code> parameter to deal with class imbalance, however, I am seeking for the best parameters using <code>keras-tuner</code> library. In their hyperparameter tuners, they do not have such an option. Therefore, I am seeking other options for dealing with class imbalance.</p>
<p>Is there an option to use class weights in <code>keras-tuner</code>? To add, I am already using the <code>precision@recall</code> metric. I could also try a data resampling method, such as <code>imblearn.over_sampling.SMOTE</code>, but as <a href=""https://www.kaggle.com/theoviel/dealing-with-class-imbalance-with-smote"" rel=""nofollow noreferrer"">this</a> Kaggle post mentions:</p>
<blockquote>
<p>It appears that SMOTE does not help improve the results. However, it makes the network learning faster. Moreover, there is one big problem, this method is not compatible larger datasets. You have to apply SMOTE on embedded sentences, which takes way too much memory.</p>
</blockquote>
",0
64326029,Load tensorflow images and create patches,"<p>I am using <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory"" rel=""nofollow noreferrer"">image_dataset_from_directory</a> to load a very large RGB imagery dataset from disk into a <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">Dataset</a>. For example,</p>
<pre><code>dataset = tf.keras.preprocessing.image_dataset_from_directory(
    &lt;directory&gt;,
    label_mode=None,
    seed=1,
    subset='training',
    validation_split=0.1)
</code></pre>
<p>The Dataset has, say, 100000 images grouped into batches of size 32 yielding a <code>tf.data.Dataset</code> with spec <code>(batch=32, width=256, height=256, channels=3)</code></p>
<p>I would like to extract patches from the images to create a new <code>tf.data.Dataset</code> with image spatial dimensions of, say, 64x64.</p>
<p>Therefore, I would like to create a new Dataset with 400000 patches still in batches of 32 with a <code>tf.data.Dataset</code> with spec <code>(batch=32, width=64, height=64, channels=3)</code></p>
<p>I've looked at the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window"" rel=""nofollow noreferrer"">window</a> method and the <a href=""https://www.tensorflow.org/api_docs/python/tf/image/extract_patches"" rel=""nofollow noreferrer"">extract_patches</a> function but it's not clear from the documentation how to use them to create a new Dataset I need to start training on the patches. The <code>window</code> seems to be geared toward 1D tensors and the <code>extract_patches</code> seems to work with arrays and not with Datasets.</p>
<p>Any suggestions on how to accomplish this?</p>
<p>UPDATE:</p>
<p>Just to clarify my needs. I am trying to avoid manually creating the patches on disk. One, that would be untenable disk wise. Two, the patch size is not fixed. The experiments will be conducted over several patch sizes. So, I do not want to manually perform the patch creation either on disk or manually load the images in memory and perform the patching. I would prefer to have tensorflow handle the patch creation as part of the pipeline workflow to minimize disk and memory usage.</p>
",1
64356209,How does Model.fit() method's shuffle deals with Batches when using a tf.data.Dataset?,"<p>I am using tensorflow 2.</p>
<p>When using the <code>Model.fit()</code> method with a <code>tf.data.Dataset</code>, the argument '<code>batch_size</code>' is ignored. Thus to train my model on batches, I have to first change my dataset of samples into a dataset of batches of samples by calling <code>tf.data.Dataset.batch(batch_size)</code>.</p>
<p>Then, after reading the documentation, I don't understand clearly how the <code>.fit()</code> method will shuffle my dataset at each epoch.</p>
<p><strong>Since my dataset is a dataset of batches, will it shuffle the batches among each other</strong> (the batches remain unchanged) <strong>? Or will it shuffle all the samples and then regroup them into new batches</strong> (which is the desired behaviour) <strong>?</strong></p>
<p>Thanks a lot for your help.</p>
",1
64380057,TF 2.3.0 training keras model using tf dataset with sample weights does not apply to metrics,"<p>I am passing in sample_weight as the 3rd tuple in tf.data.Dataset (using it in the context of mask, so my sample_weight are either 0, or 1. The problem is that this sample_weight doesn't seem to get applied to metrics calculation. (Ref: <a href=""https://www.tensorflow.org/guide/keras/train_and_evaluate#sample_weights"" rel=""noreferrer"">https://www.tensorflow.org/guide/keras/train_and_evaluate#sample_weights</a>)</p>
<p>Here's code snippet:</p>
<pre><code>train_ds = tf.data.Dataset.from_tensor_slices((imgs, labels, masks))
train_ds = train_ds.shuffle(1024).repeat().batch(32).prefetch(buffer_size=AUTO)

model.compile(optimizer = Adam(learning_rate=1e-4),
             loss = SparseCategoricalCrossentropy(),
             metrics = ['sparse_categorical_accuracy'])

model.fit(train_ds, steps_per_epoch = len(imgs)//32, epochs = 20)
</code></pre>
<p>The loss after training is very close to zero, but sparse_categorical_accuracy is not (about 0.89). So I highly suspect whatever sample_weight (masks) that's passed in to construct the tf.dataset, does NOT get applied when the metrics is reported during training, while loss seems to be correct. I further confirmed by running prediction on the subset that are not masked separately, and confirmed the accuracy is 1.0</p>
<p>Also, according to documentation:</p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalAccuracy"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalAccuracy</a></p>
<p>the metric has 3 args: y_true, y_pred, sample_weight</p>
<p>So how does one pass the sample_weight during metric computation? Is this the responsibility of model.fit(...) within the keras framework? I can't find any example googling around so far.</p>
",1
64424397,Keras - Custom layer with multiple inputs,"<p>I would like to implement a custom <code>tf.keras</code> layer called <code>MyLayer</code> which has three inputs and contains a sub layer which in turn has three inputs, like in the figure below:</p>
<p><a href=""https://i.stack.imgur.com/Um7yn.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Um7yn.jpg"" alt=""MyLayer"" /></a></p>
<p>I assume that the right thing to do would be to create a <code>MyLayer</code> class that extends <code>tf.keras.layers.Layer</code> and implement the <code>__init__</code>, <code>build</code> and <code>call</code> methods, as mentioned in the <a href=""https://www.tensorflow.org/guide/keras/custom_layers_and_models"" rel=""nofollow noreferrer"">official documentation</a>.</p>
<p>Now, the examples provided in the documentation are relative to pretty simple layers that are composed of several sublayers connected in a sequential manner, that is one after the other. For instance, the <a href=""https://www.tensorflow.org/guide/keras/custom_layers_and_models#layers_are_recursively_composable"" rel=""nofollow noreferrer""><code>MLPBlock</code></a> layer consists of 3 linear layers ordered sequentially.</p>
<p>In general, however, sublayers are not ordered sequentially, but can form branches. This suggests that those layers could be run in parallel, since they are not connected to one another.
Going back to the custom layer I would like to implement, you can see that <code>Layer1</code>, <code>Layer2</code> and <code>Layer3</code> could be run in parallel. Once their outputs are computed, they can be fed to <code>Layer4</code>. The point is: how do I run them in parallel? I couldn't find any &quot;ParallelCombinator&quot; or things like that among the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers"" rel=""nofollow noreferrer"">available Keras layers</a>.</p>
<p>If I were to follow the examples provided in the documentation, I would write something along these lines:</p>
<pre class=""lang-py prettyprint-override""><code>class MyLayer(keras.layers.Layer):
    def __init__(self, ...):
        super(MyLayer, self).__init__()
        self.layer_1 = Layer1(...)
        self.layer_2 = Layer2(...)
        self.layer_3 = Layer3(...)
        self.layer_4 = Layer4(...)

    def call(self, inputs):
        tmp_1 = self.layer_1(inputs[0])
        tmp_2 = self.layer_2(inputs[1])
        tmp_3 = self.layer_3(inputs[2])
        return self.layer4([tmp_1, tmp_2, tmp_3])
</code></pre>
<p>This, however, would imply that <code>Layer1</code>, <code>Layer2</code> and <code>Layer3</code> are run sequentially, not in parallel.</p>
<p>One possible solution that I came up with involves structuring <code>MyLayer</code> as a <code>tf.keras.Model</code> built with Keras's functional API rather than as a subclass of <code>tf.keras.Layer</code>, like so:</p>
<pre class=""lang-py prettyprint-override""><code>def MyLayer(...):
    input_1 = tf.keras.layers.Input(...)
    input_2 = tf.keras.layers.Input(...)
    input_3 = tf.keras.layers.Input(...)

    layer_1 = Layer1(...)(input_1)
    layer_2 = Layer2(...)(input_2)
    layer_3 = Layer3(...)(input_3)
    output_1 = Layer4(...)([layer_1, layer_2, layer_3])

    return tf.keras.Model(inputs=[input_1, input_2, input_3], outputs=output_1)

if __name__ == '__main__':
    my_layer = MyLayer(...)
    input_1 = ...
    input_2 = ...
    input_3 = ...
    output = my_layer([input_1, input_2, input_3])
</code></pre>
<p>The reason why I think this would work is that I assume that when I feed some inputs to a <code>tf.keras.Model</code>, as in <code>output = my_layer([input_1, input_2, input_3])</code>, the layers that <em>can</em> be run in parallel are effectively run in parallel (or are they?). This solution, however, feels like a hack to me, as <code>MyLayer</code> is supposed to be a layer, <em>not</em> a model. In fact, a <code>tf.keras.Model</code> instance exposes methods like <code>fit(...)</code> that aren't meant to be called on a layer.</p>
<p>Does anybody know what's the best approach to implement <code>MyLayer</code>?</p>
",1
64552543,Tensorflow 2.2.0 :- WARNING:tensorflow:Gradients do not exist for variables when minimizing the loss,"<p>After implementing Custom Loss class as per the tensorflow api documentation and when invoking model.fit , facing this warning alongwith below error:- This is reference link on github and they have asked to raise here in stack overflow.https://github.com/tensorflow/tensorflow/issues/42542# TypeError: An op outside of the function building code is being passed a &quot;Graph&quot; tensor. It is possible to have Graph tensors leak out of the function building context by including a tf.init_scope in your function building code. For example, the following function will fail:</p>
<hr />
<blockquote>
<p>@tf.function   def has_init_scope():
my_constant = tf.constant(1.)
with tf.init_scope():
added = my_constant * 2 The graph tensor has name: ident33/Relu_5:0</p>
</blockquote>
",1
64611137,port TensorFlow 1 code to TensorFlow 2 (model learning process without sess.run),"<p>I have this piece of tf1 code, which was taken from nice book &quot;Deep Learning&quot; by S. Nikolenko.</p>
<p>It's a simple linear regression that learns <code>k</code> and <code>b</code> to 2 and 1 respectively.</p>
<pre><code>%tensorflow_version 1.x

import numpy as np,tensorflow as tf
import pandas as pd

n_samples, batch_size, num_steps = 1000, 100, 20000 #set learning constants
X_data = np.random.uniform(1, 10, (n_samples, 1)) #generate array x from 1 to 10 of shape (1000,1)
print(X_data.shape)
y_data = 2 * X_data + 1 + np.random.normal(0, 2, (n_samples, 1)) #generate right answer and add noise to it (to make it scatter)

X = tf.placeholder(tf.float32, shape=(batch_size, 1)) #defining placeholders to put into session.run
y = tf.placeholder(tf.float32, shape=(batch_size, 1))

with tf.variable_scope('linear-regression'):
  k = tf.Variable(tf.random_normal((1, 1)), name='slope') #defining 2 variables with shape (1,1)
  b = tf.Variable(tf.zeros((1,)), name='bias') # and (1,)
  print(k.shape,b.shape)

y_pred = tf.matmul(X, k) + b # all predicted y in batch, represents linear formula k*x + b
loss = tf.reduce_sum((y - y_pred) ** 2)  # mean square
optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)
display_step = 100

with tf.Session() as sess:
  sess.run(tf.initialize_variables([k,b]))
  for i in range(num_steps):
    indices = np.random.choice(n_samples, batch_size) # taking random indices
    X_batch, y_batch = X_data[indices], y_data[indices] # taking x and y from generated examples
    _, loss_val, k_val, b_val = sess.run([optimizer, loss, k, b ],
      feed_dict = { X : X_batch, y : y_batch })
    if (i+1) % display_step == 0:
      print('Epoch %d: %.8f, k=%.4f, b=%.4f' %
        (i+1, loss_val, k_val, b_val))

</code></pre>
<p>I'm striving to port it on TensorFlow 2</p>
<p>And for long time I can't wrap my head what should I use instead of <code>sess.run()</code> and <code>feed_dict</code>, which doing magic behind the scenes, official documentation go into to details with writing model class and so on, but I'm want to keep this as flat as possible.</p>
<p>Also it's suggested to calculate derivatives with <code>tf.GradientTape</code>, but I'm struggling with applying it right to my example</p>
<pre><code>%tensorflow_version 2.x

import numpy as np,tensorflow as tf
import pandas as pd

n_samples, batch_size, num_steps = 1000, 100, 200
X_data = np.random.uniform(1, 10, (n_samples, 1))
y_data = 2 * X_data + 1 + np.random.normal(0, 2, (n_samples, 1))

X = tf.Variable(tf.zeros((batch_size, 1)), dtype=tf.float32, shape=(batch_size, 1))
y = tf.Variable(tf.zeros((batch_size, 1)), dtype=tf.float32, shape=(batch_size, 1))

k = tf.Variable(tf.random.normal((1, 1)), name='slope')
b = tf.Variable(tf.zeros((1,)), name='bias')

loss = lambda: tf.reduce_sum((y - (tf.matmul(X, k) + b)) ** 2)
optimizer = tf.keras.optimizers.SGD(0.01).minimize(loss, [k, b, X, y])
display_step = 100


for i in range(num_steps):
  indices = np.random.choice(n_samples, batch_size)
  X_batch, y_batch = X_data[indices], y_data[indices]
  
</code></pre>
<p>I need SGD optimizer minimize that given loss function and learn k and b values, how can I achieve it from this point?</p>
",1
64612657,Manually change weights of Keras convolutional layer,"<p>There is a way to change manually the weights for the tf.layers.Conv2d (<a href=""https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers/Conv2D"" rel=""nofollow noreferrer"">https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers/Conv2D</a>)? Because this class takes in only the input, the number of kernels to use, etc... and the weights are automatically stored and computed by Tensorflow, but I would a way (like in tf.nn.conv2d - <a href=""https://www.tensorflow.org/api_docs/python/tf/nn/conv2d"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/nn/conv2d</a>) to pass directly the weights to the class.</p>
<p>Anyone have a suggestion?</p>
<p>Maybe one could be to load and change manually the value for the variable associate at that layer? I found this solution very bad but it could work.</p>
<p>Thank you.</p>
",0
64642944,Steps of tf.summary.* operations in TensorBoard are always 0,"<p>When I'm training my model with TensorFlow 2.3, I want to visualize some intermediate tensors calculated using the weight in the computation graph of my customized <code>tf.keras.layers.Layer</code>.</p>
<p>So I use <code>tf.summary.image()</code> to record these tensors and visualize them as images like this:</p>
<pre><code>class CustomizedLayer(tf.keras.layers.Layer):
    def call(self, inputs, training=None):
        # ... some code ...
        tf.summary.image(name=&quot;some_weight_map&quot;, data=some_weight_map)
        # ... some code ...
</code></pre>
<p>But in TensorBoard, no matter how many steps passed, there is only one image of step 0 shown.</p>
<p>And I tried to set the parameter <em><strong>step</strong></em> of <code>tf.summary.image()</code> to the value obtained from <code>tf.summary.experimental.get_step()</code>:</p>
<pre><code>tf.summary.image(name=&quot;weight_map&quot;, data=weight_map, step=tf.summary.experimental.get_step())
</code></pre>
<p>And update the step by calling <strong>tf.summary.experimental.set_step</strong> from a customized Callback using a tf.Variable like codes shown below:</p>
<pre><code>class SummaryCallback(tf.keras.callbacks.Callback):
def __init__(self, step_per_epoch):
    super().__init__()
    self.global_step = tf.Variable(initial_value=0, trainable=False, name=&quot;global_step&quot;)
    self.global_epoch = 0
    self.step_per_epoch = step_per_epoch
    tf.summary.experimental.set_step(self.global_step)

def on_batch_end(self, batch, logs=None):
    self.global_step = batch + self.step_per_epoch * self.global_epoch
    tf.summary.experimental.set_step(self.global_step)  
    # whether the line above is commented, calling tf.summary.experimental.get_step() in computation graph code always returns 0.
    # tf.print(self.global_step)

def on_epoch_end(self, epoch, logs=None):
    self.global_epoch += 1
</code></pre>
<p>This Callback's instance is passed in the argument <em><strong>callbacks</strong></em> in <code>model.fit()</code> function.</p>
<p>But the value <code>tf.summary.experimental.get_step()</code> returned is still 0.</p>
<p>The TensorFlow document of &quot;<code>tf.summary.experimental.set_step()</code>&quot; says:</p>
<blockquote>
<p>when using this with @tf.functions, the step value will be captured at the time the function is traced, so changes to the step outside the function will not be reflected inside the function unless using a tf.Variable step.</p>
</blockquote>
<p>Accroding to the document, I am already using a Variable to store the steps, but it's changes are still not reflected inside the function (or keras.Model).</p>
<p>Note: My code produces expected results in TensorFlow 1.x with just a simple line of <code>tf.summary.image()</code> before I migrate it to TensorFlow 2.</p>
<p>So I want to know if my approach is wrong in TensorFlow 2?</p>
<p>In TF2, how can I <strong>get training steps inside the computation graph</strong>?</p>
<p>Or there is other solution to <strong>summarize tensors (as scalar, image, etc.) inside a model in TensorFlow 2</strong>?</p>
",1
64662676,why tf.random.log_uniform_candidate_sampler gives true class?,"<p>I'm reading tensorflow's word2vec tutorial: <a href=""https://www.tensorflow.org/tutorials/text/word2vec#define_loss_function_and_compile_model"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/word2vec#define_loss_function_and_compile_model</a></p>
<p>In this tutorial, nagative sampling is conducted using <code>tf.random.log_uniform_candidate_sampler</code>. Given the context class (true class), the goal is to sample negative classes from the whole vocabulary list. To my understanding, The negative classes must differ from the given context class. However, I found that the context class may appear in the negative classes sampled by <code>tf.random.log_uniform_candidate_sampler</code>. Here is the code:</p>
<pre><code>import tensorflow as tf
SEED = 42 

# encode the words
sentence = &quot;The wide road shimmered in the hot sun&quot;
tokens = list(sentence.lower().split())
vocab, index = {}, 1 # start indexing from 1
vocab['&lt;pad&gt;'] = 0 # add a padding token 
for token in tokens:
  if token not in vocab: 
    vocab[token] = index
    index += 1
vocab_size = len(vocab)
print(vocab)
inverse_vocab = {index: token for token, index in vocab.items()}
print(inverse_vocab)


# make (hot, the) as a context pair
target_word, context_word = 6, 1
print(&quot;target: {}, context: {}&quot;.format(inverse_vocab[target_word], inverse_vocab[context_word]))


# negative sampling
# Set the number of negative samples per positive context. 
num_ns = 4

context_class = tf.reshape(tf.constant(context_word, dtype=&quot;int64&quot;), (1, 1))
negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(
    true_classes=context_class, # class that should be sampled as 'positive'
    num_true=1, # each positive skip-gram has 1 positive context class
    num_sampled=num_ns, # number of negative context words to sample
    unique=True, # all the negative samples should be unique
    range_max=vocab_size, # pick index of the samples from [0, vocab_size]
    seed=SEED, # seed for reproducibility
    name=&quot;negative_sampling&quot; # name of this operation
)
print(&quot;negative samples\' index&quot;, negative_sampling_candidates)
print(&quot;negetive samples: &quot;, [inverse_vocab[index.numpy()] for index in negative_sampling_candidates])
# &quot;the&quot; will show in negative samples, if not, run it several times.
</code></pre>
<p>The word <code>the</code> is the context class of word <code>hot</code>, why it could show in the sampled negative classes? Moreover, the target word <code>hot</code> could also be sampled as negative class. Do I misunderstand something?</p>
",0
64687375,Get labels from dataset when using tensorflow image_dataset_from_directory,"<p>I wrote a simple CNN using tensorflow (v2.4) + keras in python (v3.8.3). I am trying to optimize the network, and I want more info on what it is failing to predict. I am trying to add a confusion matrix, and I need to feed tensorflow.math.confusion_matrix() the test labels.</p>
<p>My problem is that I cannot figure out how to access the labels from the dataset object created by tf.keras.preprocessing.image_dataset_from_directory()</p>
<p>My images are organized in directories having the label as the name. The documentation says the function returns a tf.data.Dataset object.</p>
<blockquote>
<pre><code>If label_mode is None, it yields float32 tensors of shape (batch_size, image_size[0], image_size[1], num_channels), encoding
</code></pre>
<p>images (see below for rules regarding num_channels).
Otherwise, it yields a tuple (images, labels), where images has shape (batch_size, image_size[0], image_size[1], num_channels), and
labels follows the format described below.</p>
</blockquote>
<p>Here is the code:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras import layers
#import matplotlib.pyplot as plt
import numpy as np
import random

import PIL
import PIL.Image

import os
import pathlib

#load the IMAGES
dataDirectory = '/p/home/username/tensorflow/newBirds'

dataDirectory = pathlib.Path(dataDirectory)
imageCount = len(list(dataDirectory.glob('*/*.jpg')))
print('Image count: {0}\n'.format(imageCount))

#test display an image
# osprey = list(dataDirectory.glob('OSPREY/*'))
# ospreyImage = PIL.Image.open(str(osprey[random.randint(1,100)]))
# ospreyImage.show()

# nFlicker = list(dataDirectory.glob('NORTHERN FLICKER/*'))
# nFlickerImage = PIL.Image.open(str(nFlicker[random.randint(1,100)]))
# nFlickerImage.show()

#set parameters
batchSize = 32
height=224
width=224

(trainData, trainLabels) = tf.keras.preprocessing.image_dataset_from_directory(
    dataDirectory,
    labels='inferred',
    label_mode='categorical',
    validation_split=0.2,
    subset='training',
    seed=324893,
    image_size=(height,width),
    batch_size=batchSize)

testData = tf.keras.preprocessing.image_dataset_from_directory(
    dataDirectory,
    labels='inferred',
    label_mode='categorical',
    validation_split=0.2,
    subset='validation',
    seed=324893,
    image_size=(height,width),
    batch_size=batchSize)

#class names and sampling a few images
classes = trainData.class_names
testClasses = testData.class_names
#plt.figure(figsize=(10,10))
# for images, labels in trainData.take(1):
#     for i in range(9):
#         ax = plt.subplot(3, 3, i+1)
#         plt.imshow(images[i].numpy().astype(&quot;uint8&quot;))
#         plt.title(classes[labels[i]])
#         plt.axis(&quot;off&quot;)
# plt.show()

#buffer to hold the data in memory for faster performance
autotune = tf.data.experimental.AUTOTUNE
trainData = trainData.cache().shuffle(1000).prefetch(buffer_size=autotune)
testData = testData.cache().prefetch(buffer_size=autotune)

#augment the dataset with zoomed and rotated images
#use convolutional layers to maintain spatial information about the images
#use max pool layers to reduce
#flatten and then apply a dense layer to predict classes
model = tf.keras.Sequential([
    #layers.experimental.preprocessing.RandomFlip('horizontal', input_shape=(height, width, 3)),
    #layers.experimental.preprocessing.RandomRotation(0.1),
    #layers.experimental.preprocessing.RandomZoom(0.1),
    layers.experimental.preprocessing.Rescaling(1./255, input_shape=(height, width, 3)),
    layers.Conv2D(16, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(32, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(128, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    layers.Conv2D(256, 3, padding='same', activation='relu'),
    layers.MaxPooling2D(),
    # layers.Conv2D(512, 3, padding='same', activation='relu'),
    # layers.MaxPooling2D(),
    #layers.Conv2D(1024, 3, padding='same', activation='relu'),
    #layers.MaxPooling2D(),
    #dropout prevents overtraining by not allowing each node to see each datapoint
    #layers.Dropout(0.5),
    layers.Flatten(),
    layers.Dense(512, activation='relu'),
    layers.Dense(len(classes))
    ])

model.compile(optimizer='adam',
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])
model.summary()
    
epochs=2
history = model.fit(
    trainData,
    validation_data=testData,
    epochs=epochs
    )

#create confusion matrix
predictions = model.predict_classes(testData)
confusionMatrix = tf.math.confusion_matrix(labels=testClasses, predictions=predictions).numpy()
</code></pre>
<p>I have tried using (foo, foo1) = tf.keras.preprocessing.image_dataset_from_directory(dataDirectory, etc), but I get
(trainData, trainLabels) = tf.keras.preprocessing.image_dataset_from_directory(
ValueError: too many values to unpack (expected 2)</p>
<p>And if I try to return as one variable and then split it as so:</p>
<pre><code>train = tf.keras.preprocessing.image_dataset_from_directory(
    dataDirectory,
    labels='inferred',
    label_mode='categorical',
    validation_split=0.2,
    subset='training',
    seed=324893,
    image_size=(height,width),
    batch_size=batchSize)
trainData = train[0]
trainLabels = train[1]
</code></pre>
<p>I get TypeError: 'BatchDataset' object is not subscriptable</p>
<p>I can access the labels via testClasses = testData.class_names, but I get:</p>
<blockquote>
<p>2020-11-03 14:15:14.643300: W
tensorflow/core/framework/op_kernel.cc:1740] OP_REQUIRES failed at
cast_op.cc:121 : Unimplemented: Cast string to int64 is not supported
Traceback (most recent call last):   File &quot;birdFake.py&quot;, line 115, in

confusionMatrix = tf.math.confusion_matrix(labels=testClasses, predictions=predictions).numpy()   File
&quot;/p/home/username/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py&quot;,
line 201, in wrapper
return target(*args, **kwargs)   File &quot;/p/home/username/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/confusion_matrix.py&quot;,
line 159, in confusion_matrix
labels = math_ops.cast(labels, dtypes.int64)   File &quot;/p/home/username/miniconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py&quot;,
line 201, in wrapper
return target(*args, **kwargs)   File &quot;/p/home/username/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py&quot;,
line 966, in cast
x = gen_math_ops.cast(x, base_type, name=name)   File &quot;/p/home/username/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py&quot;,
line 1827, in cast
_ops.raise_from_not_ok_status(e, name)   File &quot;/p/home/username/miniconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py&quot;,
line 6862, in raise_from_not_ok_status
six.raise_from(core._status_to_exception(e.code, message), None)   File &quot;&quot;, line 3, in raise_from
tensorflow.python.framework.errors_impl.UnimplementedError: Cast
string to int64 is not supported [Op:Cast]</p>
</blockquote>
<p>I am open to any method to get those labels into the confusion matrix. Any ideas as to why what I am doing is not working would also be appreciated.</p>
<p>UPDATE: I tried the method proposed by Alexandre Catalano, and I get the following error</p>
<blockquote>
<p>Traceback (most recent call last):   File &quot;./birdFake.py&quot;, line 118,
in 
labels = np.concatenate([labels, np.argmax(y.numpy(), axis=-1)])   File &quot;&lt;<strong>array_function</strong> internals&gt;&quot;, line 5, in concatenate
ValueError: all the input arrays must have same number of dimensions,
but the array at index 0 has 1 dimension(s) and the array at index 1
has 0 dimension(s)</p>
</blockquote>
<p>I printed the first element of the labels array, and it is zero</p>
",1
64700704,How to save a seq2seq model in TensorFlow 2.x?,"<p>I'm following the &quot;Neural machine translation with attention&quot; tutorial from TensorFlow docs, but can't figure out how to save the model as a <code>SavedModel</code> file.</p>
<p>As seen in the docs, I can save a checkpoint fairly easily, but afaik that's not very useful when integrating with other applications.  Does anyone know to save the whole &quot;model&quot;, even though they're not using <code>tf.keras.Model</code>?</p>
<p>Docs: <a href=""https://www.tensorflow.org/tutorials/text/nmt_with_attention"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/nmt_with_attention</a></p>
",0
64744618,How to clip layer output in MLP with `tf.keras.activations.relu()`?,"<p>According to the <a href=""https://keras.io/api/layers/activations/"" rel=""nofollow noreferrer"">documentation</a>, <code>tf.keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0)</code> seems to <strong>clip</strong> <code>x</code> within [threshold, max_value], but <code>x</code> must be specified. How can I use it for clipping the output of a layer in neural network? Or is there a more convenient way to do so?</p>
<p>Suppose I want to output the linear combination of all elements of a 10-by-10 2D-array only when the result is between 0 and 5.</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
from tensorflow import keras

model = keras.models.Sequential()
model.add(keras.layers.Flatten(input_shape=[10, 10]))
model.add(keras.layers.Dense(1, activation='relu')    # output layer
</code></pre>
",1
64759627,TensorFlow custom training step with different loss functions,"<h1>Background</h1>
<p>According to the <a href=""https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch"" rel=""nofollow noreferrer"">TensorFlow documentation</a>, a custom training step can be performed with the following</p>
<pre><code># Fake sample data for testing
x_batch_train = tf.zeros([32, 3, 1], dtype=&quot;float32&quot;)
y_batch_train = tf.zeros([32], dtype=&quot;float32&quot;)
</code></pre>
<pre><code>loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)
with tf.GradientTape() as tape:
    logits = model(x_batch_train, training=True)
    loss_value = loss_fn(y_batch_train, logits)

grads = tape.gradient(loss_value, model.trainable_weights)
optimizer.apply_gradients(zip(grads, model.trainable_weights))
</code></pre>
<br>
<p>But if I want to use a different loss function like categorical cross-entropy I would need to argmax the logits created in the gradient tape:</p>
<pre><code>loss_fn = tf.keras.lossees.get(&quot;categorical_crossentropy&quot;)
with tf.GradientTape() as tape:
    logits = model(x_batch_train, training=True)
    prediction = tf.cast(tf.argmax(logits, axis=-1), y_batch_train.dtype)
    loss_value = loss_fn(y_batch_train, prediction)

grads = tape.gradient(loss_value, model.trainable_weights)
optimizer.apply_gradients(zip(grads, model.trainable_weights))
</code></pre>
<hr>
<h1>Problem</h1>
<p>The problem with this is that the <code>tf.argmax</code> function is not differentiable, so TensorFlow wouldn't be able to compute the gradients and you would get the error:</p>
<pre><code>ValueError: No gradients provided for any variable: [...]
</code></pre>
<hr>
<p><strong>My question:</strong> Without changing the loss function how could I make the second example work?</p>
",1
64769427,Problems installing / using CleverHans library,"<p>I try to install or use the cleverhans library with tensorflow but it fails. (Cleverhans 3.0.1)</p>
<p>I tried TF 1.12, 1.13, 1.15, 2.0, 2.3.1 but everytime another error occured when using the tutorial code.</p>
<p>For example:
TF 2.3.1 --&gt; <em>module 'tensorflow' has no attribute 'GraphKeys'</em>
TF 1.13.1 --&gt; some tf.keras.v1 api calls do not work</p>
<p>Is there any working configuration for using cleverhans with tensorflow? Should I use an older version of cleverhans?</p>
",0
64771384,How to validate data during inference using tfx / tfdv / tensorflow serving?,"<p>I'm building a tfx pipeline and using tensorflow serving to serve my model. I save the signature with <code>model.save(...)</code>.</p>
<p>So far I was able to use the transform layer to transform the feature before prediction with <code>tf_transform_output.transform_features_layer()</code> (see my code below).</p>
<p>However, I'm wondering how one can detect anomalies in the input data? For instance, I don't want to predict for an input value that is too far away from the distribution that a feature was trained with before.</p>
<p>The <code>tfdv</code> library offers functions like <code>generate_statistics_from_[csv|dataframe|tfrecord]</code> but I was not able to find any good example to generate statistics for serialized <code>tf.Example</code>s (or something that is not saved in a file, like csv, tfrecords etc.).</p>
<p>I'm aware of the following <a href=""https://www.tensorflow.org/tfx/data_validation/get_started"" rel=""nofollow noreferrer"">example in the documentation</a>:</p>
<pre><code>   import tensorflow_data_validation as tfdv
   import tfx_bsl
   import pyarrow as pa
   decoder = tfx_bsl.coders.example_coder.ExamplesToRecordBatchDecoder()
   example = decoder.DecodeBatch([serialized_tfexample])
   options = tfdv.StatsOptions(schema=schema)
   anomalies = tfdv.validate_instance(example, options)
</code></pre>
<p>But in this example <code>serialized_tfexample</code> is a string, whereas in my code below the argument <code>serialized_tf_examples</code> is a Tensor of strings.</p>
<p>Sorry if this is an obvious question. I spent all day to find a solution without success. Maybe I'm getting this all thing wrong. Maybe this is not the right place to put validations. So my more generalized question is actually: How do you validate incoming input data before prediction when you serve a model, which you created through a tfx pipeline, in production?
I'm thankful for any lead into the right direction.</p>
<p>Here is my code to which I want to add validation:</p>
<pre><code>...

tf_transform_output = tft.TFTransformOutput(...)
model.tft_layer = tf_transform_output.transform_features_layer()

@tf.function(input_signature=[
    tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')
])
def serve_tf_examples_fn(serialized_tf_examples):

    #### How can I generate stats and validate serialized_tf_examples? ###
    #### Is this the right place? ###

    feature_spec = tf_transform_output.raw_feature_spec()
    feature_spec.pop(TARGET_LABEL)
    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)

    transformed_features = model.tft_layer(parsed_features)

    return model(transformed_features)

...

model.save(serving_model_dir,
           save_format='tf',
           signatures={
               'serving_default': serve_tf_examples_fn
           })
</code></pre>
",1
64775312,Custom gradient with state,"<p>I'm trying to implement <a href=""https://arxiv.org/pdf/2007.14469.pdf"" rel=""nofollow noreferrer"">this</a> gradient clipping paper in tensorflow, which entails storing the history of gradient norms.</p>
<p>I assume I need to do this using the <code>tf.custom_gradient</code> decorator, but how do I maintain a running list of the gradient norm history? Can I use a closure as in the pytorch version?</p>
<p>For reference, here is the implementation in <a href=""https://github.com/pseeth/autoclip"" rel=""nofollow noreferrer"">pytorch</a>.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import torch
from enum import Enum

def _get_grad_norm(model):
    total_norm = 0
    for p in model.parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
    total_norm = total_norm ** (1. / 2)
    return total_norm 

# written for pytorch ignite
# fire this on backwards pass
class BackwardsEvents(Enum):
    BACKWARDS_COMPLETED = 'backwards_completed'

def add_autoclip_gradient_handler(engine, model, clip_percentile):
    # Keep track of the history of gradients and select a cutoff
    # to clip values to based on percentile.
    grad_history = []

    @engine.on(BackwardsEvents.BACKWARDS_COMPLETED)
    def autoclip_gradient(engine):
        obs_grad_norm = _get_grad_norm(model)
        grad_history.append(obs_grad_norm)
        clip_value = np.percentile(grad_history, clip_percentile)
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)
</code></pre>
",0
64780564,How to fix AttributeError: module 'tensorflow' has no attribute 'Session' in Handwriting OCR,"<p>there are a lot of questions about this, but I didn't find the solution. I want to make Handwriting OCR from this site <a href=""https://github.com/Breta01/handwriting-ocr"" rel=""nofollow noreferrer"">handwriting-ocr</a></p>
<p>When I import the library, I found this error</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-22-1c5011de3819&gt; in &lt;module&gt;
      8 sys.path.append('../src/')
      9 from ocr.normalization import word_normalization, letter_normalization
---&gt; 10 from ocr import page, words, characters
     11 from ocr.helpers import implt, resize
     12 from ocr.tfhelpers import Model

D:\Master\handwriting-ocr-master\handwriting-ocr-master\src\ocr\characters.py in &lt;module&gt;
     14 location = os.path.dirname(os.path.abspath(__file__))
     15 CNN_model = Model(
---&gt; 16     os.path.join(location, '../../models/gap-clas/CNN-CG'))
     17 CNN_slider = (60, 30)
     18 RNN_model = Model(

D:\Master\handwriting-ocr-master\handwriting-ocr-master\src\ocr\tfhelpers.py in __init__(self, loc, operation, input_name)
     18         self.input = input_name + &quot;:0&quot;
     19         self.graph = tf.Graph()
---&gt; 20         self.sess = tf.Session(graph=self.graph)
     21         with self.graph.as_default():
     22             saver = tf.train.import_meta_graph(loc + '.meta', clear_devices=True)

AttributeError: module 'tensorflow' has no attribute 'Session
</code></pre>
<p>Because I use <code>tensorflow 2.1.0</code> so I try to change Into this library</p>
<pre><code>import tensorflow.compat.v1 as tf
</code></pre>
<p>And try this</p>
<pre><code>hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
print(sess.run(hello))
</code></pre>
<p>It's successful. The output is <code>b'Hello, TensorFlow!'</code>.
If use <code>import tensorflow as tf</code> I change <code>tf.Session()</code> into this <code>tf.compat.v1.Session()</code> successful but if I implement it in ocr.py it still doesn't work and returns the same error <code>no session</code></p>
<p>I have try re-install <code>tensorflow</code> too.</p>
<p>I use <code>jupyter notebook</code> , <code>python 3.6</code>,  and <code>opencv 3.3.1</code>
Thanks for the help guys.</p>
",0
64780641,Whats the equivalent of tf.keras.Input() in pytorch?,"<p>can someone tell me what the equivalent of tf.keras.Input() in pytorch is?</p>
<p>At the documentation it says, &quot;Initiates a Keras Tensor&quot;, so does it just creates a new empty tensor?</p>
<p>Thanks</p>
",0
64813162,read `wav` file with `tf.audio.decode_wav`,"<p>I am following the tensorflow tutorial for audio recognition at <a href=""https://www.tensorflow.org/tutorials/audio/simple_audio"" rel=""nofollow noreferrer"">simple_audio</a>. The notebook works very well.</p>
<p>As a next step, I wanted to record my own voice and then run it through the model trained in tensorflow.
I first generated a recording:</p>
<pre class=""lang-py prettyprint-override""><code>seconds=1
sr=16000
nchannels=1
myrecording = sd.rec(int(seconds * sr), samplerate=sr, channels=nchannels)
sd.wait()
wavfile.write(filename, sr, myrecording)
</code></pre>
<p>So far so good, I can play my recording. But when I try to load the file with <code>tf.audio.decode_wav</code> similar to this:</p>
<pre class=""lang-py prettyprint-override""><code>audio_binary = tf.io.read_file(filename)
audio, _ = tf.audio.decode_wav(audio_binary)
</code></pre>
<p>I get the following error:</p>
<blockquote>
<p>InvalidArgumentError: Bad audio format for WAV: Expected 1 (PCM), but got3 [Op:DecodeWav]</p>
</blockquote>
<p>Any pointers on what might be going wrong are greatly appreciated.</p>
",0
64938970,"WARNING: This property should not be used in TensorFlow 2.0, as updates are applied automatically","<p>I was running some code in Google Colab. I defined my own model &quot;MyModel()&quot; and some functions (not shown because it's too long), which is inherited from 'tf.keras.Model'.</p>
<p>'''</p>
<pre><code>save_model_path='./models' # path to save trained model
save_mat_folder='./results' # path to save reconstruction examples
log_path='./tensorboard_log' # path to log training process
load_model_path = save_model_path

model = MyModel()

summary_writer = tf.summary.create_file_writer(log_path)
tf.summary.trace_on(graph = True,profiler = False)

variables = [model.phi1,model.phi2] # write variables in a list

# define optimizer
optimizer =  tf.keras.optimizers.Adam(learning_rate= 1e-3)
for i in tf.range(50):
    # print(i)
    # below for TF 1.x:
    # loss,summary,_=sess.run([L,merged,train_op],feed_dict) #run(fetches, feed_dict=None, options=None, run_metadata=None)
    # model1_writer.add_summary(summary,global_step = i)
    # below for TF2.x:
    with tf.GradientTape() as tape:
        # loss function
        loss = model.call(Ein)
    # The tape is automatically erased immediately after you call its gradient() method
    grads = tape.gradient(loss, variables) ## auto-differentiation，powerful !!
    # TensorFlow will update parameters automatically
    optimizer.apply_gradients(grads_and_vars=zip(grads, variables))
    # train_op = optimizer.minimize(L) # calculates gradients automatically
    with summary_writer.as_default():
        tf.summary.scalar('loss', loss, step = tf.cast(i,tf.int64))
    if i % 10 == 0:
        print(loss)
# export trace 
with summary_writer.as_default():
    tf.summary.trace_export(name ='model_trace',step=0 ) #, profiler_outdir = log_path) 
    tf.saved_model.save(model, save_model_path)
# save_path=saver.save(sess,save_model_path)
</code></pre>
<p>'''</p>
<p>The code looks worked, but got unexpected warnings. Can anyone tell me source of the warning?</p>
<p>Below are the running outputs:
**</p>
<pre><code>tf.Tensor(-8.2480165e-06, shape=(), dtype=float32)
tf.Tensor(-8.653108e-06, shape=(), dtype=float32)
tf.Tensor(-9.343687e-06, shape=(), dtype=float32)
tf.Tensor(-1.0216764e-05, shape=(), dtype=float32)
tf.Tensor(-1.1233077e-05, shape=(), dtype=float32)
WARNING:tensorflow:Skipping full serialization of Keras layer &lt;__main__.MyModel object at 0x7fea4a9e9e48&gt;, because it is not built.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
INFO:tensorflow:Assets written to: ./models/assets
</code></pre>
<p>**</p>
",0
64956013,How can I select a specific GPU with TensorFlow and Yolov3? (tensorflow-gpu-2.3.1),"<p>I have two GPUS in my system (RTX 2070s). I am trying to apply Yolov3 using TensorFlow according to the following
<a href=""https://github.com/theAIGuysCode/Object-Detection-API"" rel=""nofollow noreferrer"">tutorial</a>.</p>
<p>However, it seems TensorFlow can't use a specific GPU. I had searched on the internet and found that you would need to change <code>'GPU'</code> to <code>/GPU:0</code> in  these lines of code:</p>
<pre><code>physical_devices = tf.config.experimental.list_physical_devices('GPU')
if len(physical_devices) &gt; 0:
tf.config.experimental.set_memory_growth(physical_devices[0], True)
</code></pre>
<p>However, that doesn't work and keeps giving me the following error:</p>
<pre><code>raise ValueError(&quot;Memory growth cannot differ between GPU devices&quot;)
ValueError: Memory growth cannot differ between GPU devices
</code></pre>
<p>Any directions on how to solve this issue?</p>
",0
65073434,Why Keras Lambda-Layer cause problem Mask_RCNN?,"<p>I'm using the Mask_RCNN package from this repo: <code>https://github.com/matterport/Mask_RCNN</code>.</p>
<p>I tried to train my own dataset using this package but it gives me an error at the beginning.</p>
<pre><code>2020-11-30 12:13:16.577252: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-11-30 12:13:16.587017: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-11-30 12:13:16.587075: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (7612ade969e5): /proc/driver/nvidia/version does not exist
2020-11-30 12:13:16.587479: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-11-30 12:13:16.593569: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300000000 Hz
2020-11-30 12:13:16.593811: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1b2aa00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-11-30 12:13:16.593846: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Traceback (most recent call last):
  File &quot;machines.py&quot;, line 345, in &lt;module&gt;
    model_dir=args.logs)
  File &quot;/content/Mask_RCNN/mrcnn/model.py&quot;, line 1837, in __init__
    self.keras_model = self.build(mode=mode, config=config)
  File &quot;/content/Mask_RCNN/mrcnn/model.py&quot;, line 1934, in build
    anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=&quot;anchors&quot;)(input_image)
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 926, in __call__
    input_list)
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 1117, in _functional_construction_call
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py&quot;, line 904, in call
    self._check_variables(created_variables, tape.watched_variables())
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py&quot;, line 931, in _check_variables
    raise ValueError(error_str)
ValueError: 
The following Variables were created within a Lambda layer (anchors)
but are not tracked by said layer:
  &lt;tf.Variable 'anchors/Variable:0' shape=(1, 261888, 4) dtype=float32&gt;
The layer cannot safely ensure proper Variable reuse across multiple
calls, and consquently this behavior is disallowed for safety. Lambda
layers are not well suited to stateful computation; instead, writing a
subclassed Layer is the recommend way to define layers with
Variables.
</code></pre>
<p>I looked up the part of code responsible for the problem (located at <code>file: /mrcnn/model.py</code>  <code>line: 1935</code> in the repo):
<code>IN[0]: anchors = KL.Lambda(lambda x: tf.Variable(anchors), name=&quot;anchors&quot;)(input_image)</code></p>
<p>If anyone have an idea how to solve it or have already solved it, please mention the solution.</p>
",0
65120399,Migration to tf.keras: lambdas how-to compile?,"<p>So I had next code in old keras-2.2 tf-1.13:</p>
<pre><code>def RN_GlobalAveragePooling2D_r(f):
    def func(x):
        x    =  layers.multiply([x, f])
        repx =  int(x.shape[2])
        repy =  int(x.shape[3])
        x    = (backend.sum(x, axis=[2, 3], keepdims=True) / (backend.sum(f, axis=[2, 3], keepdims=True)))
        x    =  backend.repeat_elements(x, repx, axis = 2)
        x    =  backend.repeat_elements(x, repy, axis = 3)    
        return x
    return Lambda(func) 
</code></pre>
<p>And called it like so: <code>s_1  = RN_GlobalAveragePooling2D_r(S_mask)(DOG1) </code></p>
<p>now I want to migrate to tf-2.2, yet I  get a compile error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;./Train_DOGLSTM.py&quot;, line 24, in &lt;module&gt;
    model = M.my_model(encoder = encoder, input_size = (options.img_h, options.img_w, 3), k_shot = options.kshot, learning_rate = options.learning_rate)
  File &quot;/content/drive/MyDrive/sc/model.py&quot;, line 178, in my_model
    s_1  = RN_GlobalAveragePooling2D_r(S_mask)(DOG1)   
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 954, in __call__
    input_list)
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 1093, in _functional_construction_call
    inputs, input_masks, args, kwargs)
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 824, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 871, in _infer_output_signature
    keras_tensor.keras_tensor_from_tensor, outputs)
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py&quot;, line 810, in map_structure
    structure[0], [func(*x) for x in entries],
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py&quot;, line 810, in &lt;listcomp&gt;
    structure[0], [func(*x) for x in entries],
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/keras_tensor.py&quot;, line 606, in keras_tensor_from_tensor
    out = keras_tensor_cls.from_tensor(tensor)
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/keras_tensor.py&quot;, line 205, in from_tensor
    type_spec = type_spec_module.type_spec_from_value(tensor)
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/type_spec.py&quot;, line 564, in type_spec_from_value
    (value, type(value).__name__))
TypeError: Could not build a TypeSpec for &lt;KerasTensor: shape=(None, 5, 7, 7, 128) dtype=float16 (created by layer 'tf.concat_1')&gt; with type KerasTensor
</code></pre>
<p>So how one writes <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda?version=nightly"" rel=""nofollow noreferrer"">lambdas</a> that take two arguments like shown here for TF.keras in 2.x?</p>
",0
65155283,PySpark 3.0.1 Failing to run Distributed training in Tensorflow 2.1.0,"<p>I'm trying to train a simple fashion_mnist model on tensorflow as per the original TensorBoard Api docs on hyper parameter tuning you can find <a href=""https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams"" rel=""nofollow noreferrer"">here</a></p>
<p>Currently, for testing purposes, I'm running on standalone mode so. <code>master = 'local[*]'</code></p>
<p>I have installed <code>pyspark==3.0.1</code> and <code>tensorflow==2.1.0</code>. The following is what I'm trying to run:</p>
<pre><code># For a given hyper parameter, this will run the train &amp; return the model + accuracy which I'm looking for. 
# This works when I run without spark.

def train(hparam) -&gt; Tuple[Model, Any]:
    fashion_mnist = fashion
    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0
    model = Sequential([
        Flatten(),
        Dense(hparam['num_units'], activation=tf.nn.relu),
        Dropout(hparam['dropout']),
        Dense(10, activation=tf.nn.softmax),
    ])
    model.compile(
        optimizer=hparam['optimizer'],
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'],
    )
    model.fit(x_train, y_train, epochs=1)  # Run with 1 epoch to speed things up for demo purposes
    _, accuracy = model.evaluate(x_test, y_test)
    return model, accuracy
</code></pre>
<p>Here's my spark code which I run.</p>
<pre><code>if __name__ == '__main__':

     hp_nums = hp.HParam('num_units', hp.Discrete([16, 32]))
     hp_dropouts = hp.HParam('dropout', hp.RealInterval(0.1, 0.2))
     hp_opts = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))

     all_params = [] ##contains a list of different hparams
     
     for num_units in hp_nums.domain.values:
         for dropout_rate in (hp_dropouts.domain.min_value, hp_dropouts.domain.max_value):
             for optimizer in hp_opts.domain.values:
                 hparams = {
                    'num_units': num_units,
                    'dropout': dropout_rate,
                    'optimizer': optimizer,
                 }
                 all_params.append(hparams)
     

    
     spark_sess = SparkSession.builder.master(
         'local[*]'
     ).appName(
         'LocalTraining'
     ).getOrCreate()
     
     res = spark_sess.sparkContext.parallelize(
          all_hparams, len(all_hparams)
     ).map(
          train #above function
     ).collect()
     
     temp = 0.0
     best_model = None
     for model, acc in res:
         if acc &gt; temp:
             best_model = model
     
     print(&quot;best accuracy is -&gt; &quot; + str(temp))


</code></pre>
<p>This looks alright to me and works for any simple mapreduce (like the basic examples). Which makes me believe my environment is perfect and alright.</p>
<p>My Environment:</p>
<pre><code>java : Java 11.0.8 2020-07-14 LTS
python: Python 3.6.5
pyspark: 3.0.1
tensorflow: 2.1.0
Keras: 2.3.1
windows: 10 (if this really matters)
cores : 8 (i5 10th gen)
Memory: 6G
</code></pre>
<p>But When I run the above piece of code. I get the following error. I can see the training run and it just stops after 1 executor runs</p>
<pre><code>59168/60000 [============================&gt;.] - ETA: 0s - loss: 0.7350 - accuracy: 0.7471
60000/60000 [==============================] - 3s 42us/step - loss: 0.7331 - accuracy: 0.7477
20/12/05 14:03:57 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.net.SocketException: Connection reset
    at java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)
    at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
    at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
    at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
0/12/05 14:03:57 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
20/12/05 14:03:57 INFO TaskSchedulerImpl: Cancelling stage 0
20/12/05 14:03:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled
20/12/05 14:03:57 INFO Executor: Executor is trying to kill task 1.0 in stage 0.0 (TID 1), reason: Stage cancelled
20/12/05 14:03:57 INFO TaskSchedulerImpl: Stage 0 was cancelled
20/12/05 14:03:57 INFO DAGScheduler: ResultStage 0 (collect at C:/Users/&lt;&gt;/&lt;&gt;/&lt;&gt;/main.py:&lt;&gt;) failed in 7.506 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, host.docker.internal, executor driver): java.net.SocketException: Connection reset
    at java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)
    at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
    at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
    at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
    at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
    at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:628)
    at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)
    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
    at scala.collection.Iterator.foreach(Iterator.scala:941)
    at scala.collection.Iterator.foreach$(Iterator.scala:941)

py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, host.docker.internal, executor driver): java.net.SocketException: Connection reset
    at java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)
    at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
    at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
    at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
    at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
    at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:628)
    at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)
    at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)
    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
    at scala.collection.Iterator.foreach(Iterator.scala:941)

Driver stacktrace:
20/12/05 14:03:57 INFO DAGScheduler: Job 0 failed: collect at C:/&lt;&gt;/&lt;&gt;/&lt;&gt;/main.py, took 7.541442 s
Traceback (most recent call last):
  File &quot;C:/&lt;&gt;/&lt;&gt;/&lt;&gt;/main.py&quot;, line 68, in main
    return res.collect()
  File &quot;C:\Users\&lt;&gt;\&lt;&gt;\&lt;&gt;\venv\lib\site-packages\pyspark\rdd.py&quot;, line 889, in collect
    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File &quot;C:\Users\&lt;&gt;\&lt;&gt;\&lt;&gt;\venv\lib\site-packages\py4j\java_gateway.py&quot;, line 1305, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File &quot;C:\Users\&lt;&gt;\&lt;&gt;\&lt;&gt;\venv\lib\site-packages\pyspark\sql\utils.py&quot;, line 128, in deco
    return f(*a, **kw)
  File &quot;C:\Users\&lt;&gt;\&lt;&gt;\&lt;&gt;\venv\lib\site-packages\py4j\protocol.py&quot;, line 328, in get_return_value
    format(target_id, &quot;.&quot;, name), value)
</code></pre>
<p>The error is on line <code>model.fit()</code>. [It only happens when I do <code>model.fit</code> If I comment it out and have something else there, it works perfectly fine. I'm unsure why it fails on model.fit()]</p>
",1
65157852,How to mix tensorflow keras model and transformers,"<p>I am trying to import a pretrained model from Huggingface's transformers library and extend it with a few layers for classification using tensorflow keras. When I directly use transformers model (Method 1), the model trains well and reaches a validation accuracy of 0.93 after 1 epoch. However, when trying to use the model as a layer within a tf.keras model (Method 2), the model can't get above 0.32 accuracy. As far as I can tell based on the documentation, the two approaches should be equivalent. My goal is to get Method 2 working so that I can add more layers to it instead of directly using the logits produced by Huggingface's classifier head but I'm stuck at this stage.</p>
<pre><code>import tensorflow as tf

from transformers import TFRobertaForSequenceClassification
</code></pre>
<p>Method 1:</p>
<pre><code>model = TFRobertaForSequenceClassification.from_pretrained(&quot;roberta-base&quot;, num_labels=6)
</code></pre>
<p>Method 2:</p>
<pre><code>input_ids = tf.keras.Input(shape=(128,), dtype='int32')

attention_mask = tf.keras.Input(shape=(128, ), dtype='int32')

transformer = TFRobertaForSequenceClassification.from_pretrained(&quot;roberta-base&quot;, num_labels=6)

encoded = transformer([input_ids, attention_mask])

logits = encoded[0]

model = tf.keras.models.Model(inputs = [input_ids, attention_mask], outputs = logits)

</code></pre>
<p>Rest of the code for either method is identical,</p>
<pre><code>model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), 
              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])
</code></pre>
<p>I am using Tensorflow 2.3.0 and have tried with transformers versions 3.5.0 and 4.0.0.</p>
",1
65201252,How to shuffle TFrecords files before feeding them to the model,"<p>I am fitting a Neural network model using TFrecords and keras. I have a relatively big dataset which is pretty heterogeneous. I already used the shuffle my dataset during the training of the model like in the documentation exemple :<a href=""https://keras.io/examples/keras_recipes/tfrecord/"" rel=""nofollow noreferrer"">https://keras.io/examples/keras_recipes/tfrecord/</a>  (but can't shuffle all because it would cost too much memory) and I also separated my dataset into small shards each of equal size.</p>
<p>However I have reasons to think that this &quot;approximate&quot; shuffling is not enough and I also think that feeding already shuffled data would increase training speed.</p>
<p>So now my question is: After I have separated my dataset into Tfrecords shards, Is it possible to efficiently make code that takes randomly 2 shards, load them, shuffle them and then rewrite 2 shards (which are now shuffled between two shards). So that I can repeat this process a lot of time, which would result in correctly shuffled TFrecords files.</p>
<p>More precisely, I take 2 shards: shard1.tfrec and shard2.tfrec, load them into one tf.data.dataset, shuffle it, and then output 2 shards of equal size again.</p>
",0
65260289,Feature Normalization/Standard Scalar in Keras,"<p>I am working with a Sequential Keras model and I trying to figure out the best method for feature scaling.</p>
<pre><code>model = Sequential()
model.add(Masking(mask_value=-50, input_shape=(None,10)))
model.add(LayerNormalization(axis=-1))
model.add(LSTM(100, input_shape=(None,10)))
model.add(Dense(100, activation='relu'))
model.add(Dense(3, activation='softmax'))
print(model.summary()) 
</code></pre>
<p>In line 3, I have a LayerNormalization layer which according to documentation, scales to mean and standard deviation. However, I have also come across Batch normalization and <code>tf.keras.layers.experimental.preprocessing.Normalization</code>. My question is is this method similar to Sklearn's <code>StandardScalar()</code> or is there another method I could use to feature scale within the model?</p>
",1
65277703,image normalization and TPU,"<p>I'm trying to incorporate image normalization in my keras model to run on Google's cloud TPU. Therefore I inserted a line into my code:</p>
<pre><code>with strategy.scope():
     input_shape=(128,128,3)
     image_0 = Input(shape=input_shape)
     **image_1 = tf.image.per_image_standardization(image_0)**
     ...
</code></pre>
<p>There was nor error thrown, but according the documentation of google tf.image.per_image_standardization
is not a supported function. Does anybody know if it works anyhow, or does anybody have an idea how to check if it works?</p>
",1
65341878,How to ensure that TensorFlow saves the average loss across the whole dataset in a training loop written from scratch,"<p>I am training my conditional GAN network using the code from TensorFlow <a href=""https://www.tensorflow.org/tutorials/generative/pix2pix"" rel=""nofollow noreferrer"">tutorial</a>  that uses a written from scratch training loop</p>
<pre><code>def fit(train_ds, epochs, test_ds):
  for epoch in range(epochs):
    start = time.time()

    display.clear_output(wait=True)

    for example_input, example_target in test_ds.take(1):
      generate_images(generator, example_input, example_target)
    print(&quot;Epoch: &quot;, epoch)

    # Train
    for n, (input_image, target) in train_ds.enumerate():
      print('.', end='')
      if (n+1) % 100 == 0:
        print()
      train_step(input_image, target, epoch)
    print()

    # saving (checkpoint) the model every 20 epochs
    if (epoch + 1) % 20 == 0:
      checkpoint.save(file_prefix = checkpoint_prefix)

    print ('Time taken for epoch {} is {} sec\n'.format(epoch + 1,
                                                        time.time()-start))
  checkpoint.save(file_prefix = checkpoint_prefix)
</code></pre>
<p>and the train step is defined like this</p>
<pre><code>@tf.function
def train_step(input_image, target, epoch):
  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
    gen_output = generator(input_image, training=True)

    disc_real_output = discriminator([input_image, target], training=True)
    disc_generated_output = discriminator([input_image, gen_output], training=True)

    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)
    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)

  generator_gradients = gen_tape.gradient(gen_total_loss,
                                          generator.trainable_variables)
  discriminator_gradients = disc_tape.gradient(disc_loss,
                                               discriminator.trainable_variables)

  generator_optimizer.apply_gradients(zip(generator_gradients,
                                          generator.trainable_variables))
  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,
                                              discriminator.trainable_variables))

  with summary_writer.as_default():
    tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)
    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)
    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)
    tf.summary.scalar('disc_loss', disc_loss, step=epoch)
</code></pre>
<p>now my <strong>question</strong> is for the summary writer is it saving the loss of the batch only or the average across the whole dataset and if it is for the batch which batch loss is it saving and how can I get the average across the whole dataset if the batches are not the same size ?
I assumed it was the average because I got the code from a tensorflow tutorial so I trusted it but when I think about it I am not sure that that is the case.</p>
",0
65413136,Tensorflow — Cannot call `tf.keras.Model.add_metric` when `tf.distribute.MirroredStrategy` is used,"<p>I have a model class that inherits from <code>tf.keras.Model</code>. I can train, evaluate, and <em>export</em> it using 8 GPUs, distributing it with <code>tf.distribute.MirroredStrategy</code>. However, I need custom metrics, and when I call the <a href=""https://www.tensorflow.org/guide/keras/custom_layers_and_models#the_add_metric_method"" rel=""nofollow noreferrer"">add_metric</a> method, it throws an error when trying to <strong>export</strong>.</p>
<pre><code>Traceback (most recent call last):
  File &quot;repro/vae.py&quot;, line 80, in &lt;module&gt;
    vae.save(&quot;vae&quot;)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py&quot;, line 1979, in save
    signatures, options)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py&quot;, line 134, in save_model
    signatures, options)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save.py&quot;, line 80, in save
    save_lib.save(model, filepath, signatures, options)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py&quot;, line 976, in save
    obj, export_dir, signatures, options, meta_graph_def)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py&quot;, line 1047, in _build_meta_graph
    checkpoint_graph_view)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_serialization.py&quot;, line 75, in find_function_to_export
    functions = saveable_view.list_functions(saveable_view.root)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/saved_model/save.py&quot;, line 145, in list_functions
    self._serialization_cache)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py&quot;, line 2590, in _list_functions_for_serialization
    Model, self)._list_functions_for_serialization(serialization_cache)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 3019, in _list_functions_for_serialization
    .list_functions_for_serialization(serialization_cache))
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py&quot;, line 87, in list_functions_for_serialization
    fns = self.functions_to_serialize(serialization_cache)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py&quot;, line 79, in functions_to_serialize
    serialization_cache).functions_to_serialize)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py&quot;, line 95, in _get_serialized_attributes
    serialization_cache)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py&quot;, line 51, in _get_serialized_attributes_internal
    default_signature = save_impl.default_save_signature(self.obj)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/save_impl.py&quot;, line 205, in default_save_signature
    fn.get_concrete_function()
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 1167, in get_concrete_function
    concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 1073, in _get_concrete_function_garbage_collected
    self._initialize(args, kwargs, add_initializers_to=initializers)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 697, in _initialize
    *args, **kwds))
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 2855, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 3213, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 3075, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 986, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 600, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/saving/saving_utils.py&quot;, line 134, in _wrapped_model
    outputs = model(inputs, training=False)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 985, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py&quot;, line 302, in wrapper
    return func(*args, **kwargs)
  File &quot;repro/vae.py&quot;, line 63, in call
    self.add_metric([0.], name=&quot;foo&quot;)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 1705, in add_metric
    metric_obj(value)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py&quot;, line 231, in __call__
    replica_local_fn, *args, **kwargs)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/distribute/distributed_training_utils.py&quot;, line 1133, in call_replica_local_fn
    return fn(*args, **kwargs)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py&quot;, line 211, in replica_local_fn
    update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/utils/metrics_utils.py&quot;, line 90, in decorated
    update_op = update_state_fn(*args, **kwargs)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py&quot;, line 176, in update_state_fn
    return ag_update_state(*args, **kwargs)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py&quot;, line 302, in wrapper
    return func(*args, **kwargs)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/keras/metrics.py&quot;, line 373, in update_state
    update_total_op = self.total.assign_add(value_sum)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/distribute/values.py&quot;, line 1015, in assign_add
    self, value, read_value=read_value)
  File &quot;/Users/acarlson/anaconda3/envs/ed-autocoder-dev/lib/python3.7/site-packages/tensorflow/python/distribute/values_util.py&quot;, line 95, in on_read_assign_add_cross_replica
    &quot;SyncOnReadVariable does not support `assign_add` in &quot;
ValueError: SyncOnReadVariable does not support `assign_add` in cross-replica context when aggregation is set to `tf.VariableAggregation.SUM`.
</code></pre>
<p>I have created a simple reproduction which shows this error here:</p>
<pre><code>import tensorflow as tf


class Sampling(tf.keras.layers.Layer):
    &quot;&quot;&quot;Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.&quot;&quot;&quot;

    def call(self, inputs):
        z_mean, z_log_var = inputs
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon


class Encoder(tf.keras.layers.Layer):
    &quot;&quot;&quot;Maps MNIST digits to a triplet (z_mean, z_log_var, z).&quot;&quot;&quot;

    def __init__(self, latent_dim=32, intermediate_dim=64, name=&quot;encoder&quot;, **kwargs):
        super(Encoder, self).__init__(name=name, **kwargs)
        self.dense_proj = tf.keras.layers.Dense(intermediate_dim, activation=&quot;relu&quot;)
        self.dense_mean = tf.keras.layers.Dense(latent_dim)
        self.dense_log_var = tf.keras.layers.Dense(latent_dim)
        self.sampling = Sampling()

    def call(self, inputs):
        x = self.dense_proj(inputs)
        z_mean = self.dense_mean(x)
        z_log_var = self.dense_log_var(x)
        z = self.sampling((z_mean, z_log_var))
        return z_mean, z_log_var, z


class Decoder(tf.keras.layers.Layer):
    &quot;&quot;&quot;Converts z, the encoded digit vector, back into a readable digit.&quot;&quot;&quot;

    def __init__(self, original_dim, intermediate_dim=64, name=&quot;decoder&quot;, **kwargs):
        super(Decoder, self).__init__(name=name, **kwargs)
        self.dense_proj = tf.keras.layers.Dense(intermediate_dim, activation=&quot;relu&quot;)
        self.dense_output = tf.keras.layers.Dense(original_dim, activation=&quot;sigmoid&quot;)

    def call(self, inputs):
        x = self.dense_proj(inputs)
        return self.dense_output(x)


class VariationalAutoEncoder(tf.keras.Model):
    &quot;&quot;&quot;Combines the encoder and decoder into an end-to-end model for training.&quot;&quot;&quot;

    def __init__(self, original_dim, intermediate_dim=64, latent_dim=32, name=&quot;autoencoder&quot;, **kwargs):
        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)
        self.original_dim = original_dim
        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)
        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)

    def call(self, inputs):
        z_mean, z_log_var, z = self.encoder(inputs)
        reconstructed = self.decoder(z)
        # Add KL divergence regularization loss.
        kl_loss = -0.5 * tf.reduce_mean(
            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1
        )
        self.add_loss(kl_loss)
        self.add_metric([0.], name=&quot;foo&quot;)
        return reconstructed


(x_train, _), _ = tf.keras.datasets.mnist.load_data()
x_train = x_train.reshape(60000, 784).astype(&quot;float32&quot;) / 255

original_dim = 784

strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    vae = VariationalAutoEncoder(original_dim, 64, 32)
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
    vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())

vae.fit(x_train, x_train, epochs=3, batch_size=64)
vae.save(&quot;vae&quot;)
</code></pre>
<p>I apologize for so much code, but most of it isn't important. The important part is that this model is instantiated and compiled inside the <code>tf.distribute.MirroredStrategy</code> scope. There is also a <code>self.add_metric([0.], name=&quot;foo&quot;)</code> in the model. If you remove that <code>add_metric</code> call, then it works. It will export correctly.</p>
<p>Therefore, using the <code>tf.keras.Model.add_metric</code> method with <code>tf.distribute.MirroredStrategy</code>. I need to be able to add my custom metrics with a distributed model.</p>
<p>Note: Metrics are <strong>supposed</strong> to be calculated in the strategy scope, as mentioned in <a href=""https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#scope"" rel=""nofollow noreferrer"">the docs</a></p>
<blockquote>
<p>&quot;Common things that create variables in TF: models, optimizers, metrics. These should always be created inside the scope.&quot;</p>
</blockquote>
<p>As for versions, I'm using the Google <a href=""https://cloud.google.com/ai-platform/training/docs/runtime-version-list#2.3"" rel=""nofollow noreferrer"">AI platform runtime version 2.3</a></p>
",0
65436819,Keras: How to use `image_dataset_from_directory` to load test set?,"<p>I am using <code>tf.keras.preprocessing.image_dataset_from_directory</code> to load dataset as follows,</p>
<pre><code>train_dataset = tf.keras.preprocessing.image_dataset_from_directory(train_dir, 
                                                                    labels='inferred', 
                                                                    label_mode='categorical',
                                                                    batch_size=32,
                                                                    image_size=(224, 224))


val_dataset = tf.keras.preprocessing.image_dataset_from_directory(val_dir, 
                                                                  labels='inferred', 
                                                                  label_mode='categorical',
                                                                  batch_size=32,
                                                                  image_size=(224, 224))

</code></pre>
<p>However, when I check the document looks like this argument <code>labels</code> seem to be a must-have one,  but my test data has no labels, so how can I load test data? Is there a convenient and unified way to do this?</p>
",1
65464004,"Running a flask app that uses tensorflow on an Ubuntu 16.04 instance on GCP, model runs but predictions are different than on local host","<p>I trained a BERT model using huggingface for tensorflow on my localhost. Running predictions on my localhost works fine.</p>
<p>I then implemented a solution so I can call my model from a GCP VM instance (Ubuntu 16.04) via flask. The process seems to work as I can successfully make the calls to my app on the VM.</p>
<p>However, the prediction I receive from the VM differs from the one I receive on my localhost (which is the expected output), yet I use identical code. I use a model for Sequence Classification, and when trying to get the probabilities for both labels on my localhost I get: <code>array([0.67829543, 0.32170454], dtype=float32)</code> while the VM returns <code>array([1, 1], dtype=float32)</code>.
This snippet is what I use to predict the model just for reference:</p>
<pre><code>predict_input = tokenizer.encode(sentence,
                                  truncation=True,
                                  padding=True,
                                  return_tensors=&quot;tf&quot;
                                  )
tf_output = model.predict(predict_input)[0]
    
tf_prediction = tf.nn.softmax(tf_output, axis=0).numpy()
</code></pre>
<p>On my localhost I trained the model using tf with GPU support, the VM of course only has two vCPUs. When loading tensorflow on the VM I get the following warnings:</p>
<pre><code>2020-12-27 07:57:55.533847: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-12-27 07:57:55.533896: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2020-12-27 07:57:56.792914: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-12-27 07:57:56.792966: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2020-12-27 07:57:56.793002: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bertvm-1): /proc/driver/nvidia/version does not exist
2020-12-27 07:57:56.793316: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 A
VX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-12-27 07:57:56.801469: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2000129999 Hz
2020-12-27 07:57:56.801693: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x64b8fe0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-12-27 07:57:56.801805: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
</code></pre>
<p>I'm not sure if that is the root of the error or if it's because I trained the model using tf for GPU and am predicting on an instance that runs tf for CPU but that doesn't seem to make too much sense to me.
The warnings only seem to pertain to CUDA 'issues' which I believe is related to GPU support.</p>
<p>Any idea or tips as to what could be the cause of the different predictions?
Thanks for your help in advance!</p>
<p>EDIT:
It seems that the model returns the same logits on both, the VM and the localhost. When I then apply <code>tf.nn.softmax(tf_output, axis=0).numpy()</code> I get different results.
tf_output being <code>[1.9530067 1.2070574]</code> on both instances while the above function returns <code>[0.67829543 0.32170454]</code> on localhost and <code>[[1. 1.]]</code> on the VM (both formatted here as a string) as mentioned above.</p>
",0
65464181,An alternative to tf.distribute.cluster_resolver.TPUClusterResolver( tpu_name) to be used in Sagemaker?,"<ol>
<li><p>task : object_detection</p>
</li>
<li><p>environment: AWS sagemaker</p>
</li>
<li><p>instance type: 'ml.p2.xlarge' | num_instances = 1</p>
</li>
<li><p>Main file to be run: <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/model_main_tf2.py"" rel=""nofollow noreferrer"">original</a></p>
</li>
<li><p>Problematic code segment from the main file:</p>
<pre><code>    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(
    FLAGS.tpu_name)
    tf.config.experimental_connect_to_cluster(resolver)
    tf.tpu.experimental.initialize_tpu_system(resolver)
    strategy = tf.distribute.experimental.TPUStrategy(resolver)
    elif FLAGS.num_workers &gt; 1:
        strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
    else:
        strategy = tf.compat.v2.distribute.MirroredStrategy()
</code></pre>
</li>
<li><p>Problem : Can't find the proper value to be given as <code>tpu_name</code> argument.</p>
</li>
<li><p>My research on the problem:</p>
</li>
</ol>
<p>According to the tensorflow documentation in <a href=""https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver"" rel=""nofollow noreferrer"">tf.distribute.cluster_resolver.TPUClusterResolver</a>, it says that this resolver works only on Google Cloud platform.</p>
<blockquote>
<p>This is an implementation of cluster resolvers for the Google Cloud
TPU service.</p>
<p>TPUClusterResolver supports the following distinct environments:
Google Compute Engine Google Kubernetes Engine Google internal</p>
<p>It can be passed into tf.distribute.TPUStrategy to support TF2
training on Cloud TPUs.</p>
</blockquote>
<p>But from <a href=""https://github.com/tensorflow/tensorflow/issues/39721"" rel=""nofollow noreferrer"">this issue in github</a>, I found out that a similar code also works in Azure.</p>
<ol start=""8"">
<li>My question :</li>
</ol>
<p>Is there a way I can bypass this resolver and initialize my tpu in <strong>sagemaker</strong> ?</p>
<p>Even better, if I can find a way to insert the name or url of sagemaker gpu to the resolver and initiate it from there ?</p>
",1
65475057,Keras data augmentation pipeline for image segmentation dataset (image and mask with same manipulation),"<p>I am building a preprocessing and data augmentation pipeline for my image segmentation dataset
There is a powerful API from keras to do this but I ran into the problem of reproducing same augmentation on image as well as segmentation mask (2nd image). Both images must undergo the exact same manipulations. Is this not supported yet?</p>
<p><a href=""https://www.tensorflow.org/tutorials/images/data_augmentation"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/images/data_augmentation</a></p>
<p><strong>Example / Pseudocode</strong></p>
<pre><code>data_augmentation = tf.keras.Sequential([
layers.experimental.preprocessing.RandomFlip(mode=&quot;horizontal_and_vertical&quot;, seed=SEED_VAL),
layers.experimental.preprocessing.RandomRotation(factor=0.4, fill_mode=&quot;constant&quot;, fill_value=0, seed=SEED_VAL),
layers.experimental.preprocessing.RandomZoom(height_factor=(-0.0,-0.2), fill_mode='constant', fill_value=0, seed=SEED_VAL)])

(train_ds, test_ds), info = tfds.load('somedataset', split=['train[:80%]', 'train[80%:]'], with_info=True)
</code></pre>
<p>This code does not work but illustrates how my dream api would work:</p>
<pre><code>train_ds = train_ds.map(lambda datapoint: data_augmentation((datapoint['image'], datapoint['segmentation_mask']), training=True))
</code></pre>
<p><strong>Alternative</strong></p>
<p>The alternative is to code a custom load and manipulation / randomization method as is proposed in the image segmentation tutorial (<a href=""https://www.tensorflow.org/tutorials/images/segmentation"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/images/segmentation</a>)</p>
<p>Any tips on state of the art data augmentation for this type of dataset is much appreciated :)</p>
",0
65481591,Keras Generator to tf.data.Dataset,"<p>I am working with the Mask RCNN keras implementation but the data generator hard locks on my systems when using <code>use_multiprocessing=True</code>. The data generator runs fine in single thread. I am trying to convert the data generator to a <code>tf.data.Dataset</code> as recommended by tensorflow. I have no idea how to do this and have been unable to find any documentation on this.</p>
<p>Mask RCNN data generator:</p>
<pre><code>class DataGenerator(KU.Sequence):
    &quot;&quot;&quot;An iterable that returns images and corresponding target class ids,
        bounding box deltas, and masks. It inherits from keras.utils.Sequence to avoid data redundancy
        when multiprocessing=True.

        dataset: The Dataset object to pick data from
        config: The model config object
        shuffle: If True, shuffles the samples before every epoch
        augmentation: Optional. An imgaug (https://github.com/aleju/imgaug) augmentation.
            For example, passing imgaug.augmenters.Fliplr(0.5) flips images
            right/left 50% of the time.
        random_rois: If &gt; 0 then generate proposals to be used to train the
                     network classifier and mask heads. Useful if training
                     the Mask RCNN part without the RPN.
        detection_targets: If True, generate detection targets (class IDs, bbox
            deltas, and masks). Typically for debugging or visualizations because
            in trainig detection targets are generated by DetectionTargetLayer.

        Returns a Python iterable. Upon calling __getitem__() on it, the
        iterable returns two lists, inputs and outputs. The contents
        of the lists differ depending on the received arguments:
        inputs list:
        - images: [batch, H, W, C]
        - image_meta: [batch, (meta data)] Image details. See compose_image_meta()
        - rpn_match: [batch, N] Integer (1=positive anchor, -1=negative, 0=neutral)
        - rpn_bbox: [batch, N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.
        - gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs
        - gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)]
        - gt_masks: [batch, height, width, MAX_GT_INSTANCES]. The height and width
                    are those of the image unless use_mini_mask is True, in which
                    case they are defined in MINI_MASK_SHAPE.

        outputs list: Usually empty in regular training. But if detection_targets
            is True then the outputs list contains target class_ids, bbox deltas,
            and masks.
        &quot;&quot;&quot;

    def __init__(self, dataset, config, shuffle=True, augmentation=None,
                 random_rois=0, detection_targets=False):

        self.image_ids = np.copy(dataset.image_ids)
        self.dataset = dataset
        self.config = config

        # Anchors
        # [anchor_count, (y1, x1, y2, x2)]
        self.backbone_shapes = compute_backbone_shapes(config, config.IMAGE_SHAPE)
        self.anchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,
                                                      config.RPN_ANCHOR_RATIOS,
                                                      self.backbone_shapes,
                                                      config.BACKBONE_STRIDES,
                                                      config.RPN_ANCHOR_STRIDE)

        self.shuffle = shuffle
        self.augmentation = augmentation
        self.random_rois = random_rois
        self.batch_size = self.config.BATCH_SIZE
        self.detection_targets = detection_targets

    def __len__(self):
        return int(np.ceil(len(self.image_ids) / float(self.batch_size)))

    def __getitem__(self, idx):
        b = 0
        image_index = -1
        while b &lt; self.batch_size:
            
            # Increment index to pick next image. Shuffle if at the start of an epoch.
            image_index = (image_index + 1) % len(self.image_ids)

            if self.shuffle and image_index == 0:
                np.random.shuffle(self.image_ids)

            # Get GT bounding boxes and masks for image.
            image_id = self.image_ids[image_index]
            image, image_meta, gt_class_ids, gt_boxes, gt_masks = \
                load_image_gt(self.dataset, self.config, image_id,
                              augmentation=self.augmentation)

            # Skip images that have no instances. This can happen in cases
            # where we train on a subset of classes and the image doesn't
            # have any of the classes we care about.
            if not np.any(gt_class_ids &gt; 0):
                continue

            # RPN Targets
            rpn_match, rpn_bbox = build_rpn_targets(image.shape, self.anchors,
                                                    gt_class_ids, gt_boxes, self.config)

            # Mask R-CNN Targets
            if self.random_rois:
                rpn_rois = generate_random_rois(
                    image.shape, self.random_rois, gt_class_ids, gt_boxes)
                if self.detection_targets:
                    rois, mrcnn_class_ids, mrcnn_bbox, mrcnn_mask = \
                        build_detection_targets(
                            rpn_rois, gt_class_ids, gt_boxes, gt_masks, self.config)

            # Init batch arrays
            if b == 0:
                batch_image_meta = np.zeros((self.batch_size,) + image_meta.shape, dtype=image_meta.dtype)
                batch_rpn_match = np.zeros([self.batch_size, self.anchors.shape[0], 1], dtype=rpn_match.dtype)
                batch_rpn_bbox = np.zeros([self.batch_size, self.config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4], dtype=rpn_bbox.dtype)
                batch_images = np.zeros((self.batch_size,) + image.shape, dtype=np.float32)
                batch_gt_class_ids = np.zeros((self.batch_size, self.config.MAX_GT_INSTANCES), dtype=np.int32)
                batch_gt_boxes = np.zeros((self.batch_size, self.config.MAX_GT_INSTANCES, 4), dtype=np.int32)
                batch_gt_masks = np.zeros((self.batch_size, gt_masks.shape[0], gt_masks.shape[1],self.config.MAX_GT_INSTANCES), dtype=gt_masks.dtype)
                if self.random_rois:
                    batch_rpn_rois = np.zeros((self.batch_size, rpn_rois.shape[0], 4), dtype=rpn_rois.dtype)
                    if self.detection_targets:
                        batch_rois = np.zeros((self.batch_size,) + rois.shape, dtype=rois.dtype)
                        batch_mrcnn_class_ids = np.zeros((self.batch_size,) + mrcnn_class_ids.shape, dtype=mrcnn_class_ids.dtype)
                        batch_mrcnn_bbox = np.zeros((self.batch_size,) + mrcnn_bbox.shape, dtype=mrcnn_bbox.dtype)
                        batch_mrcnn_mask = np.zeros((self.batch_size,) + mrcnn_mask.shape, dtype=mrcnn_mask.dtype)

            # If more instances than fits in the array, sub-sample from them.
            if gt_boxes.shape[0] &gt; self.config.MAX_GT_INSTANCES:
                ids = np.random.choice(
                    np.arange(gt_boxes.shape[0]), self.config.MAX_GT_INSTANCES, replace=False)
                gt_class_ids = gt_class_ids[ids]
                gt_boxes = gt_boxes[ids]
                gt_masks = gt_masks[:, :, ids]

            # Add to batch
            batch_image_meta[b] = image_meta
            batch_rpn_match[b] = rpn_match[:, np.newaxis]
            batch_rpn_bbox[b] = rpn_bbox
            batch_images[b] = mold_image(image.astype(np.float32), self.config)
            batch_gt_class_ids[b, :gt_class_ids.shape[0]] = gt_class_ids
            batch_gt_boxes[b, :gt_boxes.shape[0]] = gt_boxes
            batch_gt_masks[b, :, :, :gt_masks.shape[-1]] = gt_masks
            if self.random_rois:
                batch_rpn_rois[b] = rpn_rois
                if self.detection_targets:
                    batch_rois[b] = rois
                    batch_mrcnn_class_ids[b] = mrcnn_class_ids
                    batch_mrcnn_bbox[b] = mrcnn_bbox
                    batch_mrcnn_mask[b] = mrcnn_mask
            b += 1

        inputs = [batch_images, batch_image_meta, batch_rpn_match, batch_rpn_bbox,
                  batch_gt_class_ids, batch_gt_boxes, batch_gt_masks]
        outputs = []

        if self.random_rois:
            inputs.extend([batch_rpn_rois])
            if self.detection_targets:
                inputs.extend([batch_rois])
                # Keras requires that output and targets have the same number of dimensions
                batch_mrcnn_class_ids = np.expand_dims(
                    batch_mrcnn_class_ids, -1)
                outputs.extend(
                    [batch_mrcnn_class_ids, batch_mrcnn_bbox, batch_mrcnn_mask])

        return inputs, outputs
</code></pre>
<p>I have tried to use the <code>tf.data.Dataset.from_generator()</code> however it requires the <code>output_types=</code> argument and the Mask RCNN outputs a number of lists, I can not figure out how to define <code>output_types=</code>.</p>
<p>I am using <code>python3.7</code>, <code>keras==2.2.5</code>, <code>tensorflow==2.2.0</code></p>
",1
65486078,How to find the len() of a tf.Dataset,"<p>I have started using the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer""><code>tf.data.Dataset</code></a> as a way to load data into keras models, as they appear to be much faster than keras' <code>ImageDataGenerator</code> and much more memory efficient than training on arrays.</p>
<p>One think I can't get my head around is that I can't seem to find a way to access the <code>len()</code> of the dataset. Keras' <code>ImageDataGenerator</code> has an attribute called <code>n</code> which I used to use for this purpose. This makes my code very ugly, as I need to hard-code the length in various parts of the scripy (e.g. to find out how many iterations an epoch has).</p>
<p>Any ideas I can work around this issue?</p>
<p>An example script:</p>
<pre class=""lang-py prettyprint-override""><code># Generator
def make_mnist_train_generator(batch_size):
    (x_train, y_train), (_,_) = tf.keras.mnist.load_data()

    x_train = x_train.reshape((-1, 28, 28, 1))
    x_train = x_train.astype(np.float32) / 225.

    y_train = tf.keras.utils.to_categorical(y_train, 10)

    ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    ds = ds.shuffle(buffer_size=len(x_train))
    ds = ds.repeat()
    ds = ds.batch(batch_size=batch_size)
    ds = ds.prefetch(buffer_size=1)

    return ds


model = ...  # create a tf.keras model

batch_size 256
gen = make_mnist_train_generator(batch_size)

# Training
model.fit(gen, epochs=50, steps_per_epoch=60000//batch_size+1)  # Hard coded size of generator
</code></pre>
",0
65521065,How to load tensorflow dataset?,"<p>I am trying to follow the Tensorflow text classification example <a href=""https://www.tensorflow.org/tutorials/keras/text_classification"" rel=""nofollow noreferrer"">Here</a>. I want to save the tokenized dataset by using tf.data.experimental.save(). However, I can't load it properly. What should I do to load it properly? The offical Colab link is <a href=""https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/text_classification.ipynb"" rel=""nofollow noreferrer"">Here</a>.
<a href=""https://i.stack.imgur.com/OgYFJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/OgYFJ.png"" alt=""enter image description here"" /></a></p>
",0
65547937,Tensorflow inference too slow when loading multiple models,"<p>I finetuned two Mobilenet models on diferent datasets based on the tensorflow object_detection API example from <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb"" rel=""nofollow noreferrer"">here</a>. When I use eager mode (tf.executing_eagerly() is True) using only one model then the inference runs at 0.036 seconds per image. When I load two models Keras required to convert to graph mode (tf.executing_eagerly() is False) and the inference runs at 1.8 seconds per image. What I'm doing wrong?</p>
<pre><code>def inference(pipeline_config, checkpoint_path):

  print('Building model and restoring weights', flush=True)
  num_classes = 3

  # Load pipeline config and build a detection model.
  configs = config_util.get_configs_from_pipeline_file(pipeline_config)
  model_config = configs['model']
  model_config.ssd.num_classes = num_classes

  detection_model = model_builder.build(
  model_config=model_config, is_training=False)

  ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)

  ckpt.restore(checkpoint_path).expect_partial()

  # Run model through a dummy image so that variables are created
  image, shapes = detection_model.preprocess(tf.zeros([1, 320, 320, 3]))
  prediction_dict = detection_model.predict(image, shapes)
  _ = detection_model.postprocess(prediction_dict, shapes)
  print('Weights restored!')

  return detection_model

def get_model_detection_function(detection_model):
  &quot;&quot;&quot;Get a tf.function for detection.&quot;&quot;&quot;

  # Again, uncomment this decorator if you want to run inference eagerly
  @tf.function
  def detect(input_tensor):
    &quot;&quot;&quot;Run detection on an input image.

    Args:
    input_tensor: A [1, height, width, 3] Tensor of type tf.float32.
      Note that height and width can be anything since the image will be
    immediately resized according to the needs of the model within this
    function. 

    Returns:
      A dict containing 3 Tensors (`detection_boxes`, `detection_classes`,
      and `detection_scores`).
    &quot;&quot;&quot;
    preprocessed_image, shapes = detection_model.preprocess(input_tensor)
    prediction_dict = detection_model.predict(preprocessed_image, shapes)
    return detection_model.postprocess(prediction_dict, shapes)

  return detect


def mainProcess():
  
  print('Loading model 1...') 
  g1 = tf.Graph()
  s1 = tf.compat.v1.Session(graph=g1)
  with g1.as_default(), s1.as_default():
    detection_model_1 = inference('config_1/pipeline.config', 'Checkpoint_1/ckpt-1')
    detect_fn_1 = get_model_detection_function(detection_model_1)
    s1.run(tf.compat.v1.global_variables_initializer())


  print('Loading model 2...') 
  g2 = tf.Graph()
  s2 = tf.compat.v1.Session(graph=g2)
  with g2.as_default():
    detection_model_2  = inference('config_2/pipeline.config', 'Checkpoint_2/ckpt-1')
    detect_fn_2 = get_model_detection_function(detection_model_2)
    s2.run(tf.compat.v1.global_variables_initializer())

  for i, f in enumerate(listdir('images_dir/')):
    ...
    ... read the image
    ...
    with g1.as_default():
      with s1.as_default():
        sec = time.time()
        input_tensor = tf.convert_to_tensor(test_img, dtype=tf.float32)
        detections = detect_fn_1(input_tensor)
        detections = s1.run(detections)
        curr = time.time()
        print(&quot;Finished iterating in: &quot; + str(curr - sec) + &quot; seconds&quot;)

    # the same for detection_model_2
</code></pre>
<p>For eager mode with only one model the mainProcess is:</p>
<pre><code>def mainProcess():
  
  print('Loading model...') 
  detection_model_1 = inference('config_1/pipeline.config', 'Checkpoint_1/ckpt-1')
  detect_fn_1 = get_model_detection_function(detection_model_1)

  for i, f in enumerate(listdir('images_dir/')):
    ...
    ... read the image
    ...
    sec = time.time()
    input_tensor = tf.convert_to_tensor(test_img, dtype=tf.float32)
    detections = detect_fn_1(input_tensor)
    print(detections['detection_boxes'][0].numpy())
    print(detections['detection_scores'][0].numpy())
    curr = time.time()
    print(&quot;Finished iterating in: &quot; + str(curr - sec) + &quot; seconds&quot;)
</code></pre>
",0
65640885,CRNN tf.keras.backend.ctc_decode. What is log probability?,"<p>Based on <a href=""https://docs.w3cub.com/tensorflow%7Epython/tf/keras/backend/ctc_decode"" rel=""nofollow noreferrer"">the documentation</a>, the function <code>tf.keras.backend.ctc_decode</code> is supposed to return a <code>Tuple</code>. Its first field contains the best path (let's assume we use greedy search), whereas the second one contains its <code>log probability</code>.</p>
<p>Is this probability actually the accuracy of the prediction?</p>
<p>If not how am I supposed to calculate it?</p>
<p>I've tried on some test images and this was my output:</p>
<pre><code>True value: test0, prediction: test0, acc: 1.841524362564087
True value: test1, prediction: test1, acc: 0.9661365151405334
True value: test2, prediction: test2, acc: 1.0634151697158813
True value: test3, prediction: test3, acc: 2.471940755844116
True value: test4, prediction: test4, acc: 1.4866207838058472
True value: test5, prediction: test5, acc: 0.7630811333656311
True value: test6, prediction: test6, acc: 0.35642576217651367
True value: test7, prediction: test7, acc: 1.5693446397781372
True value: test8, prediction: test8, acc: 0.9700028896331787
True value: test9, prediction: test9, acc: 1.4783780574798584
</code></pre>
<p>During the training part, the final CTC loss was around 0.1 and the prediction is always correct. However what I think it's the probability seems not to be what I expect. They looks like completely random numbers, even grater than 1 or 2! What am I doing wrong?</p>
",1
65723891,"How to free TF/Keras memory in Python after a model has been deleted, while other models are still in memory and in use?","<p>I have a Python server application, which provides TensorFlow / Keras model inference services. Multiple different such models can be loaded and used at the same time, for multiple different clients. A client can request to load another model, but this has no effect on the other clients (i.e. their models stay in memory and use as they are, so each client can ask to load another model regardless of the state of any other client).</p>
<p>The logic and implementation works, however, I am not sure how to correctly free memory in this setup. When a client asks for a new model to load, then the previously loaded model will simply be deleted from memory (via the Python <code>del</code> command), then the new model is being loaded via <code>tensorflow.keras.models.load_model()</code>.</p>
<p>From what I read in the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session"" rel=""nofollow noreferrer"">Keras documentation</a> one might want to clear a Keras session in order to free memory via calling <code>tf.keras.backend.clear_session()</code>. However, that seems to release <strong>all TF memory</strong>, which is a problem in my case, since other Keras models for other clients are still in use at the same time, as described above.</p>
<p>Moreover, it seems I cannot put every model into their own process, since I cannot access the single GPU from different running processes in parallel (or at all).</p>
<p>So in other words: When loading a new TensorFlow / Keras model while other models are also in memory and in use, how can I free the TF memory from the previsouly loaded model, without interferring with the other currently loaded models?</p>
",0
65737401,Sagemaker endpoint servving doesnt work for multiple inputs (mulit input-output LSTM),"<p>I have a LSTM network that has 3 inputs and 3 outputs(built with <a href=""https://keras.io/guides/functional_api/"" rel=""nofollow noreferrer"">functional api</a> in Tf.keras) , that I am trying to deploy as sagemaker endpoint. I have input shape of (None,10,1) for each input/feature, which means 10 timesteps.(I later concatenate the embeddings, but its irrelevant here)</p>
<p>Everything works fine on training time on sagemaker training jobs as well and training completes and artifacts are made successfully. But at time of invocation, endpoint is not working to predict <code>1 example, having 10 timesteps with 3 inputs</code> , I have tried multiple things but cant provide three inputs for prediction(input_1,input_2,input_1).</p>
<p>As I said that each input has 10 timesteps, so have shape (10,1). Endpoint only returns the output if I format my payload as below, but by doing so, it treats each time-step as separate example/instance and return 10 predictions for each output</p>
<pre><code>{'inputs':{
           'input_1': [[0], [0], [0], [0], [2], [12], [11], [7], [7], [2]],
           'input_2': [[0], [0], [0], [0], [30], [21], [2], [15], [27], [30]],
           'input_3': [[0], [0], [0], [0], [6], [2], [3], [13], [15], [6]]}
          } # gives len(pred['output_1&quot;])) == 10
</code></pre>
<p>This is expected as it consider this request as 10 examples, but in my case it is one example with 10-timesteps for each feature (1,10,1). So I tried different things from the <a href=""https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/deploying_tensorflow_serving.html"" rel=""nofollow noreferrer"">documentation</a>. Like using instances.</p>
<pre><code>{'instances': [
                {
                      'input_1': [[0],[0], [0],[0],[2],[12],[11], [7], [7], [2]],
                      'input_2': [[0], [0], [0], [0], [30], [21], [2], [15], [27], [30]],
                      'input_3': [[0], [0], [0], [0], [6], [2], [3], [13], [15], [6]]
                }
              ]
}
</code></pre>
<p>But it gives this error.</p>
<blockquote>
<p>transpose expects a vector of size 4. But input(1) is a vector of
size 3\n\t [[{{node transpose_1}}]]\n\t
[[functional_1/lstm/PartitionedCall]]\n\t
[[StatefulPartitionedCall/StatefulPartitionedCall]]&quot;\n}&quot;}&quot;</p>
</blockquote>
<p>Document also gives example and says</p>
<blockquote>
<p>for models with multiple named inputs, just include all the keys in the input dict</p>
</blockquote>
<p>but when I use this, I get error saying <code>Missing 'inputs' or 'instances' key\&quot;\n}&quot;</code></p>
<pre><code>{'input_1': [[0], [0], [0], [0], [2], [12], [11], [7], [7], [2]],
 'input_2': [[0], [0], [0], [0], [30], [21], [2], [15], [27], [30]],
 'input_3': [[0], [0], [0], [0], [6], [2], [3], [13], [15], [6]]}
</code></pre>
<p>My invocation code is below.</p>
<pre><code>import boto3
import json

sm = boto3.client('sagemaker-runtime')    
endpoint_name = &quot;tensorflow--------------------4&quot;
response = sm.invoke_endpoint(EndpointName=endpoint_name, 
                              Body=json.dumps(payload),
                              ContentType='application/json')
</code></pre>
<p>I am not sure how to solve this issue, looking forward for help</p>
",1
65779087,How to use tf.gradients within a model and still use a custom training loop?,"<p>I would like to make a TensorFlow model where the outputs respect a mathematical condition, namely that output 0 is a scalar function and all subsequent outputs are its partial derivatives w.r.t. the input. This is because my observations are the scalar function and its partials, and not using the partials for training would be a waste of information.</p>
<p>For now, using simply tf.gradients works if I don't build a custom training loop, i.e. when I don't utilize eager execution. The model is built like this, and training works as expected:</p>
<pre><code>import tensorflow as tf


from tensorflow.keras import losses
from tensorflow.keras import optimizers
from tensorflow.keras import callbacks

# Creating a model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Dense,
    Dropout,
    Flatten,
    Concatenate,
    Input,
    Lambda,
)

# Custom activation function
from tensorflow.keras.layers import Activation
from tensorflow.keras import backend as K

import numpy
import matplotlib.pyplot as plt

import tensorboard

layer_width = 200
dense_layer_number = 3

def lambda_gradient(args):
    layer = args[0]
    inputs = args[1]
    return tf.gradients(layer, inputs)[0]

# Input is a 2 dimensional vector
inputs = tf.keras.Input(shape=(2,), name=&quot;coordinate_input&quot;)

# Build `dense_layer_number` times a dense layers of width `layer_width`
stream = inputs
for i in range(dense_layer_number):
    stream = Dense(
        layer_width, activation=&quot;relu&quot;, name=f&quot;dense_layer_{i}&quot;
    )(stream)

# Build one dense layer that reduces the 200 nodes to a scalar output
scalar = Dense(1, name=&quot;network_to_scalar&quot;, activation=custom_activation)(stream)

# Take the gradient of the scalar w.r.t. the model input
gradient = Lambda(lambda_gradient, name=&quot;gradient_layer&quot;)([scalar, inputs])

# Combine them to form the model output
concat = Concatenate(name=&quot;concat_scalar_gradient&quot;)([scalar, gradient])

# Wrap everything in a model
model = tf.keras.Model(inputs=inputs, outputs=concat)

loss = &quot;MSE&quot;
optimizer = &quot;Adam&quot;

# And compile
model.compile(loss=loss, optimizer=optimizer)
</code></pre>
<p>However, them problem now comes when I want to do online training (i.e. with an incremental dataset). In this case, I wouldn't compile my model at the very end. Instead, I write a loop as such (before calling model.compile):</p>
<pre><code># ... continue from previous minus model.compile

loss_fn = tf.keras.losses.MeanSquaredError()
optimizer = tf.keras.optimizers.Adam()

# Iterate over the batches of a dataset and train.
for i_batch in range(number_of_batches):

    with tf.GradientTape() as tape:
        # Predict w.r.t. the inputs X
        prediction_Y = model(batches_X[i_batch])
        
        # Compare batch prediction to batch observation
        loss_value = loss_fn(batches_Y[i_batch], prediction_Y)

    gradients = tape.gradient(loss_value, model.trainable_weights)
    optimizer.apply_gradients(zip(gradients, model.trainable_weights))
</code></pre>
<p>This however gives the following exception at <code>prediction_Y = model(batches_X[i_batch])</code>:</p>
<pre><code>RuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.
</code></pre>
<p>As most examples, tutorials and documentation solely deal with using gradients to do training, and not within the model, I can't find any good resources how to deal with this. I tried to find how to use gradient tape, but I can't figure out how to use it in the model design phase. Any pointers would be appreciated!</p>
<p>Versions used:</p>
<pre><code>$ python --version                                         
Python 3.8.5
$ python -c &quot;import tensorflow as tf;print(tf.__version__);print(tf.keras.__version__)&quot;
2.2.0
2.3.0-tf
</code></pre>
",1
65794527,"Example of output_signature , output_types & output_shapes for complex object called by tf.data.Dataset.from_generator","<p>I've a generator function that yields the following tuple: <code>yield (transformed_input_array, set_y)</code></p>
<p><em>transformed_input_array</em> is a list of ndarrays with the following shape: <em>(1024, 104), (1024, 142), (1024, 1), (1024, 1), (1024, 1), (1024, 1), (1024, 140)</em>  and the following types: <em>tf.float64, tf.float64, tf.int8, tf.int16, tf.int8, tf.int8, tf.float64</em>
<em>set_y</em> is a ndarray of shape <em>1024</em> and type of <em>int64</em></p>
<p>I've wrapped my generator with tf.data.Dataset.from_generator function, here is the code:</p>
<pre><code>dataset = tf.data.Dataset.from_generator(
    generator,
    # output_signature=(
    #     tf.TensorSpec(shape=(), dtype=(tf.float64, tf.float64, tf.int8, tf.int16, tf.int8, tf.int8, tf.float64)),
    #     tf.TensorSpec(shape=1024, dtype=tf.int64))
    output_types=(tf.float64, tf.float64, tf.int8, tf.int16, tf.int8, tf.int8, tf.float64, tf.int64),
    output_shapes=((1024, 104), (1024, 142), (1024, 1), (1024, 1), (1024, 1), (1024, 1), (1024, 140), 1024)
)
</code></pre>
<p>But when I run the training, I get the following error:</p>
<blockquote>
<p>ValueError: Data is expected to be in format <code>x</code>, <code>(x,)</code>, <code>(x, y)</code>,
or <code>(x, y, sample_weight)</code>, found: (&lt;tf.Tensor 'IteratorGetNext:0'
shape=(1024, 104) dtype=float64&gt;, &lt;tf.Tensor 'IteratorGetNext:1'
shape=(1024, 142) dtype=float64&gt;, &lt;tf.Tensor 'IteratorGetNext:2'
shape=(1024, 1) dtype=int8&gt;, &lt;tf.Tensor 'It eratorGetNext:3'
shape=(1024, 1) dtype=int16&gt;, &lt;tf.Tensor 'IteratorGetNext:4'
shape=(1024, 1) dtype=int8&gt;, &lt;tf.Tensor 'IteratorGetNext:5'
shape=(1024, 1) dtype=int8&gt;, &lt;tf.Tensor 'IteratorGetNext:6'
shape=(1024, 140) dtype=float64&gt;, &lt;tf.Tensor 'ExpandDims:0'
shape=(1024, 1) dtype=int64&gt;)</p>
</blockquote>
<p>If I try to run with output_signature param (commented out code), I get the following error:</p>
<blockquote>
<p>TypeError: Cannot convert value (tf.float64, tf.float64, tf.int8,
tf.int16, tf.int8, tf.int8, tf.float64) to a TensorFlow DType.</p>
</blockquote>
<p><strong>Can someone provide an example, of how I should treat complex type (list of ndarrays)?</strong> Couldn't find any example in TF documentation..</p>
",1
65835387,ValueError: too many values to unpack (expected 2) when using tf.keras.preprocessing.image_dataset_from_directory,"<p>I want to create a dataset-variable as well as a labels-variable using the function tf.keras.preprocessing.image_dataset_from_directory (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory</a>).
The documentation states:</p>
<blockquote>
<p>Returns:
A tf.data.Dataset object.
If label_mode is None, it yields
float32 tensors of shape (batch_size, image_size[0], image_size[1],
num_channels), encoding images (see below for rules regarding
num_channels).
Otherwise, it yields a tuple (images, labels), where
images has shape (batch_size, image_size[0], image_size[1],
num_channels), and labels follows the format described below.</p>
</blockquote>
<p>My code is the following:</p>
<pre><code>train_ds, labels = tf.keras.preprocessing.image_dataset_from_directory(
  directory = data_dir,
  labels='inferred',
  label_mode = &quot;int&quot;,
  validation_split=0.2,
  subset=&quot;training&quot;,
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)
</code></pre>
<p>I expect to get a tuple as return values, but instead I get the error message:</p>
<pre><code>Found 2160 files belonging to 2160 classes.
Using 1728 files for training.
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-168-ed9d42ed2ab9&gt; in &lt;module&gt;
      7   seed=123,
      8   image_size=(img_height, img_width),
----&gt; 9   batch_size=batch_size)

ValueError: too many values to unpack (expected 2)
</code></pre>
<p>When I save the output in one variable (just train_ds) and I inspect the variable, I get the following output:</p>
<pre><code>&lt;BatchDataset shapes: ((None, 120, 30, 3), (None,)), types: (tf.float32, tf.int32)&gt;
</code></pre>
<p>How can I access the two tuples inside seperatly?</p>
",1
65863738,Changing order of Input Image in 3D convolutions,"<p>According to the official documentation of tf.keras.layers.Conv3D</p>
<blockquote>
<p>5+D tensor with shape: batch_shape + (channels, conv_dim1, conv_dim2,
conv_dim3) if data_format='channels_first' or 5+D tensor with shape:
batch_shape + (conv_dim1, conv_dim2, conv_dim3, channels) if
data_format='channels_last'</p>
</blockquote>
<p>. Now the whole idea around channels and batch shape makes sense, but will changing the general order of (conv_dim1, conv_dim2,conv_dim2) as (x,y,z) to say (z,x,y) affect the performance.</p>
<p>Does Conv3D worry about order of x-y-z dimension ?</p>
<p>I was training a U-net segmentation model and upon changing the order of axis I saw difference in performance. (x,y,z) order converges faster as compared to (y,x,z).</p>
<p>I just wanted to make sure what's the correct way..</p>
",1
65944097,Keras: How does reprocessing scale images?,"<p>For loading image data from disk, <em>Keras</em> is providing the <code>tf.keras.preprocessing.image_dataset_from_directory()</code> method, which is documented on <a href=""https://keras.io/api/preprocessing/image/"" rel=""nofollow noreferrer"">https://keras.io/api/preprocessing/image/</a>.
I would like to use this method to load data for generating a image classifier.
<a href=""https://machinelearningmastery.com/how-to-load-large-datasets-from-directories-for-deep-learning-with-keras/"" rel=""nofollow noreferrer"">https://machinelearningmastery.com/how-to-load-large-datasets-from-directories-for-deep-learning-with-keras/</a> contains some information on how to best organize the data.</p>
<p>The <code>image_dataset_from_directory()</code> takes a mandatory argument <code>image_size=(..., ...)</code> to give all images to the same size (which is required for further steps).<br />
Where can I find details about how the pictures are scaled?<br />
Would pictures with extreme ratios become distorted and negatively impact the classfier?</p>
",0
65953591,Which format should have time series input for LSTM-Model in Tensorflow?,"<p>I have a problem with the input for the fit-function of an LSTM-Model in TensorFlow. I have an input with the following shape:<br />
(5, 128, 78, 80)<br />
The fields are: (number of samples, timesteps, feature1, feature2)</p>
<p>The output has the shape: (5, 128, 78, 2)</p>
<p>This is my model:</p>
<pre><code>from tensorflow.keras.layers import LSTM, Dense, Dropout, Activation

batch_size=5

time_model = tf.keras.Sequential()
time_model.add(tf.keras.layers.LSTM(512,return_sequences=True,input_shape=(128,2)))
time_model.add(Activation('sigmoid'))
time_model.add(Dense(2,name=&quot;dense&quot;))
time_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')


time_model.fit(x=time_input,y=time_output, epochs=10, batch_size=batch_size)
</code></pre>
<p>I get the following error:<br />
<code>ValueError: Input 0 of layer sequential_38 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (5, 128, 78, 80)</code></p>
<p>So I think, I have to change the shape of my data, but I don't know how. I tried already different values for input and  input_shape-attribute.<br />
I read in <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM</a> that the input has to be a tensor with shape <code>[batch, timesteps, feature]</code>. So I put the two features in a nested array, and gave <code>[batch, timesteps, array of the features]</code> to the fit-function. But it told me that the data could not be converted to a tensor. Also explicit converting with <code>tf.convert_to_tensor</code> did not work.</p>
<p>I would be really glad, if someone could explain me, how I can pass input data with two features to an LSTM-model.</p>
",1
66019998,"How to get a processed dataset, if the processing steps are not tensor operations?","<p>I have an instance of <code>tf.data.Dataset()</code>, of images, basically, acquired this way:</p>
<pre><code>import tensorflow as tf

dataset = tf.keras.preprocessing.image_dataset_from_directory(
    data_directory,
    image_size = (image_height, image_width),
    batch_size = batch_size
)
</code></pre>
<p>So, this dataset has <code>(data, label)</code> where the data is a tensor of shape <code>(batch_size, image_height, image_width, channels)</code> [I don't really need the labels it assigns]. So far so good. The problem is, I need to process this dataset, applying certain operations to the images, and, this dataset is too big to load everything in memory (that's why <code>batch_size</code> is there). According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map"" rel=""nofollow noreferrer"">tensorflow documentation</a>, <code>tf.data.Dataset.map()</code> is the function I need (or so I assume....).</p>
<pre><code>def image_processing(data):
    print(data.shape)
    
    # Do some operations.
    # Do some copies [because np.arrays help me more...].
    copy = np.array(data, copy=True)

    # Change some pixels, like, zero out a square in this image
    # It sad that TensorFlow can't do this assignment if it were a tf.Tensor:
    copy[10:80,10:80] = np.array([0,0,0])

    # Do more things, and when done return.
    return something


processed_dataset = dataset.map(lambda image, label: (image_processing(image), label))
</code></pre>
<p>First of all, the shape returned by the print: <code>(None, 200, 200, 3)</code> instead of <code>(32, 200, 200, 3)</code>, or, instead of <code>(200, 200, 3)</code> [which is what I'd expect from reading the documentation] [let's assume batch of 32, and images 200x200], and this is messing my code, because, I need to do assigments, like, take the ith image, and change a couple pixels: <code>data[i][12:15,40:50] = np.array([1,2,3])</code> and things like that.</p>
<p>Basically, that's the error message: <code>TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'</code>.</p>
<hr />
<p><strong>In summary, my question:</strong> How can I get a <code>processed_dataset</code>, where the processing steps will not be whole tensor operations, but instead, will be changing individual values in the data (say, individual pixels), for certain images (say, the ith image, jth image, etc)?</p>
<hr />
<p>If you must know, I am running this in Ubuntu. Tensorflow version is:</p>
<pre><code>&gt;&gt;&gt; tf.__version__
'2.4.0'
</code></pre>
",1
66030439,TensorFlow profiler using tf.profiler.experimental.client.trace gives empty trace data,"<p>I'm unable to collect trace data using <code>tf.profiler.experimental.client.trace</code> Please can someone help? I'm following the (CPU/GPU) example usage here <a href=""https://www.tensorflow.org/api_docs/python/tf/profiler/experimental/client/trace"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/profiler/experimental/client/trace</a> which looks simple enough.</p>
<p>I have a very simple model, and I'm able to collect trace data from it using <code>tf.profiler.experimental.start</code> and <code>tf.profiler.experimental.stop</code>.</p>
<p>But <code>tf.profiler.experimental.client.trace</code> gives me empty trace data.</p>
<p>My code is as follows:</p>
<pre><code>import tensorflow as tf
import numpy as np
                                                                                                    
def mnist_dataset(batch_size):
    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()                                              
    x_train = x_train / np.float32(255)
    y_train = y_train.astype(np.int64)
    train_dataset = tf.data.Dataset.from_tensor_slices(
        (x_train, y_train)).shuffle(60000).repeat().batch(batch_size)
    return train_dataset

batch_size = 64
dataset = mnist_dataset(batch_size)

model = tf.keras.Sequential([
    tf.keras.Input(shape=(28, 28)),
    tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
    tf.keras.layers.Conv2D(32, 3, activation='relu'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10)
])
model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
    metrics=['accuracy'])
                                                                                           
#tf.profiler.experimental.start('./logs/tb_log')                                                                        
tf.profiler.experimental.server.start(6009)

model.fit(dataset, epochs=10, steps_per_epoch=70)

tf.profiler.experimental.client.trace('grpc://localhost:6009', './logs/tbc_log', 20000)
#tf.profiler.experimental.stop()         
</code></pre>
<p>The code runs through the epochs, and then outputs</p>
<pre><code>2021-02-02 17:49:44.943933: I tensorflow/core/profiler/rpc/client/capture_profile.cc:198] Profiler delay_ms was 0, start_timestamp_ns set to 1612288184943887718 [2021-02-02T17:49:44.943887718+00:00]
Starting to trace for 20000 ms. Remaining attempt(s): 2
2021-02-02 17:49:44.944037: I tensorflow/core/profiler/rpc/client/remote_profiler_session_manager.cc:75] Deadline set to 2021-02-02T17:50:44.890124419+00:00 because max_session_duration_ms was 60000 and session_creation_timestamp_ns was 1612288184890124419 [2021-02-02T17:49:44.890124419+00:00]
2021-02-02 17:49:44.944197: I tensorflow/core/profiler/rpc/client/profiler_client.cc:113] Asynchronous gRPC Profile() to localhost:6009
2021-02-02 17:49:44.944316: I tensorflow/core/profiler/rpc/client/remote_profiler_session_manager.cc:96] Issued Profile gRPC to 1 clients
2021-02-02 17:49:44.944340: I tensorflow/core/profiler/rpc/client/profiler_client.cc:131] Waiting for completion.
2021-02-02 17:49:44.946274: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-02-02 17:49:44.947547: W tensorflow/core/profiler/lib/profiler_session.cc:144] Profiling is late (2021-02-02T17:49:44.946338176+00:00) for the scheduled start (2021-02-02T17:49:44.943887718+00:00) and will start immediately.
2021-02-02 17:49:44.947582: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-02-02 17:49:44.947660: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 2 GPUs
2021-02-02 17:49:44.949656: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcupti.so.11.0
2021-02-02 17:50:08.435260: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.
2021-02-02 17:50:08.435591: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed
2021-02-02 17:50:08.635192: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 0 callback api events and 0 activity events. 
2021-02-02 17:50:08.648616: I tensorflow/core/profiler/rpc/profiler_service_impl.cc:67] Collecting XSpace to repository: ./logs/tbc_log/plugins/profile/2021_02_02_17_49_44/localhost_6009.xplane.pb
2021-02-02 17:50:08.650309: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
2021-02-02 17:50:08.650676: W tensorflow/core/profiler/rpc/client/capture_profile.cc:133] No trace event is collected from localhost:6009
No trace event is collected. Automatically retrying.

2021-02-02 17:50:08.651046: I tensorflow/core/profiler/rpc/client/capture_profile.cc:198] Profiler delay_ms was 0, start_timestamp_ns set to 1612288208651017638 [2021-02-02T17:50:08.651017638+00:00]
Starting to trace for 20000 ms. Remaining attempt(s): 1
2021-02-02 17:50:08.651123: I tensorflow/core/profiler/rpc/client/remote_profiler_session_manager.cc:75] Deadline set to 2021-02-02T17:50:44.890124419+00:00 because max_session_duration_ms was 60000 and session_creation_timestamp_ns was 1612288184890124419 [2021-02-02T17:49:44.890124419+00:00]
2021-02-02 17:50:08.651274: I tensorflow/core/profiler/rpc/client/profiler_client.cc:113] Asynchronous gRPC Profile() to localhost:6009
2021-02-02 17:50:08.651391: I tensorflow/core/profiler/rpc/client/remote_profiler_session_manager.cc:96] Issued Profile gRPC to 1 clients
2021-02-02 17:50:08.651420: I tensorflow/core/profiler/rpc/client/profiler_client.cc:131] Waiting for completion.
2021-02-02 17:50:08.652492: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-02-02 17:50:08.652570: W tensorflow/core/profiler/lib/profiler_session.cc:144] Profiling is late (2021-02-02T17:50:08.652539729+00:00) for the scheduled start (2021-02-02T17:50:08.651017638+00:00) and will start immediately.
2021-02-02 17:50:08.652591: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-02-02 17:50:31.280828: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.
2021-02-02 17:50:31.281134: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed
2021-02-02 17:50:31.510697: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 0 callback api events and 0 activity events. 
2021-02-02 17:50:31.515475: I tensorflow/core/profiler/rpc/profiler_service_impl.cc:67] Collecting XSpace to repository: ./logs/tbc_log/plugins/profile/2021_02_02_17_49_44/localhost_6009.xplane.pb
2021-02-02 17:50:31.518037: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
2021-02-02 17:50:31.518440: W tensorflow/core/profiler/rpc/client/capture_profile.cc:133] No trace event is collected from localhost:6009
No trace event is collected. Automatically retrying.

2021-02-02 17:50:31.518819: I tensorflow/core/profiler/rpc/client/capture_profile.cc:198] Profiler delay_ms was 0, start_timestamp_ns set to 1612288231518793164 [2021-02-02T17:50:31.518793164+00:00]
Starting to trace for 20000 ms. Remaining attempt(s): 0
2021-02-02 17:50:31.518889: I tensorflow/core/profiler/rpc/client/remote_profiler_session_manager.cc:75] Deadline set to 2021-02-02T17:50:44.890124419+00:00 because max_session_duration_ms was 60000 and session_creation_timestamp_ns was 1612288184890124419 [2021-02-02T17:49:44.890124419+00:00]
2021-02-02 17:50:31.519021: I tensorflow/core/profiler/rpc/client/profiler_client.cc:113] Asynchronous gRPC Profile() to localhost:6009
2021-02-02 17:50:31.519124: I tensorflow/core/profiler/rpc/client/remote_profiler_session_manager.cc:96] Issued Profile gRPC to 1 clients
2021-02-02 17:50:31.519147: I tensorflow/core/profiler/rpc/client/profiler_client.cc:131] Waiting for completion.
2021-02-02 17:50:31.520067: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-02-02 17:50:31.520136: W tensorflow/core/profiler/lib/profiler_session.cc:144] Profiling is late (2021-02-02T17:50:31.520095781+00:00) for the scheduled start (2021-02-02T17:50:31.518793164+00:00) and will start immediately.
2021-02-02 17:50:31.520152: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-02-02 17:50:44.891412: W tensorflow/core/profiler/rpc/client/profiler_client.cc:152] Deadline exceeded: Deadline Exceeded
2021-02-02 17:50:44.891501: W tensorflow/core/profiler/rpc/client/capture_profile.cc:133] No trace event is collected from localhost:6009
2021-02-02 17:50:44.891526: W tensorflow/core/profiler/rpc/client/capture_profile.cc:145] localhost:6009 returned Deadline exceeded: Deadline Exceeded
No trace event is collected after 3 attempt(s). Perhaps, you want to try again (with more attempts?).
Tip: increase number of attempts with --num_tracing_attempts.
2021-02-02 17:50:44.891848: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
Traceback (most recent call last):
  File &quot;keras_singleworker_2.py&quot;, line 37, in &lt;module&gt;
2021-02-02 17:50:44.893228: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed
    tf.profiler.experimental.client.trace('grpc://localhost:6009', './logs/tbc_log', 20000)
  File &quot;/fserver/jonathanb/miniconda3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/profiler/profiler_client.py&quot;, line 131, in trace
    _pywrap_profiler.trace(
tensorflow.python.framework.errors_impl.UnavailableError: No trace event was collected because there were no responses from clients or the responses did not have trace data.
</code></pre>
<p>I've tried locating tf.profiler.experimental.server.start and tf.profiler.experimental.client.trace in other locations in the code, but with no success.</p>
",1
66038861,Why are both branches in tf.cond being executed? And why does tf.while_loop finish the loop even though the condition still true?,"<p>I am using keras for a while now, but usually I don't have to use customized layers or perform some more complex flow control, so I'm struggling trying to understand somethings.</p>
<p>I am modeling a neural network with a customized layer on the top. This customized layer calls another function (<code>search_sigma</code>)  and inside this function I execute <code>tf.while_loop</code> and inside of <code>tf.while_loop</code> I execute <code>tf.cond</code>.</p>
<p>I cannot understand why the conditions are not working.</p>
<ul>
<li><code>tf.while_loop</code> stops even though the condition (<code>l1</code>) still true</li>
<li><code>tf.cond executes</code> both <code>f1</code> and <code>f2</code> (callables <code>true_fn</code> and <code>false_fn</code>)</li>
</ul>
<p>Could someone help me understand what I am missing?</p>
<p>I already tried to change both tf.cond and tf.while_loop conditions for true tensors, just to see what would happen. The behavior (exactly same errors) remained the same.</p>
<p>I also tried to write this code without implementing a class (using just functions). Nothing changed.</p>
<p>I tried to find solutions looking at tensorflow documentation, other stack overflow doubts and websites talking about tf.while_loop and tf.cond.</p>
<p>I left some <code>print()</code>s in the body of the code to try to track what was happening.</p>
<pre><code>class find_sigma:
    
    def __init__ (self, t_inputs,  inputs,  expected_perp=10. ):       
        self.sigma, self.cluster = t_inputs
        self.inputs = inputs
        self.expected_perp = expected_perp
        self.min_sigma=tf.constant([0.01],tf.float32)
        self.max_sigma=tf.constant([50.],tf.float32)
 

    def search_sigma(self):

        
        def cond(s,sigma_not_found): return sigma_not_found


        def body(s,sigma_not_found):   

            print('loop')
            pi = K.exp( - K.sum( (K.expand_dims(self.inputs, axis=1) - self.cluster)**2, axis=2  )/(2*s**2) )        
            pi = pi / K.sum(pi)
            MACHINE_EPSILON = np.finfo(np.double).eps
            pi = K.maximum(pi, MACHINE_EPSILON)
            H = - K.sum ( pi*(K.log(pi)/K.log(2.)) , axis=0 )
            perp = 2**H

            print('0')

            l1 = tf.logical_and (tf.less(perp , self.expected_perp), tf.less(0.01, self.max_sigma-s))
            l2 = tf.logical_and (tf.less(  self.expected_perp , perp) , tf.less(0.01, s-self.min_sigma) )
    
            def f1():
                print('f1')
                self.min_sigma = s 
                s2 = (s+self.max_sigma)/2 
                return  [s2, tf.constant([True])]
                

            def f2(l2): 
                tf.cond( l2, true_fn=f3 , false_fn = f4)

            def f3(): 
                print('f3')
                self.max_sigma = s 
                s2 = (s+self.min_sigma)/2
                return [s2, tf.constant([True])]

            def f4(): 
                print('f4')
                return [s, tf.constant([False])]
            
            output = tf.cond( l1, f1 ,  f4 ) #colocar f2 no lugar de f4

            s, sigma_not_found = output
            
            print('sigma_not_found = ',sigma_not_found)
            return [s,sigma_not_found]

        print('01')

        sigma_not_found = tf.constant([True])

        new_sigma,sigma_not_found=sigma_not_found = tf.while_loop(
            cond , body, loop_vars=[self.sigma,sigma_not_found]
        )

        print('saiu')
        
        print(new_sigma)

        return new_sigma
</code></pre>
<p>The piece of code that calls the above code is:</p>
<pre><code>self.sigma = tf.map_fn(fn=lambda t: find_sigma(t,  inputs).search_sigma() , elems=(self.sigma,self.clusters), dtype=tf.float32)
</code></pre>
<p>'inputs' is a <code>(None, 10)</code> size tensor</p>
<p>'self.sigma' is a <code>(10,)</code> size tensor</p>
<p>'self.clusters' is a <code>(N, 10)</code> size tensor</p>
",1
66049816,Custom layer in sequential model tensorflow,"<p>I'm trying to create a custom layer for my model, which can be used the classic Dense layer of Keras. Here my custom layer:</p>
<pre><code>class MyDenseLayer(tf.keras.layers.Layer):
    def __init__(self, num_outputs):
        super(MyDenseLayer, self).__init__()
        self.num_outputs = num_outputs
    def build(self, input_shape):
        self.kernel = self.add_weight(&quot;kernel&quot;, 
                                      shape=[int(input_shape[-1]),
                                      self.num_outputs])
    def call(self, input):
        return tf.matmul(input, self.kernel)
</code></pre>
<p>It does not do anything 'custom' for now.</p>
<p>But when I add it to my model</p>
<pre><code>def build_model():
    model = keras.Sequential([
        MyDenseLayer(10)(normed_x_train),
        layers.Activation(tf.nn.relu),
        layers.Dense(1, activation=tf.nn.relu)
        ])
    return model
</code></pre>
<p>I get this:</p>
<pre><code>The added layer must be an instance of class Layer. Found: tf.Tensor(
[....])
</code></pre>
<p>Because probably I'm creating directly the object of class Custom Layer. But I do not find in the tf documentation how to add other properties to make it work as a normal layer, i.e. as something like <code>layers.Dense(100, activation=tf.nn.relu)</code></p>
<p>Is there a way to make it work like that ?</p>
",1
66052849,Storing pickle object in Google Cloud Storage using Tensorflow.io.gfile,"<p>I am trying to store a pickle object in a Google Cloud Storage bucket. This is a part of a machine learning pipeline [tutorial][1] provided by Google that I am following. I broke down the code to a minimal example that still throws the same error. In my actual code, the object is a class instance.</p>
<pre><code>import tensorflow as tf
import dill as pickle
obj = {'foo': 'bar'}
with tf.io.gfile.GFile(filename, 'wb') as f:
    pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)
</code></pre>
<p>When I run this, I get</p>
<blockquote>
<p>TypeError: Expected binary or unicode string, got &lt;memory at 0x7fdc66d7d7c0&gt;</p>
</blockquote>
<p>On the tutorial this step worked fine as it used Tensorflow 1 and tf.io.gfile.Open(), which was removed in Tensorflow 2 and replaced by the command above. Simply using open() also works, but of course that doesn't help me writing to a bucket. I also tried</p>
<pre><code>with tf.io.gfile.GFile(filename, 'wb') as f:
   f.write(obj)
</code></pre>
<p>but it returns the same error. Please let me know know what I am doing wrong or if there is an alternative approach to store a pickled object directly to a bucket? Many thanks for your help!
[1]: <a href=""https://cloud.google.com/dataflow/docs/samples/molecules-walkthrough#overview"" rel=""nofollow noreferrer"">https://cloud.google.com/dataflow/docs/samples/molecules-walkthrough#overview</a></p>
",0
66064840,Tensorflow 2.0: tf.feature_columns are triggering tf.function retracing warning,"<p>I'm using tensorflow 2.4.0 to train a dnn but I've been getting the following waring after saving and then loading the model:</p>
<pre><code>WARNING:tensorflow:11 out of the last 11 calls to &lt;function recreate_function.&lt;locals&gt;.restored_function_body at 0x7f14b49933b0&gt; triggered tf.function retracing. 
Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. 
For (1), please define your @tf.function outside of the loop. 
For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. 
For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for more details.
</code></pre>
<p>The surprising thing is that I don't have any custom functions in my code. After debugging by including only one input at a time, I found that these few lines are triggering the warning.</p>
<pre><code>categorical_column = tf.feature_column.categorical_column_with_vocabulary_list(col_name, ['A', 'B', 'C', 'D'])
feature_columns[col_name] = tf.feature_column.indicator_column(categorical_column=categorical_column)
</code></pre>
<p>But there is no way to add the <code>@tf.function</code> decorator to this code, since these are not custom functions but <code>tf.feature_columns</code>.</p>
<p>Is this the expected behaviour? Is there a way to remove the warnings?</p>
<p><strong>Update:</strong>
This is the complete code:</p>
<pre><code>import tempfile

import numpy as np
import tensorflow as tf

col_name = 'vocab'

inputs = {}
inputs[col_name] = tf.keras.layers.Input(name=col_name, shape=(1,), dtype=tf.dtypes.string)

feature_columns = {}
categorical_column = tf.feature_column.categorical_column_with_vocabulary_list(col_name, ['A', 'B', 'C', 'D'])
feature_columns[col_name] = tf.feature_column.indicator_column(categorical_column=categorical_column)

dense_features = tf.keras.layers.DenseFeatures(feature_columns.values())(inputs)

x = tf.keras.layers.Dense(1)(dense_features)
model = tf.keras.models.Model(inputs=inputs, outputs=x)
model.compile(optimizer='adam', loss='mse')

es = tf.keras.callbacks.EarlyStopping(
      monitor='val_loss', 
      mode='min', 
      restore_best_weights=True, 
      patience=5
  )

model.fit(
    np.array(['A', 'B', 'C', 'D']),
    np.array([1, 2, 3, 4]),
    epochs=1,
    callbacks=[es],
)

# Save the model
temp_dir = tempfile.mkdtemp()
model.save(temp_dir)

# Load it again
tf.saved_model.load(temp_dir)

# ------------------
# ERRORS THAT I GET:
# ------------------
# WARNING:tensorflow:5 out of the last 5 calls to &lt;function recreate_function.&lt;locals&gt;.restored_function_body at 0x7f8f810c9050&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
# WARNING:tensorflow:6 out of the last 6 calls to &lt;function recreate_function.&lt;locals&gt;.restored_function_body at 0x7f8f810c9680&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
# &lt;tensorflow.python.saved_model.load.Loader._recreate_base_user_object.&lt;locals&gt;._UserObject at 0x7f8f7a1b80d0&gt;
</code></pre>
",0
66069038,`tf.keras.model.evaluate()` provides different results when fed the same data in different formats,"<h1>Backgound</h1>
<p>I am looking at the following Tensorflow time series tutorial:
<a href=""https://www.tensorflow.org/tutorials/structured_data/time_series#single-shot_models"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/structured_data/time_series#single-shot_models</a></p>
<p>For the discussion here, I am going to consider the ‘multi_linear_model’ in the <strong>Multi-step models</strong> section.</p>
<p>I add the following line</p>
<p><code>multi_val_performance_new['Linear'] = multi_linear_model.evaluate(next(iter(multi_window.val))[0], next(iter(multi_window.val))[1]) </code></p>
<p>after this line</p>
<p><code>multi_val_performance['Linear'] = multi_linear_model.evaluate(multi_window.val)</code>.</p>
<p>When I inspect the loss and mean absolute errors, they are different.</p>
<h1>Question</h1>
<p>Why - when I feed the original <code>tf.Dataset</code> to <code>model.evaluate</code> - do I get one set of loss and mean absolute error, but when I feed its components - <code>(next(iter(tf.Dataset))[0], next(iter(tf.Dataset)[1])</code> to <code>model.evaluate</code> - do I get a different set of loss and mean absolute errors?</p>
<h2>NB</h2>
<p>I have set <code>shuffle = false</code> in <code>def make_dataset(self, data):</code>.</p>
",0
66102920,Tensorflow loss function having no gradients,"<p>I might need some help with implementing an specific regularization term for the loss function. It does however not have a gradient and I wonder if there is any way of changing that. I have read this approach in a paper, however it is not important to read this paper for actually helping me. I will just describe the method where the problem is and show a testing code in a Google-Colab.</p>
<p>The Neural Network is just made of 2 convolutional layers and the final layer is with the Sigmoid activation function. Therefore the output is between 0 and 1 due to the Sigmoid. This value will be treated as a probability for every neuron in the output layer to be either 0 or 1. So I want to implement this with the 'tf.keras.backend.switch' function in such a way:</p>
<pre><code>def regularization_term(y_true, y_pred):
    zeros = tf.zeros_like(y_pred)
    ones = tf.ones_like(y_pred)
    random = tf.random.uniform(tf.shape(y_pred),minval=0,maxval=1,dtype=tf.dtypes.float64)
    y_pred_new = tf.keras.backend.switch(random &gt; y_pred, zeros, ones)
    return y_pred_new
</code></pre>
<p>I draw a random number and check with a condition to make each value 0 or 1. This should be clear from the code. However when I do this, the term actually has no gradient:</p>
<blockquote>
<p>ValueError: No gradients provided for any variable: ['conv2d_4/kernel:0', 'conv2d_4/bias:0', 'conv2d_5/kernel:0', 'conv2d_5/bias:0'].</p>
</blockquote>
<p>The full testing code can be found in this colab: <a href=""https://colab.research.google.com/drive/1YuX00BUAj-BVCZRbr4opo5wHbcaVbYvx?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1YuX00BUAj-BVCZRbr4opo5wHbcaVbYvx?usp=sharing</a></p>
<p>Is there any way of implementing this method while keeping a gradient for making the network learn? If anything is unclear, please ask, I tried to put some effort in to make my question as understandable as possible. I'm really glad for any help.</p>
<p>Thank you very much!</p>
<p>[EDIT, COPY PASTE FROM GOOGLE_COLAB, ignore otherwise:]</p>
<pre><code>#just importing some libraries
import tensorflow as tf
import numpy as np
from tensorflow.python.framework import ops
from tensorflow.keras import datasets, layers, models
tf.keras.backend.set_floatx('float64')
#length of the dataset
L=16
#THIS REGULARIZATION TERM NEEDS SOME AID

def regularization_term(y_true, y_pred):
    zeros = tf.zeros_like(y_pred)
    ones = tf.ones_like(y_pred)
    random = tf.random.uniform(tf.shape(y_pred),minval=0,maxval=1,dtype=tf.dtypes.float64)
    y_pred_new = tf.keras.backend.switch(random &gt; y_pred, zeros, ones)
    #here is actually some additional operatios, but they dont need to be taken into consideration so I've removed them
    return tf.reduce_sum(y_pred_new)

def my_custom_loss(y_true, y_pred):
    # I try this only with the regularization term to get the 'No gradients provided' error message
    return regularization_term(y_true, y_pred)
    #actually i would add a binary crossentropy term to this, i did not here for showcase purpose
    enter code here
#creating a dataset for input (initial) and true data (target) for testing purposes
initial = np.random.randint(2,size=(10000,L+2,L+2)).astype(&quot;float&quot;)
target = np.random.randint(2,size=(10000,L,L)).astype(&quot;float&quot;)

#adding a model with CNN and 1 hidden layer with relu and 1 output layer with sigmoid
EPOCHS = 2
BATCH_SIZE = 1000
model = models.Sequential()
model.add(layers.Conv2D(1,2,activation='relu',input_shape=[L+2,L+2,1]))
model.add(layers.Conv2D(1,2,activation='sigmoid',input_shape=[L,L,1]))
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),loss=my_custom_loss)
model.fit(initial.reshape(10000,L+2,L+2,1),target.reshape(10000,L,L,1),batch_size = BATCH_SIZE, epochs=EPOCHS, verbose=1)
</code></pre>
",0
66144466,Tensorflow Serve SignatureDefs for classifier,"<p>I trained a BERT text classifier following these steps, with own texts and some modifications:</p>
<p><a href=""https://www.tensorflow.org/tutorials/text/classify_text_with_bert"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/classify_text_with_bert</a></p>
<p>To export the model and run it with Tensorflow Serve works well:</p>
<p><a href=""https://www.tensorflow.org/tfx/serving/docker"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tfx/serving/docker</a></p>
<p>Unfortunately, I can not really figure out how to define the SignatureDefs for a classifier, so that the classifier endpoint for Tensorflow Serve is defined. The <code>:predict</code> endpoint works well and seems to be the default signature.</p>
<p>Obviously, I have to define the signatures, when I save the model. Since the documentation is not very exhaustive on this topic, I am not sure how to define the classifier signature.</p>
<p><a href=""https://www.tensorflow.org/tfx/serving/signature_defs"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tfx/serving/signature_defs</a></p>
<p>In the above example for BERT, the <code>serving_results</code> just define the <code>reloaded_model</code> with <code>tf.constant(examples)</code> and instantiates it with <code>serving_results = tf.sigmoid(serving_results['classifier')]</code>.</p>
<p>So, I assume I have to give the activation function and the <code>classifier</code> signature as arguments, when calling the <code>model.save</code> method.</p>
<p><code>predict</code>endpoint works, <code>classify</code> endpoint gives the error:</p>
<p><code>{&quot;error&quot;: &quot;No classification inputs found in SignatureDef: inputs {\n  key: \&quot;text\&quot;\n  value {\n    name: \&quot;serving_default_text:0\&quot;\n    dtype: DT_STRING\n    tensor_shape {\n      dim {\n        size: -1\n      }\n    }\n  }\n}\noutputs {\n  key: \&quot;classifier\&quot;\n  value {\n    name: \&quot;StatefulPartitionedCall_2:0\&quot;\n    dtype: DT_FLOAT\n    tensor_shape {\n      dim {\n        size: -1\n      }\n      dim {\n        size: 1\n      }\n    }\n  }\n}\nmethod_name: \&quot;tensorflow/serving/predict\&quot;\n&quot;}</code></p>
<p>I would be grateful for any hints.</p>
",0
66211959,TensorFlow crashes with Failed to create cuSolverDN instance when tf.linalg.inv is called,"<p>I'm running the <code>tensorflow/tensoflow:latest-gpu</code> docker container. I can run simple vector operations like <code>@</code> for matrix multiplication without a problem. However, when I run the following minimal example:</p>
<pre><code>import tensorflow as tf
tf.linalg.inv(tf.eye(10))
</code></pre>
<p>I get the following error:</p>
<pre><code>2021-02-15 16:18:20.375254: I tensorflow/core/util/cuda_solvers.cc:180] Creating CudaSolver handles for stream 0x528cf90
2021-02-15 16:18:20.375365: I tensorflow/stream_executor/platform/default/dso_loader.cc:49]
Successfully opened dynamic library libcusolver.so.10
2021-02-15 16:18:21.854945: I tensorflow/stream_executor/platform/default/dso_loader.cc:49]
Successfully opened dynamic library libcublas.so.11
2021-02-15 16:18:21.934489: F tensorflow/core/util/cuda_solvers.cc:120]
Check failed: cublasCreate(&amp;cublas_handle) == CUBLAS_STATUS_SUCCESS Failed to create cuBlas instance.
</code></pre>
<p>The Python interpreter crashes. The CUDA and cuBLAS libraries are successfully opened, so I'm not sure what's causing the error.</p>
<p>The crash also happens with the <code>tensorflow/tensorflow:devel-gpu</code> image. When I try an earlier TensorFlow version (2.3), I do not get the error. However, I need to use &gt;=2.4 because that's required by <code>tensorflow_probability</code>.</p>
<p>I'm on Pop! OS (Ubuntu 20.10), using a GTX 1650.</p>
<hr />
<p>Edit: Installing tf-nightly natively on the host system doesn't produce the error; <code>tf.linalg.inv(tf.eye(10))</code> works fine. This does not solve the problem with the docker image (nightly image still produces the error), but I have a working GPU tensorflow environment for now.</p>
",0
66275500,Cannot find the source code for `tf.quantization.fake_quant_with_min_max_args`,"<p>Where one can find the github source code for <code>tf.quantization.fake_quant_with_min_max_args</code>. Checking the <a href=""https://www.tensorflow.org/api_docs/python/tf/quantization/fake_quant_with_min_max_args"" rel=""nofollow noreferrer"">TF API documentation</a>, there is no link to the github source file, and I could not find one on github.</p>
",1
66287320,Tensorflow 2.0: flat_map() to flatten Dataset of Dataset returns cardinality -2,"<p>I am trying to run the following code (as given in Tensorflow documentation) to create windows of my data and then flatten the dataset of datasets.</p>
<pre><code>window_size = 5

windows = range_ds.window(window_size, shift=1)
for sub_ds in windows.take(5):
    print(sub_ds)

flat_windows = windows.flat_map(lambda x: x)
</code></pre>
<p>The problem is that <code>flat_windows.cardinality().numpy()</code> returns cardinality to be -2 which is creating problem for me during training. I tried looking for ways to set_cardinality of a dataset but couldn't find anything. I also tried other ways of flattening a dataset of datasets, but again no success.</p>
<p><strong>Edit-1:</strong> The problem with the training is that the shape is unknown (at Linear and Dense layers) when I am training a subclass model (given below). The model trains well when I train the model eagerly (through <code>tf.config.run_functions_eagerly(True)</code>) but that is slow. Therefore I want the input data to be known for the model training.</p>
<h1>Neural Network</h1>
<pre><code>class NeuralNetworkModel(tf.keras.Model): 
    def __init__(self):
        super(NeuralNetworkModel, self).__init__()
        self.encoder = Encoder()        
    
    def train_step(self, inputs):       
        X        = inputs[0]
        Y        = inputs[1] 
        
        with tf.GradientTape() as tape:
            enc_X    = self.encoder(X)
            enc_Y    = self.encoder(Y)    

            # loss:        
            loss   = tf.norm(enc_Y - enc_X, axis = [0, 1], ord = 'fro')
                
        # Compute gradients
        trainable_vars = self.encoder.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)

        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))

        # Compute our own metrics
        loss_tracker.update_state(loss)
        
        # Return a dict mapping metric names to current value.
        # Note that it will include the loss (tracked in self.metrics).
        return {&quot;loss&quot;: loss_tracker.result()}
        
    @property
    def metrics(self):
        # We list our `Metric` objects here so that `reset_states()` can be
        # called automatically at the start of each epoch
        # or at the start of `evaluate()`.
        # If you don't implement this property, you have to call
        # `reset_states()` yourself at the time of your choosing.
        return [loss_tracker]
    
    def test_step(self, inputs):       
        X = inputs[0]
        Y = inputs[1] 

        Psi_X    = self.encoder(X)
        Psi_Y    = self.encoder(Y)    

        # loss:        
        loss   = tf.norm(Psi_Y - Psi_X, axis = [0, 1], ord = 'fro')

        # Compute our own metrics
        loss_tracker.update_state(loss)
        
        # Return a dict mapping metric names to current value.
        # Note that it will include the loss (tracked in self.metrics).
        return {&quot;loss&quot;: loss_tracker.result()}
        
class Encoder(tf.keras.Model):
    def __init__(self):
        super(Encoder, self).__init__(dtype = 'float64', name = 'Encoder')
        self.input_layer   = DenseLayer(128)
        self.hidden_layer1 = DenseLayer(128)
        self.hidden_layer2 = DenseLayer(64)        
        self.hidden_layer3 = DenseLayer(64)
        self.output_layer  = LinearLayer(64)
        
    def call(self, input_data, training):
        fx = self.input_layer(input_data)        
        fx = self.hidden_layer1(fx)
        fx = self.hidden_layer2(fx)
        fx = self.hidden_layer3(fx)
        return self.output_layer(fx)    

class LinearLayer(tf.keras.layers.Layer):
    def __init__(self, units):
        super(LinearLayer, self).__init__(dtype = 'float64')
        self.units = units

    def build(self, input_shape):
        input_dim = input_shape[-1]
        self.w = self.add_weight(shape = (input_dim, self.units), 
                             initializer = &quot;random_normal&quot;, 
                             trainable = True)
        self.b = self.add_weight(shape = (self.units,),    
                             initializer = tf.zeros_initializer(),
                             trainable = True)

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b

class DenseLayer(tf.keras.layers.Layer):
    def __init__(self, units):
        super(DenseLayer, self).__init__(dtype = 'float64')
        self.units = units
    
    def build(self, input_shape):
        input_dim = input_shape[-1]
        self.w = self.add_weight(shape = (input_dim, self.units), 
                             initializer = &quot;random_normal&quot;, 
                             trainable = True)
        self.b = self.add_weight(shape = (self.units,),    
                             initializer = tf.zeros_initializer(),
                             trainable = True)

    def call(self, inputs):
        x = tf.matmul(inputs, self.w) + self.b
        return tf.nn.elu(x)
</code></pre>
",0
66290910,issue with tf.data.experimental.rejection_resample?,"<p>I'm working on image classification(multi_labels) model, and dataset is imbalanced, I'm trying to balance the data by using the &quot;tf.data.experimental.rejection_resample&quot; method, but i keep getting the below error:</p>
<p><em><strong>&quot;ValueError:Shape must be rank 3 but is rank 2 for '{{node Tile}} = Tile[T=DT_FLOAT, Tmultiples=DT_INT32](ExpandDims, Tile/multiples)' with input shapes: [1,9,9], [2].&quot;</strong></em></p>
<p>please find the blocks of code:</p>
<p>1)Building function to get the label from dataset:</p>
<pre><code>def class_func(features, label):
return label
</code></pre>
<p>2)Create the re-sampler (target dist argument is set according to the number of (labels=9):</p>
<pre><code>resampler = tf.data.experimental.rejection_resample(
class_func, target_dist=[0.115, 0.109,0.115, 0.109,0.115, 0.109,0.115, 0.109,0.104])
</code></pre>
<p>3)Run the resampler by using .apply function (dataset has been unpatched as per the tensorflow recomendaition(see the reference <a href=""https://www.tensorflow.org/guide/data#rejection_resampling"" rel=""nofollow noreferrer"">link</a>)</p>
<pre><code>resample_train_dataset = train_dataset.unbatch().apply(resampler).batch(10)
</code></pre>
<p><em>Note: I tried to change the dims for tensor by using tf.expand_dims but it didn't work, Any idea how to resolve this issue?</em></p>
",0
66298147,How to use keras image_dataset_from_directory with custom structures?,"<p>Keras <code>image_dataset_from_directory</code> inside the <code>preprocessing</code> module takes a path as an argument and automatically infers the classes when those images are stored in separate subfolders. In my case, however, I have a single folder and image classes are then specified in a DataFrame.</p>
<pre><code>.
├── datasets
│   ├── sample_submit.csv
│   ├── test_images
│   │   ├── test_0000.jpg
│   │   ├── test_0001.jpg
│   │   ├── test_0002.jpg
│   │   └── ...
│   ├── test_images.csv
│   ├── train_images
│   │   ├── train_0000.jpg
│   │   ├── train_0001.jpg
│   │   ├── train_0002.jpg
│   │   └── ...
│   └── train_images.csv
└── model.py
</code></pre>
<p>Tensorflow's <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory"" rel=""nofollow noreferrer"">documentation</a> specifies that when you are not inferring the labels, a list or tuple must be specified, which I get from the DataFrame <code>df</code>. However, when I specify the image folder, TensorFlow returns a <code>ValueError</code> because it has found no images:</p>
<pre><code>In [1]: df = pd.read_csv('datasets/train_images.csv')
   ...: tds = keras.preprocessing\
   ...:    .image_dataset_from_directory('datasets/train_images', list(df['class']),
   ...:                                  validation_split=0.2, subset='training',
   ...:                                  seed=123, image_size(180, 180))

ValueError: Expected the lengths of `labels` to match the number of files in the target directory. len(labels) is 1102 while we found 0 files in datasets/train_images.
</code></pre>
<p>Why does keras not recognise the images within the folder? I have tried setting the &quot;full&quot; relative path with <code>./datasets/train_images</code>, adding a slash with <code>datasets/train_images/</code> and also the absolute path, to no avail. What is missing here? Alternatively, is there a more efficient approach in this case where I can still get the train/test split?</p>
<hr />
<p><strong>EDIT:</strong> It seems there is a limitation with keras and this <a href=""https://stackoverflow.com/q/63906723/9148880"">question</a> originally laid it out, but remained too vague to get to the heart of the matter.</p>
<p>Plain and clear: <strong>keras seems to always scrape the subfolders of the <code>directory</code> argument for images and build the dataset.</strong> The workaround to enable the loading of images is to wrap an additional folder (<code>outer_train</code>) and pass it to <code>directory</code>.</p>
<p>However, I <em>still</em> have problems with this approach, because now keras seems unable to take the custom classes passed as a list and outputs <code>Found 1102 files belonging to 1 classes.</code> (in this case, the name of the <em>now</em> subfolder <code>train_images</code>), so any help is still appreciated.</p>
",1
66321763,TF 2.4.1 Hanging while training using the pretrained object detection model,"<p>I am fairly new to TF and ML in general, so I have relied heavily on the documentation and tutorials provided by TF. I have been following along with the <a href=""https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/index.html"" rel=""nofollow noreferrer"">Tensorflow 2.0 Objection Detection API</a> tutorial to the letter and have encountered an issue while training: everytime I run the training script <code>model_main_tf2.py</code>, it always hangs after the output:<br>
<code>I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)</code> after a number of depreciation warnings. I have tried many different ways of fixing this, including modifying the train script and pipeline.config files. My dataset isn't very large, less than 100 images with a max of 15 labels per image.</p>
<p>useful info:<br>
Python 3.8.0 <br>
Tensorflow 2.4.4 (Non GPU) <br>
Windows 10 Pro <br></p>
<p>Any and all help is appreciated!</p>
",0
66395670,"tf.keras(tf2) problem: can't run custom gradient layers, OperatorNotAllowedInGraphError. May caused by tf.Keras API","<p>I've arranged my code on <a href=""https://github.com/xiqxin1/dann-tf.keras-test"" rel=""nofollow noreferrer"">github</a>(with avaiable dataset).</p>
<p>The problem is that I want to implement an unsupervised domain adversarial training network (DANN) <a href=""https://arxiv.org/abs/1505.07818"" rel=""nofollow noreferrer"">(see paper)</a> using tf2.keras code, while most of the answers are tf1 or pure <a href=""https://github.com/michetonu/DA-RNN_manoeuver_anticipation/blob/93706abc3a45d99e027bc8277b7c20a61bdf11c8/da_rnn/DA_RNN_anticipation.py#L314"" rel=""nofollow noreferrer"">keras</a> versions, they didn't directly consider the change between tf1 and tf2, and simply disabled eager execution.</p>
<p>When I try to add a custom gradient_reversal layer at the beginning of my domain_classifier like this:</p>
<pre><code>@tf.custom_gradient
def reverse_gradient(X, hp_lambda):
    &quot;&quot;&quot;Flips the sign of the incoming gradient during training.&quot;&quot;&quot;
    try:
        reverse_gradient.num_calls += 1
    except AttributeError:
        reverse_gradient.num_calls = 1

    grad_name = &quot;GradientReversal%d&quot; % reverse_gradient.num_calls

    @ops.RegisterGradient(grad_name)
    def _flip_gradients(grad):
        return [tf.negative(grad) * hp_lambda]

    # g = K.get_session().graph
    with tf.Graph().as_default() as g:
        with g.gradient_override_map({'Identity': grad_name}):
            y = tf.identity(X)
    return y

from tensorflow.python.keras.engine.base_layer import Layer # I use base_layer, and most errors are coming from here.
class GradientReversal(Layer):
    &quot;&quot;&quot;Layer that flips the sign of gradient during training.&quot;&quot;&quot;

    def __init__(self, hp_lambda, **kwargs):
        super(GradientReversal, self).__init__(**kwargs)
        self.supports_masking = True
        self.hp_lambda = hp_lambda

    # @staticmethod
    def get_output_shape_for(input_shape):
        return input_shape

    def build(self, input_shape):
        # self.trainable_weights = []
        return

    def call(self, x, mask=None):
        return reverse_gradient(x, self.hp_lambda)

    def compute_output_shape(self, input_shape):
        return input_shape

    def get_config(self):
        config = {}
        base_config = super(GradientReversal, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))
</code></pre>
<p>There are a lot of errors ocurred:</p>
<pre><code>Traceback (most recent call last):
File &quot;D:\Users\xiqxi\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\keras\engine\base_layer.py&quot;, line 1117, in _functional_construction_call
outputs = call_fn(cast_inputs, *args, **kwargs)
File &quot;D:\Users\xiqxi\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\autograph\impl\api.py&quot;, line 258, in wrapper
raise e.ag_error_metadata.to_exception(e)
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: in user code:

D:/Skill-worker-research/Python code and example data/SupplementarySoftware_DeepHL_python/DeepHL_python/danntest/main.py:117 call  *
    return reverse_gradient(x, self.hp_lambda)
D:\Users\xiqxi\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\ops\custom_gradient.py:264 __call__  **
    return self._d(self._f, a, k)
D:\Users\xiqxi\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\ops\custom_gradient.py:220 decorated
    return _graph_mode_decorator(wrapped, args, kwargs)
D:\Users\xiqxi\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\ops\custom_gradient.py:325 _graph_mode_decorator
    result, grad_fn = f(*args)
D:\Users\xiqxi\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\framework\ops.py:503 __iter__
    self._disallow_iteration()
D:\Users\xiqxi\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\framework\ops.py:499 _disallow_iteration
    self._disallow_in_graph_mode(&quot;iterating over `tf.Tensor`&quot;)
D:\Users\xiqxi\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\framework\ops.py:479 _disallow_in_graph_mode
    &quot; this function with @tf.function.&quot;.format(task))

OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File &quot;D:\Users\xiqxi\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\keras\engine\base_layer.py&quot;, line 1124, in _functional_construction_call
'\n&quot;&quot;&quot;')
TypeError: You are attempting to use Python control flow in a layer that was not declared to be dynamic. Pass `dynamic=True` to the class constructor.
Encountered error:
&quot;&quot;&quot;
in user code:

D:/Skill-worker-research/Python code and example data/SupplementarySoftware_DeepHL_python/DeepHL_python/danntest/main.py:117 call  *
    return reverse_gradient(x, self.hp_lambda)
D:\Users\xiqxi\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\ops\custom_gradient.py:264 __call__  **
    return self._d(self._f, a, k)
D:\Users\xiqxi\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\ops\custom_gradient.py:220 decorated
    return _graph_mode_decorator(wrapped, args, kwargs)
D:\Users\xiqxi\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\ops\custom_gradient.py:325 _graph_mode_decorator
    result, grad_fn = f(*args)
D:\Users\xiqxi\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\framework\ops.py:503 __iter__
    self._disallow_iteration()
D:\Users\xiqxi\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\framework\ops.py:499 _disallow_iteration
    self._disallow_in_graph_mode(&quot;iterating over `tf.Tensor`&quot;)
D:\Users\xiqxi\Anaconda3\envs\tf2\lib\site-packages\tensorflow\python\framework\ops.py:479 _disallow_in_graph_mode
    &quot; this function with @tf.function.&quot;.format(task))

OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed in Graph execution. Use 
Eager execution or decorate this function with @tf.function.

&quot;&quot;&quot;

Process finished with exit code -1
</code></pre>
<p>I noticed that @tf.custom_gradient is tf2 code, but tf2 no longer uses tf.Graph() to generate static graph when building network structure, so I try this code anyway:</p>
<pre><code>@tf.custom_gradient
def GradientReversalOperator(x):
    def grad(dy):
        return -1 * dy
    return x, grad
</code></pre>
<p>But another bug raised. I guess this is because class GradientReversal inherites tf.keras.base_layer which contains tf1's gramma. I cannot fix such problem even I tried so many ways.</p>
<p>I uploaded my code and put forward this question. Hopefully, there are someone who can help me fix the problem and tell me how the tf.keras.base_layer works as well as why it doesn't available on my code.</p>
<p>I'll be very appretiate if you can give me any advices.</p>
<p>Thanks again for your help!</p>
",0
66420109,Keras training and trainable attribute in BN Layer,"<p>As stated <a href=""https://keras.io/api/layers/normalization_layers/batch_normalization/"" rel=""nofollow noreferrer"">here</a></p>
<blockquote>
<p>However, in the case of the BatchNormalization layer, setting trainable = False on the layer means that the layer will be subsequently run in inference mode (meaning that it will use the moving mean and the moving variance to normalize the current batch, rather than using the mean and variance of the current batch).</p>
</blockquote>
<p>If I set <code>trainable = False</code> initially then it will make the model run in inference mode i.e. <code>training = False</code> as stated in the official docs above. But, what will happen if I reset <code>trainable = True</code> after the above step? Will the <code>training</code> attribute will be reset to <code>True</code>? or it will continue to run in inference mode?</p>
",1
66437022,TF.keras.metrics.AUC does not work on multiclass,"<p>I have a multiclass problems.
Three classes with labels 0 1 and 2.</p>
<p>If I create a model netowrk and I run this at the end</p>
<pre><code>model.add(Activation('softmax')) 
model.compile(optimizer = 'adam',loss='sparse_categorical_crossentropy',
                  metrics=['sparse_categorical_accuracy',tf.keras.metrics.AUC(name = 'auc')])
</code></pre>
<p>I always receive immediately the error</p>
<pre><code>ValueError: Shapes (None, 3) and (None, 1) are incompatible
</code></pre>
<p>If I remove</p>
<pre><code>tf.keras.metrics.AUC(name = 'auc')
</code></pre>
<p>from the metrics everything works.</p>
<p>I read the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC"" rel=""nofollow noreferrer"">documentation</a> about <code>tf.keras.metrics.AUC</code> and it should work in the case of multi-class, but to me it does not.</p>
",1
66461478,TensorFlow 2: detecting to which graph a certain node belong,"<p>I just started (self) learning TensorFlow and I decided to follow the book <a href=""https://www.oreilly.com/library/view/learning-tensorflow/9781491978504/"" rel=""nofollow noreferrer"">&quot;Learning TensorFlow&quot;</a> which I found in my local library.
Unfortunately in the book they are using TensorFlow 1.x, while I want to use the 2.4 version.</p>
<p>I have some troubles in replicating an example from Chapter 3. The point of the code is to create a new empty computing graph, create a node (i.e. in this case a constant) and then figure out whether the node belongs to the default graph or to the newly created one.
Here is the code from the book, which should work fine with TensorFlow1:</p>
<pre><code>import tensorflow as tf
print(tf.get_default_graph())

g = tf.Graph()      # This creates a new empty graph
a = tf.constant(5)  # This creates a node

print(a.graph is g)
print(a.graph is tf.get_default_graph())
</code></pre>
<p>I did realize that the attribute <strong>get_default_graph()</strong> is no longer available in TensorFlow 2 and I substituted with <strong>tf.compat.v1.get_default_graph()</strong> but I still get the following error:</p>
<blockquote>
<p>AttributeError: Tensor.graph is meaningless when eager execution is enabled.</p>
</blockquote>
<p>Any help will be very much appreciated! Thanks in advance!</p>
",0
66498898,tf.gradients() returns a list of [None],"<p>Sorry if this sounds like a repeat. I have been through all of the related questions and found no suitable solutions to my problem's context.</p>
<p>I am trying to build a generative model that outputs probabilities for each tracked day of COVID to input into an SEIR-based epidemiology model.</p>
<p>The generation works. However, I cannot figure out how to train the model. I have to write a custom loss function that runs the day-by-day parameters through a step function for the epidemiology model with will populate a dataset of &quot;confirmed&quot; and &quot;removed&quot; for each day. I then compare that data to the recorded &quot;confirmed&quot; and &quot;removed&quot; from <a href=""https://github.com/datasets/covid-19/blob/main/data/worldwide-aggregate.csv"" rel=""nofollow noreferrer"">John Hopkin's COVID dataset</a> on GitHub.</p>
<p>I use Mean Absolute Error to calculate a loss between the &quot;confirmed&quot; and &quot;removed&quot; based on the generated probabilities and the actual values from the JHU dataset. The issue I am running into is when I call <code>the tf.gradient()</code> function it returns a list of <code>None</code>s. I am stuck here and any assistance would be greatly appreciated.</p>
<p>Here is the code I am using:</p>
<p><strong>Training Step</strong></p>
<pre class=""lang-py prettyprint-override""><code># Define function to train the model based on one input
loss_fn = MeanAbsoluteError()
optimizer = Adam(learning_rate=0.005)

@tf.function
def train_step(x, y):

  y_pred = np.zeros((3, latent_dim))

  N = tf.constant(int(7_000_000_000), dtype=tf.float64)
  E0 = tf.Variable(int(1000), trainable=False, dtype=tf.float64)
  I0 = tf.Variable(covid_df.iloc[0][&quot;Confirmed&quot;], trainable=False, dtype=tf.float64)
  R0 = tf.Variable(covid_df.iloc[0][&quot;Removed&quot;], trainable=False, dtype=tf.float64)
  S0 = tf.Variable(N - E0 - I0 - R0, trainable=False, dtype=tf.float64)
  u0 = tf.Variable(0, trainable=False, dtype=tf.float64)

  SuEIRs = tf.stack([S0,u0,E0,I0,R0])

  with tf.GradientTape() as tape:
    logits = generator(tf.reshape(x, (batch_size, 4, latent_dim)), training=True)

    betas = logits[0][0]
    sigmas = logits[0][1]
    mus = logits[0][2]
    gammas = logits[0][3]

    for t in range(latent_dim):
      SuEIR_diffs = SuEIR_step(SuEIRs, t, N, betas, sigmas, mus, gammas)

      SuEIRs = SuEIRs + SuEIR_diffs

      confirmed = SuEIRs[3]
      removed = SuEIRs[4]

      # update y_pred
      y_pred[0,t] = float(t+1)
      y_pred[1,t] = confirmed.numpy()
      y_pred[2,t] = removed.numpy()

    # Convert predictions
    y_pred = tf.convert_to_tensor(y_pred)

    # Calculate loss
    loss_value = loss_fn(y[1], y_pred[1]) + loss_fn(y[2], y_pred[2])

  # Calculate the gradient
  grads = tape.gradient(loss_value, generator.trainable_weights)

  print(grads) ##==&gt;&gt; outputs [None, None, None, None]

  # Apply gradients to model
  optimizer.apply_gradients(zip(grads, generator.trainable_weights))
  return loss_value
</code></pre>
<p><strong>Training Loop</strong></p>
<pre class=""lang-py prettyprint-override""><code>import time

epochs = 2
for epoch in range(epochs):
  print(&quot;\nStart of epoch %d&quot; % (epoch,))
  start_time = time.time()

  # Iterate over the batches of the dataset.
  for step in range(sample_size):
    loss_value = train_step(x_input[step], y_true)

    # Log every 5 batches.
    if step % 5 == 0:
      print(
        &quot;Training loss (for one batch) at step %d: %.4f&quot;
        % (step, float(loss_value))
      )
    print(&quot;Time taken: %.2fs&quot; % (time.time() - start_time))
</code></pre>
<p><strong>Error output</strong></p>
<pre><code>ValueError: No gradients provided for any variable: ['dense/kernel:0', 'dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0'].
</code></pre>
<p><code>loss_value</code> and <code>generator.trainable_weights</code> are populated as expected.</p>
<p><strong>EDIT:</strong> Updated code to reflect the suggestions of <a href=""https://stackoverflow.com/a/66498964/12127309"">Myrl Marmarelis</a> and the architecture of <a href=""https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch#speeding-up_your_training_step_with_tffunction"" rel=""nofollow noreferrer"">TensorFlow's custom training loop guide</a>. Still having the same issue of gradients being a list of <code>None</code>'s.</p>
",0
66532339,What is the purpose and constraints of tf.selectV2,"<p>TF documentation simply gives the API without any description beyond, unlike TF.Select which is more descriptive</p>
<p>See <a href=""https://www.tensorflow.org/api_docs/python/tf/raw_ops/SelectV2"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/raw_ops/SelectV2</a> vs. <a href=""https://www.tensorflow.org/api_docs/python/tf/raw_ops/Select"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/raw_ops/Select</a></p>
",1
66546321,Proper way to input a scalar into a Tensorflow 2 model,"<p>In my Tensorflow 2 model, I want my batch size to be parametric, such that I can build tensors which have appropriate batch size dynamically. I have the following code:</p>
<pre><code>batch_size_param = 128

tf_batch_size = tf.keras.Input(shape=(), name=&quot;tf_batch_size&quot;, dtype=tf.int32)
batch_indices = tf.range(0, tf_batch_size, 1)

md = tf.keras.Model(inputs={&quot;tf_batch_size&quot;: tf_batch_size}, outputs=[batch_indices])
res = md(inputs={&quot;tf_batch_size&quot;: batch_size_param})
</code></pre>
<p>The code throws an error in <code>tf.range</code>:</p>
<pre><code>ValueError: Shape must be rank 0 but is rank 1
 for 'limit' for '{{node Range}} = Range[Tidx=DT_INT32](Range/start, tf_batch_size, Range/delta)' with input shapes: [], [?], []
</code></pre>
<p>I think the problem is with the fact that <code>tf.keras.Input</code> automatically tries to expand the input array at the first dimension, since it expects the partial shape of the input without the batch size and will attach the batch size according to the shape of the input array, which in my case a scalar. I can just feed the scalar value as a constant integer into <code>tf.range</code> but this time, I won't be able to change it after the model graph has been compiled.</p>
<p>Interestingly, I failed to find a proper way to input only a scalar into a TF-2 model even though I checked the documentation, too. So, what would be the best way to handle such a case?</p>
",1
66630606,"tensorflow-addons.image error, wrong input?","<p>How do I use <code>translate</code> in <code>tensorflow_addons.image</code>?
I tried the following with no success</p>
<p>(Versions: <code>tensorflow: 2.4.1</code>, <code>tensorflow-addons: 0.12.1</code>, <code>python: 3.8.8</code>)</p>
<pre><code>import tensorflow as tf
import numpy as np
from tensorflow_addons.image import translate as tfa_translate

# 5 images 10x10, BW:
imgs = np.random.rand(5, 10, 10, 1).astype(np.float32)

# vector for x-y translations:
vec = np.random.randn(5, 2).astype(np.float32)

# both of these give me the same error below:
tfa_translate(imgs, vec)
tfa_translate(tf.convert_to_tensor(imgs), tf.convert_to_tensor(vec))
</code></pre>
<p>The error I get is about using tf or np tensors. I tried with <code>tf.convert_to_tensor(imgs)</code> but no success:</p>
<pre><code>NotImplementedError: Cannot convert a symbolic Tensor 
(translate/translations_to_projective_transforms/strided_slice:0) to a numpy array. 
This error` may indicate that you're trying to pass a Tensor to a NumPy call, 
which is not supported
</code></pre>
<p>Edit: All the ops on images in these <a href=""https://github.com/tensorflow/addons/blob/master/docs/tutorials/image_ops.ipynb"" rel=""nofollow noreferrer"">examples</a> work in my systems. But translate does not.</p>
",0
66635350,How does tensorflow determine which LSTM units will be selected as outputs?,"<p>I created a LSTM model using the below code:</p>
<p><code>model = tensorflow1.keras.Sequential()</code></p>
<p><code>model.add(tensorflow1.keras.layers.LSTM(128, input_shape=(720, 4), return_sequences=True))</code>
<code>model.add(tensorflow1.keras.layers.LeakyReLU(alpha=0.5)) </code>
<code>model.add(tensorflow1.keras.layers.LSTM(128, return_sequences=True))</code>
<code>model.add(tensorflow1.keras.layers.LeakyReLU(alpha=0.5)) </code>
<code>model.add(tensorflow1.keras.layers.Dropout(0.3)) </code>
<code>model.add(tensorflow1.keras.layers.LSTM(64, return_sequences=False))</code>
<code>model.add(tensorflow1.keras.layers.Dropout(0.3)) </code>
<code>model.add(tensorflow1.keras.layers.Dense(1))</code></p>
<p>For the code model.add(tf.keras.layers.LSTM(128, input_shape=(720,4), return_sequences=True)), in my understanding, the code will have 128 LSTM output units, with an input shape of 720 time steps and 4 features.
According to tensorflow keras documentation, units refers to &quot;dimensionality of the output space&quot;. (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM</a>)</p>
<p>My question is: Which of the 128 cells as output units out of the 720 cells will be selected as output cells? How does tensorflow choose which cells to select as an output?</p>
",1
66637269,Invalid argument: Dimension -972891 must be >= 0,"<p>I have created a data pipeline using tf.data for speech recognition using the following code snippets:</p>
<pre><code>def get_waveform_and_label(file_path):
    label = tf.strings.split(file_path, os.path.sep)[-2]

    audio_binary = tf.io.read_file(file_path)
    audio, _ = tf.audio.decode_wav(audio_binary)
    waveform = tf.squeeze(audio, axis=-1)
    
    return waveform, label

def get_spectrogram(waveform):
    # Padding for files with less than 16000 samples
    # Generate zeros w.r.t how many the waveform lacks
    zero_padding = tf.zeros([16000] - tf.shape(waveform), dtype=tf.float32)

    # Concatenate audio with padding so that all audio clips will be of the same length
    waveform = tf.cast(waveform, tf.float32)
    waveform = tf.concat([waveform, zero_padding], 0)

    spectrogram = tf.signal.stft(waveform, frame_length=255, frame_step=128)
    spectrogram = tf.abs(spectrogram)

    return spectrogram

def get_spectrogram_and_label_id(audio, label):
    spectrogram = get_spectrogram(audio)
    spectrogram = tf.expand_dims(spectrogram, -1)
    
    label_id = tf.argmax(label == np.array(labels))
    label_onehot = tf.one_hot(label_id, len(labels))
    
    return spectrogram, label_onehot

files_ds = tf.data.Dataset.from_tensor_slices(files)
waveform_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=tf.data.AUTOTUNE)
spectrogram_ds = waveform_ds.map(get_spectrogram_and_label_id, num_parallel_calls=tf.data.AUTOTUNE)

</code></pre>
<p>These snippets are borrowed from <a href=""https://www.tensorflow.org/tutorials/audio/simple_audio#build_and_train_the_model"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/audio/simple_audio#build_and_train_the_model</a>.</p>
<p>And my model is defined as below:</p>
<pre><code>import tensorflow as tf

inputs = tf.keras.layers.Input(shape=(input_shape))
x = tf.keras.layers.BatchNormalization()(inputs)

x = tf.keras.layers.Conv2D(8,13, padding='same', activation='relu', strides=1)(x)
x = tf.keras.layers.MaxPooling2D(3)(x)
x = tf.keras.layers.Dropout(0.4)(x)
x = tf.keras.layers.BatchNormalization()(x)

x = tf.keras.layers.Conv2D(32, 11, padding='same', activation='relu', strides=1)(x)
x = tf.keras.layers.MaxPooling2D(3)(x)
x = tf.keras.layers.Dropout(0.4)(x)
x = tf.keras.layers.BatchNormalization()(x)

x = tf.keras.layers.Conv2D(256, 9, padding='same', activation='relu', strides=1)(x)
x = tf.keras.layers.MaxPooling2D(3)(x)
x = tf.keras.layers.Dropout(0.4)(x)
x = tf.keras.layers.BatchNormalization()(x)

x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(512, activation='relu')(x)
outputs = tf.keras.layers.Dense(len(labels), activation=&quot;softmax&quot;)(x)

model = tf.keras.models.Model(inputs, outputs)

model.compile(loss=&quot;categorical_crossentropy&quot;,
              optimizer=tf.keras.optimizers.Adam(), 
              metrics=['accuracy'])
model.summary()
</code></pre>
<p>When I start training process this error appears after a few iterations:</p>
<pre><code>&gt; InvalidArgumentError: 2 root error(s) found.   

&gt; (0) Invalid argument: 
&gt; Dimension -972891 must be &gt;= 0     [[{{node zeros}}]]     
&gt; [[IteratorGetNext]]   
&gt; [[categorical_crossentropy/softmax_cross_entropy_with_logits/Shape_2/_6]]

&gt; (1) Invalid argument:  Dimension -972891 must be &gt;= 0      [[{{node
&gt; zeros}}]]      [[IteratorGetNext]] 0 successful operations. 0 derived
&gt; errors ignored. [Op:__inference_train_function_6412]
&gt; 
&gt; Function call stack: train_function -&gt; train_function
</code></pre>
",0
66659610,How does tf.keras.metrics.TopKCategoricalAccuracy differ from Precision@k?,"<p>Coming from recommender systems, precision@k is a popular metric.</p>
<blockquote>
<p>precision@k = number of relevant predictions in top k / k</p>
</blockquote>
<p>On the tensorflow docs for tf.keras.metrics.TopKCategoricalAccuracy it states</p>
<blockquote>
<p>Computes how often targets are in the top K predictions.</p>
</blockquote>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TopKCategoricalAccuracy"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TopKCategoricalAccuracy</a></p>
<p>Which seems to be exactly the same as precision@k. Am I missing something or are they equivalent and it just comes down to TF/recommender terminology?</p>
",0
66850636,"Is it the desired way to periodically saving checkpoints with Keras model and ""SavedModel"" format in Tensorflow 2","<p>The Tensorflow 2 documentation states that users could save a Tensorflow Keras Model by calling the API <code>model.save()</code> with either &quot;SavedModel&quot; or &quot;h5&quot; format (latest version 2.4.1: <a href=""https://www.tensorflow.org/guide/keras/save_and_serialize#whole-model_saving_loading"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/keras/save_and_serialize#whole-model_saving_loading</a>). Now assuming to use the &quot;SavedModel&quot; format, I am wondering if it is by design to periodically save checkpoints with the &quot;SavedModel&quot; format. For example,</p>
<pre><code>import numpy as np
import tensorflow as tf
from tensorflow.keras import Input, Model
from tensorflow.keras.layers import Dense


def get_model() -&gt; Model:
    &quot;&quot;&quot;
    Define a TF Keras Model with layers having loss associated.
    &quot;&quot;&quot;
    x_in = Input(shape=(4,), name=&quot;input&quot;)
    layer1 = Dense(64, name=&quot;l1&quot;)(x_in)
    layer2 = Dense(64, name=&quot;l2&quot;)(layer1)
    x_out = Dense(2, name=&quot;output&quot;)(layer2)
    model = Model(inputs=x_in, outputs=x_out, name=&quot;m&quot;)
    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  optimizer=tf.keras.optimizers.Adam(),
                  metrics=[])
    return model


def train_step(input_data: np.ndarray, label_data: np.ndarray, model: Model) -&gt; None:
    &quot;&quot;&quot;
    Perform the training steps for the built Keras model, and periodically save
    a &quot;SavedModel&quot; checkpoint -- Is it desired?
    &quot;&quot;&quot;
    # Perform a training step with input and label data.
    with tf.GradientTape() as tape:
        # A simple pass to mock the training step.
        pass
    # At the end of each training step, we save the updated model.
    # But... is &quot;SavedModel&quot; the desired format for periodic saving checkpoints?
    model.save(&quot;/tmp/saved_model/&quot;, save_format=&quot;tf&quot;)


def main() -&gt; None:
    model = get_model()
    # Train 100 epoch
    for epoch in range(100):
        # each for inputs, labels in train_data:
        train_step(input_data=np.array([1.0]), label_data=np.array([1.0]), model=model)
        print(f&quot;Epoch {epoch}, the training metric is...&quot;)


main()

</code></pre>
<p>I'm asking because my understanding is &quot;SavedModel&quot; is designed for saving a model only when the model is &quot;ready&quot; for deployment for inference (i.e. the model is trained well), and users save the &quot;SavedModel&quot; model only once (or <code>O(1)</code> times) which usually happens in the end. One evidence on this is in Tensorflow 1 directly using <code>tf.Session</code> with Tensorflow graph, if you periodically save the &quot;SavedModel&quot; model like the code below, then <code>SavedModelBuilder</code> leaks one &quot;Saver&quot; node in the Tensorflow graph every time creating the builder:</p>
<pre><code>import tensorflow as tf
from tensorflow import saved_model


def _save_to_saved_model(input_tensor: tf.Tensor, output_tensor: tf.Tensor, tf_session: tf.Session, saved_model_path: str) -&gt; None:
    &quot;&quot;&quot;
    Save the Tensorflow graph to the Tensorflow saved model.
    &quot;&quot;&quot;
    # Create the saved model builder.
    builder = saved_model.builder.SavedModelBuilder(saved_model_path)

    # Build the tensor info proto using the tensors.
    tensor_info_obs = saved_model.utils.build_tensor_info(input_tensor)
    tensor_info_output = saved_model.utils.build_tensor_info(output_tensor)

    # Get the default method name.
    method_name = saved_model.signature_constants.PREDICT_METHOD_NAME
    policy_signature = (
        saved_model.signature_def_utils.build_signature_def(
            inputs={&quot;input&quot;: tensor_info_obs},
            outputs={&quot;output&quot;: tensor_info_output},
            method_name=method_name))

    # Get the signature def map key.
    serving_signature_key = (
        saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)
    builder.add_meta_graph_and_variables(
        tf_session, [saved_model.tag_constants.SERVING],
        signature_def_map={serving_signature_key: policy_signature})

    # Save the saved model.
    builder.save()


def main() -&gt; None:
    for _ in range(100):
        # Mock each train step by only saving the &quot;SavedModel&quot;.
        _save_to_saved_model(...)


main()
</code></pre>
<p>Another question posted a few years ago seems also mentioned the same point: <a href=""https://stackoverflow.com/questions/49287202/how-to-periodically-save-tensorflow-model-using-saved-model-api"">How to periodically save tensorflow model using saved_model API?</a>. However, Tensorflow Keras doesn't seem to have the same leaking issue, as looks <code>Model.save()</code> creates <code>TrackableSaver</code> rather than <code>Saver</code>, which doesn't leak the saver node in the Tensorflow graph, but I want to know using Tensorflow Keras Model if it is desired to periodically save checkpoints with &quot;SavedModel&quot; format.</p>
<p><strong>NOTE: &quot;SavedModel&quot; format is being considered because looks it is the only Keras model persistence format that is allowed to restore model without accessing the custom model code.</strong></p>
<p>Thanks!</p>
",0
66874943,Why iterations over the same tf.data.Dataset give different data each iteration?,"<p>I'm trying to understand how <strong>tf.data.Dataset</strong> works.</p>
<p>It says on the documentation that <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take"" rel=""nofollow noreferrer"">take</a> returns a dataset with a certain amount of elements from that dataset. You can then iterate over a single sample (in this case a batch):</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow.compat.v2 as tf
import tensorflow_datasets as tfds

# Construct a tf.data.Dataset
ds = tfds.load('mnist', split='train', shuffle_files=True)

# Build your input pipeline
ds = ds.shuffle(1024).batch(32).prefetch(tf.data.experimental.AUTOTUNE)

single_batch_dataset = ds.take(1)

for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)
# ...
</code></pre>
<p>Outputs:</p>
<pre><code>tf.Tensor([2 0 6 6 8 8 6 0 3 4 8 7 5 2 5 7 8 7 1 1 1 8 6 4 0 4 3 2 4 2 1 9], shape=(32,), dtype=int64)
</code></pre>
<p>However, iterating over it again, gives different labels: (continuation of last code)</p>
<pre class=""lang-py prettyprint-override""><code>for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)

for example in single_batch_dataset:
  image, label = example[&quot;image&quot;], example[&quot;label&quot;]
  print(label)

</code></pre>
<p>Outputs:</p>
<pre><code>tf.Tensor([7 3 5 6 3 1 7 9 6 1 9 3 9 8 6 7 7 1 9 7 5 2 0 7 8 1 7 8 7 0 5 0], shape=(32,), dtype=int64)
tf.Tensor([1 3 6 1 8 8 0 4 1 3 2 9 5 3 8 7 4 2 1 8 1 0 8 5 4 5 6 7 3 4 4 1], shape=(32,), dtype=int64)
</code></pre>
<p>Shouldn't the labels be the same, given that the dataset is the same?</p>
",1
66910103,What is a trackable object?,"<p>I'm browsing the documentation around <code>SavedModels</code>, and <code>tf.saved_model.save</code> (<a href=""https://www.tensorflow.org/api_docs/python/tf/saved_model/save"" rel=""noreferrer"">docs</a>) does:</p>
<blockquote>
<p>Exports the Trackable object obj to SavedModel format.</p>
</blockquote>
<p><strong>What is a trackable object</strong>, I have not been able to find anything regarding this. Except for my PyCharm debugger printing it.</p>
<hr />
<p>I am reading the documentation because I need to understand how SavedModels work. I've tried to load some SavedModels and subsequently convert these to TensorFlow lite/ coreML. Things are not going well in this respect, and I'm completely lost there so I won't bother mentioning it much detail in this question.</p>
",1
66942311,"In Tensorflow, what does tf.GradientTape.gradients do when its ""target"" attribute is a multi-dimensional tensor?","<p>In my model, I'm using tf.keras.losses.MSE to calculate the <strong>mean squared error</strong> of my <strong>BATCH_SIZE x 256 x 256 x 3</strong> output and my <strong>BATCH_SIZE x 256 x 256 x 3</strong> input.</p>
<p>The <strong>output</strong> of this function appears to be <strong>(None,256,256)</strong>.</p>
<p>I then use tf.GradientTape.gradients, with the MSE <strong>output</strong> as the &quot;<strong>target</strong>&quot; attribute. In the documentation, it says that this attribute can be a tensor.</p>
<p>My understanding is that <strong>loss is a scalar number</strong> which is differentiated against each of the weights during backpropagation.</p>
<p>Therefore, my question is: <strong>What happens when a multi-dimensional tensor is passed into the gradients function? Is the sum of all elements in the tensor simple calculated?</strong></p>
<p>I ask this because my model is not training at the moment, with loss reading at 1.0 at every epoch. My assumption is that I am not calculating the gradients correctly, as all my gradients are reading as 0.0 for each weight.</p>
",0
66946438,Tensorflow TypeError: `generator` must be callable in tf.data.Dataset.from_generator(gen),"<p>I am slightly losging my mind about a simple task. I want to implement a simple RandomForestClassifier on Images using the tf.estimator.BoostedTreesClassifier (Gradient Boosted Tree is good enough although the difference is clear to me). I'm following the <strong><a href=""https://www.tensorflow.org/tutorials/estimator/boosted_trees_model_understanding"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/estimator/boosted_trees_model_understanding</a></strong> guide. I swapped the</p>
<pre><code># Use entire batch since this is such a small dataset.
NUM_EXAMPLES = len(y_train)

def make_input_fn(X, y, n_epochs=None, shuffle=True):
  def input_fn():
    dataset = tf.data.Dataset.from_tensor_slices((X.to_dict(orient='list'), y))
    if shuffle:
      dataset = dataset.shuffle(NUM_EXAMPLES)
    # For training, cycle thru dataset as many times as need (n_epochs=None).
    dataset = (dataset
      .repeat(n_epochs)
      .batch(NUM_EXAMPLES))
    return dataset
  return input_fn
</code></pre>
<p>with my own function looking like this</p>
<pre><code># LOADING IMAGES USING TENSORFLOW
labels = ['some','fancy','labels']
batch_size = 32
datagen = ImageDataGenerator(
    rescale=1. / 255,
    data_format='channels_last',
    validation_split=0.1,
    dtype=tf.float32
)

train_generator = datagen.flow_from_directory(
    './images',
    classes=labels,
    target_size=(128, 128),
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=True,
    subset='training',
    seed=42
)

valid_generator = datagen.flow_from_directory(
    './images',
    classes=labels,
    target_size=(128, 128),
    batch_size=batch_size,
    class_mode='categorical',
    shuffle=False,
    subset='validation',
    seed=42
)

# THE SWAPPED FUNCTION:
NUM_FEATURES = 128 * 128
NUM_EXAMPLES = len(train_generator)

def make_input_fn(gen, n_epochs=None, shuffle=True):
    def input_fn():
        dataset = tf.data.Dataset.from_generator(gen, (tf.float32, tf.int32))
        if shuffle:
            dataset = dataset.shuffle(NUM_EXAMPLES)
        # For training, cycle thru dataset as many times as need (n_epochs=None).
        dataset = (dataset
                   .repeat(n_epochs)
                   .batch(NUM_EXAMPLES))
        return dataset
    return input_fn


def _generator_(tf_gen):
    print(len(tf_gen))
    def arg_free():
        for _ in range(len(tf_gen)):
            X, y = next(iter(tf_gen))
            X = X.reshape((len(X), -1))
            print(X.shape)
            yield X, y
    return arg_free()


_gen = _generator_(train_generator)
print(callable(g_gen)) # returns Fals WHY?!
</code></pre>
<p>I dont understand why this is not working and why on earth nobody ever thaught about making a simple enough tutorial (or why I am not able to find it :D). If you are asking yourself, why I want to use the RandomForest and not regular Deep Learning aproaches. The RF is set by the supervising Authorithy as well as it has to be TF (and not e.g. sklearn).</p>
<p>Anyway, any help will be appreciated.</p>
",0
66966492,Is there a way to use bert-large as a text classification tool without fine-tuning?,"<p>I'm currently have a task of converting a keras BERT-based model for any text classification problem to the .pb file. For this I already have a function, that takes in the keras model, but the point is that when I'm trying to download any pre-trained versions of BERT they always end up without any top layers for classification, hence I should manually add <code>tf.keras.layers.Input</code> layers before and any neural network architecture above the BERT (after [CLS]'s embedding). My goal is ultimately escape the need for fine-tuning and get some ready model, that has already been fine-tuned. I've found that transformer library might be useful for this, as they have some BERT-based models ready for some datasets. Anyway, using the following code from their documentation gives back the tensor of shape 1 by number of tokens by hidden dimensionality.</p>
<pre><code>from transformers import BertTokenizer, TFBertModel
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')
model = TFBertModel.from_pretrained(&quot;bert-large-uncased&quot;)
text = &quot;Replace me by any text you'd like.&quot;
encoded_input = tokenizer(text, return_tensors='tf')
output = model(encoded_input)
</code></pre>
<p>So, I eventually have to find some dataset and do fine-tuning. Even usage of models like distilbert-base-uncased-finetuned-sst-2-english still produce the embedding for each input token. Is there a way of getting ready model?</p>
",0
67066760,Configuring labels in TensorFlow BinaryCrossentropy loss function,"<p>I want to compute cross-entropy loss using <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy"" rel=""nofollow noreferrer"">tf.keras.losses.BinaryCrossentropy</a>. The documentation has the following example, and specifies that true labels and predicted labels should have the shape <code>[batch_size]</code>:</p>
<pre><code>y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]

bce = tf.keras.losses.BinaryCrossentropy()
bce(y_true, y_pred).numpy()
</code></pre>
<p>From the example, it is inferred that each sample's label should be formatted as [probability of belonging to Class 0, probability of belonging to Class 1]. Is it correct? If it is, why <code>y_true[1]</code> probabilities do not add up to 1?</p>
",1
67088200,Is it okay to use complex control flow in tf.function?,"<p>I have the following Python function and I want to wrap it into <code>@tf.function</code> (originally the input arguments are numpy arrays, but for the sake of executing on GPU it's not a problem to convert them to TF tensors).</p>
<pre><code>def reproject(before_frame, motion_vecs):
    reprojected_image = np.zeros((before_frame.shape[0], before_frame.shape[1], before_frame.shape[2]))
    for row_idx in range(before_frame.shape[0]):
        for col_idx in range(before_frame.shape[1]):
            for c_idx in range(before_frame.shape[2]):
                diff_u = int(round(
                         (before_frame.shape[1] * motion_vecs[row_idx][col_idx][0])
                     ))
                diff_v  = int(round(
                         (before_frame.shape[0] * motion_vecs[row_idx][col_idx][1])
                     ))
                before_pixel_position = (
                    row_idx + diff_v,
                    col_idx + diff_u
                )
                if before_pixel_position[0] &lt; before_frame.shape[0] and before_pixel_position[1] &lt; before_frame.shape[1] \
                    and before_pixel_position[0] &gt; 0 and before_pixel_position[1] &gt; 0:
                    reprojected_image[row_idx][col_idx][c_idx] = before_frame[
                        before_pixel_position[0]
                    ][
                        before_pixel_position[1]
                    ][c_idx]
    return reprojected_image
</code></pre>
<p>I can see that in Tensorflow tutorials people use <code>vectorized_map</code> or <code>map_fn</code> instead of loops, and <code>tf.cond</code> instead of the <code>if</code> operator. So is using these functions the only option for control flow, and if so, what are the reasons behind it?</p>
",0
67101417,Using saved models from TF hub to extract feature vectors,"<p>I've been playing with different models from TF hub to extract feture vectors:</p>
<pre><code>module = hub.load('https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4')
features = module(image)
</code></pre>
<p>What i don't quite understand is how input image should be preprocessed.
Every model from the hub has this generic instruction:</p>
<blockquote>
<p>The input image1 are expected to have color values in the range [0,1], following the common image input conventions. The expected size of the input images is height x width = 299 x 299 pixels by default, but other input sizes are possible (within limits).
where &quot;common image input&quot; is a link to a the following:
A signature that takes a batch of images as input accepts them as a dense 4-D tensor of dtype float32 and shape [batch_size, height, width, 3] whose elements are RGB color values of pixels normalized to the range [0, 1]. This is what you get from tf.image.decode_*() followed by tf.image.convert_image_dtype(..., tf.float32).</p>
</blockquote>
<p>and this is indeed what i see quite often online:</p>
<pre><code>image = tf.io.read_file(path)

# Decodes the image to W x H x 3 shape tensor with type of uint8
image = tf.io.decode_jpeg(image, channels=3)

# Resize the image to for model
image = tf.image.resize(image, [model_input_size, model_input_size])

# 1 x model_input_size x model_input_size x 3 tensor with the data type of float32
image = tf.image.convert_image_dtype(image, tf.float32)[tf.newaxis, ...]
</code></pre>
<p>BUT, color values are expected to be in range [0,1], in this case colors are in range [0,255] and should be scaled down:</p>
<pre><code>image = numpy.array(image) * (1. / 255)
</code></pre>
<p>Is it just a common mistake or is the TF documentation is not up to date?</p>
<p>I was playing with models from tf.keras.applications and reading source code in github. I noticed in some of the models (EfficientNet) first layer is:</p>
<pre><code>x = layers.Rescaling(1. / 255.)(x)
</code></pre>
<p>but in some models there is no such layer, instead and an utility function scales colors to [0,1] range, for example tf.keras.applications.mobilenet.preprocess_input.</p>
<p>So, how important for TF hub saved models image colors to be in [0,1] range?</p>
",0
67103766,bounding box format in tensorflow object detection api,"<p>thanks in advance.<br />
I try to use tensorflow object detection api with manual and web.<br />
But I confused about bounding box format in tensorflow object detection api.<br />
in tutorial, TODA(tensorflow object detection api) serve several pretrained model,
and its trained with coco dataset.</p>
<p>in coco dataset,<br />
bbox foramt is [xmin, ymin, width, height],<br />
there are many bbox format,
centerx, centery,  width, height, or xmin, ymin, xmax,ymax</p>
<p>which bbox format should I use for TODA??
(should I use coco format??)<br />
I cant find any info regarding this.</p>
<p>and x axis and y axis, this is also confused.
I understand X means width, Y means height.</p>
<p>bun TODA code,
I found this.<br />
def assert_or_prune_invalid_boxes(boxes):<br />
...<br />
ymin, xmin, ymax, xmax = tf.split(
boxes, num_or_size_splits=4, axis=1)</p>
<p>why x, y switching??<br />
TODA axis is different from others??</p>
<p>thanks.</p>
",0
67116476,Memory leak for custom tensorflow training using @tf.function,"<p>I am trying to write my own training loop for <code>TF2/Keras</code>, following the official Keras walkthrough. The vanilla version works like a charm, but when I try to add the <code>@tf.function</code> decorator to my training step, some memory leak grabs all my memory and I lose control of my machine, does anyone know what is going on?.</p>
<p>The important parts of the code look like this:</p>
<pre><code>@tf.function
def train_step(x, y):
    with tf.GradientTape() as tape:
        logits = siamese_network(x, training=True)
        loss_value = loss_fn(y, logits)
    grads = tape.gradient(loss_value, siamese_network.trainable_weights)
    optimizer.apply_gradients(zip(grads, siamese_network.trainable_weights))
    train_acc_metric.update_state(y, logits)
    return loss_value

@tf.function
def test_step(x, y):
    val_logits = siamese_network(x, training=False)
    val_acc_metric.update_state(y, val_logits)
    val_prec_metric.update_state(y_batch_val, val_logits)
    val_rec_metric.update_state(y_batch_val, val_logits)


for epoch in range(epochs):
        step_time = 0
        epoch_time = time.time()
        print(&quot;Start of {} epoch&quot;.format(epoch))
        for step, (x_batch_train, y_batch_train) in enumerate(train_ds):
            if step &gt; steps_epoch:
                break
           
            loss_value = train_step(x_batch_train, y_batch_train)
        train_acc = train_acc_metric.result()
        train_acc_metric.reset_states()
        
        for val_step,(x_batch_val, y_batch_val) in enumerate(test_ds):
            if val_step&gt;validation_steps:
                break
            test_step(x_batch_val, y_batch_val)
         
        val_acc = val_acc_metric.result()
        val_prec = val_prec_metric.result()
        val_rec = val_rec_metric.result()

        val_acc_metric.reset_states()
        val_prec_metric.reset_states()
        val_rec_metric.reset_states()
</code></pre>
<p>If I comment on the <code>@tf.function</code> lines, the memory leak doesn't occur, but the step time is 3 times slower. My guess is that somehow the graph is bean created again within each epoch or something like that, but I have no idea how to solve it.</p>
<p>This is the tutorial I am following: <a href=""https://keras.io/guides/writing_a_training_loop_from_scratch/"" rel=""noreferrer"">https://keras.io/guides/writing_a_training_loop_from_scratch/</a></p>
",0
67197448,How to extract multiple rows from tensor at the same time?,"<p>TL;DR:
TensorFlow tensor is of shape <code>(50, 50, 6)</code>, want these indices (:, :, (0, 2, 3)). How to extract them?</p>
<p>Here is an example array I am working with:</p>
<pre><code>import numpy as np

a = np.random.randint(0,10, (50, 50, 6))
</code></pre>
<p>I want to extract the the first, third, and fourth row; in other words I need all these entries <code>(:, :, (1, 3))</code>, which works for numpy arrays:</p>
<pre><code>out = a[:,:, [0, 2, 3]]
out.shape #(50, 50, 3)

</code></pre>
<p>Working with a tensor <code>t = tf.convert_to_tensor(a)</code> and then calling the index like</p>
<pre><code>t[:,:, [0, 2, 3]]
</code></pre>
<p>throws an error:</p>
<pre><code>TypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got [0, 1, 3]
</code></pre>
<p>For numpy I have found the following relevant questions, but they naturally focus on numpy arrays:</p>
<p><a href=""https://stackoverflow.com/questions/46227095/how-to-slice-a-2d-array-non-consecutively-in-python?noredirect=1&amp;lq=1"">How to slice a 2D array non-consecutively in Python</a></p>
<p><a href=""https://stackoverflow.com/questions/24398708/slicing-a-numpy-array-along-a-dynamically-specified-axis"">Slicing a numpy array along a dynamically specified axis</a></p>
<p>Looking at the TF documentation I found <code>gather_nd</code> and <code>boolean_mask</code>, which I feel are helpful, but I must freely admit that I have not understood the docs at this part. On SO I found this question <a href=""https://stackoverflow.com/questions/58052967/how-to-select-elements-of-a-tensor-along-a-specific-axis-in-tensorflow"">How to select elements of a tensor along a specific axis in TensorFlow</a>, which focuses on single elements; I am looking for complete dimensions (if that's the right wording here).</p>
<p>How can I do the numpy thing in TensorFlow?</p>
",1
67202194,"ValueError: Layer expects 2 input(s), but it received 1 input tensors when training a CNN","<p>I am new to <code>tensorflow</code>, trying to build a Siamese CNN similar to what's done in <a href=""https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/siamese_network.ipynb"" rel=""nofollow noreferrer"">this guide</a>.<br />
My model is built using a base model, which is then fed twice with two different pictures that go through the same network.<br />
This is the code for building the network:</p>
<pre class=""lang-py prettyprint-override""><code>class BaseModel(Model):

  def __init__(self, base_network):
    super(BaseModel, self).__init__()
    self.network = base_network
  
  def call(self, inputs):
    print(inputs)
    return self.network(inputs)

def get_base_model():
  inputs = tf.keras.Input(shape=INPUT)

  conv2d_1 = layers.Conv2D(name='seq_1', filters=64, 
            kernel_size=20, 
            activation='relu')(inputs)
  maxpool_1 = layers.MaxPooling2D(pool_size=(2, 2))(conv2d_1)

  conv2d_2 = layers.Conv2D(filters=128, 
            kernel_size=20, 
            activation='relu')(maxpool_1)
  maxpool_2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2d_2)

  conv2d_3 = layers.Conv2D(filters=128, 
            kernel_size=20, 
            activation='relu')(maxpool_2)
  maxpool_3 = layers.MaxPooling2D(pool_size=(2, 2))(conv2d_3)

  conv2d_4 = layers.Conv2D(filters=256, 
            kernel_size=10, 
            activation='relu')(maxpool_3)

  flatten_1 = layers.Flatten()(conv2d_4)
  outputs = layers.Dense(units=4096,
                        activation='sigmoid')(flatten_1)
  
  model = Model(inputs=inputs, outputs=outputs)

  return model
</code></pre>
<p>Then, I'm building the Siamese network using the previous method like that:</p>
<pre class=""lang-py prettyprint-override""><code>INPUT = (250, 250, 3)

def get_siamese_model():
  left_input = layers.Input(name='img1', shape=INPUT)
  right_input = layers.Input(name='img2', shape=INPUT)
  
  base_model = get_base_model()
  base_model = BaseModel(base_model)

  # bind the two input layers to the base network
  left = base_model(left_input)
  right = base_model(right_input)

  # build distance measuring layer
  l1_lambda = layers.Lambda(lambda tensors:abs(tensors[0] - tensors[1]))
  l1_dist = l1_lambda([left, right])

  pred = layers.Dense(1,activation='sigmoid')(l1_dist)

  return Model(inputs=[left_input, right_input], outputs=pred)

class SiameseNetwork(Model):

  def __init__(self, siamese_network):
    super(SiameseNetwork, self).__init__()
    self.siamese_network = siamese_network
  
  def call(self, inputs):
    print(inputs)
    return self.siamese_network(inputs)
</code></pre>
<p>I'm then training the network by passing a <code>tf.data.Dataset</code> to it:</p>
<pre class=""lang-py prettyprint-override""><code>net.fit(x=train_dataset, epochs=10 ,verbose=True)
</code></pre>
<p><code>train_dataset</code> is of type:</p>
<blockquote>
<p>&lt;PrefetchDataset shapes: ((None, 250, 250, 3), (None, 250, 250, 3)), types: (tf.float32, tf.float32)&gt;</p>
</blockquote>
<p>It seems like the shape of the input is defined well, but I'm still encountering an error:</p>
<pre class=""lang-py prettyprint-override""><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-144-6c5586e1e205&gt; in &lt;module&gt;()
----&gt; 1 net.fit(x=train_dataset, epochs=10 ,verbose=True)

9 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1098                 _r=1):
   1099               callbacks.on_train_batch_begin(step)
-&gt; 1100               tmp_logs = self.train_function(iterator)
   1101               if data_handler.should_sync:
   1102                 context.async_wait()

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    826     tracing_count = self.experimental_get_tracing_count()
    827     with trace.Trace(self._name) as tm:
--&gt; 828       result = self._call(*args, **kwds)
    829       compiler = &quot;xla&quot; if self._experimental_compile else &quot;nonXla&quot;
    830       new_tracing_count = self.experimental_get_tracing_count()

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    869       # This is the first call of __call__, so we have to initialize.
    870       initializers = []
--&gt; 871       self._initialize(args, kwds, add_initializers_to=initializers)
    872     finally:
    873       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    724     self._concrete_stateful_fn = (
    725         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 726             *args, **kwds))
    727 
    728     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2967       args, kwargs = None, None
   2968     with self._lock:
-&gt; 2969       graph_function, _ = self._maybe_define_function(args, kwargs)
   2970     return graph_function
   2971 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3359 
   3360           self._function_cache.missed.add(call_context_key)
-&gt; 3361           graph_function = self._create_graph_function(args, kwargs)
   3362           self._function_cache.primary[cache_key] = graph_function
   3363 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3204             arg_names=arg_names,
   3205             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 3206             capture_by_value=self._capture_by_value),
   3207         self._function_attributes,
   3208         function_spec=self.function_spec,

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    988         _, original_func = tf_decorator.unwrap(python_func)
    989 
--&gt; 990       func_outputs = python_func(*func_args, **func_kwargs)
    991 
    992       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    632             xla_context.Exit()
    633         else:
--&gt; 634           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    635         return out
    636 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    975           except Exception as e:  # pylint:disable=broad-except
    976             if hasattr(e, &quot;ag_error_metadata&quot;):
--&gt; 977               raise e.ag_error_metadata.to_exception(e)
    978             else:
    979               raise

ValueError: in user code:

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *
        return step_function(self, iterator)
    &lt;ipython-input-125-de3a74f810c3&gt;:9 call  *
        return self.siamese_network(inputs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__  **
        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:207 assert_input_compatibility
        ' input tensors. Inputs received: ' + str(inputs))

    ValueError: Layer model_16 expects 2 input(s), but it received 1 input tensors. Inputs received: [&lt;tf.Tensor 'IteratorGetNext:0' shape=(None, 250, 250, 3) dtype=float32&gt;]
</code></pre>
<p>I do undertand that <code>model_16</code> is the BaseModel, however I can't figure out what am I doing wrong here.</p>
",0
67279286,TensorFlow Extended data_accessor.tf_dataset_factory() shape discrepancies,"<p>I am facing a perplexing issue while attempting to convert a vanilla tensorflow/keras workflow into a tensorflow extended pipeline.</p>
<p>In short: the datasets generated using tfx’s <a href=""https://www.tensorflow.org/tfx/guide/examplegen"" rel=""nofollow noreferrer"">ExampleGen</a> component have different shapes from those created manually using <code>tf.data.Dataset.from_tensor_slices()</code> from the same data, and cannot be fed into a keras model.</p>
<h2>Reproducible example</h2>
<h3>1. Data generation</h3>
<p>Let’s assume we create a sample dataset using:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import random

df = pd.DataFrame({
    'a': [float(x) for x in range(100)],
    'b': [float(x + 1) for x in range(100)],
    'c': [float(x**2) for x in range(100)],
    'target': [random.randint(0, 2) for _ in range(100)],
})

df.to_parquet({my_path})

</code></pre>
<h3>2. Model generation</h3>
<p>Let's use a dummy dense model for simplicity's sake.</p>
<pre class=""lang-py prettyprint-override""><code>from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD

def build_model():

    model = Sequential()
    model.add(Dense(8, input_shape=(3,), activation='relu'))
    model.add(Dense(3, activation='softmax'))

    model.compile(
        optimizer=SGD(),
        loss=&quot;sparse_categorical_crossentropy&quot;,
        metrics=[&quot;sparse_categorical_accuracy&quot;],
    )

    return model
</code></pre>
<h3>3. What works: manual dataset creation</h3>
<p>This parquet file can then be loaded back into a pandas df and converted into a tensorflow dataset using:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

_BATCH_SIZE = 4

dataset = tf.data.Dataset.from_tensor_slices((
    tf.cast(df[['a', 'b', 'c']].values, tf.float32),
    tf.cast(df['target'].values, tf.int32),
)).batch(_BATCH_SIZE, drop_remainder=True)
</code></pre>
<p>This gives a dataset with <code>cardinality() = &lt;tf.Tensor: shape=(), dtype=int64, numpy=25&gt;</code>, which can be fed to the toy model above.</p>
<h3>4. What doesn't work: making a tensorflow extended pipeline</h3>
<p>I have tried to replicate those results by applying a slightly modified tfx <a href=""https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple"" rel=""nofollow noreferrer"">starter pipeline</a>:</p>
<pre class=""lang-py prettyprint-override""><code>from tfx_bsl.tfxio import dataset_options
from tfx.components import SchemaGen
from tfx.components import StatisticsGen
from tfx.components import Trainer
from tfx.dsl.components.base import executor_spec
from tfx.components.example_gen.component import FileBasedExampleGen
from tfx.components.example_gen.custom_executors import parquet_executor
from tfx.components.trainer.executor import GenericExecutor
from tfx.orchestration import metadata
from tfx.orchestration import pipeline
from tfx.proto import trainer_pb2
from tfx.proto import example_gen_pb2
from tfx.utils.io_utils import parse_pbtxt_file


_BATCH_SIZE = 4
_LABEL_KEY = 'target'
_EPOCHS = 10


def _input_fn(file_pattern, data_accessor, schema) -&gt; Dataset:

    dataset = data_accessor.tf_dataset_factory(
        file_pattern,
        dataset_options.TensorFlowDatasetOptions(
            batch_size=_BATCH_SIZE,
            label_key=_LABEL_KEY,
            num_epochs=_EPOCHS,
        ),
        schema,
    )
    
    return dataset


def build_model():
    &quot;&quot;&quot;Same as above&quot;&quot;&quot;

    ...

    return model


def run_fn(fn_args):
    schema = parse_pbtxt_file(fn_args.schema_file, schema_pb2.Schema())
    
    train_dataset = _input_fn(
        fn_args.train_files,
        fn_args.data_accessor,
        schema,
    )
    eval_dataset = _input_fn(
        fn_args.eval_files,
        fn_args.data_accessor,
        schema,
    )

    model = build_model()
    model.fit(
        train_dataset,
        steps_per_epoch=fn_args.train_steps,
        validation_data=eval_dataset,
        validation_steps=fn_args.eval_steps,
        epochs=_EPOCHS,
    )

    model.save(fn_args.serving_model_dir, save_format='tf')


def _create_pipeline(
    pipeline_name: str,
    pipeline_root: str,
    data_root: str,
    module_file: str,
    metadata_path: str,
    split: dict,
) -&gt; pipeline.Pipeline:

    split_config = example_gen_pb2.SplitConfig(
        splits=[
            example_gen_pb2.SplitConfig.Split(name=name, hash_buckets=buckets)
            for name, buckets in split.items()
        ]
    )

    example_gen = FileBasedExampleGen(
        input_base=data_root,
        custom_executor_spec=executor_spec.ExecutorClassSpec(parquet_executor.Executor),
        output_config=example_gen_pb2.Output(split_config=split_config),
    )

    statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])
    infer_schema = SchemaGen(statistics=statistics_gen.outputs['statistics'])

    trainer = Trainer(
        module_file=module_file,
        custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),
        examples=example_gen.outputs['examples'],
        schema=infer_schema.outputs['schema'],
        train_args=trainer_pb2.TrainArgs(),
        eval_args=trainer_pb2.EvalArgs()
    )

    components = [example_gen, statistics_gen, infer_schema, trainer]
    metadata_config = metadata.sqlite_metadata_connection_config(metadata_path)

    _pipeline = pipeline.Pipeline(
        pipeline_name=pipeline_name,
        pipeline_root=pipeline_root,
        components=components,
        metadata_connection_config=metadata_config,
    )

    return _pipeline
</code></pre>
<p>However, the dataset generated by ExampleGen has cardinality <code>tf.Tensor(-2, shape=(), dtype=int64)</code>, and gives the following error message when fed to the same model:</p>
<pre><code>ValueError: Layer sequential expects 1 inputs, but it received 3 input tensors. Inputs received: [&lt;tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7f40353373d0&gt;, &lt;tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7f4035337710&gt;, &lt;tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7f40352e3190&gt;]
</code></pre>
<p>Importantly: the problem persists even when the data are stored as a <code>csv</code> file and read using <code>CsvExampleGen</code>, which makes the issue very unlikely to arise from the data themselves.</p>
<p>Also, batching the tfx output dataset has no effect on the results.</p>
<p>I’ve tried everything I could think of to no benefit. The relative obscurity of what's happening under tfx's hood doesn't help with the debugging either. Has anyone any idea of what the problem is?</p>
<h3>Edit 1</h3>
<p>Two points have come to my attention since writing this question:</p>
<ul>
<li><p><code>data_accessor.tf_dataset_factory()</code> doesn't actually output a <code>tensorflow.python.data.ops.dataset_ops.TensorSliceDataset</code>, but a <code>tensorflow.python.data.ops.dataset_ops.PrefetchDataset</code> instead.</p>
</li>
<li><p>There's actually a small bunch of as yet unanswered questions that look somewhat related to my problem discussing the pains of working with <code>PrefetchDataset</code>s:</p>
</li>
</ul>
<p><a href=""https://stackoverflow.com/questions/64579137/tfds-audio-preprocessing-prefetchdataset-problems"">TFDS Audio Preprocessing PrefetchDataset Problems</a></p>
<p><a href=""https://stackoverflow.com/questions/66302936/how-to-feed-tf-prefetch-dataset-into-lstm"">How to feed tf.prefetch dataset into LSTM?</a></p>
<p><a href=""https://stackoverflow.com/questions/64978453/change-prefetchdataset-shapes"">Change PrefetchDataset shapes</a></p>
<p>Considering none of those questions have found an answer, and that the crux of the problem seems to be the lack of documentation regarding <code>PrefetchDataset</code>s and how to use them, I'll open an issue on tfx's board and see how it goes if this doesn't get answered here within a few days.</p>
<h3>Edit 2: version and environment details</h3>
<p>As requested by <a href=""https://stackoverflow.com/users/11530462/tensorflow-support"">TensorFlow Support</a>, here are the details regarding the versions of all my TensorFlow-related installs:</p>
<ul>
<li><p><strong>Core components</strong>:</p>
<ul>
<li>tensorflow==2.3.0</li>
<li>tfx==0.25.0</li>
<li>tfx-bsl==0.25.0</li>
</ul>
</li>
<li><p><strong>TensorFlow-related stuff</strong>:</p>
<ul>
<li>tensorflow-cloud==0.1.7</li>
<li>tensorflow-data-validation==0.25.0</li>
<li>tensorflow-datasets==3.0.0</li>
<li>tensorflow-estimator==2.3.0</li>
<li>tensorflow-hub==0.9.0</li>
<li>tensorflow-io==0.15.0</li>
<li>tensorflow-metadata==0.25.0</li>
<li>tensorflow-model-analysis==0.25.0</li>
<li>tensorflow-probability==0.11.0</li>
<li>tensorflow-serving-api==2.3.0</li>
<li>tensorflow-transform==0.25.0</li>
</ul>
</li>
<li><p><strong>Environment and other miscellaneous details:</strong></p>
<ul>
<li>Python version: 3.7.9</li>
<li>OS: Debian GNU/Linux 10 (buster)</li>
<li>Running from an N1 GCP instance</li>
</ul>
</li>
</ul>
",1
67280717,Is there a way to define the gradient of a 3D FFT using tensorflow's custom_gradient decorator,"<p><strong>Context &amp; problem</strong></p>
<p>I am using the Hamiltonian Monte Carlo (HMC) method of the <code>tensorflow-probability</code> module to explore the most probable states of a self-written probability function. Amongst the parameters I am trying to fit are Fourier modes of a real three dimensional field.</p>
<p>For the HMC to run, each computing block needs to have its gradient implemented. However, the implementation of the inverse real FFT <code>tf.signal.irfft3d</code> does not have a gradient method associated by default.</p>
<p><strong>Question</strong></p>
<p>Is there a way to implement the gradient of the function <code>irfft3d</code>? I already have a running, self-implemented <code>irfft3d</code> with more basic blocks of <code>tensorflow</code> on which automatic differentiation seems to work, but I would like to wrap the actual optimized and stable implementation of <code>tf.signal.irfft3d</code> using the decorator <code>@tf.custom_gradient</code> to make the automatic differentiation work.</p>
<p><strong>My guess</strong></p>
<p>The Fourier transform is linear, this problem is then theoretically trivial. However, writing the Jacobian of the Fourier transform on a grid is numerically unfeasible (as its dimensions would be huge). Luckily, <code>tensorflow</code> demands only for a functional that evaluates the Jacobian on a input vector. I believe this can be efficiently done thanks to the FFT algorithm. Unfortunately, it seems to me that <code>tensorflow</code> demands a functional that computes the <em>invert</em> of the Jacobian applied to the &quot;upstream gradient&quot;, which I do not understand:</p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient?version=nightly"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/custom_gradient?version=nightly</a></p>
<blockquote>
<p>function f(*x) that returns a tuple (y, grad_fn) where:</p>
<ul>
<li>x is a sequence of (nested structures of) Tensor inputs to the function.</li>
<li>y is a (nested structure of) Tensor outputs of applying TensorFlow operations in f to x.</li>
<li>grad_fn is a function with the signature g(*grad_ys) which returns a list of Tensors the same size as (flattened) x - the derivatives of Tensors in y with respect to the Tensors in x. grad_ys is a sequence of Tensors the same size as (flattened) y holding the initial value gradients for each Tensor in y.</li>
</ul>
<p>In a pure mathematical sense, a vector-argument vector-valued function f's derivatives should be its Jacobian matrix J. Here we are expressing the Jacobian J as a function grad_fn which defines how J will transform a vector grad_ys when left-multiplied with it (grad_ys * J, the vector-Jacobian product, or VJP). This functional representation of a matrix is convenient to use for chain-rule calculation (in e.g. the back-propagation algorithm).</p>
</blockquote>
<p>Complying with the dimensions and the formats given in the doc, I cannot imagine any other solution that:</p>
<pre><code>#!/usr/bin/env python3

# set up
import tensorflow as tf

n = 64

noise = tf.random.normal((n, n, n))
modes = tf.signal.rfft3d(noise)

# the function 
@tf.custom_gradient
def irfft3d(x):
    def grad_fn(dy):
        return (tf.signal.rfft3d(dy))

    return (tf.signal.irfft3d(x), grad_fn)

# test 
with tf.GradientTape() as gt:
    gt.watch(modes)
    rec_noise = irfft3d(modes)

dn_dm = gt.gradient(rec_noise, modes)

print(dn_dm)
</code></pre>
<p>Which does run and return:</p>
<pre><code>tf.Tensor(
[[[262144.+0.j      0.+0.j      0.+0.j ...      0.+0.j      0.+0.j
        0.+0.j]
  [     0.+0.j      0.+0.j      0.+0.j ...      0.+0.j      0.+0.j
        0.+0.j]
  [     0.+0.j      0.+0.j      0.+0.j ...      0.+0.j      0.+0.j
        0.+0.j]
  ...
  [     0.+0.j      0.+0.j      0.+0.j ...      0.+0.j      0.+0.j
        0.+0.j]
  [     0.+0.j      0.+0.j      0.+0.j ...      0.+0.j      0.+0.j
        0.+0.j]
  [     0.+0.j      0.+0.j      0.+0.j ...      0.+0.j      0.+0.j
        0.+0.j]]], shape=(64, 64, 33), dtype=complex64)
</code></pre>
<p>I cannot really wrap up my mind around it. First, this would be such a simple solution that I would not understand why it has not been natively  implemented. But more importantly, I am simply lost in what <code>tensorflow</code> expects from this self-written gradient function and I am not able to express its result in a mathematical way that makes sense to me.</p>
<p>Is there anybody out there that understands the way <code>tensorflow</code> handles differentiation and could help or correct me?</p>
",1
67318143,Evaluate trained and loaded CNN keras model problem,"<p>I am new in the deep neuron network world. I tried to train my one model using the TensorFlow Keras toolkit.</p>
<p>I managed to train a model using the <code>fit</code> function. The accuracy, for 50 epochs, was good - around 96% and the model predicts well with new data. The problem is when I try to evaluate the loaded model the results are like the model wasn't trained at all (accuracy around 50%).</p>
<p>I prepare the small test. I evaluate the model after a <code>fit</code>. Then I save the model, load it, and evaluate it once again. The results are so different. I thought that maybe weights aren't loaded properly, but the documentation suggests that save and load functions operate on the whole model. Here is my code:</p>
<pre><code>CNNmodelHistory = model.fit(train_data, batch_size=batch_size, validation_data=test_data, steps_per_epoch=train_data.samples // batch_size,
                            epochs=echos)


scores = model.evaluate(test_data, verbose=0)
print(f'Test loss: {scores[0]} / Test accuracy: {scores[1]}')

# save the model to disk
model.save('gender_detection.modelTest')

modelLoaded = keras.models.load_model('gender_detection.modelTest')
scores = modelLoaded.evaluate(test_data, verbose=0)
print(f'Test loss: {scores[0]} / Test accuracy: {scores[1]}')
</code></pre>
<p>And here are the results:
<a href=""https://i.stack.imgur.com/zwpLR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zwpLR.png"" alt=""enter image description here"" /></a></p>
<p>Do you have any tips on what I am doing wrong?</p>
",1
67331114,How to get the feature maps from each layers of a trained model during inference time?,"<p>I have implemented and trained the model from the <a href=""https://medium.com/analytics-vidhya/inferring-a-super-resolution-neural-network-on-raspberry-pi-gpu-89b5456d21ef"" rel=""nofollow noreferrer"">following website</a>, and using the author's <a href=""https://gist.github.com/lnstadrum/0e2fec1e96bc6791f0ee3487969632ee"" rel=""nofollow noreferrer"">source code</a>:</p>
<p>I am now running an image through the trained network and <strong>want to get the network output (feature maps etc.) at every stage</strong>.</p>
<h3>My Approach</h3>
<p>To that end, I have tried making sub-models from groups of layers from the full model (called <code>sizedModel</code> in my code) and checking their output.</p>
<p><a href=""https://i.stack.imgur.com/0Z1Dp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0Z1Dp.png"" alt=""Diagram of L1 and L2"" /></a></p>
<p>I have done that for the first <code>L1(Conv2D)</code></p>
<pre><code>L1_inp = sizedModel.layers[1].input
L1_out = sizedModel.layers[1].output
Layer1 = Model(L1_inp,L1_out)
L1_result = Layer1.predict(
    tf.expand_dims(tf.expand_dims(lrImage, axis=0), axis=3)
)

# Save feature maps to greyscale images
Xresult = tf.squeeze(L1_result)
for i in range(Xresult.shape[2]):
    data = (Xresult[:,:,i].numpy() * 255).astype(np.uint8)
    filename = 'tmp_test/result'+str(i).zfill(2)+'.png'
    Image.fromarray(data).save(filename)`
</code></pre>
<p>the result of <code>.predict()</code> on that model is tensor shaped <code>(1,360,640,12)</code> which is as expected and the images look fine.</p>
<p>I am now trying to feed that tensor into the layers marked as L2 in the diagram (sizedModel.layers[2-8]).
<a href=""https://i.stack.imgur.com/z9BhY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/z9BhY.png"" alt=""enter image description here"" /></a>
The only way I know how to do it is isolated in a new model. To that end I am doing:
# Pick up input shape from output of L1 (1,360,640,12)
_input = tf.keras.Input(shape=L1_result.shape, name=&quot;input_layer&quot;)</p>
<pre><code># First &quot;tf_op_layer_strided_slice&quot; from large model
_split = largeModel.layers[2](_input)

# First &quot;L2-0 (Conv 3x3)&quot; from large model
_conv = largeModel.layers[5](_split)
</code></pre>
<p>This results in
# ValueError: Input 0 of layer L2-0 is incompatible with the layer: expected axis -1 of input shape
# to have value 4 but received input with shape [None, 1, 360, 4, 12]</p>
<pre><code># Build sub-model from these two layers of the large model
Layer2 = Model(_input,_conv)

# Pass first sub-model (L1) output to this model
Result_L2 = Layer2.predict(L1_result)
</code></pre>
<p>Why is the <code>Input 0</code> of Layer <code>L2-0</code> incompatible?
Is there an easier way to debug the output of each layer individually?</p>
",0
67432620,"tf.data input pipeline, dynamic shaped tensors and slicing: NotImplementedError: Cannot convert a symbolic Tensor (args_0:0) to a numpy array","<p>I am trying to write an efficient data input using tensorflow and <code>tf.data</code>.
I want to replicate the functionality of <code>PIL.Image.Image.crop</code>, where one can pass negative bounding box values such that the crop is expanded with zeros.</p>
<p>For example, if I call <code>PIL.Image.Image.crop([-10, 0, img_height, img_width])</code>, the image has 10 additional rows at the beginning filled with zeros.</p>
<p>As far as I understood, using python functions in the tf.data input pipeline can slow down the code significantly, therefore I try to write everything using tensorflow functions. I also want to use prefetching, batching, shuffling etc. which is already provided by the API.</p>
<p>My plan to implement the PIL crop using tensorflow functions is to preallocate a (dynamically shaped) zero tensor and assign the cropped values using slicing.</p>
<p>This is the error I ran into: <code>NotImplementedError: Cannot convert a symbolic Tensor (args_2:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported</code></p>
<p>Here is minimal code to replicate the issue:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
ds2 = tf.data.Dataset.from_tensor_slices(np.arange(10))

def preproc(number):
    o = tf.zeros((number,number))
    # ...

ds2.map(preproc)
</code></pre>
<p><strong>Question 1:</strong> How can I solve that issue?</p>
<p><strong>Question 2:</strong> I am coming from a PyTorch background and I am confused about the complexity of the whole tf.data pipeline. Why am I restricted to  tensorflow functions in order to use all the nice features?</p>
<hr />
<p>For reference, here is my complete code so far.</p>
<pre class=""lang-py prettyprint-override""><code>class MyPreprocessing:
    def __call__(self, *data):
        # Complete code omitted for simplicity. 
        # This is called somewhere:
        self._crop(...)

    def _crop(self, img, center, body_size, res):
        tl = tf.cast(center - body_size/2, dtype=tf.int32)
        br = tf.cast(center + body_size/2, dtype=tf.int32)

        height, width = tf.shape(img)[0], tf.shape(img)[1]

        crop_tl = tf.stack([
            tf.cond(tl[0] &lt; 0, lambda: tf.constant(0, dtype=tf.int32), lambda: tl[0]),
            tf.cond(tl[1] &lt; 0, lambda: tf.constant(0, dtype=tf.int32), lambda: tl[1])])
        crop_br = tf.stack([
            tf.cond(br[0] &gt; height, lambda: height, lambda: br[0]),
            tf.cond(br[1] &gt; width, lambda: width, lambda: br[1])])

        crop = tf.image.crop_to_bounding_box(
            img,
            crop_tl[0],
            crop_tl[1],
            crop_br[0] - crop_tl[1],
            crop_br[1] - crop_tl[1])

        new_tl = crop_tl - tl
        new_br = crop_br - tl

        # Error:
        new_img = tf.zeros((body_size, body_size, tf.constant(3)), dtype=tf.float32)
        new_img[new_tl[0]:new_br[0], new_tl[1]:new_br[1]].assign(crop)

        return tf.image.resize(new_img, (res, res))
</code></pre>
<hr />
<p><strong>Edit: Full Stacktrace</strong></p>
<pre class=""lang-txt prettyprint-override""><code>---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
&lt;ipython-input-2-84d20c31b9c9&gt; in &lt;module&gt;
      8     # ...
      9 
---&gt; 10 ds2.map(preproc)

~/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls, deterministic)
   1923         warnings.warn(&quot;The `deterministic` argument has no effect unless the &quot;
   1924                       &quot;`num_parallel_calls` argument is specified.&quot;)
-&gt; 1925       return MapDataset(self, map_func, preserve_cardinality=True)
   1926     else:
   1927       return ParallelMapDataset(

~/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)
   4481     self._use_inter_op_parallelism = use_inter_op_parallelism
   4482     self._preserve_cardinality = preserve_cardinality
-&gt; 4483     self._map_func = StructuredFunctionWrapper(
   4484         map_func,
   4485         self._transformation_name(),

~/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)
   3710     resource_tracker = tracking.ResourceTracker()
   3711     with tracking.resource_tracker_scope(resource_tracker):
-&gt; 3712       self._function = fn_factory()
   3713       # There is no graph to add in eager mode.
   3714       add_to_graph &amp;= not context.executing_eagerly()

~/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py in get_concrete_function(self, *args, **kwargs)
   3132          or `tf.Tensor` or `tf.TensorSpec`.
   3133     &quot;&quot;&quot;
-&gt; 3134     graph_function = self._get_concrete_function_garbage_collected(
   3135         *args, **kwargs)
   3136     graph_function._garbage_collector.release()  # pylint: disable=protected-access

~/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_garbage_collected(self, *args, **kwargs)
   3098       args, kwargs = None, None
   3099     with self._lock:
-&gt; 3100       graph_function, _ = self._maybe_define_function(args, kwargs)
   3101       seen_names = set()
   3102       captured = object_identity.ObjectIdentitySet(

~/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3442 
   3443           self._function_cache.missed.add(call_context_key)
-&gt; 3444           graph_function = self._create_graph_function(args, kwargs)
   3445           self._function_cache.primary[cache_key] = graph_function
   3446 

~/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3277     arg_names = base_arg_names + missing_arg_names
   3278     graph_function = ConcreteFunction(
-&gt; 3279         func_graph_module.func_graph_from_py_func(
   3280             self._name,
   3281             self._python_function,

~/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    997         _, original_func = tf_decorator.unwrap(python_func)
    998 
--&gt; 999       func_outputs = python_func(*func_args, **func_kwargs)
   1000 
   1001       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py in wrapped_fn(*args)
   3685           attributes=defun_kwargs)
   3686       def wrapped_fn(*args):  # pylint: disable=missing-docstring
-&gt; 3687         ret = wrapper_helper(*args)
   3688         ret = structure.to_tensor_list(self._output_structure, ret)
   3689         return [ops.convert_to_tensor(t) for t in ret]

~/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py in wrapper_helper(*args)
   3615       if not _should_unpack(nested_args):
   3616         nested_args = (nested_args,)
-&gt; 3617       ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)
   3618       if _should_pack(ret):
   3619         ret = tuple(ret)

~/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    693       except Exception as e:  # pylint:disable=broad-except
    694         if hasattr(e, 'ag_error_metadata'):
--&gt; 695           raise e.ag_error_metadata.to_exception(e)
    696         else:
    697           raise

NotImplementedError: in user code:

    &lt;ipython-input-2-84d20c31b9c9&gt;:7 preproc  *
        o = tf.zeros((number,number))
    /home/benjs/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **
        return target(*args, **kwargs)
    /home/benjs/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2911 wrapped
        tensor = fun(*args, **kwargs)
    /home/benjs/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2960 zeros
        output = _constant_if_small(zero, shape, dtype, name)
    /home/benjs/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2896 _constant_if_small
        if np.prod(shape) &lt; 1000:
    &lt;__array_function__ internals&gt;:5 prod
        
    /home/benjs/Documents/projects/hpe/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3030 prod
        return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,
    /home/benjs/Documents/projects/hpe/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:87 _wrapreduction
        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
    /home/benjs/Documents/projects/hpe/venv/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:867 __array__
        raise NotImplementedError(

    NotImplementedError: Cannot convert a symbolic Tensor (args_0:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported
</code></pre>
",0
67523944,"Tensorflow2 - Use ""tf.data.experimental.make_csv_dataset"" with ""tf.keras.preprocessing.timeseries_dataset_from_array""","<p>I am trying to get TensorFlow to read +100 CSV files that <em><strong>don't</strong></em> fit in memory (+1GB size each). The files contain time series data (EEG signals), with the labels in the first column. From the TensorFlow documentation it seems like I should be able to use the <em>tf.data</em> API to load my data off-disk.</p>
<p>For the sake of simplicity and reproducibility, let's consider the following &quot;<em>sample_data.csv</em>&quot; dataset:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Label</th>
<th>Feature 1</th>
<th>Feature 2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Apple</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>Banana</td>
<td>3</td>
<td>4</td>
</tr>
<tr>
<td>Coconut</td>
<td>5</td>
<td>6</td>
</tr>
<tr>
<td>Durian</td>
<td>7</td>
<td>8</td>
</tr>
</tbody>
</table>
</div>
<p>I've tried using <a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset"" rel=""nofollow noreferrer"">tf.data.experimental.make_csv_dataset</a> to load the CSV files into <em>tf.data.Dataset</em> objects, and then <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array"" rel=""nofollow noreferrer"">tf.keras.preprocessing.timeseries_dataset_from_array</a> to process the data into sliding windows with overlap. For the dataset above, I would do:</p>
<pre><code>import tensorflow as tf

input_data = tf.data.experimental.make_csv_dataset(
    'sample_data.csv',
    batch_size=1,
    column_names=['Label', 'Feature 1', 'Feature 2']
    label_name='Label',
    num_epochs=1,
    shuffle=False
)
</code></pre>
<p>Which we can check works correctly by looking at the output from <code>list(input_data.as_numpy_iterator())</code>. We can then feed <code>input_data</code> to the next function:</p>
<pre><code>my_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
    input_data,
    targets=None,
    sequence_length=3,
    sequence_stride=2,
    sampling_rate=1,  
    batch_size=1,
    shuffle=False
)
</code></pre>
<p>Which unfortunately <strong>throws this error</strong>:</p>
<blockquote>
<p>TypeError: dataset length is unknown.</p>
</blockquote>
<p>I also tried using <code>my_dataset = input_data.window(3, shift=2)</code> (see the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window"" rel=""nofollow noreferrer"">tf.data.Dataset.window</a> documentation) and it didn't throw an error, but
it seems to be returning an <strong>empty dataset</strong>? See &quot;<em>_VariantDataset shapes: (None,)</em>&quot; in the output:</p>
<pre><code>list(input_data.window(3, shift=2))

[344]:
[(OrderedDict([('Feature 1',
                &lt;_VariantDataset shapes: (None,), types: tf.int32&gt;),
               ('Feature 2',
                &lt;_VariantDataset shapes: (None,), types: tf.int32&gt;)]),
  &lt;_VariantDataset shapes: (None,), types: tf.string&gt;),
 (OrderedDict([('Feature 1',
                &lt;_VariantDataset shapes: (None,), types: tf.int32&gt;),
               ('Feature 2',
                &lt;_VariantDataset shapes: (None,), types: tf.int32&gt;)]),
  &lt;_VariantDataset shapes: (None,), types: tf.string&gt;),
 (OrderedDict([('Feature 1',
                &lt;_VariantDataset shapes: (None,), types: tf.int32&gt;),
               ('Feature 2',
                &lt;_VariantDataset shapes: (None,), types: tf.int32&gt;)]),
  &lt;_VariantDataset shapes: (None,), types: tf.string&gt;)]
</code></pre>
<p>If I load the &quot;<em>sample_data.csv</em>&quot; in memory using pandas and then feed the <em>timeseries_dataset_from_array</em> function a numpy array instead, it works correctly.</p>
<p>Any ideas on how to solve this? <strong>What's the best method to input overlapping windows from off-memory time-series data into TensorFlow</strong>?</p>
<p>Thank you!</p>
",1
67557800,How to increase the rank (ndim) of input of BERT keras hub layer for learning-to-rank,"<p>I am trying to implement a learning-to-rank model using a pre-trained BERT available on tensorflow hub. I am using a variation of ListNet loss function, which requires each training instance to be a list of several ranked documents in relation to a query. I need the model to be able to accept data in a shape (batch_size, list_size, sentence_length), where the model loops over the 'list_size' axis in each training instance, returns the ranks and passes them to the loss function. In a simple model that only consists of dense layers, this is easily done by augmenting the dimensions of the input layer. For example:</p>
<pre><code>from tensorflow.keras.layers import Dense, Input
from tensorflow.keras import Model

input = Input([6,10])
x = Dense(20,activation='relu')(input)
output = Dense(1, activation='sigmoid')(x)
model = Model(inputs=input, outputs=output)
</code></pre>
<p>...now the model will perform 6 forward passes over vectors of length 10 before calculating the loss and updating gradients.</p>
<p>I am trying to do the same with the BERT model and its preprocessing layer:</p>
<pre><code>import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text

bert_preprocess_model = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1')
bert_model = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')
    
text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
processed_input = bert_preprocess_model(text_input)
output = bert_model(processed_input)
model = tf.keras.Model(text_input, output)
</code></pre>
<p>But when I try to change the shape of 'text_input' to, say, (6), or meddle with it in any way really, it always results in the same type of error:</p>
<pre><code> ValueError: Could not find matching function to call loaded from the SavedModel. Got:
      Positional arguments (3 total):
        * Tensor(&quot;inputs:0&quot;, shape=(None, 6), dtype=string)
        * False
        * None
      Keyword arguments: {}
    
    Expected these arguments to match one of the following 4 option(s):
    
    Option 1:
      Positional arguments (3 total):
        * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')
        * False
        * None
      Keyword arguments: {}
     ....
</code></pre>
<p>As per <a href=""https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer</a>, it seems like you can configure the input shape of hub.KerasLayer via tf.keras.layers.InputSpec. In my case, I guess it would be something like this:</p>
<pre><code>bert_preprocess_model.input_spec = tf.keras.layers.InputSpec(ndim=2)
bert_model.input_spec = tf.keras.layers.InputSpec(ndim=2)
</code></pre>
<p>When I run the above code, the attributes indeed get changed, but when trying to build the model, the same exact error appears.</p>
<p>Is there any way to easily resolve this without the necessity to create a custom training loop?</p>
",0
67563475,How to convert a tensorflow model and load as tfds,"<p>I need help converting my dataset from how I usually make it using
<code>tf.keras.preprocessing.image_dataset_from_directory</code>
To be used to replace this in an example</p>
<pre><code>dataset, info = tfds.load(name='mnist', split=split, with_info=True,

as_supervised=True, try_gcs=True)
</code></pre>
<p>How can I do so? I am unable to find related documentation so if you can link that it would be amazing.
Thanks</p>
<p>This is how the dataset is used in the example</p>
<pre><code>  split = 'train' if is_training else 'test'
  dataset, info = tfds.load(name='mnist', split=split, with_info=True,
                            as_supervised=True, try_gcs=True)


  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255.0

    return image, label

  dataset = dataset.map(scale)
</code></pre>
",1
67635325,"ValueError: Input 0 of layer conv2d_46 is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: (None, 64, 64)","<p>I am using input images with (64, 64, 3) shape based on the following script. I am not sure why it returns error about data dimension. I also tried <code>trainX = tf.expand_dims(trainX, axis=-1)</code> based on <a href=""https://stackoverflow.com/questions/63279168/valueerror-input-0-of-layer-sequential-is-incompatible-with-the-layer-expect"">this post</a>, but I could not solve it. Can anyone help me about that?</p>
<pre><code>inputShape = (64, 64, 3)
chanDim = -1
# define the model input
inputs = Input(shape=inputShape)
# CONV =&gt; RELU =&gt; BN =&gt; POOL
x = Conv2D(16, (3, 3), padding=&quot;same&quot;)(inputs)
x = Activation(&quot;relu&quot;)(x)
x = BatchNormalization(axis=chanDim)(x)
x = MaxPooling2D(pool_size=(2, 2))(x)
# CONV =&gt; RELU =&gt; BN =&gt; POOL
x = Conv2D(32, (3, 3), padding=&quot;same&quot;)(x)
x = Activation(&quot;relu&quot;)(x)
x = BatchNormalization(axis=chanDim)(x)
x = MaxPooling2D(pool_size=(2, 2))(x)
# CONV =&gt; RELU =&gt; BN =&gt; POOL
x = Conv2D(64, (3, 3), padding=&quot;same&quot;)(x)
x = Activation(&quot;relu&quot;)(x)
x = BatchNormalization(axis=chanDim)(x)
x = MaxPooling2D(pool_size=(2, 2))(x)

# flatten the volume, then FC =&gt; RELU =&gt; BN =&gt; DROPOUT
x = Flatten()(x)
x = Dense(16)(x)
x = Activation(&quot;relu&quot;)(x)
x = BatchNormalization(axis=chanDim)(x)
x = Dropout(0.5)(x)

# apply another FC layer, this one to match the number of nodes
# coming out of the MLP
x = Dense(4)(x)
x = Activation(&quot;relu&quot;)(x)
x = Dense(1, activation=&quot;linear&quot;)(x)

# construct the CNN
model = Model(inputs, x)

model.summary()
fileToSaveModelPlot='model.png'
plot_model(model, to_file='model.png')
print(&quot;[INFO] Model plot saved to {}&quot;.format(fileToSaveModelPlot) )

opt = Adam(lr=1e-3, decay=1e-3 / 200)
model.compile(loss=&quot;mean_absolute_percentage_error&quot;, optimizer=opt)

history=model.fit(trainX, trainY, validation_data=(testX, testY),epochs=EPOCHS_NUM, batch_size=2)
</code></pre>
<p>error:</p>
<pre><code>ValueError: Input 0 of layer conv2d_46 is incompatible with the layer: : expected min_ndim=4, found ndim=3. Full shape received: (None, 64, 64)
</code></pre>
<p>-update</p>
<pre><code>#load csv file
labelPath =  &quot;/content/drive/MyDrive/Notebook/tepm.csv&quot;
cols = [&quot;temperature&quot;]
df = pd.read_csv(labelPath, sep=&quot; &quot;, header=None, names=cols)

inputPath='/content/drive/MyDrive/Notebook/test_png_64'
images = []

# Load in the images
for filepath in os.listdir(inputPath):
    images.append(cv2.imread(inputPath+'/{0}'.format(filepath),0))

images_scaled = np.array(images, dtype=&quot;float&quot;) / 255.0
</code></pre>
<hr />
<p>here is the script to define trainY, testY, trainX, and testX</p>
<pre><code>(trainY, testY, trainX, testX) = train_test_split(df, images_scaled, test_size=0.25, random_state=42)
</code></pre>
<p>these are the code and result for their shape:</p>
<pre><code>print (trainY.shape,testY.shape,trainX.shape, testX.shape)

(224, 1) (75, 1) (224, 64, 64) (75, 64, 64)
</code></pre>
",0
67644480,"How to fix ""using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function""","<p>Recently faced with such a problem: using a <code>tf.Tensor</code> as a Python <code>bool</code> is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.</p>
<pre class=""lang-py prettyprint-override""><code>from keras.layers import Input, Reshape, Dropout, Dense, Flatten, BatchNormalization, Activation, ZeroPadding2D
from keras.layers.advanced_activations import LeakyReLU
from keras.layers.convolutional import UpSampling2D, Conv2D
from keras.models import Sequential, Model, load_model
from keras.optimizers import Adam
import tensorflow as tf
import os
import numpy as np
from PIL import Image
</code></pre>
<pre class=""lang-py prettyprint-override""><code>IMAGE_SIZE = 128
IMAGE_CHANNELS = 3
IMAGE_DIR = 'dataset/'
</code></pre>
<pre class=""lang-py prettyprint-override""><code>images_path = IMAGE_DIR 

training_data = []
</code></pre>
<pre class=""lang-py prettyprint-override""><code>for filename in os.listdir(images_path):
    path = os.path.join(images_path, filename)
    image = Image.open(path).resize((IMAGE_SIZE, IMAGE_SIZE), Image.ANTIALIAS)

    training_data.append(np.asarray(image))

training_data = np.reshape(training_data, (-1, IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS))
training_data = training_data / 127.5 - 1
np.save('cubism_data.npy', training_data)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>PREVIEW_ROWS = 4
PREVIEW_COLS = 7
PREVIEW_MARGIN = 4
SAVE_FREQ = 100
NOISE_SIZE = 100
EPOCHS = 10000 
BATCH_SIZE = 32
GENERATE_RES = 3
IMAGE_SIZE = 128 
training_data = np.load('cubism_data.npy')
</code></pre>
<pre class=""lang-py prettyprint-override""><code>def build_discriminator(image_shape):    
    model = Sequential()    
    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape = image_shape, padding=&quot;same&quot;))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))    
    model.add(Conv2D(64, kernel_size=3, strides=2, padding=&quot;same&quot;))
    model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))
    model.add(BatchNormalization(momentum=0.8))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))    
    model.add(Conv2D(128, kernel_size=3, strides=2, padding=&quot;same&quot;))
    model.add(BatchNormalization(momentum=0.8))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dropout(0.25))    
    model.add(Conv2D(256, kernel_size=3, strides=1, padding=&quot;same&quot;))
    model.add(BatchNormalization(momentum=0.8))
    model.add(LeakyReLU(alpha=0.2))    
    model.add(Dropout(0.25))
    model.add(Conv2D(512, kernel_size=3, strides=1, padding=&quot;same&quot;))
    model.add(BatchNormalization(momentum=0.8))
    model.add(LeakyReLU(alpha=0.2))    
    model.add(Dropout(0.25))
    model.add(Flatten())
    model.add(Dense(1, activation='sigmoid'))    
    input_image = Input(shape=image_shape)
    validity = model(input_image)
    return Model(input_image, validity)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>def build_generator(noise_size, channels):
    model = Sequential()
    model.add(Dense(4 * 4 * 256, activation=&quot;relu&quot;, input_dim=noise_size))
    model.add(Reshape((4, 4, 256)))    
    model.add(UpSampling2D())
    model.add(Conv2D(256, kernel_size=3, padding=&quot;same&quot;))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Activation(&quot;relu&quot;))    
    model.add(UpSampling2D())
    model.add(Conv2D(256, kernel_size=3, padding=&quot;same&quot;))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Activation(&quot;relu&quot;))    
    for i in range(GENERATE_RES):
         model.add(UpSampling2D())
         model.add(Conv2D(256, kernel_size=3, padding=&quot;same&quot;))
         model.add(BatchNormalization(momentum=0.8))
         model.add(Activation(&quot;relu&quot;))    
    model.summary()
    model.add(Conv2D(channels, kernel_size=3, padding=&quot;same&quot;))
    model.add(Activation(&quot;tanh&quot;))    
    input = Input(shape=(noise_size,))
    generated_image = model(input)
    
    return Model(input, generated_image)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>def save_images(cnt, noise):
    image_array = np.full((
        PREVIEW_MARGIN + (PREVIEW_ROWS * (IMAGE_SIZE + PREVIEW_MARGIN)),
        PREVIEW_MARGIN + (PREVIEW_COLS * (IMAGE_SIZE + PREVIEW_MARGIN)), 3), 255, dtype=np.uint8)
    generated_images = generator.predict(noise)
    generated_images = 0.5 * generated_images + 0.5
    image_count = 0
    for row in range(PREVIEW_ROWS):
        for col in range(PREVIEW_COLS):
            r = row * (IMAGE_SIZE + PREVIEW_MARGIN) + PREVIEW_MARGIN
            c = col * (IMAGE_SIZE + PREVIEW_MARGIN) + PREVIEW_MARGIN
            image_array[r:r + IMAGE_SIZE, c:c + IMAGE_SIZE] = generated_images[image_count] * 255
            image_count += 1
    output_path = 'output'
    if not os.path.exists(output_path):
        os.makedirs(output_path)
            
    filename = os.path.join(output_path, f&quot;trained-{cnt}.png&quot;)
    im = Image.fromarray(image_array)
    im.save(filename)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>image_shape = (IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS)
optimizer = Adam(1.5e-4, 0.5)
discriminator = build_discriminator(image_shape)
validity = discriminator(generated_image)
discriminator.compile(loss=tf.keras.losses.binary_crossentropy,
                      optimizer=optimizer, metrics=[&quot;accuracy&quot;])
generator = build_generator(NOISE_SIZE, IMAGE_CHANNELS)
random_input = Input(shape=(NOISE_SIZE,))
generated_image = generator(random_input)
discriminator.trainable = Falsevalidity = discriminator(generated_image)
combined = Model([random_input, validity])
combined.compile(loss=tf.keras.losses.binary_crossentropy,
                 optimizer=optimizer, metrics=[&quot;accuracy&quot;])
</code></pre>
<p>And there this problem</p>
<pre class=""lang-py prettyprint-override""><code>y_real = tf.ones((BATCH_SIZE, 1))
y_fake = tf.zeros((BATCH_SIZE, 1))
fixed_noise = np.random.normal(0, 1, (PREVIEW_ROWS * PREVIEW_COLS, NOISE_SIZE))
cnt = 1

for epoch in range(EPOCHS):
    idx = np.random.randint(0, training_data.shape[0], BATCH_SIZE)
    x_real = training_data[idx]
 
    noise= np.random.normal(0, 1, (BATCH_SIZE, NOISE_SIZE))
    x_fake = generator.predict(noise)
 
    
    discriminator_metric_real = discriminator.train_on_batch(x_real, y_real)
    
discriminator_metric_generated = discriminator.train_on_batch(x_fake, y_fake)
 
discriminator_metric = 0.5 * np.add(discriminator_metric_real, discriminator_metric_generated)
generator_metric = combined.train_on_batch(noise, y_real)
    
if epoch % SAVE_FREQ == 0:
    save_images(cnt, fixed_noise)
    cnt += 1
    print(f&quot;{epoch} epoch, Discriminator accuracy: {100*  discriminator_metric[1]}, Generator accuracy: {100 * generator_metric[1]}&quot;)
</code></pre>
<pre><code>WARNING:tensorflow:8 out of the last 8 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x000001C884439D30&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
</code></pre>
<pre><code>---------------------------------------------------------------------------
OperatorNotAllowedInGraphError            Traceback (most recent call last)
&lt;ipython-input-62-c0178347a645&gt; in &lt;module&gt;
     12 
     13 
---&gt; 14     discriminator_metric_real = discriminator.train_on_batch(x_real, y_real)
     15 
     16 discriminator_metric_generated = discriminator.train_on_batch(x_fake, y_fake)

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\keras\engine\training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)
   1687     self._check_call_args('train_on_batch')
   1688     _disallow_inside_tf_function('train_on_batch')
-&gt; 1689     with self.distribute_strategy.scope(), \
   1690          training_utils.RespectCompiledTrainableState(self):
   1691       iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\keras\engine\training_utils.py in __enter__(self)
   1230     for layer, trainable in self._compiled_trainable_state.items():
   1231       if (layer in self._current_trainable_state and
-&gt; 1232           trainable != self._current_trainable_state[layer]):
   1233         self._should_set_trainable = True
   1234         break

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\framework\ops.py in __bool__(self)
    875       `TypeError`.
    876     &quot;&quot;&quot;
--&gt; 877     self._disallow_bool_casting()
    878 
    879   def __nonzero__(self):

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\framework\ops.py in _disallow_bool_casting(self)
    488     else:
    489       # Default: V1-style Graph execution.
--&gt; 490       self._disallow_in_graph_mode(&quot;using a `tf.Tensor` as a Python `bool`&quot;)
    491 
    492   def _disallow_iteration(self):

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\framework\ops.py in _disallow_in_graph_mode(self, task)
    475 
    476   def _disallow_in_graph_mode(self, task):
--&gt; 477     raise errors.OperatorNotAllowedInGraphError(
    478         &quot;{} is not allowed in Graph execution. Use Eager execution or decorate&quot;
    479         &quot; this function with @tf.function.&quot;.format(task))

OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.
</code></pre>
<p>If I add <code>@tf.function</code> before def build_discriminator (image_shape), then this error appears in the penultimate block of code(it does not disappear in the last):</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\framework\tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
    547     try:
--&gt; 548       str_values = [compat.as_bytes(x) for x in proto_values]
    549     except TypeError:

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\framework\tensor_util.py in &lt;listcomp&gt;(.0)
    547     try:
--&gt; 548       str_values = [compat.as_bytes(x) for x in proto_values]
    549     except TypeError:

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\util\compat.py in as_bytes(bytes_or_text, encoding)
     85   else:
---&gt; 86     raise TypeError('Expected binary or unicode string, got %r' %
     87                     (bytes_or_text,))

TypeError: Expected binary or unicode string, got &lt;tensorflow.python.keras.engine.functional.Functional object at 0x000001C884818D30&gt;

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\framework\func_graph.py in convert(x)
    941         try:
--&gt; 942           x = ops.convert_to_tensor_or_composite(x)
    943         except (ValueError, TypeError):

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor_or_composite(value, dtype, name)
   1620   &quot;&quot;&quot;
-&gt; 1621   return internal_convert_to_tensor_or_composite(
   1622       value=value, dtype=dtype, name=name, as_ref=False)

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\framework\ops.py in internal_convert_to_tensor_or_composite(value, dtype, name, as_ref)
   1655   else:
-&gt; 1656     return convert_to_tensor(
   1657         value,

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\framework\ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)
   1498     if ret is None:
-&gt; 1499       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1500 

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    337   _ = as_ref
--&gt; 338   return constant(v, dtype=dtype, name=name)
    339 

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\framework\constant_op.py in constant(value, dtype, shape, name)
    262   &quot;&quot;&quot;
--&gt; 263   return _constant_impl(value, dtype, shape, name, verify_shape=False,
    264                         allow_broadcast=True)

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    279   tensor_value.tensor.CopyFrom(
--&gt; 280       tensor_util.make_tensor_proto(
    281           value, dtype=dtype, shape=shape, verify_shape=verify_shape,

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\framework\tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
    549     except TypeError:
--&gt; 550       raise TypeError(&quot;Failed to convert object of type %s to Tensor. &quot;
    551                       &quot;Contents: %s. Consider casting elements to a &quot;

TypeError: Failed to convert object of type &lt;class 'tensorflow.python.keras.engine.functional.Functional'&gt; to Tensor. Contents: &lt;tensorflow.python.keras.engine.functional.Functional object at 0x000001C884818D30&gt;. Consider casting elements to a supported type.

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
&lt;ipython-input-64-bf1d58de645b&gt; in &lt;module&gt;
      1 image_shape = (IMAGE_SIZE, IMAGE_SIZE, IMAGE_CHANNELS)
      2 optimizer = Adam(1.5e-4, 0.5)
----&gt; 3 discriminator = build_discriminator(image_shape)
      4 validity = discriminator(generated_image)
      5 discriminator.compile(loss=tf.keras.losses.binary_crossentropy,

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = &quot;nonXla&quot;
--&gt; 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\def_function.py in _call(self, *args, **kwds)
    821       # This is the first call of __call__, so we have to initialize.
    822       initializers = []
--&gt; 823       self._initialize(args, kwds, add_initializers_to=initializers)
    824     finally:
    825       # At this point we know that the initialization is complete (or less

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\def_function.py in _initialize(self, args, kwds, add_initializers_to)
    694     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)
    695     self._concrete_stateful_fn = (
--&gt; 696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
    697             *args, **kwds))
    698 

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2853       args, kwargs = None, None
   2854     with self._lock:
-&gt; 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2856     return graph_function
   2857 

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\function.py in _maybe_define_function(self, args, kwargs)
   3211 
   3212       self._function_cache.missed.add(call_context_key)
-&gt; 3213       graph_function = self._create_graph_function(args, kwargs)
   3214       self._function_cache.primary[cache_key] = graph_function
   3215       return graph_function, args, kwargs

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3063     arg_names = base_arg_names + missing_arg_names
   3064     graph_function = ConcreteFunction(
-&gt; 3065         func_graph_module.func_graph_from_py_func(
   3066             self._name,
   3067             self._python_function,

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\framework\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,
    989       # TensorArrays and `None`s.
--&gt; 990       func_outputs = nest.map_structure(convert, func_outputs,
    991                                         expand_composites=True)
    992 

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\util\nest.py in map_structure(func, *structure, **kwargs)
    633 
    634   return pack_sequence_as(
--&gt; 635       structure[0], [func(*x) for x in entries],
    636       expand_composites=expand_composites)
    637 

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\util\nest.py in &lt;listcomp&gt;(.0)
    633 
    634   return pack_sequence_as(
--&gt; 635       structure[0], [func(*x) for x in entries],
    636       expand_composites=expand_composites)
    637 

c:\users\zealottv\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\framework\func_graph.py in convert(x)
    942           x = ops.convert_to_tensor_or_composite(x)
    943         except (ValueError, TypeError):
--&gt; 944           raise TypeError(
    945               &quot;To be compatible with tf.eager.defun, Python functions &quot;
    946               &quot;must return zero or more Tensors; in compilation of %s, found &quot;

TypeError: To be compatible with tf.eager.defun, Python functions must return zero or more Tensors; in compilation of &lt;function build_discriminator at 0x000001C884736AF0&gt;, found return value of type &lt;class 'tensorflow.python.keras.engine.functional.Functional'&gt;, which is not a Tensor.
</code></pre>
",0
67720270,How to use WeightNormalization wrapper for LocallyConnected2D layer,"<p>I'm trying to use the <a href=""https://www.tensorflow.org/addons/api_docs/python/tfa/layers/WeightNormalization"" rel=""nofollow noreferrer""><code>tfa.layers.WeightNormalization</code></a> wrapper around a <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/LocallyConnected2D"" rel=""nofollow noreferrer""><code>tf.layers.LocallyConnected2D</code></a> layer like so:</p>
<pre><code>from tensorflow_addons.layers import WeightNormalization
import tensorflow as tf

x = tf.ones((1, 32, 32, 3))
x = WeightNormalization(tf.keras.layers.LocallyConnected2D(3, 3))(x)
</code></pre>
<p>It gives the following error:</p>
<pre><code>TypeError: 'NoneType' object is not callable
</code></pre>
<p>For the record, this does work with a <code>Conv2D</code> layer. Any idea how to get this working with a <code>LocallyConnected2D</code> layer?</p>
",0
67747314,Finding precision and recall for the tutorial federated learning model on MNIST,"<p>I'm using this tutorial to try to learn how federated models work through TensorFlow's tutorial here: <a href=""https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynb</a></p>
<p>Currently, the model is defined like this which uses accuracy as its metric.</p>
<pre><code>def model_fn():
  keras_model = create_keras_model()
  return tff.learning.from_keras_model(
      keras_model,
      input_spec = preprocessed_example_dataset.element_spec,
      loss = tf.keras.losses.SparseCategoricalCrossentropy(),
      metrics = [tf.keras.metrics.SparseTopKCategoricalAccuracy()]
  )
</code></pre>
<p>I want to either use precision and recall as metrics, or find them after the model is trained, but I can't figure out how to do so.</p>
<p>I tried adding precision to metrics like this <code>metrics = [tf.keras.metrics.SparseTopKCategoricalAccuracy(), tf.keras.metrics.Precision()]</code> and run this code but it gives me an error.</p>
<pre><code>iterative_process = tff.learning.build_federated_averaging_process(
    model_fn,
    client_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate=0.01),
    server_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate=1.5))
</code></pre>
<p>Error output:</p>
<pre><code>ValueError                                Traceback (most recent call last)

&lt;ipython-input-13-f8ac3534e325&gt; in &lt;module&gt;()
      2     model_fn,
      3     client_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate=0.01),
----&gt; 4     server_optimizer_fn = lambda: tf.keras.optimizers.SGD(learning_rate=1.5))

ValueError: Shapes (None, 10) and (None, 1) are incompatible
</code></pre>
<p>Previously, I asked a similar question for a regular <a href=""https://stackoverflow.com/questions/67729973/finding-precision-and-recall-for-mnist-dataset-using-tensorflow/67730576#67730576"">centralized model here</a>, but I don't think I can use that same method since you can't get the results of the predictions back in the same way from what I've found.
I've also tried looking at other documentation <a href=""https://www.tensorflow.org/federated/tutorials/tff_for_federated_learning_research_compression#training_the_model_and_outputting_training_metrics"" rel=""nofollow noreferrer"">such as this</a>, but it also uses accuracy as the metric, so that wasn't helpful. How can I get the precision and recall of this federated model?</p>
",1
67830747,Difference between tf.data.TextLineDataset and tf.data.experimental.make_csv_dataset,"<p>Open a Google colab notebook and run below statements</p>
<pre><code>#
import tensorflow as tf
import pathlib
import os
dataset = tf.data.TextLineDataset('/content/sample_data/california_housing_test.csv')
dataset ## output is &lt;TextLineDatasetV2 shapes: (), types: tf.string&gt;
</code></pre>
<p>Then run below</p>
<pre><code>import tensorflow as tf
import pathlib
import os
dataset = tf.data.experimental.make_csv_dataset('/content/sample_data/california_housing_test.csv',batch_size=5)
dataset ## output is &lt;PrefetchDataset shapes: OrderedDict([(longitude, (5,)), (latitude, (5,)), (housing_median_age, (5,)), (total_rooms, (5,)), (total_bedrooms, (5,)), (population, (5,)), (households, (5,)), (median_income, (5,)), (median_house_value, (5,))]), types: OrderedDict([(longitude, tf.float32), (latitude, tf.float32), (housing_median_age, tf.float32), (total_rooms, tf.float32), (total_bedrooms, tf.float32), (population, tf.float32), (households, tf.float32), (median_income, tf.float32), (median_house_value, tf.float32)])&gt;
</code></pre>
<p>Clearly there is huge difference in the way tf.data.TextLineDataset and tf.data.experimental.make_csv_dataset handles text file. Why does tensorflow has these two one under experimental and other outside.</p>
",0
67947583,"Defining a callable ""loss"" function","<p>I am trying to optimize a loss function (defined using evidence lower bound) with <code>tf.train.AdamOptimizer.minimize()</code> on Tensorflow version <code>1.15.2</code> with eager execution enabled. I tried the following:</p>
<pre><code>learning_rate = 0.01
optim = tf.train.AdamOptimizer(learning_rate=learning_rate)
train_op = optim.minimize(loss)
</code></pre>
<p>and got the following : <code>RuntimeError: &quot;loss&quot; passed to Optimizer.compute_gradients should be a function when eager execution is enabled.</code></p>
<p>This works fine if I disable eager execution but since I need to save a tensorflow variable as a <code>numpy</code> array so I need eager execution enabled. The documentation mentions that when eager execution is enabled, the loss must be a <strong>callable</strong>. So the loss function should be defined in a way that it takes no inputs but gives out loss. I am not exactly sure how do I achieve such a thing.</p>
<p>I tried <code>train_op = optim.minimize(lambda: loss)</code> but got <code>ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [] and loss &lt;function &lt;lambda&gt; at 0x7f3c67a93b00&gt;</code></p>
",1
68058651,convert CSR format to dense/COO format in tensorflow,"<p><code>tf.sparse_to_dense()</code> fucntion in tensorflow only support <code>((data, (row_ind, col_ind)), [shape=(M, N)])</code> format. How can I convert standard CSR tensor <code>(((data, indices, indptr), [shape=(M, N)]))</code> to dense representation in tensorflow?</p>
<p>For example given, <code>data</code>, <code>indices</code> and <code>indptr</code> the function will return <code>dense</code> tensor.</p>
<p>e.g., inputs:</p>
<pre><code>indices = [1 3 3 0 1 2 2 3]
indptr = [0 2 3 6 8]
data = [2 4 1 3 2 1 1 5]
</code></pre>
<p>expected output:</p>
<pre><code>[[0, 2, 0, 4],
 [0, 0, 0, 1],
 [3, 2, 1, 0], 
 [0, 0, 1, 5]]
</code></pre>
<p>According to Scipy documentation, we can convert it back by the following:</p>
<blockquote>
<p>the column indices for row i are stored in indices[indptr[i]:indptr[i+1]] and their
corresponding values are stored in data[indptr[i]:indptr[i+1]].
If the shape parameter is not supplied, the matrix dimensions are
inferred from the index arrays.</p>
</blockquote>
",0
68151368,"ValueError: Input 0 of layer sequential_10 is incompatible with the layer: expected ndim=5, found ndim=4. Alexnet(cnn) + LSTM","<p>currently making  human action recognition to detect a cheating kind on exam from CCTV using AlexNet+LSTM</p>
<p>My Data are raw image in a each class folder with like <a href=""https://drive.google.com/drive/folders/1PVi6_U1cNseuZNsh0Eso9CF9XcvXIt-U?usp=sharing"" rel=""nofollow noreferrer"">this</a></p>
<p>but i got error like this</p>
<p>ValueError: in user code:</p>
<pre><code>/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:830 train_function  *
    return step_function(self, iterator)
/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:813 run_step  *
    outputs = model.train_step(data)
/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:770 train_step  *
    y_pred = self(x, training=True)
/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:989 __call__  *
    input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)
/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py:212 assert_input_compatibility  *
    raise ValueError('Input ' + str(input_index) + ' of layer ' +

ValueError: Input 0 of layer sequential_10 is incompatible with the layer: expected ndim=5, found ndim=4. Full shape received: (None, None, None, None)
</code></pre>
<p>The error come when i do the model.fit</p>
<p>from what i read it said that the problem at the input_shape but i am still doesnt found the solution of my problem
the link of my code at colab can be found <a href=""https://colab.research.google.com/drive/1Z2zCae8BRCbTqDk0J-R9hhjUCX6S3cHN?usp=sharing"" rel=""nofollow noreferrer"">here</a>,</p>
<p>i still dont understand what is the problem, i check the documentation for input_shape in TimeDistributed and it's the same for (timeSteps, height, width, channels)</p>
<p>is it from my ImageDataGenerator?or did i do wrong on making my model?</p>
<p>i would appreciate if anybody have the experience in this matter and try to help my problem</p>
<p>Thank you so much !</p>
",1
68175469,ValueError: setting an array element with a sequence. ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type Series),"<p>I want to extract signals from time series data for machine learning using tensorflow. I got this error when I try to run the program.</p>
<blockquote>
<p>/Users/renzha/Library/Application Support/JetBrains/PyCharmCE2021.1/scratches/pywavelet.py:38: DeprecationWarning: <code>np.float</code> is a deprecated alias for the builtin <code>float</code>. To silence this warning, use <code>float</code> by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use <code>np.float64</code> here.
Deprecated in NumPy 1.20; for more details and guidance: <a href=""https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"" rel=""nofollow noreferrer"">https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations</a>
X = np.asarray(X).astype(np.float)
Traceback (most recent call last):
File &quot;/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/series.py&quot;, line 129, in wrapper
raise TypeError(f&quot;cannot convert the series to {converter}&quot;)
TypeError: cannot convert the series to &lt;class 'float'&gt;
The above exception was the direct cause of the following exception:
Traceback (most recent call last):
File &quot;/Users/renzha/Library/Application Support/JetBrains/PyCharmCE2021.1/scratches/pywavelet.py&quot;, line 38, in 
X = np.asarray(X).astype(np.float)
ValueError: setting an array element with a sequence.</p>
</blockquote>
<p>However, if I use <code>X = np.array(X)</code>, then the error will be</p>
<blockquote>
<p>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type Series).</p>
</blockquote>
<p>I have tried <code>tf.convert_to_tensor(X)</code>, but it will return the same error.</p>
<p>The code is here:</p>
<pre><code>HBT_data = []
for i in range (0,10):
    files = glob.glob('/Users/renzha/Documents/work/preparing papers/non-contact HCI/s letter' + str(i) +'/*')

    for file in files:
        names = ['time', 'signal']
        data = pd.read_csv (file, names=names)
        data = data.drop (data.index [0])
        data = data.dropna (axis=0, how='any')
        x = data.iloc[:,1]
        filename = os.path.basename (file)
        label = os.path.splitext(filename)[0]
        labeledArray = [label, x]
        HBT_data.append(labeledArray)

HBT_data = pd.DataFrame(HBT_data, dtype=object)
y = HBT_data.iloc[:, 0]
y = np.asarray(y)
X = HBT_data.iloc[:, 1:]
X = np.asarray(X).astype(np.float) # tried but did not work
X = tf.convert_to_tensor (X) # tried but did not work

X_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.2, random_state=8)

X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_validation.reshape((X_validation.shape[0], X_validation.shape[1], 1))
num_classes = len(np.unique(Y_train))

idx = np.random.permutation(len(X_train))
x_train = X_train[idx]
y_train = X_train[idx]

Y_train[Y_train == -1] = 0
Y_validation[Y_validation == -1] = 0


def make_model(input_shape):
    input_layer = keras.layers.Input(input_shape)

    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=&quot;same&quot;)(input_layer)
    conv1 = keras.layers.BatchNormalization()(conv1)
    conv1 = keras.layers.ReLU()(conv1)

    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=&quot;same&quot;)(conv1)
    conv2 = keras.layers.BatchNormalization()(conv2)
    conv2 = keras.layers.ReLU()(conv2)

    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=&quot;same&quot;)(conv2)
    conv3 = keras.layers.BatchNormalization()(conv3)
    conv3 = keras.layers.ReLU()(conv3)

    gap = keras.layers.GlobalAveragePooling1D()(conv3)

    output_layer = keras.layers.Dense(num_classes, activation=&quot;softmax&quot;)(gap)

    return keras.models.Model(inputs=input_layer, outputs=output_layer)


model = make_model(input_shape=X_train.shape[1:])
keras.utils.plot_model(model, show_shapes=True)

epochs = 500
batch_size = 32

callbacks = [
    keras.callbacks.ModelCheckpoint(
        &quot;best_model.h5&quot;, save_best_only=True, monitor=&quot;val_loss&quot;
    ),
    keras.callbacks.ReduceLROnPlateau(
        monitor=&quot;val_loss&quot;, factor=0.5, patience=20, min_lr=0.0001
    ),
    keras.callbacks.EarlyStopping(monitor=&quot;val_loss&quot;, patience=50, verbose=1),
]
model.compile(
    optimizer=&quot;adam&quot;,
    loss=&quot;sparse_categorical_crossentropy&quot;,
    metrics=[&quot;sparse_categorical_accuracy&quot;],
)
history = model.fit(
    X_train,
    Y_train,
    batch_size=batch_size,
    epochs=epochs,
    callbacks=callbacks,
    validation_split=0.2,
    verbose=1,
)

model = keras.models.load_model(&quot;best_model.h5&quot;)

test_loss, test_acc = model.evaluate(X_validation, Y_validation)

print(&quot;Test accuracy&quot;, test_acc)
print(&quot;Test loss&quot;, test_loss)```

</code></pre>
",0
68354367,Getting an error when using tf.keras.metrics.Mean in functional Keras API,"<p>I'm trying to add a Mean metric to a Keras functional model (Tensorflow 2.5), and am getting the following error:</p>
<pre><code>ValueError: Expected a symbolic Tensor for the metric value, received: tf.Tensor(0.0, shape=(), dtype=float32)
</code></pre>
<p>Here is the code:</p>
<pre><code>x = [1, 2, 3, 4, 5, 6, 7, 8]
y = [5 + i * 3 for i in x]
a = Input(shape=(1,))
output = Dense(1)(a)
model = Model(inputs=a,outputs=output)
model.add_metric(tf.keras.metrics.Mean()(output))
model.compile(loss='mse')
model.fit(x=x, y=y, epochs=100)
</code></pre>
<p>If I remove the following line (from which the exception is thrown):</p>
<pre><code>model.add_metric(tf.keras.metrics.Mean()(output))
</code></pre>
<p>the code works as expected.
<br><br>I Tried disabling eager execution, but I get the following error instead:</p>
<pre><code>ValueError: Using the result of calling a `Metric` object when calling `add_metric` on a Functional Model is not supported. Please pass the Tensor to monitor directly.
</code></pre>
<p>The above usage was pretty much copied from the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Mean"" rel=""nofollow noreferrer"">tf.keras.metrics.Mean</a> documentation (see <em>Usage with compile() API</em>)</p>
",1
68379996,How to profile tensorflow memory?,"<p>No thanks to the wonderful <a href=""https://www.tensorflow.org/guide/profiler"" rel=""noreferrer"">tensorflow profiler documentation</a> displaying features rather than how the actual thing can be used. I'm experiencing a memory leak which I described earlier in my <a href=""https://stackoverflow.com/questions/68349724/memory-leak-that-persists-after-colab-cell-executes"">post</a>. I've been trying to find a way to profile memory of the things going on inside a <code>tf.function</code> ever since. But as it has always been the case, the documentation is as useless as never existing. For those who ever experienced memory leaks while using <code>tf.function</code>, is there a way to actually see what memory is allocated to which objects? Forget about tracemalloc, objgraph, memory-profiler which won't work, and of course the official tensorflow profiler which doesn't have a documentation.  Let's say the leak won't happen unless there is a <code>tf.function</code> somewhere, what are the options here?</p>
",1
68431633,tf.image.stateless_random_crop VS. tf.image.random_crop. Shouldn't these be the same thing?,"<p>In tf 2.5, there are two functions for cropping an image: <code>tf.image.stateless_random_crop</code>, and <code>tf.image.random_crop</code>. The documentation states that <code>stateless_random_crop</code> is deterministic (always returns the same crop given one seed). However, <code>random_crop</code> has a seed parameter and is also deterministic, one would think. What is the actual difference between these two functions? I cannot find information about statelessness in Tensorflow anywhere.</p>
<p>The differences between <code>tf.image.stateless_random_crop</code>, and <code>tf.image.random_crop</code> are one line where stateless_random_uniform is used instead of a random_uniform:
stateless_random_crop: <a href=""https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L415-L465"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L415-L465</a>
random_crop: <a href=""https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L360-L412"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L360-L412</a></p>
<p>I always thought that <code>random_crop</code> would always return the same crop given a seed, but it looks like maybe that wasn't always true? Any enlightenment about statelessness in Tensorflow is greatly appreciated!</p>
",1
68532412,"Tensorflow object detection uses CPU for training, but it detects and uses GPU at the beginning of script","<p>So I have run in to an issue while trying to follow this tutorial:
<a href=""https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html"" rel=""nofollow noreferrer"">https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html</a></p>
<p>While I am able to run the training script using CPU device, I simply cannot get it to work with my GPU. Specifically, it is the model_main_tf2.py (under the &quot;Training Custom Object Detector&quot; part of the tutorial) that is giving me issues. I have also added the line &quot;tf.debugging.set_log_device_placement(True)&quot; in the script to hopefully get at more comprihensive log.</p>
<p>Oddly enough, it detects the GPU in the beginning of the execution, and (as per my understanding), it uses the GPU for some tasks, but then at some point, it switches to the CPU without any errors... this part of the log is further down the post.</p>
<p>Some system, hardware and software specs:<br />
OS: Ubuntu 20.4<br />
GPU: GTX 1660 Ti Mobile<br />
nvidia-smi output: | NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |<br />
tensorflow-gpu version: 2.5.0<br />
tensorflow version: 2.5.0<br />
Python version: 3.9.5</p>
<p>Log where the script switches to CPU use:</p>
<pre><code>2021-07-26 17:00:34.027672: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-07-26 17:00:34.028022: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-07-26 17:00:34.028148: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-07-26 17:00:34.028357: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-07-26 17:00:34.028418: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-07-26 17:00:34.028506: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-07-26 17:00:34.028683: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-07-26 17:00:34.028741: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0
2021-07-26 17:00:34.028817: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0
WARNING:tensorflow:From /home/[user]/miniconda3/envs/tensorflowGPU4/lib/python3.9/site-packages/object_detection/model_lib_v2.py:557: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.
Instructions for updating:
rename to distribute_datasets_from_function
W0726 17:00:34.090738 140198244909888 deprecation.py:330] From /home/[user]/miniconda3/envs/tensorflowGPU4/lib/python3.9/site-packages/object_detection/model_lib_v2.py:557: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.
Instructions for updating:
rename to distribute_datasets_from_function
2021-07-26 17:00:34.091483: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op HashTableV2 in device /job:localhost/replica:0/task:0/device:CPU:0
2021-07-26 17:00:34.091597: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op LookupTableImportV2 in device /job:localhost/replica:0/task:0/device:CPU:0
2021-07-26 17:00:34.091910: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op HashTableV2 in device /job:localhost/replica:0/task:0/device:CPU:0
2021-07-26 17:00:34.091969: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op LookupTableImportV2 in device /job:localhost/replica:0/task:0/device:CPU:0
2021-07-26 17:00:34.092269: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op HashTableV2 in device /job:localhost/replica:0/task:0/device:CPU:0
2021-07-26 17:00:34.092327: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op LookupTableImportV2 in device /job:localhost/replica:0/task:0/device:CPU:0
2021-07-26 17:00:34.092609: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op HashTableV2 in device /job:localhost/replica:0/task:0/device:CPU:0
2021-07-26 17:00:34.092666: I tensorflow/core/common_runtime/eager/execute.cc:733] Executing op LookupTableImportV2 in device /job:localhost/replica:0/task:0/device:CPU:0
</code></pre>
<p>It would be highly appreciated if anyone could help. At this point, I have pretty much scoured the internet for anyone with the same issue without any luck :/</p>
<p><strong>EDIT:</strong><br />
Just an update on my issue. I've tried the exact same setup, with the same OS, graphics driver and CUDA versions and all, on my old computer with a GTX 1060, but I still get the same behavior. I would then guess, that it is just a configuration issue or a misunderstanding somewhere?</p>
",0
68536630,Is it possible to use a custom generator to train multi input architecture with keras tensorflow 2.0.0?,"<p>With TF 2.0.0, I can train an architecture with one input, I can train an architecture with one input using a custom generator, and I can train an architecture with two inputs. But I can't train an architecture with two inputs using a custom generator.</p>
<p>To keep it minimalist, here's a simple example, with no generator and no multiple inputs to start with:</p>
<pre><code>from tensorflow.keras import layers, models, Model, Input, losses
from numpy import random, array, zeros

input1 = Input(shape=2)
dense1 = layers.Dense(5)(input1)
fullModel = Model(inputs=input1, outputs=dense1)
fullModel.summary()

# Generate random examples:
nbSamples = 21
X_train = random.rand(nbSamples, 2)
Y_train = random.rand(nbSamples, 5)

batchSize = 4
fullModel.compile(loss=losses.LogCosh())
fullModel.fit(X_train, Y_train, epochs=10, batch_size=batchSize)
</code></pre>
<p>It's a simple dense layer which takes in input vectors of size 2. The randomly generated dataset contains 21 examples and the batch size is 4. Instead of loading all the data and giving them to <code>model.fit()</code>,  we can also give a custom generator in input. The main advantage (for RAM consumption) of this is to load only batch by batch rather that the whole dataset. Here is a simple example with the previous architecture and a custom generator:</p>
<pre><code>import json
# Save the last dataset in a file:
with open(&quot;./dataset1input.txt&quot;, 'w') as file:
    for i in range(nbSamples):
        example = {&quot;x&quot;: X_train[i].tolist(), &quot;y&quot;: Y_train[i].tolist()}
        file.write(json.dumps(example) + &quot;\n&quot;)

def generator1input(datasetPath, batch_size, inputSize, outputSize):
    X_batch = zeros((batch_size, inputSize))
    Y_batch = zeros((batch_size, outputSize))
    i=0
    while True:
        with open(datasetPath, 'r') as file:
            for line in file:
                example = json.loads(line)
                X_batch[i] = array(example[&quot;x&quot;])
                Y_batch[i] = array(example[&quot;y&quot;])
                i+=1
                if i % batch_size == 0:
                    yield (X_batch, Y_batch)
                    i=0

fullModel.compile(loss=losses.LogCosh())
my_generator = generator1input(&quot;./dataset1input.txt&quot;, batchSize, 2, 5)
fullModel.fit(my_generator, epochs=10, steps_per_epoch=int(nbSamples/batchSize))
</code></pre>
<p>Here, the generator opens the dataset file, but loads only batch_size examples (not nbSamples examples) each time it is called and slides into the file while looping.</p>
<p>Now, I can build a simple functional architecture with 2 inputs, and no generator:</p>
<pre><code>input1 = Input(shape=2)
dense1 = layers.Dense(5)(input1)
subModel1 = Model(inputs=input1, outputs=dense1)
input2 = Input(shape=3)
dense2 = layers.Dense(5)(input2)
subModel2 = Model(inputs=input2, outputs=dense2)
averageLayer = layers.average([subModel1.output, subModel2.output])
fullModel = Model(inputs=[input1, input2], outputs=averageLayer)
fullModel.summary()

# Generate random examples:
nbSamples = 21
X1 = random.rand(nbSamples, 2)
X2 = random.rand(nbSamples, 3)
Y = random.rand(nbSamples, 5)

fullModel.compile(loss=losses.LogCosh())
fullModel.fit([X1, X2], Y, epochs=10, batch_size=batchSize)
</code></pre>
<p>Until here, all models compile and run, but I'm not able to use a generator with the last architecture and its 2 inputs... By trying the following code (which should logically work in my opinion):</p>
<pre><code># Save data in a file:
with open(&quot;./dataset.txt&quot;, 'w') as file:
    for i in range(nbSamples):
        example = {&quot;x1&quot;: X1[i].tolist(), &quot;x2&quot;: X2[i].tolist(), &quot;y&quot;: Y[i].tolist()}
        file.write(json.dumps(example) + &quot;\n&quot;)

def generator(datasetPath, batch_size, inputSize1, inputSize2, outputSize):
    X1_batch = zeros((batch_size, inputSize1))
    X2_batch = zeros((batch_size, inputSize2))
    Y_batch = zeros((batch_size, outputSize))
    i=0
    while True:
        with open(datasetPath, 'r') as file:
            for line in file:
                example = json.loads(line)
                X1_batch[i] = array(example[&quot;x1&quot;])
                X2_batch[i] = array(example[&quot;x2&quot;])
                Y_batch[i] = array(example[&quot;y&quot;])
                i+=1
                if i % batch_size == 0:
                    yield ([X1_batch, X2_batch], Y_batch)
                    i=0

fullModel.compile(loss=losses.LogCosh())
my_generator = generator(&quot;./dataset.txt&quot;, batchSize, 2, 3, 5)
fullModel.fit(my_generator, epochs=10, steps_per_epoch=(nbSamples//batchSize))
</code></pre>
<p>I obtain the following error:</p>
<pre><code>File &quot;C:\Anaconda\lib\site-packages\tensorflow_core\python\keras\engine\training.py&quot;, line 729, in fit
    use_multiprocessing=use_multiprocessing)
File &quot;C:\Anaconda\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py&quot;, line 224, in fit
    distribution_strategy=strategy)
File &quot;C:\Anaconda\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py&quot;, line 547, in _process_training_inputs
    use_multiprocessing=use_multiprocessing)
File &quot;C:\Anaconda\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py&quot;, line 606, in _process_inputs
    use_multiprocessing=use_multiprocessing)
File &quot;C:\Anaconda\lib\site-packages\tensorflow_core\python\keras\engine\data_adapter.py&quot;, line 566, in __init__
    reassemble, nested_dtypes, output_shapes=nested_shape)
File &quot;C:\Anaconda\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py&quot;, line 540, in from_generator
    output_types, tensor_shape.as_shape, output_shapes)
File &quot;C:\Anaconda\lib\site-packages\tensorflow_core\python\data\util\nest.py&quot;, line 471, in map_structure_up_to
    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]
File &quot;C:\Anaconda\lib\site-packages\tensorflow_core\python\data\util\nest.py&quot;, line 471, in &lt;listcomp&gt;
    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]
File &quot;C:\Anaconda\lib\site-packages\tensorflow_core\python\framework\tensor_shape.py&quot;, line 1216, in as_shape
    return TensorShape(shape)
File &quot;C:\Anaconda\lib\site-packages\tensorflow_core\python\framework\tensor_shape.py&quot;, line 776, in __init__
    self._dims = [as_dimension(d) for d in dims_iter]
File &quot;C:\Anaconda\lib\site-packages\tensorflow_core\python\framework\tensor_shape.py&quot;, line 776, in &lt;listcomp&gt;
    self._dims = [as_dimension(d) for d in dims_iter]
File &quot;C:\Anaconda\lib\site-packages\tensorflow_core\python\framework\tensor_shape.py&quot;, line 718, in as_dimension
    return Dimension(value)
File &quot;C:\Anaconda\lib\site-packages\tensorflow_core\python\framework\tensor_shape.py&quot;, line 193, in __init__
    self._value = int(value)
TypeError: int() argument must be a string, a bytes-like object or a number, not 'tuple'
</code></pre>
<p>As explain in the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">doc</a>, <code>x</code> argument of <code>model.fit()</code> can be <code>A generator or keras.utils.Sequence returning (inputs, targets)</code>, and <code>The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively</code>. Thus, I think that it can not take in input more than one generator. Perhaps multiple inputs are not possible with custom generator. Please, would you have an explanation? A solution?</p>
<p>(otherwise, it seems possible to go through <code>tf.data.Dataset.from_generator()</code> with a less custom approach, but I have difficulties to understand what to indicate in the <code>output_signature</code> argument)</p>
<br/> 
<br/> 
<p>[<strong>EDIT</strong>] Thank you for your response @Francis Tang. In fact, it's possible to use a custom generator, but it allowed me to understand that I just had to change the line:</p>
<pre><code>yield ([X1_batch, X2_batch], Y_batch)
</code></pre>
<p>To:</p>
<pre><code>yield (X1_batch, X2_batch), Y_batch
</code></pre>
<p>Nevertheless, it is indeed perhaps better to use <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence"" rel=""nofollow noreferrer""><code>tf.keras.utils.Sequence</code></a>. But I find it a bit restrictive.
In particular, I understand in the example given (as well as in most of the examples I could find about <code>Sequence</code>) that <code>__init__()</code> is first used to load the full dataset, which is against the interest of the generator.
But maybe it was a particular example about Sequence(), and there is no need to use <code>__init__()</code> like that: you can directly read a file and load the desired batch into the <code>__getitem__()</code>.
In this case, it seems to push to browse each time the data file, or else it is necessary to create a file per batch beforehand (not really optimal).</p>
",0
68545187,Detect channels first/last of tensorflow saved model?,"<p>Is there any way to detect the channels first or last format for TF saved model loaded as <code>model=tf.saved_model.load(path)</code>?</p>
<p>In Keras and can go over <code>model.layers</code> and check for a layer l <code>l.data_format == 'channels_last'</code></p>
<p>Is there something like this for TF saved model? I can't find any suitable documentation of TF model details - everything goes back to Keras.</p>
",1
68622639,How is the AUC calculated for multi-class data in tensorflow?,"<p>The documentation for <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC"" rel=""nofollow noreferrer""><code>tf.keras.metrics.AUC</code></a> says that when estimating the AUC for multi-class data (i.e., <code>multi_label=False</code>),</p>
<blockquote>
<p>the data should be flattened into a single label before AUC computation.</p>
</blockquote>
<p>What exactly does this mean?</p>
<hr />
<p>Also, if you have multi-class data, it's possible to train a model without pre-flatting the labels, e.g., for a given <code>model</code>, you can run</p>
<pre class=""lang-py prettyprint-override""><code>model.compile(loss=&quot;binary_crossentropy&quot;, metrics=[tf.keras.metrics.AUC()])
</code></pre>
<p>and the model will calculate an AUC for you for each epoch. I know using binary cross-entropy loss in a multi-class problem tells tensorflow to setup a multi-label classification problem (see <a href=""https://stackoverflow.com/questions/47877083/keras-binary-crossentropy-categorical-crossentropy-confusion"">here</a>), but I haven't told <code>tf.keras.metrics.AUC</code> that the data is multilabel. So what exactly is it calculating in this case?</p>
",1
68638911,Ragged Tensors have no len() after conversion to Tensor,"<p>I am training a deep learning model on stacks of images with variable dimensions. <code>(Shape = [Batch, None, 256, 256, 1])</code>, where None can be variable.</p>
<p>I use  <code>tf.RaggedTensor.merge_dimsions(0,1)</code> to convert the ragged Tensor to a shape of <code>[None, 256, 256, 1]</code> to run into a pretrained keras CNN model.</p>
<p>However, using the KerasLayer API results in the following error: <code>TypeError: the object of type 'RaggedTensor' has no len()</code></p>
<p>When I apply <code>.merge_dimsions</code> outside of the KerasLayer and pass the tensors to the same pretrained model I do not get this error.</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

# Synthetic Data Pipeline
def synthetic_gen():
  varShape = tf.random.uniform((), minval=1, maxval=12, dtype=tf.int32)
  image = tf.random.normal((varShape, 256, 256, 1))
  image = tf.RaggedTensor.from_tensor(image, ragged_rank=1)
  yield image

ds = tf.data.Dataset.from_generator(synthetic_gen, output_signature=(tf.RaggedTensorSpec(shape=(None, 256, 256, 1), dtype=tf.float32, ragged_rank=1)))
ds = ds.repeat().batch(8)
print(next(iter(ds)).shape)

# Build Model
inputs = tf.keras.Input(
    type_spec=tf.RaggedTensorSpec(
        shape=(8, None, 256, 256, 1), 
        dtype=tf.float32, 
        ragged_rank=1))

ResNet50 = tf.keras.applications.ResNet50(
    include_top=True, 
    input_shape=(256, 256, 1),
    weights=None)

def merge(x):
  x = x.merge_dims(0, 1)
  return x
x = tf.keras.layers.Lambda(merge)(inputs)
merged_inputs = x
# x = ResNet50(x) # Uncommenting this will result in `model` producing an error when run for inference.

model = tf.keras.Model(inputs, x)

# Run inference
data = next(iter(ds))
model(data).shape # Will be an error if ResNet50 is used
</code></pre>
<p>Here is a colab notebook that demonstrates the problem. <a href=""https://colab.research.google.com/drive/1kN78mf4_oNqxWOluV054NlqmakC5msli?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1kN78mf4_oNqxWOluV054NlqmakC5msli?usp=sharing</a></p>
",0
68645231,Tokenize dataset using map on tf.data.Dataset.from_tensor_slices(....),"<p><strong>Note</strong>: I am using the free TPU provided on Kaggle.</p>
<p>I want to tokenize the text using transformers such that I tokenize only the batch while training the model instead of first tokenizing the whole dataset and then creating batches from the tokenized dataset as it flows OOM and is also inefficient. Below is a basic overview of what I want</p>
<pre><code>tokenizer = transformers.RobertaTokenizerFast.from_pretrained('roberta-base')

def tokenize(text, labels):
    tokenized = tokenizer(text, padding=True, truncation=True, max_length=MAX_LEN)
    ids = tokenized['input_ids']
    mask = tokenized['attention_mask']
    return (ids, mask), labels

train_dataset = tf.data.Dataset.from_tensor_slices((text, train_label_chunk)).batch(BATCH_SIZE)
train_dataset = train_dataset.map(tokenize)
</code></pre>
<p>Below is the error it gives. I won't share the whole trace as the error is pretty clear</p>
<pre><code>ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)
</code></pre>
<p>while should be solved by something like this</p>
<pre><code>for i in train_dataset:
    sample = i[0]
    break

sample.numpy()[0].decode()
</code></pre>
<p>which gives a proper string but decoding every single <code>tf.string</code> is not possible. Also, it gives an error anyway when I try this</p>
<pre><code>def tokenize(text, labels):
    text = text.numpy()
    tokenized = tokenizer(text, padding=True, truncation=True, max_length=MAX_LEN)
    ids = tokenized['input_ids']
    mask = tokenized['attention_mask']
    return (ids, mask), labels
</code></pre>
<p>error</p>
<pre><code>AttributeError: in user code:
    &lt;ipython-input-37-857b904b7110&gt;:2 tokenize  *
        text = text.numpy()
    AttributeError: 'Tensor' object has no attribute 'numpy'
</code></pre>
<p>I am not sure why is it there but in any case, this can't be done. The following GitHub trace can also be seen on the same topic <a href=""https://github.com/huggingface/transformers/issues/3851"" rel=""nofollow noreferrer"">here</a></p>
<p>Below are some other things that I tried. First I created a new dataset class</p>
<pre><code>class TrainDataset():
    def __init__(self, text, label, batch_size):
        self.text = text
        self.label = label
        self.batch_size = batch_size
        
    def __len__(self):
        return len(self.text) // self.batch_size
    
    def __getitem__(self, idx):
        text = self.text[idx*self.batch_size:(idx+1)*self.batch_size]
        label = self.label[idx*self.batch_size:(idx+1)*self.batch_size]
        return text, label

ds = TrainDataset()

def train_loop(train_dataset):
    with strategy.scope():
        for step, (x, y) in enumerate(train_dataset):
            train_data = tokenizer(x, padding=True, truncation=True, max_length=MAX_LEN, return_tensors='tf')
            inputs = (train_data['input_ids'], train_data['attention_mask'])
            with tf.GradientTape() as tape:
                preds = model(inputs, training=True)
                loss_value = loss_fun(y, preds)

            grads = tape.gradient(loss_value, model.trainable_weights)
            optimizer.apply_gradients(zip(grads, model.trainable_weights))
            break

train_loop(ds)
</code></pre>
<p>which yields the following error</p>
<pre><code>ValueError: Please use `tf.keras.losses.Reduction.SUM` or `tf.keras.losses.Reduction.NONE` for loss reduction when losses are used with `tf.distribute.Strategy` outside of the built-in training loops. You can implement `tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE` using global batch size like:
</code></pre>
<p>with strategy.scope():
loss_obj = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)
....
loss = tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size)</p>
<pre><code>Please see https://www.tensorflow.org/tutorials/distribute/custom_training for more details.
</code></pre>
<p>After which, I changed <code>loss_fun</code> to <code>loss_object</code> as below (Also changed the activation of the last layer to get the logits)</p>
<pre><code>loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)
</code></pre>
<p>which gives the below mentioned error</p>
<pre><code>RuntimeError: `apply_gradients() cannot be called in cross-replica context. Use `tf.distribute.Strategy.run` to enter replica context.
</code></pre>
<p>At this point I wrote all custom function</p>
<pre><code>def train_step(inputs):
    x, y = inputs
    with tf.GradientTape() as tape:
        predictions = model(x, training=True)
        loss = compute_loss(y, predictions)

    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    train_accuracy.update_state(y, predictions)
    return loss 

@tf.function
def distributed_train_step(dataset_inputs):
    x, y = dataset_inputs
    train_data = tokenizer(x, padding=True, truncation=True, max_length=MAX_LEN, return_tensors='tf')
    inputs = (train_data['input_ids'], train_data['attention_mask'])
    dataset_inputs = (inputs, y)
    per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))
    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,
                         axis=None)

for epoch in range(2):
    # TRAIN LOOP
    total_loss = 0.0
    num_batches = 0
    for x in tqdm(ds):
        total_loss += distributed_train_step(x)
        num_batches += 1
    train_loss = total_loss / num_batches
    
    template = (&quot;Epoch {}, Loss: {}, Accuracy: {}&quot;)
    print(template.format(epoch+1, train_loss, train_accuracy.result()*100))

    train_accuracy.reset_states()
</code></pre>
<p>which fortunately did ran but gave the below error</p>
<pre><code>StagingError                              Traceback (most recent call last)
&lt;ipython-input-24-2cda132cf9fa&gt; in &lt;module&gt;
      4     num_batches = 0
      5     for x in tqdm(ds):
----&gt; 6         total_loss += distributed_train_step(x)
      7         num_batches += 1
      8     train_loss = total_loss / num_batches

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    826     tracing_count = self.experimental_get_tracing_count()
    827     with trace.Trace(self._name) as tm:
--&gt; 828       result = self._call(*args, **kwds)
    829       compiler = &quot;xla&quot; if self._experimental_compile else &quot;nonXla&quot;
    830       new_tracing_count = self.experimental_get_tracing_count()

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    860       # In this case we have not created variables on the first call. So we can
    861       # run the first trace but we should fail if variables are created.
--&gt; 862       results = self._stateful_fn(*args, **kwds)
    863       if self._created_variables:
    864         raise ValueError(&quot;Creating variables on a non-first call to a function&quot;

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   2939     with self._lock:
   2940       (graph_function,
-&gt; 2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)
   2942     return graph_function._call_flat(
   2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3359 
   3360           self._function_cache.missed.add(call_context_key)
-&gt; 3361           graph_function = self._create_graph_function(args, kwargs)
   3362           self._function_cache.primary[cache_key] = graph_function
   3363 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3204             arg_names=arg_names,
   3205             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 3206             capture_by_value=self._capture_by_value),
   3207         self._function_attributes,
   3208         function_spec=self.function_spec,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    988         _, original_func = tf_decorator.unwrap(python_func)
    989 
--&gt; 990       func_outputs = python_func(*func_args, **func_kwargs)
    991 
    992       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    632             xla_context.Exit()
    633         else:
--&gt; 634           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    635         return out
    636 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    975           except Exception as e:  # pylint:disable=broad-except
    976             if hasattr(e, &quot;ag_error_metadata&quot;):
--&gt; 977               raise e.ag_error_metadata.to_exception(e)
    978             else:
    979               raise

StagingError: in user code:

    &lt;ipython-input-19-9d8bdb5f7f7c&gt;:4 distributed_train_step  *
        train_data = tokenizer(x, padding=True, truncation=True, max_length=MAX_LEN, return_tensors='tf')
    /opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2305 __call__  *
        **kwargs,
    /opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2490 batch_encode_plus  *
        **kwargs,
    /opt/conda/lib/python3.7/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:163 _batch_encode_plus  *
        return super()._batch_encode_plus(*args, **kwargs)
    /opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py:418 _batch_encode_plus  *
        for key in tokens_and_encodings[0][0].keys():

    IndexError: list index out of range
</code></pre>
<p>The <code>IndexError: list index out of range</code> might be solved but the speed of training was really really slow and hence I think something is wrong.
At this point, any help will be highly appreciated.</p>
",0
68839011,Python/Keras: LeakyRelu using tensorflow,"<p>I am having problems installing keras. The following are giving me too much trouble to get around (even when doing updates on the terminal):</p>
<pre><code>from keras.layers import Dense, Activation
from keras.models import Sequential
</code></pre>
<p>So instead of initialising a ANN with <code>ann = Sequential()</code>, I do <code>ann = tf.keras.models.Sequential()</code>. This by importing:</p>
<pre><code>import tensorflow as tf
from tensorflow import keras
</code></pre>
<p>I would like to use LeakyReLU as an activation function. However, this one seems to be different to implement and the keras documentation is not helping me that much compared to how others tend to do.</p>
<p>I've seen that ann.add(LeakyReLU(alpha=0.05)) is needed. However, what about the other parameters like unit or input_dim? How can I implement this using my code?</p>
<pre><code># Initialising the ANN
ann = tf.keras.models.Sequential()

# Adding the input layer and the first hidden layer
ann.add(tf.keras.layers.Dense(units=32, activation='relu'))

# Adding the second hidden layer
ann.add(tf.keras.layers.Dense(units=32, activation='relu'))

# Adding the output layer
ann.add(tf.keras.layers.Dense(units=1))
</code></pre>
",0
68878231,tf.gradients() vs tf.gradientTape.gradient() in graph mode,"<p>I had a question regarding the behavior of tf.gradients() as opposed tf.gradientTape.gradient() in graph mode.</p>
<p>Given a differentiable function y = f(x), where x and y are each single tensorflow tensors, is there any difference between the behavior of tf.gradient(y, x) vs tape.gradient(y, x) where tape is an instance of tf.gradientTape (assuming the use of graph mode) ?</p>
<p>Not sure why tensorflow has two different gradient methods which can be used with graph mode - maybe there are some subtle differences in the implementations? I’ve looked at the documentation for gradientTape and tf.gradients but it’s not clear whether there is any difference between the behavior of these methods for a single (x, y) pair, or whether it’s just that tf.gradients() can be used in this case for a speedup when using graph mode.</p>
<p>Thank you so much for your help!</p>
",1
68958028,"TensorFlow image processing error: ""TypeError: 'MapDataset' object is not subscriptable""","<p>I am trying to follow this guide here on semantic segmentation : <a href=""https://yann-leguilly.gitlab.io/post/2019-12-14-tensorflow-tfdata-segmentation/"" rel=""nofollow noreferrer"">https://yann-leguilly.gitlab.io/post/2019-12-14-tensorflow-tfdata-segmentation/</a> and I'm stuck at building a pipeline.</p>
<p>I am running into an error when I'm applying a script to resize and rotate images from the training set. The transformation is defined in the function <code>load_image_train()</code>. The error reads :</p>
<pre><code>raise errors.OperatorNotAllowedInGraphError(
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert 
this function. This might indicate you are trying to use an unsupported feature.
</code></pre>
<p>Tracing the error in <code>load_image_train()</code>, I am stuck at running this line:
<code>input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))</code></p>
<p>Here <code>datapoint</code> is a <code>dict</code> item of four <code>tf.Dataset</code>s:</p>
<pre><code>&gt;&gt;&gt; dataset
{'train': &lt;MapDataset shapes: {image: (None, None, 3), segmentation_mask: (None, None, 1)}, types: {image: tf.uint8, segmentation_mask: tf.uint8}&gt;, 'val': &lt;MapDataset shapes: {image: (None, None, 3), segmentation_mask: (None, None, 1)}, types: {image: tf.uint8, segmentation_mask: tf.uint8}&gt;} 
</code></pre>
<p>I can subset <code>dataset['train']</code> but I can't subset further. subsetting <code>dataset['train']['image']</code> is not allowed for <code>MapDataset</code> objects:</p>
<pre><code>&gt;&gt;&gt; dataset['train']['image']  
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
TypeError: 'MapDataset' object is not subscriptable
</code></pre>
<p>I suspect this is where the error comes from. So how can I go about choosing only the <code>image</code> from the training set and apply the transformations? Thanks!</p>
",0
68984841,How can I understand the kernel of tf.keras.layers.Dense for rank >2?,"<p>How can I understand the kernel of <code>tf.keras.layers.Dense</code> for rank &gt;2?</p>
<p>The official API doc states that:</p>
<blockquote>
<p>Note: If the input to the layer has a rank greater than 2, then Dense
computes the dot product between the inputs and the kernel along the
last axis of the inputs and axis 0 of the kernel (using tf.tensordot).
For example, if input has dimensions (batch_size, d0, d1), then we
create a kernel with shape (d1, units), and the kernel operates along
axis 2 of the input, on every sub-tensor of shape (1, 1, d1) (there
are batch_size * d0 such sub-tensors). The output in this case will
have shape (batch_size, d0, units).</p>
</blockquote>
<p>My understanding is that for a rank larger than 2 (for example rank 3) only <strong>one</strong> kernel is created and thus the same kernel is applied on all slices of the second dimension, like above.
That would consequently mean that the outputs for different indices of the second dimension are <strong>not independent</strong> of each other (especially during training).</p>
<p>Is my understanding correct? And if yes, is there a simple way to use a stack of kernels instead or do I have to implement the tensor multiplication?</p>
",1
69136518,"Tensorflow 2 getting ""WARNING:tensorflow:x out of the last x calls to <function> triggered tf.function retracing.""","<p>I'm working on a project where I have trained a series of binary classifiers with <strong>Keras</strong>, with <strong>Tensorflow</strong> as the backend engine. The input data I have is a series of images, where each binary classifier must make the prediction on the images, later I save the predictions on a CSV file.</p>
<p>The problem I have is when I get the predictions from the first series of binary classifiers there isn't any warning, but when the 5th or 6th binary classifier calls the method <strong>predict</strong> on the input data I get the following warning:</p>
<blockquote>
<p>WARNING:tensorflow:5 out of the last 5 calls to &lt;function
Model.make_predict_function..predict_function at
0x2b280ff5c158&gt; triggered tf.function retracing. Tracing is expensive
and the excessive number of tracings could be due to (1) creating
@tf.function repeatedly in a loop, (2) passing tensors with different
shapes, (3) passing Python objects instead of tensors. For (1), please
define your @tf.function outside of the loop. For (2), @tf.function
has experimental_relax_shapes=True option that relaxes argument shapes
that can avoid unnecessary retracing. For (3), please refer to
<a href=""https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args</a>
and <a href=""https://www.tensorflow.org/api_docs/python/tf/function"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/function</a> for  more
details.</p>
</blockquote>
<p>To answer each point in the parenthesis, here are my answers:</p>
<ol>
<li>The <strong>predict</strong> method is called inside a for loop.</li>
<li>I don't pass tensors but a list of <strong>NumPy arrays</strong> of gray scale images, all of them with the same size in width and height. The only thing that can change is the batch size because the list can have only 1 image or more than one.</li>
<li>As I wrote in point 2, I pass a list of NumPy arrays.</li>
</ol>
<p>I have debugged my program and found that this warning always happens when the method predict is called. To summarize the code I have written is the following:</p>
<pre><code>import cv2 as cv
import tensorflow as tf
from tensorflow.keras.models import load_model
# Load the models
binary_classifiers = [load_model(path) for path in path2models]
# Get the images
images = [#Load the images with OpenCV]
# Apply the resizing and reshapes on the images.
my_list = list()
for image in images:
    image_reworked = # Apply the resizing and reshaping on images
    my_list.append(image_reworked)

# Get the prediction from each model
# This is where I get the warning
predictions = [model.predict(x=my_list,verbose=0) for model in binary_classifiers]
</code></pre>
<h3>What I have tried</h3>
<p>I have defined a function as tf.function and putted the code of the predictions inside the tf.function like this</p>
<pre><code>@tf.function
def testing(models, faces):
    return [model.predict(x=faces,verbose=0) for model in models]
</code></pre>
<p>But I ended up getting the following error:</p>
<blockquote>
<p>RuntimeError: Detected a call to <code>Model.predict</code> inside a
<code>tf.function</code>. Model.predict is a high-level endpoint that manages
its own <code>tf.function</code>. Please move the call to <code>Model.predict</code> outside
of all enclosing <code>tf.function</code>s. Note that you can call a <code>Model</code>
directly on Tensors inside a <code>tf.function</code> like: <code>model(x)</code>.</p>
</blockquote>
<p>So calling the method <code>predict</code> is basically already a tf.function. So it's useless to define a tf.function when the warning I get it's from that method.</p>
<p>I have also checked those other two questions:</p>
<ol>
<li><a href=""https://stackoverflow.com/questions/61647404/tensorflow-2-getting-warningtensorflow9-out-of-the-last-9-calls-to-function"">Tensorflow 2: Getting &quot;WARNING:tensorflow:9 out of the last 9 calls to  triggered tf.function retracing. Tracing is expensive&quot;</a></li>
<li><a href=""https://stackoverflow.com/questions/65563185/loading-multiple-saved-tensorflow-keras-models-for-prediction"">Loading multiple saved tensorflow/keras models for prediction</a></li>
</ol>
<p>But neither of the two questions answers my question about how to avoid this warning. Plus I have also checked the links in the warning message but I couldn't solve my problem.</p>
<h3>What I want</h3>
<p>I simply want to avoid this warning. While I'm still getting the predictions from the models I noticed that the python program takes way too much time on doing predictions for a list of images.</p>
<h3>What I'm using</h3>
<ul>
<li>Python 3.6.13</li>
<li>Tensorflow 2.3.0</li>
</ul>
<h3>Solution</h3>
<p>After some tries to suppress the warning from the <code>predict</code> method, I have checked the documentation of Tensorflow and in one of the first tutorials on how to use Tensorflow it is explained that, by default, Tensorflow is executed in eager mode, which is useful for testing and debugging the network models. Since I have already tested my models many times, it was only required to disable the eager mode by writing this single python line of code:</p>
<p><code>tf.compat.v1.disable_eager_execution()</code></p>
<p>Now the warning doesn't show up anymore.</p>
",0
69146074,tensorflow dataset splitting by participant,"<p>I want to split a <code>tf.data.DataSet</code> on attributes, in my case participant or gesture. Currently, the dataset is a work in progress and the number of participants/gestures may grow. I originally set up a <a href=""https://github.com/tensorflow/datasets/pull/3482"" rel=""nofollow noreferrer"">tfds config</a> for this <a href=""https://tev.fbk.eu/technologies/smartwatch-gestures-dataset"" rel=""nofollow noreferrer"">gestures-dataset</a>, but I haven't figured out how to configure participant/gesture splitting here either.</p>
<p>How should I split the <code>tf.data.DataSet</code> object? Currently, my data set exists as a single tf_record. I would prefer to keep it that way instead of generating different files for each participant and gesture and have to regenerate all the gesture tf-records when a new participant is added.</p>
<p>this works (method 1, gross):</p>
<pre><code>subset = ds.filter(lambda x: (x['participant'] == 1 or x['participant'] == 2))
</code></pre>
<p>this doesn't (method 2, dream):</p>
<pre><code>subset = ds.filter(lambda x: any(x['participant'] == p for p in [1,2]))
</code></pre>
<blockquote>
<p>OperatorNotAllowedInGraphError: using a <code>tf.Tensor</code> as a Python <code>bool</code> is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.</p>
</blockquote>
<p>I also attempted the same op as a decorated @tf.function.</p>
<p>example code with publicly available mnist dataset: <a href=""https://colab.research.google.com/drive/10e8rWbbqvLL1thDU3wtxfLWwDNiPUzQR?usp=sharing"" rel=""nofollow noreferrer"">juptyer notebook on colab</a></p>
<p><strong>Is their a way to perform this operation in a similar style to method 2?</strong></p>
",0
69174182,tensorflow serving grpc regress request,"<p>Currently making rest request to tf_serving using the Classify and Regress API (see <a href=""https://www.tensorflow.org/tfx/serving/api_rest"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tfx/serving/api_rest</a>).</p>
<pre><code>{
  &quot;signature_name&quot;: &lt;string&gt;,
  // Optional: Common context shared by all examples.
  // Features that appear here MUST NOT appear in examples (below).
  &quot;context&quot;: {
    &quot;&lt;feature_name3&gt;&quot;: &lt;value&gt;|&lt;list&gt;
    &quot;&lt;feature_name4&gt;&quot;: &lt;value&gt;|&lt;list&gt;
  },
  // List of Example objects
  &quot;examples&quot;: [
    {
      &quot;&lt;feature_name1&gt;&quot;: &lt;value&gt;|&lt;list&gt;,
      &quot;&lt;feature_name2&gt;&quot;: &lt;value&gt;|&lt;list&gt;,
    },
    {
      &quot;&lt;feature_name1&gt;&quot;: &lt;value&gt;|&lt;list&gt;,
      &quot;&lt;feature_name2&gt;&quot;: &lt;value&gt;|&lt;list&gt;,
    }
  ]
}

</code></pre>
<p>I want to make the same request through grpc. But all documentations I can find are using the prediction API and the input are <code>&quot;inputs&quot;</code> or <code>&quot;instances&quot;</code>.
I'm having trouble using the <code>tf.make_tensor_proto</code> function to construct the request. I want to keep using the <code>context</code> <code>example</code> format if possible. The documentation is sparse on this. Any help would be appreciated.</p>
",1
69200644,deprication of tf.contrib.rnn.LayerNormBasicLSTMCell,"<p>From the <a href=""https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib"" rel=""nofollow noreferrer"">documentation</a> I can see that tf.contrib has been deprecated for tensorflow 2.
However, the <a href=""https://www.tensorflow.org/beta/guide/migration_guide"" rel=""nofollow noreferrer"">Migration Guide</a> page isn't found.</p>
<p>I'm essentially trying to do write an rnn in tensorflow 2.1.
Are there any similar keras libraries? or will I have to downgrade to tensorflow 1 (even though tensorflow is really telling us to start adopting v2.x)</p>
",0
69304833,"Loading a tensorflow.keras trained model using load_model returns JSON decode error, while untrained model loads normally","<p>I have a trained Keras model built and trained using the tensorflow.keras API and saved using the <code>tf.keras.save_model()</code> method with no optional arguments. Tensorflow is up to date and my Python version is 3.8. From my understanding, this method should save the model using the default &quot;tf&quot; format, which is recommended in TF 2.X, and then using <code>load_model()</code> should work fine.</p>
<p>Loading the model again, however, produces the following:</p>
<pre><code>model = tf.keras.models.load_model(&quot;/Volumes/thesis_drive/thesis_project_local_new/trained_model_640x64/&quot;)

---------------------------------------------------------------------------
JSONDecodeError                           Traceback (most recent call last)
/var/folders/c1/8tq0t8y90195qxyt5hppjjtr0000gq/T/ipykernel_933/1131710361.py in &lt;module&gt;
----&gt; 1 model = tf.keras.models.load_model(&quot;/Volumes/thesis_drive/thesis_project_local_new/trained_model_640x64/&quot;)

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/saving/save.py in load_model(filepath, custom_objects, compile, options)
    204         filepath = path_to_string(filepath)
    205         if isinstance(filepath, str):
--&gt; 206           return saved_model_load.load(filepath, compile, options)
    207 
    208   raise IOError(

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/saving/saved_model/load.py in load(path, compile, options)
    153 
    154   # Finalize the loaded layers and remove the extra tracked dependencies.
--&gt; 155   keras_loader.finalize_objects()
    156   keras_loader.del_tracking()
    157 

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/saving/saved_model/load.py in finalize_objects(self)
    624 
    625     # Initialize graph networks, now that layer dependencies have been resolved.
--&gt; 626     self._reconstruct_all_models()
    627 
    628   def _unblock_model_reconstruction(self, layer_id, layer):

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/saving/saved_model/load.py in _reconstruct_all_models(self)
    643       all_initialized_models.add(model_id)
    644       model, layers = self.model_layer_dependencies[model_id]
--&gt; 645       self._reconstruct_model(model_id, model, layers)
    646       _finalize_config_layers([model])
    647 

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/saving/saved_model/load.py in _reconstruct_model(self, model_id, model, layers)
    659   def _reconstruct_model(self, model_id, model, layers):
    660     &quot;&quot;&quot;Reconstructs the network structure.&quot;&quot;&quot;
--&gt; 661     config = json_utils.decode(
    662         self._proto.nodes[model_id].user_object.metadata)['config']
    663 

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/saving/saved_model/json_utils.py in decode(json_string)
     60 
     61 def decode(json_string):
---&gt; 62   return json.loads(json_string, object_hook=_decode_helper)
     63 
     64 

~/miniforge3/lib/python3.9/json/__init__.py in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)
    357     if parse_constant is not None:
    358         kw['parse_constant'] = parse_constant
--&gt; 359     return cls(**kw).decode(s)

~/miniforge3/lib/python3.9/json/decoder.py in decode(self, s, _w)
    335 
    336         &quot;&quot;&quot;
--&gt; 337         obj, end = self.raw_decode(s, idx=_w(s, 0).end())
    338         end = _w(s, end).end()
    339         if end != len(s):

~/miniforge3/lib/python3.9/json/decoder.py in raw_decode(self, s, idx)
    353             obj, end = self.scan_once(s, idx)
    354         except StopIteration as err:
--&gt; 355             raise JSONDecodeError(&quot;Expecting value&quot;, s, err.value) from None
    356         return obj, end

JSONDecodeError: Expecting value: line 1 column 1 (char 0)
</code></pre>
<p>To test whether this is an error with <code>save_model()</code> or <code>load_model()</code>, I built the same model again in a Jupyter notebook, saved it, and reloaded it with no error:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, Conv2D, MaxPooling2D, Input
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras import optimizers
from tensorflow.keras.models import Model

def build_model():
    _input = Input(shape=(640,64,3))
    x = Conv2D(filters=64, kernel_size=4, input_shape=(640, 64, 3))(_input)
    x = Activation('relu')(x)
    x = MaxPooling2D(pool_size=(4, 4))(x)
    x = Dropout(0.5)(x)
    x = Conv2D(filters=128, kernel_size=4, input_shape=(640, 64, 3))(_input)
    x = Activation('relu')(x)
    x = MaxPooling2D(pool_size=(4, 4))(x)
    x = Dropout(0.5)(x)
    x = Conv2D(filters=256, kernel_size=4, input_shape=(640, 64, 3))(_input)
    x = Activation('relu')(x)
    x = MaxPooling2D(pool_size=(2, 2))(x)
    x = Dropout(0.5)(x)
    x = Flatten()(x)
    output = Dense(161, activation = 'softmax')(x)
    model = Model(_input,output)
    model.compile(optimizer=optimizers.Adam(), loss=&quot;categorical_crossentropy&quot;)
    tf.keras.models.save_model(model,&quot;model_test&quot;)

model = build_model()

Metal device set to: Apple M1
2021-09-23 13:40:46.234438: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2021-09-23 13:40:46.234631: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -&gt; physical PluggableDevice (device: 0, name: METAL, pci bus id: &lt;undefined&gt;)
2021-09-23 13:40:47.112730: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
INFO:tensorflow:Assets written to: model_test/assets

del model
model = tf.keras.models.load_model(&quot;model_test&quot;)
</code></pre>
<p>Further details: the model was trained on another machine (a supercomputer I have access to through my university) running Linux, and transferred to my Apple M1 machine via SCP, where it is now exhibiting this loading error.</p>
<p>I don't know why the JSON module is being called - there doesn't appear to be a JSON file anywhere in the directory. However, given that rebuilding the model without training and loading it produced no error, I am suspicious that the save did not execute correctly.</p>
",0
69421344,"How to call the BatchNormalization layer with available statistics (mean , var , beta, gamma)?","<p>I am implementing a neural network in tf-keras, where I have to use <code>BatchNormalization</code> layer, preferably using keras but instead of training I am using it for inference but highly confused about how to provide the available statistics to <code>tf.keras.BatchNormalization</code> call.<br />
As I understood from various sources and official documentation, there is no way of providing the data explicitly to <code>tf.keras.BatchNormalization</code> call and if you desire to use it in inference mode then you have to train using the same call and making <code>training = true</code>.</p>
",1
69452672,"Does ""tf.keras.losses.SparseCategoricalCrossentropy()"" work for all classification problems?","<p>For <code>tf.keras.losses.SparseCategoricalCrossentropy()</code>, the documentation of TensorFlow says</p>
<p><code>&quot;Use this crossentropy loss function when there are two or more label classes.&quot;</code></p>
<p>Since it covers two or more labels, including binary classification, then does it mean I can use this loss function for any classification problem? When do I <strong>have to</strong> use those binary loss such as <code>tf.keras.losses.BinaryCrossentropy</code> and similar ones?</p>
<p>I am using TensorFlow 2.3.1</p>
",1
69454217,Op type not registered \'IO>BigQueryClient\' with BigQuery connector on AI platform,"<p>I'm trying to parallelize the training step of my model with tensorflow <code>ParameterServerStrategy</code>. I work with GCP <code>AI Platform</code> to create the cluster and launch the task.
As my dataset is huge, I use the bigquery tensorflow connector included in <code>tensorflow-io</code>.</p>
<p>My script is inspired by the <a href=""https://colab.research.google.com/github/tensorflow/io/blob/master/docs/tutorials/bigquery.ipynb#scrollTo=4_NlkxZt1rwR"" rel=""nofollow noreferrer"">documentation of tensorflow bigquery reader</a> and the <a href=""https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/distribute/parameter_server_training.ipynb#scrollTo=gmPvactfa6Eh"" rel=""nofollow noreferrer"">documentation of tensorflow ParameterServerStrategy</a></p>
<p>Locally my script works well but when I launch it with AI Platform I get the following error :</p>
<p><code>{&quot;created&quot;:&quot;@1633444428.903993309&quot;,&quot;description&quot;:&quot;Error received from peer ipv4:10.46.92.135:2222&quot;,&quot;file&quot;:&quot;external/com_github_grpc_grpc/src/core/lib/surface/call.cc&quot;,&quot;file_line&quot;:1056,&quot;grpc_message&quot;:&quot;Op type not registered \'IO&gt;BigQueryClient\' in binary running on gke-cml-1005-141531--n1-standard-16-2-644bc3f8-7h8p. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.&quot;,&quot;grpc_status&quot;:5}</code></p>
<p>The scripts works with fake data on AI platform and works locally with bigquery connector.
I imagine that the compilation of the model including the bigquery connector and its calls on other devices creates the bug but I don't know how to fix it.</p>
<p>I read this error happens when devices don't have same tensorflow versions so I checked tensorflow and tensorflow-io version on each device.</p>
<p><strong>tensorflow : 2.5.0</strong></p>
<p><strong>tensorflow-io : 0.19.1</strong></p>
<p>I created a similar example which reproduce the bug on AI platform</p>
<pre><code>import os
from tensorflow_io.bigquery import BigQueryClient
from tensorflow_io.bigquery import BigQueryReadSession
import tensorflow as tf

import multiprocessing
import portpicker
from tensorflow.keras.layers.experimental import preprocessing

from google.cloud import bigquery

from tensorflow.python.framework import dtypes

import numpy as np
import pandas as pd

client = bigquery.Client()

PROJECT_ID = &lt;your_project&gt;
DATASET_ID = 'tmp'
TABLE_ID = 'bq_tf_io'

BATCH_SIZE = 32

# Bigquery requirements
def init_bq_table():
    table = '%s.%s.%s' %(PROJECT_ID, DATASET_ID, TABLE_ID)
    # Create toy_data
    def create_toy_data(N):
        x = np.random.random(size = N)
        y = 0.2 + x + np.random.normal(loc=0, scale = 0.3, size = N)
        return x, y
    x, y =create_toy_data(1000)
    df = pd.DataFrame(data = {'x': x, 'y': y})

    job_config = bigquery.LoadJobConfig(write_disposition=&quot;WRITE_TRUNCATE&quot;,)
    job = client.load_table_from_dataframe( df, table, job_config=job_config )
    job.result()

# Create initial data
#init_bq_table()

CSV_SCHEMA = [
      bigquery.SchemaField(&quot;x&quot;, &quot;FLOAT64&quot;),
      bigquery.SchemaField(&quot;y&quot;, &quot;FLOAT64&quot;),
  ]

def transform_row(row_dict):
  # Trim all string tensors
  dataset_x = row_dict
  dataset_x['constant'] = tf.cast(1, tf.float64)
  # Extract feature column
  dataset_y = dataset_x.pop('y')

  #Export as tensor
  dataset_x = tf.stack([dataset_x[column] for column in dataset_x], axis=-1)

  return (dataset_x, dataset_y)

def read_bigquery(table_name):
  tensorflow_io_bigquery_client = BigQueryClient()
  read_session = tensorflow_io_bigquery_client.read_session(
      &quot;projects/&quot; + PROJECT_ID,
      PROJECT_ID, TABLE_ID, DATASET_ID,
      list(field.name for field in CSV_SCHEMA),
      list(dtypes.double if field.field_type == 'FLOAT64'
           else dtypes.string for field in CSV_SCHEMA),
      requested_streams=2)

  dataset = read_session.parallel_read_rows()
  return dataset

def get_data():
    dataset = read_bigquery(TABLE_ID)
    dataset = dataset.map(transform_row, num_parallel_calls=4)
    dataset = dataset.batch(BATCH_SIZE).prefetch(2)
    return dataset

cluster_resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()

# parameter server and worker just wait jobs from the coordinator (chief)
if cluster_resolver.task_type in (&quot;worker&quot;):
    worker_config = tf.compat.v1.ConfigProto()

    server = tf.distribute.Server(
        cluster_resolver.cluster_spec(),
        job_name=cluster_resolver.task_type,
        task_index=cluster_resolver.task_id,
        config=worker_config,
        protocol=&quot;grpc&quot;)
    server.join()

elif cluster_resolver.task_type in (&quot;ps&quot;):
    server = tf.distribute.Server(
        cluster_resolver.cluster_spec(),
        job_name=cluster_resolver.task_type,
        task_index=cluster_resolver.task_id,
        protocol=&quot;grpc&quot;)
    server.join()

elif cluster_resolver.task_type == 'chief':
    strategy = tf.distribute.experimental.ParameterServerStrategy(cluster_resolver=cluster_resolver)

if cluster_resolver.task_type == 'chief':

    learning_rate = 0.01
    with strategy.scope():
        # model
        model_input = tf.keras.layers.Input(
            shape=(2,), dtype=tf.float64)
        layer_1 = tf.keras.layers.Dense( 8, activation='relu')(model_input)
        dense_output = tf.keras.layers.Dense(1)(layer_1)
        model = tf.keras.Model(model_input, dense_output)

        #optimizer
        optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate)

        accuracy = tf.keras.metrics.MeanSquaredError()

    @tf.function
    def distributed_train_step(iterator):
        def train_step(x_batch_train, y_batch_train):
            with tf.GradientTape() as tape:
                y_predict = model(x_batch_train, training=True)
                loss_value = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)(y_batch_train, y_predict)
                grads = tape.gradient(loss_value, model.trainable_weights)
            optimizer.apply_gradients(zip(grads, model.trainable_weights))
            accuracy.update_state(y_batch_train, y_predict)
            return loss_value
        x_batch_train, y_batch_train = next(iterator)
        return strategy.run(train_step, args=(x_batch_train, y_batch_train))

    coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(strategy)


    #test
    def dataset_fn(_):
        def create_toy_data(N):
            x = np.random.random(size = N)
            y = 0.2 + x + np.random.normal(loc=0, scale = 0.3, size = N)
            return np.c_[x,y]
        def toy_transform_row(row):
            dataset_x = tf.stack([row[0], tf.cast(1, tf.float64)], axis=-1)
            dataset_y = row[1]
            return dataset_x, dataset_y
        N = 1000
        data =create_toy_data(N)
        dataset = tf.data.Dataset.from_tensor_slices(data)
        dataset = dataset.map(toy_transform_row, num_parallel_calls=4)
        dataset = dataset.batch(BATCH_SIZE)
        dataset = dataset.prefetch(2)
        return dataset

    @tf.function
    def per_worker_dataset_fn():
        return strategy.distribute_datasets_from_function(lambda x : get_data()) # &lt;-- Not working with AI platform
        #return strategy.distribute_datasets_from_function(dataset_fn) # &lt;-- Working with AI platform

    per_worker_dataset = coordinator.create_per_worker_dataset(per_worker_dataset_fn)

    # Train model
    for epoch in range(5):
        per_worker_iterator = iter(per_worker_dataset)
        accuracy.reset_states()
        for step in range(5):
            coordinator.schedule(distributed_train_step, args=(per_worker_iterator,))
        coordinator.join()
        print (&quot;Finished epoch %d, accuracy is %f.&quot; % (epoch, accuracy.result().numpy()))
</code></pre>
<p>When I create the dataset with <code>per_worker_dataset_fn()</code> I can use the bigquery connector (bugging) or create the dataset in live (working).</p>
<p><strong>AI Platform Cluster configuration :</strong></p>
<p>runtimeVersion: &quot;2.5&quot;</p>
<p>pythonVersion: &quot;3.7&quot;</p>
<p>Did someone get this issue ? Bigquery connector worked pretty well with MirroredStrategy on AI Platform. Tell me if I should report the issue somewhere else.</p>
",0
69458522,What does tf.squeeze does to the audio and how can I load an mp3?,"<p>I'm using TensorFlow and I would like to be able to load audio and generate a spectrogram from it. I have little knowledge of how audio internally works.
Currently, this is the code I'm using:</p>
<pre><code>import pathlib
import tensorflow as tf
import tensorflow_io as tfio
import matplotlib.pyplot as plt

from IPython.display import Audio

data_dir = pathlib.Path('recordings')
sample_file = data_dir/'testA.mp3'
audio = tfio.audio.AudioIOTensor(str(sample_file))

# remove last dimension
audio_slice = audio[100:]
audio_tensor = tf.squeeze(audio_slice, axis=[-1])
#audio_tensor = audio.to_tensor()
tensor = tf.cast(audio_tensor, tf.float32) / 32768.0

print(&quot;Audio Tensor: &quot; + str(tensor))

plt.figure()
plt.plot(tensor.numpy())
plt.show()

# Convert to spectrogram
spectrogram = tfio.audio.spectrogram(tensor, nfft=512, window=512, stride=256)
    
plt.figure()
plt.imshow(tf.math.log(spectrogram).numpy())
plt.show()
</code></pre>
<p>I have been reading the documentation and in order to create a tensor I need to either use the tf.squeeze method or audio.to_tensor(). I have no clue what the tf.squeeze method does, but when I use it I get the error:</p>
<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: Can not squeeze dim[1], expected a dimension of 1, got 2 [Op:Squeeze]
</code></pre>
<p>If I instead use the method audio.to_tensor(), I'm unable to display the created spectrogram on the plt and instead I get the following error:</p>
<pre><code>TypeError: Invalid shape (28224, 1, 257) for image data
</code></pre>
",1
69509388,TF BERT input packer on more than two inputs,"<p>Some of the TensorFlow examples using BERT models show a use of the BERT preprocessor to &quot;pack&quot; inputs. E.g. in <a href=""https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/bert_glue.ipynb"" rel=""nofollow noreferrer"">this example</a>,</p>
<p><code>text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok], tf.constant(20))</code></p>
<p>The documentation implies that this works equally well with more than two input sentences, such that (I would expect) one can do something like:</p>
<p><code>text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok, tok], tf.constant(20))</code></p>
<p>However, so doing causes the error at the bottom[1] of this post.</p>
<p>I get that there isn't a matching signature; if I read this correctly (and I may not!), there's a signature for a single input and one for two. But what's the recommended way to pack more than two sentences into input suitable for a classification task, as suggested in the above colab?</p>
<p>1.</p>
<pre><code>ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (2 total):
    * [tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(&quot;inputs:0&quot;, shape=(None,), dtype=int32), row_splits=Tensor(&quot;inputs_2:0&quot;, shape=(None,), dtype=int64)), row_splits=Tensor(&quot;inputs_1:0&quot;, shape=(2,), dtype=int64)), tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(&quot;inputs_3:0&quot;, shape=(None,), dtype=int32), row_splits=Tensor(&quot;inputs_5:0&quot;, shape=(None,), dtype=int64)), row_splits=Tensor(&quot;inputs_4:0&quot;, shape=(2,), dtype=int64)), tf.RaggedTensor(values=tf.RaggedTensor(values=Tensor(&quot;inputs_6:0&quot;, shape=(None,), dtype=int32), row_splits=Tensor(&quot;inputs_8:0&quot;, shape=(None,), dtype=int64)), row_splits=Tensor(&quot;inputs_7:0&quot;, shape=(2,), dtype=int64))]
    * Tensor(&quot;seq_length:0&quot;, shape=(), dtype=int32)
  Keyword arguments: {}

Expected these arguments to match one of the following 4 option(s):

Option 1:
  Positional arguments (2 total):
    * [RaggedTensorSpec(TensorShape([None, None]), tf.int32, 1, tf.int64)]
    * TensorSpec(shape=(), dtype=tf.int32, name='seq_length')
  Keyword arguments: {}

Option 2:
  Positional arguments (2 total):
    * [RaggedTensorSpec(TensorShape([None, None]), tf.int32, 1, tf.int64), RaggedTensorSpec(TensorShape([None, None]), tf.int32, 1, tf.int64)]
    * TensorSpec(shape=(), dtype=tf.int32, name='seq_length')
  Keyword arguments: {}

Option 3:
  Positional arguments (2 total):
    * [RaggedTensorSpec(TensorShape([None, None, None]), tf.int32, 2, tf.int64), RaggedTensorSpec(TensorShape([None, None, None]), tf.int32, 2, tf.int64)]
    * TensorSpec(shape=(), dtype=tf.int32, name='seq_length')
  Keyword arguments: {}

Option 4:
  Positional arguments (2 total):
    * [RaggedTensorSpec(TensorShape([None, None, None]), tf.int32, 2, tf.int64)]
    * TensorSpec(shape=(), dtype=tf.int32, name='seq_length')
  Keyword arguments: {}```
</code></pre>
",1
69591866,"tf.keras.model call() method, is it possible to use method call() with labels?","<p>I've been having this question bugging me for some time: Is it possible to use the method <code>call()</code> of <code>tf.keras.model</code> with labels? From what I've seen it is not plausible, but it just strikes me as odd that you are able to train the model using this method but you can't pass it labels like the <code>.fit()</code> method.</p>
<p>Also, this question arised when I was reading the tutorial to make a DCGAN in the tensorflow documentation.</p>
<p>Source: <a href=""https://www.tensorflow.org/tutorials/generative/dcgan"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/generative/dcgan</a></p>
",0
69628955,How to verify original image's input channels when using Keras load_image?,"<p>I am using <code>tf.keras.preprocessing.image.load_image</code> to load 3-channel images into a simple classification network. Based on the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/load_img"" rel=""nofollow noreferrer"">documentation</a> for <code>load_img</code>, this method will return a 3-channel image <strong>even if the original image had a single channel</strong> (by duplicating the channels, I suppose).</p>
<p>Since it is possible for the user of my application to accidentally provide a single-channel image, this has the potential to fail silently with no way for the user to know why the predictions are bad.</p>
<p>I would therefore like to add a line to my code that makes sure that the original loaded image does indeed have 3 channels, and raise an error otherwise.</p>
<p>Is there a way to catch 1-channel images without loading them twice separately? Is there an alternative method to <code>load_image</code> that I can use for loading the images in a way that doesn't change the number of channels?</p>
",0
69672777,Compute Hessian of lossfunction in Tensorflow,"<p>I would like to compute the hessian of a loss function of a neural network in Tensorflow with respect to all the parameters (or trainable variables). By modifying the example code from the Tensorflow documentation (<a href=""https://www.tensorflow.org/api_docs/python/tf/GradientTape"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/GradientTape</a>) I managed to compute the hessian w.r.t the weight matrix for the first layer (if I'm not mistaken):</p>
<pre><code>with tf.GradientTape(persistent=True) as tape:
    loss = tf.reduce_mean(model(x,training=True)**2)
    g = tape.gradient(loss,model.trainable_variables[0]) 
    h=tape.jacobian(g,model.trainable_variables[0])
</code></pre>
<p>If I try to compute it w.r.t model.trainable_variables instead the tape.jacobian complains that 'list object has no attribute shape'. I instead tried to flatten the model.trainable_variables and compute it w.r.t the flattened vector:</p>
<pre><code>with tf.GradientTape(persistent=True) as tape:
    loss = tf.reduce_mean(model(x,training=True)**2)
    source = tf.concat([tf.reshape(x,[-1]) for x in model.trainable_variables],axis=0)
    g = tape.gradient(loss,source) 
    h=tape.jacobian(g,source)
   
</code></pre>
<p>The problem now is that g is empty (NoneType) for some reason. I noticed that source is tf.Tensor-type but model.trainable_variables[0] was of type tf.ResourceVariable so I tried changing this by declaring source as</p>
<pre><code>source = resource_variable_ops.ResourceVariable(tf.concat([tf.reshape(x,[-1]) for x in model.trainable_variables],axis=0))
</code></pre>
<p>This didn't change anything though, so I'm guessing that this is not the issue. I also thought that the problem might be that the source-variable is not watched, but it seems that it is set to trainable and even if i do tape.watch(source), g is still empty.</p>
<p>Does anybody know how I can solve this?</p>
",1
69746393,Keras discrepancy between .evaluate and .predict,"<p>I know this question has been asked before, but I have tried all of their solutions and nothing is working for me.</p>
<p><strong>My Problem:</strong></p>
<p>I am running a CNN to classify some images, a typical task, nothing too crazy.  I have the following compilation of my model.</p>
<pre><code>model.compile(optimizer = keras.optimizers.Adam(learning_rate = exp_learning_rate),
          loss = tf.keras.losses.SparseCategoricalCrossentropy(),
          metrics = ['accuracy'])
</code></pre>
<p>I fit this on my training dataset, and evaluated on my validation dataset as follows:</p>
<pre><code>history = model.fit(train_dataset, validation_data = validation_dataset, epochs = 5)
</code></pre>
<p>And then I evaluated on a separate test set as follows:</p>
<pre><code>model.evaluate(test_dataset)
</code></pre>
<p>Which resulted in this:</p>
<blockquote>
<p>4/4 [==============================] - 30s 7s/step - loss: 1.7180 - accuracy: 0.8627</p>
</blockquote>
<p>However, when I run:</p>
<pre><code>model.predict(test_dataset)
</code></pre>
<p>I have the following confusion matrix output:</p>
<p><a href=""https://i.stack.imgur.com/P1K4N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P1K4N.png"" alt="""" /></a></p>
<p>This clearly isn't 86% accuracy like the .evaluate method tells me.  In fact, it's actually 35.39% accuracy.  To make sure it wasn't an issue with my testing dataset, I had my model predict on my training and validation datasets and I still got a similar percentage as here (~30%) despite my training, validation accuracy during fitting going up to 96%, 87%, respectively.</p>
<p><strong>Question:</strong></p>
<p>I don't know why .predict and .evaluate are outputting different results?  What's happening there?  It seems like when I call .predict, it's not using any of the weights that I trained during fitting? (in fact, given that there are 3 classes, this output is no better than just blindly guessing each label).  Are the weights from my fitting not being transferred over to my prediction?  My loss function is correct (I label encoded my data as tensorflow wishes to be used with sparse_categorical_crossentropy) and when I pass 'accuracy', it will just take the accuracy corresponding to my loss function.  All of this should be consistent.  But why is there such a discrepancy with the results of .evaluate and .predict?  Which one should I trust?</p>
<p><strong>My Attempts to Fix My Issue:</strong></p>
<p>I thought maybe the sparse categorical cross entropy wasn't right, so I one-hot encoded my target labels and used the categorical_crossentropy loss instead.  I still have the EXACT same issue as above.</p>
<p><strong>Concerns:</strong></p>
<p>If the .evaluate is incorrect, then doesn't that mean my training accuracy and validation accuracy during fitting are inaccurate as well?  Don't those use the .evaluate method as well?  If that's the case, then what can I trust?  The loss isn't a good indication of if my model is doing well because it is well-known that minimal loss does not imply good accuracy (although the converse is usually true depending on what standard of &quot;good&quot; we're using).  How do I gauge my model's effectiveness in the case that my accuracy metrics aren't correct? I don't really know what to look at anymore because I have no other way to gauge if my model is learning, if someone could please help me understand what is happening I would appreciate it so much.  I'm so frustrated.</p>
<p><strong>Edit: (10-28-2021: 12:26 AM)</strong></p>
<p>Ok, so I'll provide some more code to really troubleshoot this.</p>
<p>I originally preprocessed my data as such:</p>
<pre><code>image_size = (256, 256)
batch_size = 16

train_ds = keras.preprocessing.image_dataset_from_directory(
    directory = image_directory,
    label_mode = 'categorical',
    shuffle = True,
    validation_split = 0.2,
    subset = 'training',
    seed = 24,
    batch_size = batch_size
)

val_ds = keras.preprocessing.image_dataset_from_directory(
    directory = image_directory,
    label_mode = 'categorical',
    shuffle = True,
    validation_split = 0.2,
    subset = 'validation',
    seed = 24,
    batch_size = batch_size
)
</code></pre>
<p>Where image_directory is a string with a path containing my images.  Now you could probably read documentation, but the image_dataset_from_directory method actually returns a tf.data.Dataset object containing a bunch of batches of the respective (training, validation) data.</p>
<p>I imported the VGG16 architecture to do my classification so I called the respective preprocessing function for VGG16 as follows:</p>
<pre><code>preprocess_input = tf.keras.applications.vgg16.preprocess_input

train_ds = train_ds.map(lambda x, y: (preprocess_input(x), y))

val_ds = val_ds.map(lambda x, y: (preprocess_input(x), y))
</code></pre>
<p>This transformed the images into something that was suitable as input for VGG16.  Then, in my last processing steps, I did the following validation/test split:</p>
<pre><code>val_batches = tf.data.experimental.cardinality(val_ds)
test_dataset = val_ds.take(val_batches // 3)
validation_dataset = val_ds.skip(val_batches // 3)
</code></pre>
<p>Then I proceeded to cache and prefetch my data:</p>
<pre><code>AUTOTUNE = tf.data.AUTOTUNE

train_dataset = train_ds.prefetch(buffer_size = AUTOTUNE)
validation_dataset = validation_dataset.prefetch(buffer_size = AUTOTUNE)
test_dataset = test_dataset.prefetch(buffer_size = AUTOTUNE)
</code></pre>
<p><strong>The Problem:</strong></p>
<p>The problem occurs in the method above.  I'm still not sure whether or not .evaluate is a true indicator of accuracy for my model.  But I realized that the .evaluate and .predict always coincide when my neural network is a keras.Sequential() model.  However, (correct me if I'm wrong) what I am suspecting is that VGG16, when imported from keras.applications API, is actually <em>NOT</em> a keras.Sequential() model.  Therefore, I don't think that the .predict and .evaluate results actually coincide when I feed my data straight into my model (I was going to post this as an answer, but I don't have sufficient knowledge nor research to confirm that any of what I said is correct, someone please chime in because I like learning things I know little to nothing about, an edit this is for now).</p>
<p>In the end, I worked around my problem by calling Image_Data_Generator() instead of image_dataset_from_directory() as follows:</p>
<pre><code>train_datagen = ImageDataGenerator(
    preprocessing_function = preprocess_input,
    width_shift_range = 0.2,
    height_shift_range = 0.2,
    shear_range = 0.2,
    zoom_range = 0.2,
    horizontal_flip = True
)

val_datagen = ImageDataGenerator(
    preprocessing_function = preprocess_input
)


train_ds = train_datagen.flow_from_directory(
    train_image_directory,
    target_size = (224, 224),
    batch_size = 16,
    seed = 24,
    shuffle = True,
    classes = ['class1', 'class2', 'class3'],
    class_mode = 'categorical'
)

test_ds = val_datagen.flow_from_directory(
    test_image_directory,
    target_size = (224, 224),
    batch_size = 16,
    seed = 24,
    shuffle = False,
    classes = ['class1', 'class2', 'class3'],
    class_mode = 'categorical'
)
</code></pre>
<p>(NOTE: I got this based off the following link from tensorflow's documentation: <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_directory"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_directory</a>)</p>
<p>This completes all the preprocessing for me.  Then, when I call model.evaluate(test_ds), it returns the exact same result as when I do model.predict_generator(test_ds).  After some minor processing of the prediction output, I use the following code for my confusion matrix:</p>
<pre><code>Y_pred = model.predict(test_ds)
y_pred = np.argmax(Y_pred, axis=1)

cf = confusion_matrix(test_ds.classes, y_pred)
sns.heatmap(cf, annot= True, xticklabels = class_names,
           yticklabels = class_names)
plt.title('Performance of Model on Testing Set')
</code></pre>
<p>This eliminates the discrepancy in the confusion matrix and the result of model.evaluate(test_ds).</p>
<p><strong>The Takeaway:</strong></p>
<p>If you're loading images onto a classification model, and your loss and accuracy match, but you're getting discrepancy between your predictions and loss, accuracy, try preprocessing in every way possible.  I usually preprocess my images using the image_dataset_from_directory() method for all my keras.sequential() models, however, for the VGG16 model, which I suspect is not a sequential() model, using the ImageDataGenerator(...).flow_from_directory(...) resulted in the correct format for the model to generate a prediction that is consistent with the performance metrics.</p>
<p><strong>TLDR</strong>  I didn't answer any of my original questions, but I found a workaround.  Sorry if this is spam in any way.  As is the nature of most Stack Overflow posts, I hope my turmoil in the last few hours helps someone way in the future.</p>
",0
69775482,tf.keras.Concatenate Graph Disconnected when concatenating two input layers,"<p>Hey I have a problem that <em>seems</em> common but I am sure what I'm doing should work because it is so simple.</p>
<p>It's got to do with the Keras Concatenate layer:</p>
<p><code>Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 128, 256, 192, 1), dtype=tf.float32, name='input_1'), name='input_1', description=&quot;created by layer 'input_1'&quot;) at layer &quot;tf.concat&quot;. The following previous layers were accessed without issue: []</code></p>
<p>I am essentially trying to concatenate 2 inputs like so:</p>
<pre class=""lang-py prettyprint-override""><code>in_layer1 = Input((sizes1[1], sizes1[2], sizes1[3], 1))  # (slices, x, y, channel=1)
in_layer2 = Input((sizes2[1], sizes2[2], sizes2[3], 1))  # (slices, x, y, channel=1)
in_layer = Concatenate(axis=1)([in_layer1, in_layer2][:])  # combine the two inputs
</code></pre>
<p>problem happens when I instantiate the model :</p>
<pre class=""lang-py prettyprint-override""><code>Model(inputs=[in_layer], outputs=[out_layer])
</code></pre>
<p>it seems it was a problem before tf2.2 and I am using 2.4 so not sure why it's happening:
<a href=""https://github.com/tensorflow/tensorflow/issues/32023"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/32023</a></p>
<p>Any help or resources would be really appreciated. I checked the documentation and I don't think I'm doing it wrong but clearly there is a problem.</p>
",1
69792031,Explanation of tf.keras.layers.CategoryEncoding output_mode='multi_hot' behavior,"<h1>Question</h1>
<p>Please help understand the definition of <strong>multi hot encoding</strong> of tf.keras.layers.CategoryEncoding and the behavior of <code>output_mode='multi_hot'</code>.</p>
<h1>Background</h1>
<p>According to <a href=""https://stats.stackexchange.com/a/467672"">What exactly is multi-hot encoding and how is it different from one-hot?</a>:</p>
<blockquote>
<p>If you would use multi-hot-encoding you would first label-encode your classes, thus having only a single number which represents the presence of a class (e.g. 1 for 'dog') and then convert the numerical labels to binary vectors of size log2(5)=3.<br />
Examples:</p>
<pre><code>'cat'  = [0,0,0]  
'dog'  = [0,0,1]  
'fish' = [0,1,0]  
'bird' = [0,1,1]  
'ant'  = [1,0,0]   
</code></pre>
</blockquote>
<h1>Behaviour of <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/CategoryEncoding"" rel=""nofollow noreferrer"">tf.keras.layers.CategoryEncoding</a></h1>
<p>The document says <code>num_tokens</code> is the total number of tokens the layer should support.</p>
<blockquote>
<h3>args</h3>
<h4>num_tokens</h4>
<p>The total number of tokens the layer should support. All inputs to the layer must integers in the range 0 &lt;= value &lt; num_tokens, or an error will be thrown.</p>
<h4>output_mode</h4>
<ul>
<li>&quot;one_hot&quot;: Encodes each individual element in the input into an array of num_tokens size, containing a 1 at the element index. If the last dimension is size 1, will encode on that dimension. If the last dimension is not size 1, will append a new dimension for the encoded output.</li>
<li>&quot;multi_hot&quot;: Encodes each sample in the input into <strong>a single array of num_tokens size, containing a 1 for each vocabulary term present in the sample</strong>. Treats the last dimension as the sample dimension, if input shape is (..., sample_length), output shape will be (..., num_tokens).</li>
</ul>
</blockquote>
<p>According to the definitions of multi hot encoding above, I expected <code>tf.keras.layers.CategoryEncoding(num_tokens=5, output_mode=&quot;multi_hot&quot;)</code> encodes 5 tokens into an array of size 3.</p>
<p>However, the document says &quot;multi_hot&quot; encodes each sample into <strong>a single array of num_tokens size</strong>, containing a 1 for each vocabulary term present in the sample, and behaves as such.</p>
<pre><code>dataset = tf.data.Dataset.from_tensor_slices(tf.constant(['cat', 'dog', 'fish', 'bird']))

lookup = tf.keras.layers.StringLookup(max_tokens=5, oov_token='[UNK]')
lookup.adapt(dataset)
lookup.get_vocabulary()
---
['[UNK]', 'fish', 'dog', 'cat', 'bird']

mhe = tf.keras.layers.CategoryEncoding(num_tokens=lookup.vocabulary_size(), output_mode=&quot;multi_hot&quot;)
print(f&quot;cat: {mhe(lookup(tf.constant('cat'))).numpy()}&quot;)
print(f&quot;dog: {mhe(lookup(tf.constant('dog'))).numpy()}&quot;)
---
cat: [0. 0. 0. 1. 0.]
dog: [0. 0. 1. 0. 0.]
</code></pre>
<p>Which has no difference from One Hot Encoding.</p>
<pre><code>ohe = tf.keras.layers.CategoryEncoding(num_tokens=lookup.vocabulary_size(), output_mode=&quot;one_hot&quot;)
print(f&quot;cat: {ohe(lookup(tf.constant('cat'))).numpy()}&quot;)
print(f&quot;dog: {ohe(lookup(tf.constant('dog'))).numpy()}&quot;)
---
cat: [0. 0. 0. 1. 0.]
dog: [0. 0. 1. 0. 0.]
</code></pre>
<p>For multi value inputs, multi_hot only handles the first value.</p>
<pre><code>print(ohe(lookup(tf.constant(['cat', 'dog']))).numpy())
---
[[0. 0. 0. 1. 0.]
 [0. 0. 1. 0. 0.]]

print(mhe(lookup(tf.constant(['cat', 'dog']))).numpy())
---
[0. 0. 1. 1. 0.]
</code></pre>
<p>To handle multiple inputs, need to be 2D array.</p>
<pre><code>print(mhe(lookup(tf.constant([['cat'], ['dog']]))).numpy())
---
[[0. 0. 0. 1. 0.]
 [0. 0. 1. 0. 0.]]
</code></pre>
<p>Apparently the definition of <strong>multi hot encoding</strong> of <code>tf.keras.layers.CategoryEncoding</code> is not the same with the one in <a href=""https://stats.stackexchange.com/a/467672"">What exactly is multi-hot encoding and how is it different from one-hot?</a>.</p>
<h1>Related</h1>
<ul>
<li><a href=""https://github.com/tensorflow/tensorflow/issues/52892"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/52892</a></li>
</ul>
",1
69843239,How does tf.keras.util.array_to_image() work with regards to memory?,"<p>I have image data that I want to use in a TensorFlow model, but I have to retrieve the image as an (Numpy) array of pixel values. From what I've read, TensorFlow has to read an image in as some image format and from some location. I know that <code>tf.keras.util.array_to_image()</code> can convert an array to a PIL instance of an image, and I know that there are several other libraries that have similar functionality, such as <code>PIL.Image.fromarray()</code>.</p>
<p>My problem is that I don't want to duplicate the image data by copying it to a new format. The API documentation for the <code>tf.keras.util.array_to_image()</code> says that it returns a &quot;PIL image instance&quot;. Does that mean that it is copying all array values to a new data structure and returning that, or is it creating an image data structure that references the original array pixel values?</p>
<p>As a follow-up question, if the keras method does duplicate the data (by having both the original array and the image instance have independent values), is there a way to have TensorFlow accept an array representation of an image without needing to duplicate it as a separate image file?</p>
",1
69851165,How to create a multivariate timeseries dataset with tf.data?,"<p>I am trying to create an input pipeline for my LSTM model. I am using the <code>tf.data.Dataset.from_generator</code> API to do that. Following the <a href=""https://www.tensorflow.org/guide/data#time_series_windowing"" rel=""nofollow noreferrer"">guide</a>, my current minimal example looks like this:</p>
<pre><code>class generator:
    def __init__(self, n=5):
        self.n = n

    def __call__(self):
        for i in range(self.n):
            yield (i, 10*i)

dataset = tf.data.Dataset.from_generator(generator(), 
    output_signature=(tf.TensorSpec(shape=(), dtype=tf.uint16), tf.TensorSpec(shape=(), dtype=tf.int32)))

window_size = 3
windows = dataset.window(window_size, shift=1)

def sub_to_batch(sub):
    return sub.batch(window_size, drop_remainder=True)

final_dset = windows.flat_map(sub_to_batch)

print(list(final_dset.as_numpy_iterator()))
</code></pre>
<p>Error message</p>
<pre><code>TypeError: tf__sub_to_batch() takes 1 positional argument but 2 were given
</code></pre>
<p>This problem only occurs when using more than one feature in the generator (e.g. change the following lines).</p>
<pre><code>yield (i)

dataset = tf.data.Dataset.from_generator(generator(), 
    output_signature=(tf.TensorSpec(shape=(), dtype=tf.uint16)))
</code></pre>
<p>In the version with only 1 feature, the output looks like <code>shape=(3, 3, 1)</code></p>
<pre><code>[ [ [0], [1], [2] ],
  [ [1], [2], [3] ],
  [ [2], [3], [4] ]  ]
</code></pre>
<p>I basically try to achieve a zipping of the individual features so that I get <code>shape=(3, 3, 2)</code>:</p>
<pre><code>[ [ [0,  0], [1, 10], [2, 20] ],
  [ [1, 10], [2, 20], [3, 30] ],
  [ [2, 20], [3, 30], [4, 40] ]  ]
</code></pre>
<p>How can this be done?</p>
",0
69911866,Calculating FLOPS for Keras Models (TF 2.x),"<p>I found two solutions to calculate FLOPS of Keras models (TF 2.x):</p>
<p>[1] <a href=""https://github.com/tensorflow/tensorflow/issues/32809#issuecomment-849439287"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/32809#issuecomment-849439287</a></p>
<p>[2] <a href=""https://github.com/tensorflow/tensorflow/issues/32809#issuecomment-841975359"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/32809#issuecomment-841975359</a></p>
<p>At first glance, both seem to work perfectly when testing with <code>tf.keras.applications.ResNet50()</code>. The resulting FLOPS are identical and correspond to the FLOPS of the ResNet paper.</p>
<p>But then I built a small GRU model and found different FLOPS for the two methods:</p>
<pre><code>model = Sequential(name=self._modelName)
model.add(GRU(recurrentCells, input_shape=(10, 8), recurrent_dropout=0.2, return_sequences=True))
model.add(GRU(recurrentCells, recurrent_dropout=0.2))
model.add(Dense(outputDimension, activation='tanh'))
model.compile(loss='mse', optimizer='adam', metrics=[RootMeanSquaredError(), MeanAbsoluteError()])
</code></pre>
<p>This results in the following numbers:
<code>13206</code> for method [1] and <code>18306</code> for method [2].
That is really confusing...</p>
<p>Does anyone know how to correctly calculate FLOPS of recurrent Keras models in TF 2.x?</p>
<p><strong>EDIT</strong></p>
<p>I found another information:</p>
<p>[3] <a href=""https://github.com/tensorflow/tensorflow/issues/36391#issuecomment-596055100"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/36391#issuecomment-596055100</a></p>
<blockquote>
<p>It seems like when our model have the LSTM or GRU's layers, what we
need to do is by pass one more argument lower_control_flow=False in
order to make the convert_variables_to_constants_v2 work.</p>
</blockquote>
<p>When adding this argument to <code>convert_variables_to_constants_v2</code>, the outputs of [1] and [2] are the same when using my GRU example.</p>
<p>The tensorflow documentation explains this argument as follows (<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/convert_to_constants.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/convert_to_constants.py</a>):</p>
<blockquote>
<p>lower_control_flow: Boolean indicating whether or not to lower control
flow ops such as If and While. (default True)</p>
</blockquote>
<p>Can someone try to explain this?</p>
",0
70120519,"Python version mismatch: module was compiled for Python 3.6, but the interpreter version is incompatible: 3.9.8","<p>In order to install the newest <code>tensorflow</code>(2.7.0), I updated my <code>python3</code> verison from <code>3.6.6</code> to <code>3.9.8</code>. Here is how I do it inside my <strong>docker</strong>!!.</p>
<pre><code>Download the Python-3.9.8.tgz file

1. tar -xf Python-3.9.8.tgz

2. cd Python-3.9.8 &amp; ./configure --enable-optimizations

3. make -j 12

4. make altinstall
</code></pre>
<p>And my <code>python3 --version</code> is <code>Python 3.9.8</code>. However, as I am trying to load the newest <code>tf</code> by <code>import tensorflow.compat.v1 as tf</code>. Here comes the error:</p>
<pre><code>  File &quot;/workspaces/model/task.py&quot;, line 120, in new_model_test
    import model_api
  File &quot;/lfs/biomind/model_tmp/19bddfc44e8211ecbe172d8a58f5e38e/wmh_v2/model_api.py&quot;, line 3, in &lt;module&gt;
    import tensorflow.compat.v1 as tf
  File &quot;/usr/local/lib/python3.6/site-packages/tensorflow/__init__.py&quot;, line 99, in &lt;module&gt;
    from tensorflow_core import *
  File &quot;/usr/local/lib/python3.6/site-packages/tensorflow_core/__init__.py&quot;, line 28, in &lt;module&gt;
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File &quot;/usr/local/lib/python3.6/site-packages/tensorflow/__init__.py&quot;, line 50, in __getattr__
    module = self._load()
  File &quot;/usr/local/lib/python3.6/site-packages/tensorflow/__init__.py&quot;, line 44, in _load
    module = _importlib.import_module(self.__name__)
  File &quot;/usr/local/lib/python3.9/importlib/__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;/usr/local/lib/python3.6/site-packages/tensorflow_core/python/__init__.py&quot;, line 49, in &lt;module&gt;
    from tensorflow.python import pywrap_tensorflow
  File &quot;/usr/local/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py&quot;, line 74, in &lt;module&gt;
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File &quot;/usr/local/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow.py&quot;, line 58, in &lt;module&gt;
    from tensorflow.python.pywrap_tensorflow_internal import *
  File &quot;/usr/local/lib/python3.6/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py&quot;, line 2453, in &lt;module&gt;
    from tensorflow.python.util import deprecation
  File &quot;/usr/local/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py&quot;, line 25, in &lt;module&gt;
    from tensorflow.python.platform import tf_logging as logging
  File &quot;/usr/local/lib/python3.6/site-packages/tensorflow_core/python/platform/tf_logging.py&quot;, line 38, in &lt;module&gt;
    from tensorflow.python.util.tf_export import tf_export
  File &quot;/usr/local/lib/python3.6/site-packages/tensorflow_core/python/util/tf_export.py&quot;, line 48, in &lt;module&gt;
    from tensorflow.python.util import tf_decorator
  File &quot;/usr/local/lib/python3.6/site-packages/tensorflow_core/python/util/tf_decorator.py&quot;, line 64, in &lt;module&gt;
    from tensorflow.python.util import tf_stack
  File &quot;/usr/local/lib/python3.6/site-packages/tensorflow_core/python/util/tf_stack.py&quot;, line 29, in &lt;module&gt;
    from tensorflow.python import _tf_stack
ImportError: Python version mismatch: module was compiled for Python 3.6, but the interpreter version is incompatible: 3.9.8 (main, Nov 25 2021, 21:54:13) 
[GCC 7.5.0].
</code></pre>
<p>Is there a way to change the compiled version of <code>Python</code> or do I do something which is not the right step? Thanks in advance.</p>
",0
70137989,"Tensorflow - LSTM, TextVectorization (custom standardization) and ModelCheckpoint","<p>I'm fairly new to Tensorflow. I've been following Word Embeddings, Basic text classification and RNN articles on TF website.</p>
<p>I have <code>dataset_dir = 'aclImdb'</code> IMDB dataset as in those tutorials and everything is the same as described there. I've added a <code>ModelCheckpoint</code> with <code>save_best_only=True</code> and I'm getting two errors - first when training, between epochs, I guess when the best model is saved and second one when the best saved model is loaded.</p>
<p><strong>Warning 1:</strong> <code>WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn,lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.</code></p>
<p><code>WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x000001E4DA8BC0A0&gt; has the same name 'LSTMCell' as a built-in Keras object. Consider renaming &lt;class 'keras.layers.recurrent.LSTMCell'&gt; to avoid naming conflicts when loading with 'tf.keras.models.load_model'. If renaming is not possible, pass the object in the 'custom_objects' parameter of the load function.</code></p>
<p><strong>Error 2:</strong> <code>RuntimeError: Unable to restore a layer of class TextVectorization. Layers of class TextVectorization require that the class be provided to the model loading code, either by registering the class using '@keras.utils.register_keras_serializable' on the class def and including that file in your program, or by passing the class in a 'keras.utils.CustomObjectScope' that wraps this load call.</code></p>
<p>My <strong>variables</strong>:</p>
<pre><code>batch_size = 64
seed = 42
max_features = 10000  # vocabulary size
sequence_length = 250  # sentence length of reviews or tweets, etc.
output_dim = 16  # encoded size of a vector
</code></pre>
<p>Here's my <strong>callback</strong>:</p>
<pre><code>def create_model_checkpoint(model_name, save_path='model_experiments'):
  return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name),
                                            verbose=0, # output limited amount of info
                                            save_best_only=True)
</code></pre>
<p>Here's <strong>TextVectorization</strong> (the same as in the articles):</p>
<pre><code>def custom_standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    stripped_html = tf.strings.regex_replace(lowercase, '&lt;br/&gt;', ' ')
    return tf.strings.regex_replace(stripped_html,
                                    '[%s]' % re.escape(string.punctuation), '')

vectorize_layer = tf.keras.layers.TextVectorization(
    standardize=custom_standardization,
    max_tokens=max_features,
    output_mode='int',
    output_sequence_length=sequence_length
)
</code></pre>
<p>Here's the Model (I have added <strong>GlobalAveragePooling1D</strong> and <strong>callback</strong> compared to the article):</p>
<pre><code>model_2 = tf.keras.models.Sequential([
    vectorize_layer,
    tf.keras.layers.Embedding(len(vectorize_layer.get_vocabulary()), 64, mask_zero=False),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(1)
], name='model_2_Bidirectional')

model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['accuracy'])

history_model_2 = model_2.fit(train_ds, epochs=3,
                              validation_data=valid_ds,
                              callbacks=[create_model_checkpoint(model_name=model_2.name)])

model_2_loaded = tf.keras.models.load_model(os.path.join('model_experiments', model_2.name))
model_2_loaded.evaluate(valid_ds)
</code></pre>
<p><strong>Questions:</strong>
Regarding Warning - I have tried defining name='lstm_custom_64' and 'lstm_custom_32' for the layers but these warnings still kept on coming up. I couldn't find any clear solution to what this warning means and how to fix it.</p>
<p>Regarding Error - I have more clues on this one, I guess I have to define @keras.utils.register_keras_serializable and write custom layer class inheriting from <code>TextVectorization</code> and include <code>custom_standardization</code> into it. Should I? I would love if you could share the solution for this one as I'm not experienced with TF.</p>
<p>P.S. What about adapt? as far as I understand this creates a vocab. Is this getting saved and then loaded when performing save/load?</p>
<pre><code>text_ds = train_ds.map(lambda x, y: x)
vectorize_layer.adapt(text_ds)
</code></pre>
",0
70426044,tf.keras.losses.categorical_crossentropy() does not output what it should output,"<p>I am trying to train a classifier CNN with 3 classes. I am trying to troubleshoot my loss function. I am testing <code>tf.keras.losses.CategoricalCrossentropy()</code> and <code>tf.keras.losses.categorical_crossentropy().numpy()</code>. I am following the standealone usage guide from <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/categorical_crossentropy"" rel=""nofollow noreferrer"">the tensorflow documentation</a>.</p>
<p>I think that I am not getting the proper outputs that I should be.
When I input <code>y_true=[0.,1.,0.]</code> and <code>y_pred=[1.,0.,0.]</code> I expect a loss of infinity (output in the program: <code>nan</code>). However, the output I receive is <code>16.118095</code>. When the classification aligns with the label (i.e. <code>y_true=[1.,0.,0.]</code> and <code>y_pred=[1.,0.,0.]</code>) the output is <code>1.192093e-07</code>, even though I would expect a perfect 0.</p>
<p>I am really perplexed by this behavior. Similarly, with the 1 long vector case: <code>y_true=[1.]</code> and <code>y_pred=[0.]</code> the loss is <code>16.118095</code>, and likewise when the classification aligns <code>y_true=[1.]</code> and <code>y_pred=[1.]</code> I receive <code>1.192093e-07</code> and <code>y_true=[0.]</code> and <code>y_pred=[0.]</code> the result is <code>nan</code>.</p>
<p>I think that summarizing the results I get, the results I expect, and the values I am inputting into the loss functions would make things more readable so I will do that below:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th><code>y_true</code></th>
<th><code>y_pred</code></th>
<th>Actual Output</th>
<th>What I Expect</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>[0.,1.,0.]</code></td>
<td><code>[1.,0.,0.] </code></td>
<td>16.118095</td>
<td><code>nan</code> or infinity</td>
</tr>
<tr>
<td><code>[1.,0.,0.]</code></td>
<td><code>[1.,0.,0.]</code></td>
<td>1.192093e-07</td>
<td>True 0</td>
</tr>
<tr>
<td><code>[0.,1.]</code></td>
<td><code>[1.,0.]</code></td>
<td>16.118095</td>
<td><code>nan</code> or infinity</td>
</tr>
<tr>
<td><code>[1.,0.]</code></td>
<td><code>[1.,0.]</code></td>
<td>1.192093e-07</td>
<td>True 0</td>
</tr>
<tr>
<td><code>[1.]</code></td>
<td><code>[0]</code></td>
<td><code>nan</code> or infinity</td>
<td><code>nan</code> or infinity</td>
</tr>
<tr>
<td><code>[1.]</code></td>
<td><code>[1.]</code></td>
<td>1.192093e-07</td>
<td>True 0</td>
</tr>
</tbody>
</table>
</div>
<p>I am sorry if this is a trivial question, but I really don't know why I am getting the results that I am getting. I think something is wrong because I am only getting 16 and not infinity, but if nothing is going wrong I'd like the reassurance. If I am wrong, I would really appreciate the correction.</p>
",0
70478356,ONNX model checker fails while ONNX runtime works fine when `tf.function` is used to decorate memeber function with loop,"<p>When a tensorflow model contains <code>tf.function</code> decorated function with for loop in it, the tf-&gt;onnx conversion yields warnings:</p>
<pre><code>WARNING:tensorflow:From /Users/amit/Programs/lammps/kim/kliff/venv/lib/python3.7/site-packages/tf2onnx/tf_loader.py:706: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
Cannot infer shape for model/ex_layer/PartitionedCall/while: model/ex_layer/PartitionedCall/while:3
Cannot infer shape for model/ex_layer/PartitionedCall/Identity: model/ex_layer/PartitionedCall/Identity:0
Cannot infer shape for Func/model/ex_layer/PartitionedCall/output/_3: Func/model/ex_layer/PartitionedCall/output/_3:0
Cannot infer shape for Identity: Identity:0
missing output shape for while/Identity_3:0
missing output shape for while/Identity_3:0
missing output shape for while/Identity_3:0
missing output shape for while/Identity_3:0
...
</code></pre>
<p>And as the obtained model is run through onnxruntime it runs fine, but model checker gives the following error</p>
<pre><code>Traceback (most recent call last):
  File &quot;failed_example.py&quot;, line 85, in &lt;module&gt;
    onnx.checker.check_model(onnx.load(&quot;tmp.onnx&quot;))
  File &quot;venv/lib/python3.7/site-packages/onnx/checker.py&quot;, line 106, in check_model
    C.check_model(protobuf_string)
onnx.onnx_cpp2py_export.checker.ValidationError: Field 'shape' of type is required but missing.
</code></pre>
<p>Netron does not show any appreciable difference between the model with decorated function and without decorated function. I guess error comes from the fact that the for loop is converted to separate while-loop graph, whose input shape is not defined. But it does work perfectly without tf.function decorator. I am putting a minimal replication code below.</p>
<p>I think it is related to following issues:</p>
<ul>
<li><a href=""https://github.com/onnx/onnx/issues/2932"" rel=""nofollow noreferrer"">https://github.com/onnx/onnx/issues/2932</a></li>
<li><a href=""https://github.com/onnx/onnx/issues/2492"" rel=""nofollow noreferrer"">https://github.com/onnx/onnx/issues/2492</a></li>
<li><a href=""https://github.com/onnx/onnx/pull/2937"" rel=""nofollow noreferrer"">https://github.com/onnx/onnx/pull/2937</a></li>
</ul>
<p>Code to replicate:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np
import sys
import onnx
import onnxruntime
import tf2onnx

# =============================================================================
# Layer and its herler functions
# COMMENT IT OUT TO PASS ONNX CHECK
@tf.function(
    input_signature=[
    tf.TensorSpec(shape=[None,None], dtype=tf.int32),
    tf.TensorSpec(shape=[None,None], dtype=tf.float32),
    tf.TensorSpec(shape=None, dtype=tf.float32),
    ])
def extra_function(
    list1,
    list2,
    accum_var
    ):
    some_num = 4
    num_iter = tf.size(list1)//some_num
    for i in range(num_iter):
        xyz_i = list2[0, i * 3 : (i + 1) * 3]
        accum_var += tf.reduce_sum(xyz_i)
    return accum_var

class ExLayer(tf.keras.layers.Layer):
    def __init__(self):
        super().__init__()

    # Doesnt tf.function also create graphs out of called functions?
    # however it does not seem to do that if `call` function is decorated
    # @tf.function(
    #     input_signature=[
    #     tf.TensorSpec(shape=[None,None], dtype=tf.float32),
    #     tf.TensorSpec(shape=[None,None], dtype=tf.int32),
    #     ])
    def call(self, list2,list1):
        accum_var = tf.constant(0.0)
        accum_var = extra_function( list1, list2, accum_var)
        return accum_var
# =============================================================================


# =============================================================================
# Example implementation

layer1 = tf.keras.layers.Input(shape=(1,))
layer2 = tf.keras.layers.Input(shape=(1,), dtype=tf.int32)
EL = ExLayer()(layer1,layer2)
model = tf.keras.models.Model(inputs=[layer1, layer2], outputs=EL)

# Define input data
list2_tf = tf.constant([[0.,0.,0.,1.,1.,1.,2.,2.,2.,3.,3.,3.]],dtype=tf.float32)
list1_tf = tf.constant([[0,1,2,-1,1,0,2,-1,2,0,1,-1]],dtype=tf.int32)
list2_np = np.array([[0.,0.,0.,1.,1.,1.,2.,2.,2.,3.,3.,3.]],dtype=np.float32)
list1_np = np.array([[0,1,2,-1,1,0,2,-1,2,0,1,-1]],dtype=np.int32)

# Save to onnx
model_proto, external_tensor_storage = tf2onnx.convert.from_keras(model,
            input_signature=[
                tf.TensorSpec(shape=[None,None], dtype=tf.float32, name=&quot;list2&quot;),
                tf.TensorSpec(shape=[None,None], dtype=tf.int32, name=&quot;list1&quot;)
                ],
            opset=11,
            output_path=&quot;tmp.onnx&quot;)


# Load onnx runtime session
ort_session = onnxruntime.InferenceSession(&quot;tmp.onnx&quot;)
inputs = {&quot;list2&quot;:list2_np, &quot;list1&quot;:list1_np}

print(&quot;===================================================&quot;)
print(&quot;Original model evaluation:&quot;)
print(model([list2_tf,list1_tf]))
print(&quot;ORT session evaluation&quot;)
print(ort_session.run(None, inputs))
print(&quot;===================================================&quot;)

# Check with model checker
onnx.checker.check_model(onnx.load(&quot;tmp.onnx&quot;))
</code></pre>
<ul>
<li>ONNX version:  1.10.2</li>
<li>Python version: 3.7.7</li>
<li>TF version: 2.7.0</li>
</ul>
<p>Related github issues I submitted:</p>
<ul>
<li><a href=""https://github.com/onnx/onnx/issues/3909"" rel=""nofollow noreferrer"">https://github.com/onnx/onnx/issues/3909</a></li>
<li><a href=""https://github.com/onnx/tensorflow-onnx/issues/1812"" rel=""nofollow noreferrer"">https://github.com/onnx/tensorflow-onnx/issues/1812</a></li>
</ul>
",0
70600297,Change of type via map method,"<p>I am struggling with a detail in Tensorflow regarding the <code>map</code> method of <code>Dataset</code> as described <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map"" rel=""nofollow noreferrer"">here</a>. The example</p>
<pre><code>dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
dataset = dataset.map(lambda x: x + 2)
list(dataset.as_numpy_iterator())
</code></pre>
<p>works fine, but changing the element type by applying <code>map</code> as</p>
<pre><code>dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
dataset = dataset.map(lambda x: x / 10.0)
list(dataset.as_numpy_iterator())
</code></pre>
<p>yields the error message</p>
<pre><code>TypeError: `x` and `y` must have the same dtype, got tf.int32 != tf.float32.
</code></pre>
<p>because the return type of the applied map function is not the same as its input type. Why is that the case? Is it impossible to change the type? If so, how can I achieve the desired result of changing the element type in the dataset to <code>tf.float32</code>?</p>
<p>Note that the actual dataset is more complex, but this is a minimum example illustrating the issue.</p>
",0
70657656,spliting custom binary dataset in train/test subsets using tensorflow io,"<p>I am trying to use local binary data to train a network to perform <a href=""https://www.tensorflow.org/tutorials/keras/regression"" rel=""nofollow noreferrer"">regression inference</a>.</p>
<p>Each local binary data has the following layout:</p>
<p><a href=""https://i.stack.imgur.com/tOsja.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tOsja.png"" alt=""enter image description here"" /></a></p>
<p>and the whole data consists of several *.bin files with the layout above. Each file has a variable number of sequences of 403*4 bytes. I was able to read one of those files using the following code:</p>
<pre><code>import tensorflow as tf

RAW_N = 2 + 20*20 + 1

def convert_binary_to_float_array(register):
     return tf.io.decode_raw(register, out_type=tf.float32)

raw_dataset = tf.data.FixedLengthRecordDataset(filenames=['mydata.bin'],record_bytes=RAW_N*4)
raw_dataset = raw_dataset.map(map_func=convert_binary_to_float_array)
</code></pre>
<p>Now, I need to create 4 datasets <code>train_data</code>, <code>train_labels</code>, <code>test_data</code>, <code>test_labels</code> as follows:</p>
<pre><code>train_data, train_labels, test_data, test_labels = prepare_ds(raw_dataset, 0.8)
</code></pre>
<p>and use them to train &amp; evaluate:</p>
<pre><code>model = build_model()

history = model.fit(train_data, train_labels, ...)

loss, mse = model.evaluate(test_data, test_labels)
</code></pre>
<p>My question is: how to implement function <code>prepare_ds(dataset, frac)</code>?</p>
<pre><code>def prepare_ds(dataset, frac):
    ...
</code></pre>
<p>I have tried to use <code>tf.shape</code>, <code>tf.reshape</code>, <code>tf.slice</code>, subscription [:] with no success. I realized that those functions doesn't work properly because after the <code>map()</code> call <code>raw_dataset</code> is a <code>MapDataset</code> (as a result of the eager execution concerns).</p>
",0
70686521,"Slice of 20 elements of rank1 tensor then reshaping throws ""Input to reshape is tensor with 10272 values, but requested shape requires multiple of 20""","<p>My input tensor <code>Data = Input(shape=(856,))</code> is a vector of float32 values concatenated from many different devices. I am trying to apply different TensorFlow functions to different subslices of each input chunk. Some of these functions include a 1D Convolution which requires a reshape.</p>
<pre><code>slice = Data[:20]
reshape = tf.reshape(slice, (-1, 20, 1))
...
</code></pre>
<p>Doing this crashes after trying to fit my model. It throws the following errors:</p>
<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input to reshape is a tensor with 10272 values, but the requested shape requires a multiple of 20
         [[node model/tf.reshape_1/Reshape
 (defined at /home/.local/lib/python3.8/site-packages/keras/layers/core/tf_op_layer.py:261)
]] [Op:__inference_train_function_1858]

Errors may have originated from an input operation.
Input Source operations connected to node model/tf.reshape_1/Reshape:
In[0] model/tf.__operators__.getitem_1/strided_slice:
In[1] model/tf.reshape_1/Reshape/shape:
</code></pre>
<p>I am not sure how slicing 20 elements from a tensor of 856 could result in a tensor of 10272 values.</p>
<p>I have also tried using the <code>tf.slice</code> function a couple of different ways; both fail. Referencing the docs: <a href=""https://www.tensorflow.org/guide/tensor_slicing"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/tensor_slicing</a></p>
<pre><code>slice = tf.slice(Data, begin=[0], size=[20]) 
...
</code></pre>
<p>And fails, stating:</p>
<pre><code>Shape must be rank 1 but is rank 2 for '{{node tf.slice/Slice}} = Slice[Index=DT_INT32, T=DT_FLOAT](Placeholder, tf.slice/Slice/begin, tf.slice/Slice/size)' with input shapes: [?,856], [1], [1].
</code></pre>
<p>For reference, here is what some of the values look like in the input data</p>
<pre><code>array([-9.55784683e+01, -1.70557899e+01,  2.95967350e+01,  7.81378937e+00,
        9.02729130e+00,  5.49621725e+00,  4.19811630e+00,  5.84186697e+00,
        4.90438080e+00,  3.73845983e+00,  5.12300587e+00,  2.61530232e+00,
        2.67061424e+00,  3.91038632e+00,  2.31110978e+00,  4.20644665e+00,
        4.50000000e+00,  9.87345278e-01,  1.59740388e+00,  6.30727148e+00,
...
</code></pre>
",0
70709785,Creating input tensors with the right dimensions from data,"<p>I have 4 features, 2 continous ones taking the form of</p>
<pre><code>[1,2,3,4,1,1,...]
</code></pre>
<p>And 2 categoric in the form of</p>
<pre><code>[[&quot;A&quot;],[&quot;A&quot;,&quot;B&quot;],[&quot;A&quot;,&quot;B&quot;,&quot;C&quot;], [&quot;A&quot;, &quot;C&quot;]]
</code></pre>
<p>My labels take the same form as continous features, from 0 to 8 for multiclassification. My goal is to predict the label class based on the 4 features.</p>
<p>I extract my data from a json file into a Pandas dataframe that looks like this:</p>
<pre><code>         f1         f2   f3                f4         label
 0        1          3     [R]              [None]        1
 1        2          2     [U, W]           [Flying]      2
 2        1          4     [None]           [None]        2
 3        1          2     [B]              [Flying]      0
..     ...        ...     ...                 ...   ...
</code></pre>
<p>From my understanding of the <a href=""https://www.tensorflow.org/guide/ragged_tensor"" rel=""nofollow noreferrer"">ragged tensor</a> documentation I can directly feed all of these features into my model like so:</p>
<pre><code>import tensorflow as tf
f1 = tf.keras.Input(shape=(1,),dtype=tf.dtypes.int32)
f2 = tf.keras.Input(shape=(1,),dtype=tf.dtypes.int32)
f3 = tf.keras.Input(shape=(None,),dtype=tf.dtypes.string, ragged=True)
f4 = tf.keras.Input(shape=(None,),dtype=tf.dtypes.string, ragged=True)
</code></pre>
<p>After that I normalize f1 and f2, and use a StringLookup, Embedding and Flatten layer for f3 and f4. Then I concatenate and feed them into a couple of Dense layers and then into a final dense layer using a softmax.</p>
<p>My model builds sucesfully.</p>
<p>However when I pass my dataframe to my training function like so:</p>
<pre><code>features = [&quot;f1&quot;,&quot;f2&quot;,&quot;f3&quot;,&quot;f4&quot;]
model.fit(x=[training_set[feature] for feature in features],
             y=training_set[label],
             validation_split=0.1)
</code></pre>
<p>I get the following error</p>
<pre><code>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).
</code></pre>
<p>Next I tried manually turning my dataframe into NumPy arrays with the right type:</p>
<pre><code>import numpy as np
f1 = np.asarray(training_set[&quot;f1&quot;]).astype(np.int32)
f2 = np.asarray(training_set[&quot;f2&quot;]).astype(np.int32)
f3 = np.asarray(training_set[&quot;f3&quot;]).astype(object)
f4 = np.asarray(training_set[&quot;f4&quot;]).astype(object)
l = np.asarray(training_set[&quot;label&quot;]).astype(np.int32)
model.fit(x=[f1,f2,f3,f4],
          y=l,
          validation_split=0.1)   
</code></pre>
<p>Which produces the same error:</p>
<pre><code>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).
</code></pre>
<p>It seems to me like instead of Numpy Arrays I should convert the data to tensors and feed that into the fit method. If I interpret the <a href=""https://www.tensorflow.org/guide/data"" rel=""nofollow noreferrer"">tf.Data documentation</a> correctly this should be possible?</p>
<p>In trying to do so I got stuck at getting the dimensions to be right, e.g :</p>
<pre><code>f1 = tf.stack([tf.convert_to_tensor(i) for i in training_set[&quot;f1&quot;].values],axis=0)
# Is shape (622,) afaik I need shape(1,)?
f3 = tf.ragged.stack(data[&quot;colors&quot;])
# Is shape (622, None) afaik needs to be shape (None,)
</code></pre>
<p>What am I doing wrong?</p>
",1
70811536,How Tensorflow(2.0) distributed dataset manage data,"<p>I'm a newbie to Tensorflow. I have been learning how to use TensorFlow to train models in a distributed manner and I have access to multiple servers, each with multiple CPUs.</p>
<p>Training mechanisms are clearly outlined in <a href=""https://www.tensorflow.org/guide/distributed_training"" rel=""nofollow noreferrer"">documentation</a> and <a href=""https://www.tensorflow.org/tutorials/distribute/multi_worker_with_ctl"" rel=""nofollow noreferrer"">tutorials</a>, but there are some ambiguities regarding data management while training multiple workers. In my understanding, data <em>should be shared and stored on a single machine</em>, and <code>tf.distribute.DistributedDataset</code> distributes data among workers.</p>
<p>Is my understanding that shared data is stored on one machine correct?</p>
<p>Think of a situation where we have multiple workers in our network and we want to train a model for 10 epochs on a large dataset. Is it true that <code>tf.distribute.DistributedDataset</code> sends data to workers 10 times? Are there any mechanisms to prevent the same batches of data from being sent to the same worker ten times?</p>
<p><a href=""https://medium.datadriveninvestor.com/distributed-data-processing-with-apache-spark-2a5e473b0cb1"" rel=""nofollow noreferrer"">This post</a>, for instance, states that:</p>
<blockquote>
<p>Spark and HDFS are designed to work well together. When Spark needs some data from HDFS, it grabs the closest copy which minimizes the time data spends traveling around the network.</p>
</blockquote>
<p>I'm looking for something similar for Tensorflow's distributed training.</p>
",1
70870188,How to use legacy_seq2seq for TensorFlow 2?,"<p>I am new to TensorFlow and I am wanting to use tensorflow.config.legacy_seq2se, specifically <a href=""https://docs.w3cub.com/tensorflow%7Epython/tf/contrib/legacy_seq2seq/embedding_rnn_seq2seq"" rel=""nofollow noreferrer"">embedding_rnn_seq2seq()</a> and I can't figure out how to use it (or if there is an equivalent method) for TensorFlow 2.</p>
<p>I know that in TensorFlow 2, TensorFlow removed contrib and according to <a href=""https://docs.google.com/spreadsheets/d/1FLFJLzg7WNP6JHODX5q8BDgptKafq_slHpnHVbJIteQ/edit#gid=0"" rel=""nofollow noreferrer"">this document</a>
tf.contrib.legacy_seq2seq has been deleted and replaced with tf.seq2seq in TensorFlow 2, but I can't find embedding_rnn_seq2seq() in the <a href=""https://www.tensorflow.org/addons/api_docs/python/tfa/seq2seq"" rel=""nofollow noreferrer"">tf.seq2seq documentation</a> I have seen.</p>
<p>The reason I want to use it is I am trying to implement something similar to what is done with embedding_rnn_seq2seq() in this <a href=""https://adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me"" rel=""nofollow noreferrer"">article</a>. So is there an equivalent in tensorflow 2, or is there a different way to achieve the same goal?</p>
",1
70876534,Error remains after applying fix_shape: ValueError: slice index 0 of dimension 0 out of bounds,"<p>I'm trying to build a Neural Network with Tensorflow and I got really stuck there.</p>
<p>Here's what the network is being meant to accomplish:
A bunch of Pdfs with different contents has been converted to numpy arrays in the shape (22,512,512,3).</p>
<p>In each Pdf the first 22 pages were rendered into a 512x512px numpy array and saved as .npy file.</p>
<p>The Network is supposed detect the classification of a Pdf, for example drawings or instructions.</p>
<p>This basically works with a smaller amount of Pdfs which can be converted into numpy arrays in the RAM. But since there are about 100GB of learn data, I decided to map the preprocessed numpy files.</p>
<p>This is the code to load and map the numpy data:</p>
<pre class=""lang-python prettyprint-override""><code>class full_nn:
    def __init__(self):
        self.data_dir = &quot;E:\\numpy_arrays\\&quot;        
        self.seq_ds = tf.data.Dataset.list_files(self.data_dir+&quot;*\\*.npy&quot;, shuffle=False)        
        self.class_names = list(sorted([item.name for item in pathlib.Path(self.data_dir).glob('*')]))
        self.data_size = len(list(pathlib.Path(self.data_dir).glob('*\\*.npy')))
        self.train_size = int(self.data_size * 0.8)
        self.train_ds = self.seq_ds.take(self.train_size)
        self.test_ds = self.seq_ds.skip(self.train_size)
                    
nn = full_nn()

def get_label(file_path):    
    lbl = tf.strings.split(file_path, os.path.sep)[-2]
    return nn.class_names.index(lbl)

def process_image_batch(file_path):
    label = get_label(file_path) 
    #decode tf.string to string
    file_path = file_path.numpy()    
    file_path = file_path.decode('UTF-8') 
    seq = np.load(file_path)     
    return (seq, label)

def fix_shape(sequence, label):
    label.set_shape([])
    return sequence, label
        
train_ds = nn.train_ds.map(lambda x: tf.py_function(process_image_batch, [x], [tf.float32, tf.int32])).map(fix_shape)
test_ds = nn.test_ds.map(lambda x: tf.py_function(process_image_batch, [x], [tf.float32,  tf.int32])).map(fix_shape)

</code></pre>
<p><strong>Edit:</strong> Not converting the tf.string produced this issue:</p>
<pre class=""lang-python prettyprint-override""><code>TypeError: Target data is missing. Your model has `loss`: &lt;keras.losses.SparseCategoricalCrossentropy object at 0x0000029D9F1AFD30&gt;, and therefore expects target data to be passed in `fit()`.
</code></pre>
<p>This is the neural network so far, influenced by <a href=""https://bleedai.com/human-activity-recognition-using-tensorflow-cnn-lstm/"" rel=""nofollow noreferrer"">this</a>:</p>
<pre class=""lang-python prettyprint-override""><code>model = keras.models.Sequential([
    layers.TimeDistributed(layers.Convolution2D(32, (7,7), strides=(2, 2),
      padding='same', activation='relu'), input_shape=(22,512,512,1)),
    layers.TimeDistributed(layers.Convolution2D(32, (3,3),
      kernel_initializer=&quot;he_normal&quot;, activation='relu')),
    layers.TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2))),
    
    layers.TimeDistributed(layers.Convolution2D(64, (3,3),
      padding='same', activation='relu')),
    layers.TimeDistributed(layers.Convolution2D(64, (3,3),
      padding='same', activation='relu')),
    layers.TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2))),

   layers.TimeDistributed(layers.Convolution2D(128, (3,3),
      padding='same', activation='relu')),
   layers.TimeDistributed(layers.Convolution2D(128, (3,3),
      padding='same', activation='relu')),
   layers.TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2))),

   layers.TimeDistributed(layers.Convolution2D(256, (3,3),
      padding='same', activation='relu')),
   layers.TimeDistributed(layers.Convolution2D(256, (3,3),
      padding='same', activation='relu')),
   layers.TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2))),

   layers.TimeDistributed(layers.Flatten()),

   
   layers.LSTM(512, return_sequences=False, dropout=0.2),   
   
   layers.Dense(nn.data_size, activation='softmax')
    
    ])
</code></pre>
<p>Here's how I build the model:</p>
<pre class=""lang-python prettyprint-override""><code>loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optim = keras.optimizers.Adam(learning_rate=0.001)
metrics=[&quot;accuracy&quot;]

model.compile(loss=loss,optimizer=optim, metrics=metrics)
model.fit(train_ds, batch_size=8, epochs=5, verbose= 2, validation_data=test_ds)
</code></pre>
<p>Now here is the question: What causes the model fitting to fail? I get this error message:</p>
<pre class=""lang-python prettyprint-override""><code>ValueError: slice index 0 of dimension 0 out of bounds. for '{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)' with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = &lt;0&gt;, input[2] = &lt;1&gt;, input[3] = &lt;1&gt;.
</code></pre>
<p><strong>Note:</strong> My original question was what produced the mentioned &quot;Target data is missing&quot; error, which could be resolved by converting the tf.string into a string before loading the numpy file.</p>
",0
70880589,what does cardinality mean in relation to an image dataset?,"<p>After successfully creating a tensorflow image <code>Dataset</code> with:</p>
<p><code>dataset = tf.keras.utils.image_dataset_from_directory(...)</code></p>
<p>which returns</p>
<p><em>Found 21397 files belonging to 5 classes.
Using 17118 files for training.</em></p>
<p>There is the cardinality method:</p>
<p><code>dataset.cardinality()</code></p>
<p>which returns a tensor containing the single value</p>
<p><em>tf.Tensor(535, shape=(), dtype=int64)</em></p>
<p>I've read the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/experimental/cardinality"" rel=""nofollow noreferrer"">docs here</a> but I don't understand what 535 represents or why its different to the number of files?</p>
<p>I ask, because I would like to understand how cardinality plays into this equation:</p>
<p><code>steps_per_epoch = dataset.cardinality().numpy() // batch_size</code></p>
",1
70944546,What is the Tensorflow 2.4.1 analogous of TPU_strategy.experimental_run_v2 from version 2.1? How to replace that?,"<p>I am following <a href=""https://www.kaggle.com/riblidezso/finetune-xlm-roberta-on-jigsaw-test-data-with-mlm"" rel=""nofollow noreferrer"">this old notebook on Kaggle for BERT MLM training</a> where the <code>tensorflow</code> version is <code>2.1</code>. I cloned and tried running the code but there's an error that <code>strategy has no experimental_run_v2</code>.</p>
<p>In the <a href=""https://www.tensorflow.org/tutorials/distribute/custom_training"" rel=""nofollow noreferrer"">official documentation of Custom training in TPU's</a> this piece of information is given but i'm not able to grasp what do I have to change in my code to make it run:</p>
<pre><code># `run` replicates the provided computation and runs it
# with the distributed input.
@tf.function
def distributed_train_step(dataset_inputs):
  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))
  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,
                         axis=None)

@tf.function
def distributed_test_step(dataset_inputs):
  return strategy.run(test_step, args=(dataset_inputs,))

</code></pre>
<p>Below is the code which I am trying to run and I have commented the troublesome part. Could someone please help me with proper restructuring of this code?</p>
<pre><code>
def train_mlm(train_dist_dataset, total_steps=2000, evaluate_every=200):
    step = 0
    ### Training lopp ###
    for tensor in train_dist_dataset:
        distributed_mlm_train_step(tensor) # --------- HERE IS THE ERROR ----- 
        step+=1

        if (step % evaluate_every == 0):   
            ### Print train metrics ###  
            train_metric = train_mlm_loss_metric.result().numpy()
            print(&quot;Step %d, train loss: %.2f&quot; % (step, train_metric))     

            ### Reset  metrics ###
            train_mlm_loss_metric.reset_states()
            
        if step  == total_steps:
            break


@tf.function # What Should be replaced with this line of code?
def distributed_mlm_train_step(data):
    strategy.experimental_run_v2(mlm_train_step, args=(data,)) # this is what causing the error

</code></pre>
<p>I think I have to use something to add the total error like the one in the documentation <code>strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)</code> but using this one gave me another obvious error <code>ValueError: A non-DistributedValues value None cannot be reduced with the given reduce op ReduceOp.SUM.</code></p>
",0
70950860,How to update tf.contrib.layers.real_valued_column code to tf 2.0,"<p>I came across a Gaussian Kernel classifier tutorial in Python that utilizes tensorflow. In part of the tutorial,  <code>tf.contrib.layers.real_valued_column</code> is referenced. However, as far as I know,  <code>tf.contrib</code> is deprecated.</p>
<p>How can I change the statement to work in <code>tensorflow 2.0</code> ?</p>
<p>I tried using  <code>tf_slim</code> and  <code>tf.keras</code>, but I was unable to find the equivalent statements. Thanks in advance.</p>
",0
70970323,multiple image input training using dataset object,"<p>How to use a dataset object as input in the <code>model.fit()</code> training loop, for a model with multiple inputs?
Trying to pass the dataset itself gives me the following error:</p>
<pre><code>Failed to find data adapter that can handle input: (&lt;class 'list'&gt; containing values of types {&quot;&lt;class 'tensorflow.python.data.ops.dataset_ops.MapDataset'&gt;&quot;}), &lt;class 'NoneType'&gt;
</code></pre>
<p>My case here:</p>
<p>I have a multiple input model built with keras
The inputs are named <code>'First'</code>, <code>'Second'</code> and <code>'Third'</code></p>
<p>I have an image dataset in keras-style:</p>
<pre><code>main_directory/
...class_a/
......a_image_1.jpg
......a_image_2.jpg
...class_b/
......b_image_1.jpg
......b_image_2.jpg
</code></pre>
<p>I create the dataset object using <code>tf.keras.utils.image_dataset_from_directory</code>:</p>
<pre><code>train_dataset = image_dataset_from_directory(train_dir,
                                             shuffle=False,
                                             label_mode='categorical',
                                             batch_size=hyperparameters[&quot;BATCH_SIZE&quot;],
                                             image_size=IMG_SIZE)
</code></pre>
<p>Now, each image is divided in 3 parts, each part serving as input to each of the inputs of the model. I take care of that using some map functions. This is not relevant tot he problem and I will not include it. I cannot use the cropping layers included in TF because of unrelated reasons.</p>
<p>I then try to start the training loop:</p>
<pre><code> history = model.fit([train_dataset1, 
                      train_dataset2, 
                      train_dataset3,
                      ],
                      epochs=epochs, 
                      callbacks=callbacks,
                      validation_data=validation_dataset
                      validation_steps=steps
                      )
</code></pre>
<p>And here is where I get the error.
I have tried some other approaches, like using a dict instead of a list.
The problem seems to be that when training a model with multiple inputs, the fit() loop expects data to come as a list for x-values and a list for y-values, but I haven't been able to split the dataset object into the required formats</p>
<p>I have read many topics on this, but all use datasets that are created using the <code>tf.data.Dataset.from_tensor_slices()</code> method, which is not applicable in my case</p>
<p>Additionally, there is no indication of how the validation dataset has to be structured (at least according to the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit"" rel=""nofollow noreferrer"">model.fit() documentation</a>)</p>
<p>I have found some guidance saying that the validation dataset must have the same number of input/outputs as the training datasets (makes sense), but again, no indication on how to build or feed the validation dataset for a multiple input model</p>
",0
71002866,Difference between Tokenizer and TextVectorization layer in tensorflow,"<p>New to TensorFlow</p>
<p>I saw couple of small NLP projects where people use the 'tf.keras.preprocessing.Tokenizer' to pre-process their text (link: <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer</a> )</p>
<p>In some cases, they directly add 'tf.keras.layers.TextVectorization' layer while making the model (link : <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization</a>)</p>
<p>May I know what's the difference between the two in terms of usage and when to choose which option?</p>
",0
71006896,Fine tuning a model - base_model Dropout in inference or training mode?,"<p>In the TensorFlow documentation it is highlighted that it is important during fine tuning to set the base_model to ’inference mode’ setting the parameter <code>training = False</code> when calling the <code>base_model</code>. The reason to do so is because of the <code>tf.keras.layers.BatchNormalization</code> layers, that should be executed in inference mode during fine tuning.<br />
<a href=""https://www.tensorflow.org/tutorials/images/transfer_learning#fine_tuning"" rel=""nofollow noreferrer"">TensorFlow documentation on Fine Tuning</a></p>
<p>But setting the <code>base_model</code> to inference mode will also affect the <code>tf.keras.layers.Dropout</code> in the <code>base_model</code> as these will then also run in inference mode and will not apply any dropout at all.</p>
<p>What is useful for getting meaningful results when fine tuning a model?</p>
<p>Running the dropout layers in the <code>base_model</code> in inference mode (no dropout at all) or running them in training mode applying the dropout as defined in the <code>base_model</code>?</p>
",0
71019644,Equivalent tensorflow expression to numpy mask,"<p>I have a numpy array named PixelData of unknown shape, and I am using the following condition to filter values in the array greater than some value x using a mask:</p>
<pre><code>PixelData[PixelData&gt;=x] = PixelData[PixelData&gt;=x] - x
</code></pre>
<p>When I convert this numpy array to a tensor, I cannot perform the same masking operation. I have tried using tf.where as follows:</p>
<pre><code>PixelData = tf.where(PixelData&gt;=x, PixelData - x, PixelData)
</code></pre>
<p>In the official documentation, they always seem to define the mask dimensions in advance to equal the dimensions of the tensor being masked, but then they talk about the dimensions being broadcasted automatically, so I am a bit confused. Are these two functions equivalent? Are there any situations where they may produce different outputs?</p>
",1
71068368,"Tensorflow custom op, gpu kernel returns a tensor of zeros","<p>I am trying to implement a custom op and I am using the example in the <a href=""https://www.tensorflow.org/guide/create_op"" rel=""nofollow noreferrer"">official documentation</a> as a benchmark to test the correct compilation of the op, I've just modified the gpu kernel in order to see if it was actually executed but when I test the op it returns all zeros.</p>
<p>kernel_example.h</p>
<pre class=""lang-py prettyprint-override""><code>// kernel_example.h
#ifndef KERNEL_EXAMPLE_H_
#define KERNEL_EXAMPLE_H_

#include &lt;unsupported/Eigen/CXX11/Tensor&gt;

template &lt;typename Device, typename T&gt;
struct ExampleFunctor {
  void operator()(const Device&amp; d, int size, const T* in, T* out);
};

#if GOOGLE_CUDA
// Partially specialize functor for GpuDevice.
template &lt;typename T&gt;
struct ExampleFunctor&lt;Eigen::GpuDevice, T&gt; {
  void operator()(const Eigen::GpuDevice&amp; d, int size, const T* in, T* out);
};
#endif

#endif
</code></pre>
<p>kernel_example.cc</p>
<pre class=""lang-py prettyprint-override""><code>// kernel_example.cc
#include &quot;kernel_example.h&quot;

#include &quot;tensorflow/core/framework/op.h&quot;
#include &quot;tensorflow/core/framework/shape_inference.h&quot;
#include &quot;tensorflow/core/framework/op_kernel.h&quot;

using namespace tensorflow;

using CPUDevice = Eigen::ThreadPoolDevice;
using GPUDevice = Eigen::GpuDevice;

REGISTER_OP(&quot;Example&quot;)
    .Attr(&quot;T: numbertype&quot;)
    .Input(&quot;input: T&quot;)
    .Output(&quot;input_times_two: T&quot;)
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      c-&gt;set_output(0, c-&gt;input(0));
      return Status::OK();
    });

// CPU specialization of actual computation.
template &lt;typename T&gt;
struct ExampleFunctor&lt;CPUDevice, T&gt; {
  void operator()(const CPUDevice&amp; d, int size, const T* in, T* out) {
    for (int i = 0; i &lt; size; ++i) {
      out[i] = 2 * in[i];
    }
  }
};

// OpKernel definition.
// template parameter &lt;T&gt; is the datatype of the tensors.
template &lt;typename Device, typename T&gt;
class ExampleOp : public OpKernel {
 public:
  explicit ExampleOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor&amp; input_tensor = context-&gt;input(0);

    // Create an output tensor
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context-&gt;allocate_output(0, input_tensor.shape(),
                                                     &amp;output_tensor));

    // Do the computation.
    OP_REQUIRES(context, input_tensor.NumElements() &lt;= tensorflow::kint32max,
                errors::InvalidArgument(&quot;Too many elements in tensor&quot;));
    ExampleFunctor&lt;Device, T&gt;()(
        context-&gt;eigen_device&lt;Device&gt;(),
        static_cast&lt;int&gt;(input_tensor.NumElements()),
        input_tensor.flat&lt;T&gt;().data(),
        output_tensor-&gt;flat&lt;T&gt;().data());
  }
};

// Register the CPU kernels.
#define REGISTER_CPU(T)                                          \
  REGISTER_KERNEL_BUILDER(                                       \
      Name(&quot;Example&quot;).Device(DEVICE_CPU).TypeConstraint&lt;T&gt;(&quot;T&quot;), \
      ExampleOp&lt;CPUDevice, T&gt;);
REGISTER_CPU(float);
REGISTER_CPU(int32);

// Register the GPU kernels.
#ifdef GOOGLE_CUDA
#define REGISTER_GPU(T)                                          \
  /* Declare explicit instantiations in kernel_example.cu.cc. */ \
  extern template class ExampleFunctor&lt;GPUDevice, T&gt;;            \
  REGISTER_KERNEL_BUILDER(                                       \
      Name(&quot;Example&quot;).Device(DEVICE_GPU).TypeConstraint&lt;T&gt;(&quot;T&quot;), \
      ExampleOp&lt;GPUDevice, T&gt;);
REGISTER_GPU(float);
REGISTER_GPU(int32);
#endif  // GOOGLE_CUDA
</code></pre>
<p>kernel_example.cu.cc</p>
<pre class=""lang-py prettyprint-override""><code>// kernel_example.cu.cc
#ifdef GOOGLE_CUDA
#define EIGEN_USE_GPU
#include &quot;kernel_example.h&quot;
#include &quot;tensorflow/core/util/gpu_kernel_helper.h&quot;

using namespace tensorflow;

using GPUDevice = Eigen::GpuDevice;

// Define the CUDA kernel.
template &lt;typename T&gt;
__global__ void ExampleCudaKernel(const int size, const T* in, T* out) {
  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; size;
       i += blockDim.x * gridDim.x) {
    out[i] = 3 * __ldg(in + i); # modified to check if it is executed
  }
}

// Define the GPU implementation that launches the CUDA kernel.
template &lt;typename T&gt;
void ExampleFunctor&lt;GPUDevice, T&gt;::operator()(
    const GPUDevice&amp; d, int size, const T* in, T* out) {
  // Launch the cuda kernel.
  //
  // See core/util/gpu_kernel_helper.h for example of computing
  // block count and thread_per_block count.
  int block_count = 1024;
  int thread_per_block = 20;
  ExampleCudaKernel&lt;T&gt;
      &lt;&lt;&lt;block_count, thread_per_block, 0, d.stream()&gt;&gt;&gt;(size, in, out);
}

// Explicitly instantiate functors for the types of OpKernels registered.
template struct ExampleFunctor&lt;GPUDevice, float&gt;;
template struct ExampleFunctor&lt;GPUDevice, int32&gt;;

#endif  // GOOGLE_CUDA
</code></pre>
<p>To compile the op I use the following Makefile</p>
<pre><code># Makefile
TF_COM = `python -c &quot;import tensorflow as tf; print(' '.join(tf.sysconfig.get_compile_flags()))&quot;`
TF_INC = `python -c &quot;import tensorflow as tf; print(tf.sysconfig.get_include())&quot;`
TF_LIN = `python -c &quot;import tensorflow as tf; print(' '.join(tf.sysconfig.get_link_flags()))&quot;`

CC        = gcc -O2 -pthread
GPUDEF    = -D GOOGLE_CUDA=1

# nvcc: cuda kernel compilation to obtain .o file
GPUCC     = nvcc
GPUCFLAGS = -std=c++14 -I$(TF_INC) --expt-relaxed-constexpr -c
GPULFLAGS = -x cu -Xcompiler -fPIC

GPUSRC    = kernel_example.cu.cc
GPUPROD   = kernel_example.cu.o


# g++: combines the source .cc and the gpu prod .o to create a .so
CXX       = g++ -O2
CFLAGS    = -std=c++14 $(TF_COM)
LFLAGS    = -shared -fPIC $(TF_LIN) 
LCUDA     = -lcuda -L /usr/local/cuda-11.0/lib64/

SRC       = kernel_example.cc
PROD      = kernel_example.so


default: gpu

cpu:
    $(CXX) $(CFLAGS) $(SRC) $(LFLAGS) -o $(PROD)

gpu:
    $(GPUCC) $(GPUCFLAGS) $(GPUSRC) $(GPULFLAGS) $(GPUDEF) -o $(GPUPROD)
    $(CXX) $(CFLAGS) $(SRC) $(GPUPROD) $(LFLAGS) $(LCUDA) $(GPUDEF) -o $(PROD)

clean:
    rm -f $(PROD) $(GPUPROD)
</code></pre>
<p>I'm currently working directly on google Colab, loading the previous files in <code>/content</code> and running the compilation with the following lines</p>
<pre><code>!mkdir -p /usr/local/lib/python3.7/dist-packages/tensorflow/include/third_party/gpus/cuda/
!ln -s /usr/local/cuda/include /usr/local/lib/python3.7/dist-packages/tensorflow/include/third_party/gpus/cuda/
!make gpu
</code></pre>
<p>The cpu op compilation works fine. The gpu op compilation does not give any error but when I test the op it does not work properly.</p>
<p>With an input of <code>input = tf.ones((4,))</code>, considered the update I made in the gpu kernel, I expect to get <code>[3., 3., 3., 3.]</code> but I actually get <code>[0., 0., 0., 0.]</code>, when eager execution is active.</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

kernel_example_module = tf.load_op_library('/content/kernel_example.so')
example = kernel_example_module.example

input = tf.ones((4,))
output = .example(input)
print(output.numpy())
</code></pre>
<p>Whereas when I run the same in graph mode I get <code>[2., 2., 2., 2.]</code>, which is the result I obtain from the cpu implementation, this makes me think the gpu kernel is kind not executed.</p>
<pre><code>import tensorflow as tf
tf.compat.v1.disable_eager_execution()

kernel_example_module = tf.load_op_library('/content/kernel_example.so')
example = kernel_example_module.example

with tf.device('/gpu:0'):
    input = tf.ones((4,))
    output = example(input)

with tf.compat.v1.Session() as sess:
    print(sess.run(output))
</code></pre>
<p>I've tried to link the cuda path in the Makefile (LCUDA), as suggested in another similar question, but this did not solve my problem. I feel something is wrong with compiling options but I really cannot find what the problem could be.</p>
<p>Any idea how to make the gpu kernel work correctly ?</p>
",1
71081095,How to make the training using Tensorflow 2 Object Detection API deterministic i.e. achieve Reproducibility?,"<p>I am using TF2 Object Detection API to train a ssd_resnet50. Each time I train it I get different losses and evaluation scores (tensorboard logs -- graphs).</p>
<p>I am using VOC2012 dataset to retrain a pretrained ssd_resnet50_v1_fpn_640x640_coco17_tpu-8 model. I have religiously followed the API setup using this link: <a href=""https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html"" rel=""nofollow noreferrer"">https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html</a>.</p>
<ol>
<li>Organised my workspace/training files</li>
<li>Prepared/annotated image datasets</li>
<li>Generate tf records from such datasets</li>
<li>Configured a simple training pipeline</li>
<li>Trained a model and monitored its progress</li>
</ol>
<p>Everything is working just fine except for the reproducibility. In order to train a customized model I am using the configurations mentioned below (pipeline.config).</p>
<pre><code>model {
  ssd {
    num_classes: 20 # Set this to the number of different label classes
    image_resizer {
      fixed_shape_resizer {
        height: 640
        width: 640
      }
    }
    feature_extractor {
      type: &quot;ssd_resnet50_v1_fpn_keras&quot;
      depth_multiplier: 1.0
      min_depth: 16
      conv_hyperparams {
        regularizer {
          l2_regularizer {
            weight: 0.00039999998989515007
          }
        }
        initializer {
          truncated_normal_initializer {
            mean: 0.0
            stddev: 0.029999999329447746
          }
        }
        activation: RELU_6
        batch_norm {
          decay: 0.996999979019165
          scale: true
          epsilon: 0.0010000000474974513
        }
      }
      override_base_feature_extractor_hyperparams: true
      fpn {
        min_level: 3
        max_level: 7
      }
    }
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
        use_matmul_gather: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    box_predictor {
      weight_shared_convolutional_box_predictor {
        conv_hyperparams {
          regularizer {
            l2_regularizer {
              weight: 0.00039999998989515007
            }
          }
          initializer {
            random_normal_initializer {
              mean: 0.0
              stddev: 0.009999999776482582
            }
          }
          activation: RELU_6
          batch_norm {
            decay: 0.996999979019165
            scale: true
            epsilon: 0.0010000000474974513
          }
        }
        depth: 256
        num_layers_before_predictor: 4
        kernel_size: 3
        class_prediction_bias_init: -4.599999904632568
      }
    }
    anchor_generator {
      multiscale_anchor_generator {
        min_level: 3
        max_level: 7
        anchor_scale: 4.0
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        scales_per_octave: 2
      }
    }
    post_processing {
      batch_non_max_suppression {
        score_threshold: 9.99999993922529e-09
        iou_threshold: 0.6000000238418579
        max_detections_per_class: 100
        max_total_detections: 100
        use_static_shapes: false
      }
      score_converter: SIGMOID
    }
    normalize_loss_by_num_matches: true
    loss {
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_loss {
        weighted_sigmoid_focal {
          gamma: 2.0
          alpha: 0.25
        }
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
    encode_background_as_zeros: true
    normalize_loc_loss_by_codesize: true
    inplace_batchnorm_update: true
    freeze_batchnorm: false
  }
}
train_config {
  batch_size: 8 # Increase/Decrease this value depending on the available memory (Higher values require more memory and vice-versa)
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    random_crop_image {
      min_object_covered: 0.0
      min_aspect_ratio: 0.75
      max_aspect_ratio: 3.0
      min_area: 0.75
      max_area: 1.0
      overlap_thresh: 0.0
    }
  }
  sync_replicas: true
  optimizer {
    momentum_optimizer {
      learning_rate {
        cosine_decay_learning_rate {
          learning_rate_base: 0.03999999910593033
          total_steps: 25000
          warmup_learning_rate: 0.013333000242710114
          warmup_steps: 2000
        }
      }
      momentum_optimizer_value: 0.8999999761581421
    }
    use_moving_average: false
  }
  fine_tune_checkpoint: &quot;pre-trained-models/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0&quot; # Path to checkpoint of pre-trained model
  num_steps: 25000
  startup_delay_steps: 0.0
  replicas_to_aggregate: 8
  max_number_of_boxes: 100
  unpad_groundtruth_tensors: false
  fine_tune_checkpoint_type: &quot;detection&quot; # Set this to &quot;detection&quot; since we want to be training the full detection model
  use_bfloat16: false # Set this to false if you are not training on a TPU
  fine_tune_checkpoint_version: V2
}
train_input_reader {
  label_map_path: &quot;annotations/label_map.pbtxt&quot; # Path to label map file
  tf_record_input_reader {
    input_path: &quot;annotations/train.record&quot; # Path to training TFRecord file
  }
}
eval_config {
  metrics_set: &quot;coco_detection_metrics&quot;
  use_moving_averages: false
}
eval_input_reader {
  label_map_path: &quot;annotations/label_map.pbtxt&quot; # Path to label map file
  shuffle: false
  num_epochs: 1
  tf_record_input_reader {
    input_path: &quot;annotations/test.record&quot; # Path to testing TFRecord
  }
}
</code></pre>
<p>I tried a few things to achieve reproducibility:</p>
<ol>
<li>Set a global seed in various modules across the training pipeline</li>
<li>I tried setting up the operation-based seed for say shuffle, data augmentations etc.</li>
<li>Manually setting attributes coming from .proto files, like shuffle = False in build() function of Tensorflow/models/research/object_detection/builders/data_builder.py module
etc.</li>
<li>De-selecting or removing data_augmentation_options from pipeline.config file altogether.</li>
</ol>
<p><a href=""https://i.stack.imgur.com/jFjrw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jFjrw.png"" alt=""enter image description here"" /></a></p>
<p>The graphs above show 2 separate training runs (everything is kept the same in both experiments: global seeding is done in data_builder.py -- tf.random.set_seed(1234), no data augmentation selected i.e. &quot;data_augmentation_options&quot; removed from pipeline.config, data shuffle and related attributes adjusted as mentioned in point 3)</p>
<pre><code># Operations that rely on a random seed actually derive it from two seeds: 
# the global and operation-level seeds. Adding this on top of a module sets the global seed.
tf.random.set_seed(1234)
# Switch off shuffle, config.filenames_shuffle_buffer_size
config.shuffle = False
# Make number of readers to zero
config.num_readers = 0
# Set sample_from_datasets_weights to zero
config.sample_from_datasets_weights = 0
</code></pre>
",0
71086631,TF/Keras ways to dynamically load in memory training batches of images and text?,"<p>I am implementing a Handwritten Text Recognition system in Tensorflow using the Keras interface. In prediction phase, my system would take as an input the image of a line (.jpg format) and return as an output the corresponding transcription.
So this is not a classification problem: it is more akin to encoding-decoding, where the input is an image and the output is text.</p>
<p>To sum up, my training samples are couples of</p>
<ul>
<li>x: an image (read with cv2 from a .jpg file)</li>
<li>y: a transcription (read from a .txt file)</li>
</ul>
<p>By the way this is how they stored in the filesystem:</p>
<p><a href=""https://i.stack.imgur.com/KqfgMm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KqfgMm.png"" alt=""Just plain old files in folders"" /></a></p>
<p>In almost all the tutorials I have found, for the sake of simplicity all the training samples are loaded at once in memory at the beginning of the training process and then scanned using generator methods. With large datasets, however, loading all the training data in memory may lead to OOM before the training even begins.</p>
<p>To avoid this risk I would like to load batches in memory on the fly; so I am wondering if TF or Keras provide any built-in solution that loads the training data loading them <em>progressively</em>.</p>
<p>I have found that the Keras method <code>tf.keras.preprocessing.image_dataset_from_directory</code> does something pretty close to what I am thinking about, but apparently it only works for classification tasks (it requires your directory to have a sub-folder for each class; in my scenario, I do not have any classes).</p>
<p>I believe I can implement my own solution zipping an ImageDataGenerator and a... TextDataGenerator-something (not a real class, I should implement this one on my own). I found an example that zips and scans two ImageDataGenerators <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator"" rel=""nofollow noreferrer"">in the documentation</a>.</p>
<p>However my scenario should be quite common, so I have a hard time believing that it is not already handled by any pre-existing TF/Keras classes or methods. If possible, I would avoid re-inventing the wheel; do you have any suggestions?</p>
<p>Thank you for your help. I really appreciate it!</p>
",0
71129505,"Is it possible to split a tensorflow dataset into train, validation AND test datasets when using image_dataset_from_directory?","<p>I am using <code>tf.keras.utils.image_dataset_from_directory</code> to load a dataset of 4575 images. While this function allows to split the data into two subsets (with the <code>validation_split</code> parameter), I want to split it into training, testing, and validation subsets.</p>
<p>I have tried using <code>dataset.skip()</code> and <code>dataset.take()</code> to further split one of the resulting subsets, but these functions return a <code>SkipDataset</code> and a <code>TakeDataset</code> respectively (by the way, contrary to <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#take"" rel=""nofollow noreferrer"">the documentation</a>, where it is claimed that these functions return a <code>Dataset</code>). This leads to problems when fitting the model - the metrics calculated on validation sets (val_loss, val_accuracy) disappear from model history.</p>
<p>So, my question is: is there a way to split a <code>Dataset</code> into three subsets for training, validation and testing, so that all three subsets are also <code>Dataset</code> objects?</p>
<p><strong>Code used to load the data</strong></p>
<pre><code>def load_data_tf(data_path: str, img_shape=(256,256), batch_size: int=8):
    train_ds = tf.keras.utils.image_dataset_from_directory(
        data_path,
        validation_split=0.2,
        subset=&quot;training&quot;,
        label_mode='categorical',
        seed=123,
        image_size=img_shape,
        batch_size=batch_size)
    val_ds = tf.keras.utils.image_dataset_from_directory(
        data_path,
        validation_split=0.3,
        subset=&quot;validation&quot;,
        label_mode='categorical',
        seed=123,
        image_size=img_shape,
        batch_size=batch_size)
    return train_ds, val_ds

train_dataset, test_val_ds = load_data_tf('data_folder', img_shape = (256,256), batch_size=8)
test_dataset = test_val_ds.take(686)
val_dataset = test_val_ds.skip(686)
</code></pre>
<p><strong>Model compilation and fitting</strong></p>
<pre><code>model.compile(optimizer='sgd',
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])
history = model.fit(train_dataset, epochs=50, validation_data=val_dataset, verbose=1)
</code></pre>
<p><strong>When using a normal <code>Dataset</code>, <code>val_accuracy</code> and <code>val_loss</code> are present in the history of the model:</strong></p>
<p><a href=""https://i.stack.imgur.com/Qn1Yf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qn1Yf.png"" alt=""Expected behaviour: when using a Dataset, validation metrics are calculated"" /></a></p>
<p><strong>But when using a <code>SkipDataset</code>, they are not:</strong></p>
<p><a href=""https://i.stack.imgur.com/GMnBM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GMnBM.png"" alt=""Using the SkipDataset produced by test_val_ds.take() leads to validation metrics disappearing from model history"" /></a></p>
<p><a href=""https://i.stack.imgur.com/omU5U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/omU5U.png"" alt=""val_accuracy and val_loss are not present in history keys when using a SkipDataset or a TakeDataset"" /></a></p>
",1
71149271,"How to remove single feature from tensorflow dataset, how to use apply on single feture?","<p>I created dataset from csv file with dataset = tf.data.experimental.make_csv_dataset() function but My dataset has categorical and numeric features.</p>
<pre><code>dataset=
color  price weight
red    120    1.2
blue    80     2.0
green   90     3
</code></pre>
<p>Question 1:
The question is how can I  modify  only single feature, for example weight +2, to:</p>
<pre><code>dataset=
color  price weight
red    120    3.2
blue    80     4.0
green   90     5
</code></pre>
<p>I try to do something like:</p>
<pre><code>dataset = dataset.apply(lambda x: x['weight']+2)
</code></pre>
<p>but the error is: &quot;TypeError: 'FilterDataset' object is not subscriptable&quot;</p>
<p>Example from the documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply</a> doesn't show it.</p>
<p>Question 2:
How can I remove single feature ? Is there any equivalent to pandas drop column?</p>
",1
71179882,Runtime Error: tensorflow.python.framework.errors_impl.NotFoundError: Could not find metadata file. [[{{node LoadDataset/_1}}]] [Op:DatasetFromGraph],"<p>As <a href=""https://www.tensorflow.org/federated/tutorials/simulations"" rel=""nofollow noreferrer"">in the tutorial</a>, trying to execute <code>tff.learning.build_federated_averaging_process(model_fn, client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.02))</code> but on orchestrator (server) with data saved on edge node (client) using <code>tf.data.experimental.load()</code> method:</p>
<pre><code>@tff.tf_computation
def make_data():
    element_spec = collections.OrderedDict([('x', tf.TensorSpec(shape=(None, 784), dtype=tf.float32, name=None)),
             ('y', tf.TensorSpec(shape=(None,), dtype=tf.int32, name=None))])
    data = tf.data.experimental.load('./train_data', element_spec = element_spec)
    return data
</code></pre>
<p>However, I'm getting the following error:</p>
<pre><code>W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at dataset_ops.cc:175 : Not found: Could not find metadata file.
         [[{{node LoadDataset/_1}}]]
</code></pre>
<p>TF data was saved using <code>tf.data.experimental.save(train_data[0], './train_data')</code> method. The implementation works when executed locally: <code>tff.backends.native.set_local_execution_context()</code></p>
<p>python - 3.7</p>
<p>libraries versions:</p>
<p>tensorflow - 2.5.2</p>
<p>tensorflow-estimator - 2.5.0</p>
<p>tensorflow-federated - 0.19.0</p>
<p>Any help would be most appreciated.</p>
",0
71276062,segmentation fault in tensorflow eager execution?,"<p>I use the mirrored strategy in my Tensorflow2 code, as described in this tutorial: <a href=""https://www.tensorflow.org/guide/distributed_training"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/distributed_training</a>. I have almost the same exact code, and the setup is working well for about 1.5 years now. I regularly put the function call</p>
<pre><code>@tf.function
def distributed_train_step(dist_inputs):
</code></pre>
<p>in the eager mode for debugging purposes by simply commenting the @tf.function, worked well until now. This morning when I started the debugger, I got the following error message: Process finished with exit code 139 (interrupted by signal 11: SIGSEGV).
When I put the @tf.function in again, everything works well, it's just in the eager mode. I did even reset all my code and restored an old git commit which I know is working perfectly fine. Can someone explain why this error suddenly occurs in eager mode? I'm a bit lost here..</p>
",0
71315426,Using TFDS datasets with Keras Functional API,"<p>I'm trying to train a neural network made with the Keras Functional API with one of the default TFDS Datasets, but I keep getting dataset related errors.</p>
<p>The idea is doing a model for object detection, but for the first draft I was trying to do just plain image classification (img, label). The input would be (256x256x3) images. The input layer is as follows:</p>
<pre><code>img_inputs = keras.Input(shape=[256, 256, 3], name='image')
</code></pre>
<p>Then I'm trying to use the voc2007 dataset as available in TFDS (a very old and light version to make it faster)</p>
<pre><code>(train_ds, test_ds), ds_info = tfds.load(
'voc/2007',
split=['train', 'test'],
data_dir=&quot;/content/drive/My Drive&quot;,
with_info=True)
</code></pre>
<p>then preprocessing the data as follows:</p>
<pre><code>def resize_and_normalize_img(example):
  &quot;&quot;&quot;Normalizes images: `uint8` -&gt; `float32`.&quot;&quot;&quot;
  example['image'] = tf.image.resize(example['image'], [256, 256])
  example['image'] = tf.cast(example['image'], tf.float32) / 255.
  return example

def reduce_for_classification(example):
        for key in ['image/filename', 'labels_no_difficult', 'objects']:
            example.pop(key)
        return example

train_ds_class = train_ds.map(reduce_for_classification, num_parallel_calls=tf.data.AUTOTUNE)
train_ds_class = train_ds_class.map(resize_and_normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
train_ds_class = train_ds_class.cache()
train_ds_class = train_ds_class.shuffle(ds_info.splits['train'].num_examples)
train_ds_class = train_ds_class.batch(64)
train_ds_class = train_ds_class.prefetch(tf.data.AUTOTUNE)

test_ds_class = test_ds.map(reduce_for_classification, num_parallel_calls=tf.data.AUTOTUNE)
test_ds_class = test_ds_class.map(resize_and_normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
test_ds_class = test_ds_class.batch(64)
test_ds_class = test_ds_class.cache()
test_ds_class = test_ds_class.prefetch(tf.data.AUTOTUNE)
</code></pre>
<p>And then fitting the model like:</p>
<pre><code>epochs=8
history = model.fit(
  x=train_x, y =trian_y,
  validation_data=test_ds_clas,
  epochs=epochs
)
</code></pre>
<p>And after doing this is when I get an error saying that my model expects an input of shape [None, 256, 256, 3] but it's getting an input of shape [256, 256, 3].</p>
<p>I think it's an issue to do with the label. Before I got problems with the extra keys from the dictionary-like format of the data you get from tfds and tried to remove everything except the label, but now I'm still getting this and don't know how to go forward. I feel like after getting the dataset prepared with tfds it should be ready to be fed to a model, and after looking through the documentation, tutorials and stack overflow I haven't found the answer, I hope someone who comes across this can help.</p>
<p><strong>Update:</strong>
To give a bit more of information, this is the model I'm using:</p>
<p><strong>TLDR:</strong> Image input 256x256x3, a succession of convolutions and residual blocks, and an ending with average pooling, fully connected layer, and softmax that results in a (None, 1280) tensor. Using sparse categorical cross-entropy as loss and accuracy as metric.</p>
<pre><code>img_inputs = keras.Input(shape=[256, 256, 3], name='image')

# first convolution
conv_first = tf.keras.layers.Conv2D(32, kernel_size=(3, 3), padding='same', name='first_conv')
x = conv_first(img_inputs)

# Second convolution
x = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=2, padding='same', name='second_conv')(x)

# First residual block
res = tf.keras.layers.Conv2D(32, kernel_size=(1, 1), name='res_block1_conv1')(x)
res = tf.keras.layers.Conv2D(64, kernel_size=(3, 3), padding='same', name='res_block1_conv2')(res)
x = x + res

# Convolution after First residual block
x = tf.keras.layers.Conv2D(128, kernel_size=3, strides=2, padding='same', name='first_post_res_conv')(x)

# Second residual Block
for i in range(2):
  shortcut = x
  res = tf.keras.layers.Conv2D(64, kernel_size=1, name=f'res_block2_conv1_loop{i}')(x)
  res = tf.keras.layers.Conv2D(128, kernel_size=3, padding='same', name=f'res_block2_conv2_loop{i}')(res)

  x = res + shortcut

# Convolution after Second residual block
x = tf.keras.layers.Conv2D(256, 3, strides=2, padding='same', name='second_post_res_conv')(x)

# Third residual Block
for i in range(8):
  shortcut = x
  res = tf.keras.layers.Conv2D(128, kernel_size=1, name=f'res_block3_conv1_loop{i}')(x)
  res = tf.keras.layers.Conv2D(256, kernel_size=3, padding='same', name=f'res_block3_conv2_loop{i}')(res)

  x = res + shortcut

# Convolution after Third residual block
x = tf.keras.layers.Conv2D(512, 3, strides=2, padding='same', name='third_post_res_conv')(x)

# Fourth residual Block
for i in range(8):
  shortcut = x
  res = tf.keras.layers.Conv2D(256, kernel_size=1, name=f'res_block4_conv1_loop{i}')(x)
  res = tf.keras.layers.Conv2D(512, kernel_size=3, padding='same', name=f'res_block4_conv2_loop{i}')(res)

  x = res + shortcut

# Convolution after Fourth residual block
x = tf.keras.layers.Conv2D(1024, 3, strides=2, padding='same', name='fourth_post_res_conv')(x)

# Fifth residual Block
for i in range(4):
  shortcut = x
  res = tf.keras.layers.Conv2D(512, kernel_size=1, name=f'res_block5_conv1_loop{i}')(x)
  res = tf.keras.layers.Conv2D(1024, kernel_size=3, padding='same', name=f'res_block5_conv2_loop{i}')(res)

  x = res + shortcut

# Global avg pooling
x = tf.keras.layers.GlobalAveragePooling2D(name='average_pooling')(x)

# Fully connected layer
x = tf.keras.layers.Dense(1280, name='fully_connected_layer')(x)

# Softmax
end_result = tf.keras.layers.Softmax(name='softmax')(x)

model = tf.keras.Model(inputs=img_inputs, outputs=end_result, name=&quot;darknet53&quot;)

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

</code></pre>
<p>After trying the solution proposed by AloneTogether I'm getting the following errors (I tried changing the axis in the tf.one_hot() function many times and same result):</p>
<pre><code>Node: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'
logits and labels must have the same first dimension, got logits shape [64,1280] and labels shape [1280]
     [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_20172]

</code></pre>
<p>Which seems to be related to the batching, but don't know exactly how to fix it.</p>
<p>The whole issue really seems related to the labels encoding, because when running that line without the tf.reduce_sum() function I get the same but with:</p>
<pre><code>First element had shape [2,20] and element 1 had shape [1,20].
</code></pre>
<p>And if I run the same without the one-hot encoding line, I get this error:</p>
<p>´´´
Node: 'IteratorGetNext'
Cannot batch tensors with different shapes in component 1. First element had shape [4] and element 1 had shape [1].
[[{{node IteratorGetNext}}]] [Op:__inference_train_function_18534]
´´´</p>
",1
71332012,"What method will be called when executing embedding_layer(tf.constant([1, 2, 3]))","<p>The following code is excerpted from the following link:</p>
<p><a href=""https://www.tensorflow.org/text/guide/word_embeddings"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/guide/word_embeddings</a></p>
<pre><code>import tensorflow as tf

# Embed a 1,000 word vocabulary into 5 dimensions.
embedding_layer = tf.keras.layers.Embedding(1000, 5)
print(&quot;embedding_layer: {}&quot;.format(embedding_layer))

result = embedding_layer(tf.constant([1, 2, 3]))
print(&quot;result: {}&quot;.format(result.numpy()))

embedding_layer: &lt;keras.layers.embeddings.Embedding object at 0x7ffb180b17f0&gt;
result: [[-0.04678862 -0.03500976 -0.04254207 -0.0452533   0.04933525]
 [-0.0366199  -0.01814463  0.04166402  0.02388224  0.03472105]
 [ 0.02966919  0.04294082  0.00715581  0.0376732   0.00529655]]
</code></pre>
<p>When executing the &quot;<code>embedding_layer(tf.constant([1, 2, 3]))</code>&quot;, which method will be called from the tf.keras.layers.Embedding class?</p>
<p>Is it calling the <code>__init__</code> method?</p>
<p>The following code will throw the following error:</p>
<pre><code>embedding_layer = tf.keras.layers.Embedding(tf.constant([1, 2, 3]))

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/tmp/ipykernel_4537/3254652836.py in &lt;cell line: 10&gt;()
      8 print(&quot;result: {}&quot;.format(result.numpy()))
      9 
---&gt; 10 embedding_layer = tf.keras.layers.Embedding(tf.constant([1, 2, 3]))

TypeError: __init__() missing 1 required positional argument: 'output_dim'
</code></pre>
<p>Here is the source code of Embedding:</p>
<pre><code>class Embedding(Layer):
    
  def __init__(self,
               input_dim,
               output_dim,
               embeddings_initializer='uniform',
               embeddings_regularizer=None,
               activity_regularizer=None,
               embeddings_constraint=None,
               mask_zero=False,
               input_length=None,
               **kwargs):
    if 'input_shape' not in kwargs:
      if input_length:
        kwargs['input_shape'] = (input_length,)
      else:
        kwargs['input_shape'] = (None,)
    if input_dim &lt;= 0 or output_dim &lt;= 0:
      raise ValueError('Both `input_dim` and `output_dim` should be positive, '
                       'found input_dim {} and output_dim {}'.format(
                           input_dim, output_dim))
    if (not base_layer_utils.v2_dtype_behavior_enabled() and
        'dtype' not in kwargs):
      # In TF1, the dtype defaults to the input dtype which is typically int32,
      # so explicitly set it to floatx
      kwargs['dtype'] = backend.floatx()
    # We set autocast to False, as we do not want to cast floating- point inputs
    # to self.dtype. In call(), we cast to int32, and casting to self.dtype
    # before casting to int32 might cause the int32 values to be different due
    # to a loss of precision.
    kwargs['autocast'] = False
    super(Embedding, self).__init__(**kwargs)

    self.input_dim = input_dim
    self.output_dim = output_dim
    self.embeddings_initializer = initializers.get(embeddings_initializer)
    self.embeddings_regularizer = regularizers.get(embeddings_regularizer)
    self.activity_regularizer = regularizers.get(activity_regularizer)
    self.embeddings_constraint = constraints.get(embeddings_constraint)
    self.mask_zero = mask_zero
    self.supports_masking = mask_zero
    self.input_length = input_length
</code></pre>
",0
71484472,Tensorflow Probability VI: Discrete + Continuous RVs inference: gradient estimation?,"<p>See <a href=""https://github.com/tensorflow/probability/issues/1534"" rel=""nofollow noreferrer"">this tensorflow-probability issue</a></p>
<pre class=""lang-sh prettyprint-override""><code>tensorflow==2.7.0
tensorflow-probability==0.14.1
</code></pre>
<h2>TLDR</h2>
<p>To perform VI on discrete RVs, should I use:</p>
<ul>
<li>A- the REINFORCE gradient estimator</li>
<li>B- the Gumbel-Softmax reparametrization</li>
<li>C- another solution</li>
</ul>
<p>and how to implement it ?</p>
<h2>Problem statement</h2>
<p>Sorry in advance for the long issue, but I believe the problem requires some explaining.</p>
<p>I want to implement a Hierarchical Bayesian Model involving both continuous and <strong>discrete</strong> Random Variables. A minimal example is a Gaussian Mixture model:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import tensorflow_probability as tfp

tfd = tfp.distributions
tfb = tfp.bijectors

G = 2

p = tfd.JointDistributionNamed(
    model=dict(
        mu=tfd.Sample(
            tfd.Normal(0., 1.),
            sample_shape=(G,)
        ),
        z=tfd.Categorical(
            probs=tf.ones((G,)) / G
        ),
        x=lambda mu, z: tfd.Normal(
            loc=mu[z],
            scale=1.
        )
    )
)
</code></pre>
<p>In this example I don't use the <code>tfd.Mixture</code> API on purpose to expose the Categorical label. I want to perform <strong>Variational Inference</strong> in this context, and for instance given an observed <code>x</code> fit over the posterior of <code>z</code> a <code>Categorical</code> distribution with parametric probabilities:</p>
<pre class=""lang-py prettyprint-override""><code>q_probs = tfp.util.TransformedVariable(
    tf.ones((G,)) / G,
    tfb.SoftmaxCentered(),
    name=&quot;q_probs&quot;
)
q_loc = tf.Variable(0., name=&quot;q_loc&quot;)
q_scale = tfp.util.TransformedVariable(
    1.,
    tfb.Exp(),
    name=&quot;q_scale&quot;
)

q = tfd.JointDistributionNamed(
    model=dict(
        mu=tfd.Normal(q_loc, q_scale),
        z=tfd.Categorical(probs=q_probs)
    )
)
</code></pre>
<p>The issue is: when computing the ELBO and trying to optimize for the optimal <code>q_probs</code> I cannot use the reparameterization gradient estimators: this is AFAIK because <code>z</code> is a discrete RV:</p>
<pre class=""lang-py prettyprint-override""><code>
def log_prob_fn(**kwargs):
    return p.log_prob(
        **kwargs,
        x=tf.constant([2.])
    )


optimizer = tf.optimizers.SGD()

@tf.function
def fit_vi():
    return tfp.vi.fit_surrogate_posterior(
        target_log_prob_fn=log_prob_fn,
        surrogate_posterior=q,
        optimizer=optimizer,
        num_steps=10,
        sample_size=8
    )

_ = fit_vi() 
# This last line raises:
# ValueError: Distribution `surrogate_posterior` must be reparameterized, i.e.,a diffeomorphic transformation
# of a parameterless distribution. (Otherwise this function has a biased gradient.)

</code></pre>
<p>I'm looking into a way to make this work. I've identified at least 2 ways to circumvent the issue: using REINFORCE gradient estimator or the Gumbel-Softmax reparameterization.</p>
<h2>A- REINFORCE gradient</h2>
<p>cf <a href=""https://www.tensorflow.org/probability/api_docs/python/tfp/vi/GradientEstimators"" rel=""nofollow noreferrer"">this TFP API link</a> a classical result in VI is that the REINFORCE gradient can deal with a non-differentiable objective function, for instance due to discrete RVs.</p>
<p>I can use a <code>tfp.vi.GradientEstimators.SCORE_FUNCTION</code> estimator instead of the <code>tfp.vi.GradientEstimators.REPARAMETERIZATION</code> one using the lower-level <code>tfp.vi.monte_carlo_variational_loss</code> function?
Using the REINFORCE gradient, In only need the <code>log_prob</code> method of <code>q</code> to be differentiable, but the <code>sample</code> method needn't be differentiated.</p>
<p>As far as I understood it, the <code>sample</code> method for a <code>Categorical</code> distribution implies a gradient break, but the <code>log_prob</code> method does not. Am I correct to assume that this could help with my issue? Am I missing something here?</p>
<p>Also I wonder: why is this possibility not exposed in the <code>tfp.vi.fit_surrogate_posterior</code> API ? Is the performance bad, meaning is the variance of the estimator too large for practical purposes ?</p>
<h2>B- Gumbel-Softmax reparameterization</h2>
<p>cf <a href=""https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/RelaxedOneHotCategorical"" rel=""nofollow noreferrer"">this TFP API link</a> I could also reparameterize <code>z</code> as a variable <code>y = tfd.RelaxedOneHotCategorical(...)</code> . The issue is: I need to have a proper categorical label to use for the definition of <code>x</code>, so AFAIK I need to do the following:</p>
<pre class=""lang-py prettyprint-override""><code>p_GS = tfd.JointDistributionNamed(
    model=dict(
        mu=tfd.Sample(
            tfd.Normal(0., 1.),
            sample_shape=(G,)
        ),
        y=tfd.RelaxedOneHotCategorical(
            temperature=1.,
            probs=tf.ones((G,)) / G
        ),
        x=lambda mu, y: tfd.Normal(
            loc=mu[tf.argmax(y)],
            scale=1.
        )
    )
)
</code></pre>
<p>...but his would just move the gradient breaking problem to <code>tf.argmax</code>. This is where I maybe miss something. Following the Gumbel-Softmax (Jang et al., 2016) paper, I could then use the &quot;STRAIGHT-THROUGH&quot; (ST) strategy and &quot;plug&quot; the gradients of the variable <code>tf.one_hot(tf.argmax(y))</code> -the &quot;discrete y&quot;- onto <code>y</code> -the &quot;continuous y&quot;.</p>
<p>But again I wonder: how to do this properly ? I don't want to mix and match the gradients by hand, and I guess an autodiff backend is precisely meant to avoid me this issue. How could I create a distribution that differentiates the forward direction (sampling a &quot;discrete y&quot;) from the backward direction (gradient computed using the &quot;continuous y&quot;) ? I guess this is the meant usage of the <code>tfd.RelaxedOneHotCategorical</code> distribution, but I don't see this implemented anywhere in the API.</p>
<p>Should I implement this myself ? How ? Could I use something in the lines of <code>tf.custom_gradient</code>?</p>
<h2>Actual question</h2>
<p>Which solution -A or B or another- is meant to be used in the TFP API, if any? How should I implement said solution efficiently?</p>
",0
71505664,How to do one hot encoding in tft using hardcoded values,"<p>I want to apply one hot encoding to my categorical features. I see how one can use <code>tf.one_hot</code> to do that but <code>one_hot</code> accepts indices so I'd need to map the tokens to indices. But all of the examples I've found are computing the vocab over the entire dataset. I don't want to do that as I have hard-coded dict of possible values. Something like:</p>
<pre class=""lang-py prettyprint-override""><code>CATEG = {
    'feature1': ['a', 'b', 'c'],
    'feature2': ['foo', 'bar']
}
</code></pre>
<p>I just need the <code>proprocessing_fn</code> to simply map the tokens to an index then run it through <code>tf.one_hot</code>. How can I do that?</p>
<hr />
<p>For example, <a href=""https://www.tensorflow.org/tfx/transform/api_docs/python/tft/apply_vocabulary"" rel=""nofollow noreferrer""><code>tft.apply_vocabulary</code></a> sounds like what I need but then I see that it takes a <code>deferred_vocab_filename_tensor</code> of type <code>common_types.TemporaryAnalyzerOutputType</code>? The description says:</p>
<blockquote>
<p>The deferred vocab filename tensor as returned by tft.vocabulary, as long as the frequencies were not stored.</p>
</blockquote>
<p>And I see that <a href=""https://www.tensorflow.org/tfx/transform/api_docs/python/tft/vocabulary"" rel=""nofollow noreferrer""><code>tft.vocabulary</code></a> is again <em>computing</em> the vocab:</p>
<blockquote>
<p>Computes The unique values taken by x, which can be a Tensor or CompositeTensor of any size. The unique values will be aggregated over all dimensions of x and all instances.</p>
</blockquote>
<p>Why doesn't something simple like this exist?</p>
",0
71524470,TypeError: Reduce.update_state() got multiple values for argument 'sample_weight',"<p>I am creating a basic deep learning model and tried to compile it with tf.keras.metrics.Mean() but I keep getting this error and I am not sure how to fix it. I realized that tensorflow documentations for these metrics is supposed to use the &quot;add_metric&quot; function but I do not really understand how to use that function.</p>
<p>Here is a simple code to reproduce that error:</p>
<pre><code>from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()

model.add(Dense(4, input_shape=(1,)))
model.add(Dense(1, ))
model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.Mean()])

model.fit([1], [0])
</code></pre>
<p>If anyone knows how to use the Mean() metrics, please let me know. Thanks!</p>
",1
71556136,Data Augmentation for Inception v3,"<p>I'm trying to use inception v3 to classify images but my dataset is very small (can't have more img than that) and I'd like to augment it with transformations such as rotation or inversions. I'm new to TF and can't figure out how to do so, I've read the documentation for the <code>ImageDataGenerator</code> which should augment my data but when training I still get the error that states that I don't have enough data. I could use masks also but don't know how to implement in tf.
Can someone enlighten me ? Thanks a lot for any input</p>
<p>Here's my code:</p>
<pre><code>train_datagen = ImageDataGenerator(rescale = 1./255.,
                                   rotation_range = 180,
                                   width_shift_range = 0.2,
                                   height_shift_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True,
                                   vertical_flip = True)
 
test_datagen = ImageDataGenerator(rescale = 1./255.,
                                   rotation_range = 180,
                                   width_shift_range = 0.2,
                                   height_shift_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True,
                                   vertical_flip = True)

 
train_generator = train_datagen.flow_from_directory(train_dir,
                                                    batch_size = 100,
                                                    class_mode = 'binary',
                                                    target_size = (224, 224))


validation_generator =  test_datagen.flow_from_directory(validation_dir,
                                                          batch_size  = 100,
                                                          class_mode  = 'binary',
                                                          target_size = (224, 224))
base_model = InceptionV3(input_shape = (224, 224, 3),
                                include_top = False,
                                weights = 'imagenet')
for layer in base_model.layers:
  layer.trainable = False

%%time
x = layers.Flatten()(base_model.output)
x = layers.Dense(1024, activation='relu')(x)
x = layers.Dropout(0.2)(x)                 
x = layers.Dense(1, activation='sigmoid')(x)          
 
model = Model( base_model.input, x)
 
model.compile(optimizer = RMSprop(learning_rate=0.0001),loss = 'binary_crossentropy',metrics = ['acc'])
callbacks = myCallback()
 
history = model.fit_generator(
            train_generator,
            validation_data = validation_generator,
            steps_per_epoch = 100,
            epochs = 10,
            validation_steps = 10,
            verbose = 2,
            callbacks=[callbacks])
</code></pre>
<p>Error:</p>
<pre><code>WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 1000 batches). You may need to use the repeat() function when building your dataset.
</code></pre>
",1
71574084,How can I use 'tensorboard' with tflite_model_maker? Is it possible to use custom_callbacks with tflite_model_maker?,"<p>I have used &quot;<strong>tf.keras.callbacks.tensorboard</strong>&quot; with my custom keras-models. But I couldn't use it with tflite_model_maker for object detection. If it is possible please let me know... Thanks!</p>
<p>I could not find any information on the <a href=""https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/create"" rel=""nofollow noreferrer"">documentation</a>...</p>
",1
71574679,TensorFlow 2.3 dataset pipeline run out of memory,"<p>I am trying to train a neural network model with TF2.3 on GPU (EC2/P2.xlarge with 61GB memory).</p>
<p>My code for data pipeline:</p>
<pre><code> my_dataset = tf.data.TFRecordDataset(train_files_path, compression_type='GIP') # 500 files, each file size is 500+ KB
 my_dataset = my_dataset.map(lambda x: parse_input(x), num_parallel_calls=tf.data.experimental.AUTOTUNE)
 epoch = 10
 batch_size = 4096
 my_dataset = my_dataset.shuffle(200)
 my_dataset = my_dataset.repeat(epoch)
 my_dataset = my_dataset.batch(batch_size, drop_reminder=True)
 my_dataset = my_dataset.prefetch(batch_size)
</code></pre>
<p>After running two epochs, the GPU run out of memory and the jupyter kernel died.
I have tried the options at
<a href=""https://stackoverflow.com/questions/45124719/memory-management-in-tensorflows-dataset-api"">Memory management in Tensorflow&#39;s Dataset API</a>
<a href=""https://stackoverflow.com/questions/47117498/does-tf-data-dataset-repeat-buffer-the-entire-dataset-in-memory"">Does `tf.data.Dataset.repeat()` buffer the entire dataset in memory?</a>
<a href=""https://stackoverflow.com/questions/56896230/why-would-this-dataset-implementation-run-out-of-memory"">Why would this dataset implementation run out of memory?</a>
but, not helpful.</p>
<p>Also, followed the best practices at
<a href=""https://www.tensorflow.org/guide/effective_tf2"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/effective_tf2</a>
<a href=""https://www.tensorflow.org/guide/data"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/data</a></p>
<p>My pipeline:</p>
<pre><code>map --&gt; shuffle --&gt; repeat --&gt; batch --&gt; prefetch 
</code></pre>
<p>Did I miss something ? thanks</p>
",0
71588962,Solving a set of linear systems in tensorflow,"<p>I'm having a problem understanding the working mechanism of tensorflow's function: tf.linalg.solve.
I want to solve a set of linear systems (AX = Y), where the linear coefficients (A) were shared but there are multiple batches of Y, which are different.
Using numpy, I can simply do it via:</p>
<pre><code>np.random.seed(0)
mtx = np.random.uniform(size= (1,4,4))
vec = np.random.uniform(size= (100,4,1))
solution = np.linalg.solve(mtx,vec)
print(abs(np.matmul(mtx,solution) - vec).max())
# 5.551115123125783e-16
</code></pre>
<p>which gives me a quite consistent solution.
But when I switch to tensorflow, it gives me the results:</p>
<pre><code>mtx = tf.random.uniform(shape = (1,4,4))
vec = tf.random.uniform(shape = (100,4,1))
solution = tf.linalg.solve(mtx,vec)
print(tf.math.reduce_max(abs(tf.matmul(mtx,solution) - vec))) 
# tf.Tensor(1.3136615, shape=(), dtype=float32)
</code></pre>
<p>According to the document, I assume the solution should be solved according to the corresponding vec. But it does not seem to give me the expected results in tensorflow. Since I'm a new user, I could have messed up something.
It would be appreciated if any information could be offered.</p>
",1
71612603,How does one invert an encoded prediction in Keras for model serving?,"<p>I have a Keras model in which i have successfully added a <code>StringLookUp</code> pre-processing step as part of the model definition. This is generally a good practice because i can then feed it the raw data to get back a prediction.</p>
<p>I am feeding the model string words that are mapped to an integer. The Y values are also string words that have been mapped to an integer.</p>
<p>Here is the implementation of the encoder and decoders:</p>
<pre><code>#generate the encoder and decoders
encoder = tf.keras.layers.StringLookup(vocabulary=vocab, )
decoder = tf.keras.layers.StringLookup(vocabulary=vocab, output_mode=&quot;int&quot;, invert=True)
</code></pre>
<p>Here is the some of the code that makes the inference model</p>
<pre><code># For inference, you can export a model that accepts strings as input
inputs = Input(shape=(6,), dtype=&quot;string&quot;)
x = encoder(inputs)
outputs = keras_model(x)
inference_model = Model(inputs, outputs)

inference_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  
inference_model.summary()
</code></pre>
<p>The <code>encoder</code> above is just a function that implements <code>tf.keras.layers.StringLookup</code></p>
<p>Now, inside the notebook, I can easily convert the predictions back to the Original String representations by using a <code>decoder</code> which implements the reverse of <code>StringLookUp</code>.</p>
<p><em><strong>Here's my problem</strong></em>
While this works fine inside the notebook, this isn't very practical for deploying the model as a REST API because the calling program has no way of knowing how the encoded integer maps back to the original string representation.</p>
<p><em><strong>So the question is what strategy should I use to implement the keras predict so that it returns the original string which I can then serialize using mlflow &amp; cloudpickle to deploy it as a servable model in databricks</strong></em></p>
<p>Any guidance would be very much appreciated. I've seen a lot of example of Keras, but none that show how to do enact this kind of behavior for model deployment.</p>
",0
71619495,Image normalization by tf.image.convert_image_dtype function,"<p>According to documentation <code>tf.image.convert_image_dtype</code> &quot;Images that are represented using floating point values are expected to have values in the range [0,1).&quot;</p>
<p>But in the keras tutorial(<a href=""https://keras.io/examples/vision/cutmix/"" rel=""nofollow noreferrer"">https://keras.io/examples/vision/cutmix/</a>) i have seen the following preprocessing function:</p>
<pre><code>def preprocess_image(image, label):
    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
    image = tf.image.convert_image_dtype(image, tf.float32) / 255.0
    return image, label
</code></pre>
<p>My question is: why did they divide by 255, when <code>tf.image.convert_image_dtype</code> already did that job?</p>
",1
71667263,"Do Keras Layers input and output tensors, or other Layers?","<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer"" rel=""nofollow noreferrer"">tf.Keras documentation</a> states:</p>
<blockquote>
<p>A layer is a callable object that takes as input one or more tensors and that outputs one or more tensors.</p>
</blockquote>
<p>But then the whole <a href=""https://www.tensorflow.org/guide/keras/functional"" rel=""nofollow noreferrer"">Functional API</a> seems to be based calling a Layer object with another Layer as an input.</p>
<p>Do layers take in tensors and output tensors, or do they take in layers and output something else?</p>
",0
71714299,tensorflow 2 TextVectorization process tensor and dataset error,"<p>I would like to process text with tensorflow 2.8 on Jupyter notebook.</p>
<p>my code:</p>
<pre><code>import re
import string
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_text as tf_text

def standardize(input_data):
    lowercase_str = tf.strings.lower(input_data)
    a_str = tf.strings.regex_replace(lowercase_str, f&quot;[{re.escape(string.punctuation)}]&quot;, &quot;&quot;)
    tokenizer = tf_text.WhitespaceTokenizer()
    tokens = tokenizer.tokenize(a_str)
    return tokens

# the input data loaded from text files by TfRecordDataset(file_paths, &quot;GZIP&quot;)
# each file can be 200+MB, totally about 300 files
# each file hold the data with multiple columns
# some columns are text
# after loading, the dataset will be accessed by column name 
# e.g. one column is &quot;sports&quot;, so the input_dataset[&quot;sports&quot;] 
# return a tensor, which is like the following example
my_data_tensor = tf.constant([[&quot;SWIM 2008-07 Baseball&quot;], [&quot;Football&quot;]])

tf.print(my_data_tensor)
tf.print(my_data_tensor.shape)
tf.print(f&quot;type is {type(my_data_tensor)}&quot;)
text_layer = layers.TextVectorization(
                        standardize = standardize,
                        max_tokens = 10,
                        output_mode = 'int',
                        output_sequence_length=10
                       )

my_dataset = tf.data.Dataset.from_tensor_slices(my_data_tensor)
text_layer.adapt(my_dataset.batch(2)) # error         
processed_text = text_layer(my_dataset)

error:
 ValueError: Exception encountered when calling layer &quot;query_tower&quot; (type QueryTower).
 When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(2, 1, None) with rank=3
</code></pre>
<p>I have tried tf.unstack() and tf.reshape, tf.unbatch, but none of them work.
For the given example:</p>
<pre><code>[[&quot;SWIM 2008-07 Baseball&quot;], [&quot;Football&quot;]]
</code></pre>
<p>what I need:</p>
<pre><code>[[&quot;swim 200807 baseball&quot;], [&quot;football&quot;]]
then
it will be encoded as int by the &quot;text_layer&quot;
</code></pre>
<p>these data  (bach_size=2) will be used for a machine learning model as features.</p>
<p>Did I do something wrong ? thanks</p>
",0
71791115,Nan Loss when training Deep neural Recommender model using tensorflow,"<p>I am trying to follow <a href=""https://www.tensorflow.org/recommenders/examples/deep_recommenders"" rel=""nofollow noreferrer"">tensorflow documentation</a> and applying same technique to one of toy dataset.</p>
<p>During training I am getting all loss as Nan. I have tried to debug the same using Debugger V2 and I could see that <code>tf.keras.layers.GlobalAveragePooling1D</code> is giving Nan due to division by 0, which is causing all values to be Nan during backpropagation. But what is not clear from the debugger V2 GUI why the sum is becoming 0. I did try to reduce the number of features and the size of the dataset, but each of this activity is giving me new error (probably I shall start a separate question thread for each issues at a later point ).</p>
<p>Below is the code for reference. I am providing the dataset as well <a href=""https://drive.google.com/file/d/1z954Djz8IntSzMP6velSdMGWTW_yBUAn/view?usp=sharing"" rel=""nofollow noreferrer"">here</a>. I had tried below code on Google Colab.</p>
<pre><code>import os
import pprint
import tempfile

from typing import Dict, Text

import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_datasets as tfds

tf.debugging.experimental.enable_dump_debug_info(
    &quot;./tfdbg2_logdir&quot;,
    tensor_debug_mode=&quot;FULL_HEALTH&quot;,
    circular_buffer_size=-1)

!pip install -q tensorflow-recommenders
import tensorflow_recommenders as tfrs  
</code></pre>
<p>Preparing Data</p>
<pre><code>ds=pd.read_csv('train_recom.csv')
ds['year'].replace(0,1,inplace=True)
ds_song=ds.groupby(['song_id','title','release','artist_name','year']).size().reset_index().rename(columns={0:'count'})
ds_song.to_csv('songs_details.csv')
ds.to_csv('train_recom_transformed.csv')
</code></pre>
<p>Reading data to tensorflow dataset</p>
<pre><code>ratings = tf.data.experimental.make_csv_dataset(
    &quot;./train_recom_transformed.csv&quot;,
    batch_size=5,
    select_columns=['user_id', 'song_id', 'listen_count', 'title', 'release', 'artist_name',
       'year'],
    header=True,
    num_epochs=1,
    ignore_errors=False,)
songs = tf.data.experimental.make_csv_dataset(
    &quot;./songs_details.csv&quot;,
    batch_size=128,
    select_columns=['song_id','title','release','artist_name','year'],
    num_epochs=1,
    ignore_errors=True,)
ratings = ratings.unbatch().map(lambda x: {
    &quot;song_id&quot;: x[&quot;song_id&quot;],
    &quot;user_id&quot;: x[&quot;user_id&quot;],
    &quot;release&quot; : x[&quot;release&quot;],
    &quot;artist_name&quot; : x[&quot;artist_name&quot;],
    &quot;title&quot; : x[&quot;title&quot;],
    &quot;year&quot; : x[&quot;year&quot;],
    &quot;listen_count&quot;: x[&quot;listen_count&quot;]
})
songs = songs.unbatch().map(lambda x: x[&quot;song_id&quot;]) 
</code></pre>
<p>Preparing train and test dataset</p>
<pre><code>tf.random.set_seed(42)
shuffled = ratings.shuffle(16000, seed=42, reshuffle_each_iteration=False)

train = shuffled.take(12000)
test = shuffled.skip(12000).take(4000)
cached_train = train.shuffle(100_000).batch(1200).cache()
cached_test = test.batch(400).cache()

title = songs.batch(1000)
user_ids = ratings.batch(1_000_000).map(lambda x: x[&quot;user_id&quot;])
unique_song_titles = np.unique(np.concatenate(list(title)))
unique_user_ids = np.unique(np.concatenate(list(user_ids)))
year_data=np.concatenate(list(ratings.map(lambda x: x['year']).batch(4000)))
</code></pre>
<p>User model class</p>
<pre><code>class UserModel(tf.keras.Model):

    def __init__(self):
        super().__init__()

        max_tokens = 1_000_000

        embedding_dimension = 32
        self.user_embedding = tf.keras.Sequential([
            tf.keras.layers.StringLookup(
                vocabulary=unique_user_ids, mask_token=None),
            tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)
          ])

        self.release_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(
            max_tokens=max_tokens)
        
        self.release_text_embedding = tf.keras.Sequential([
          self.release_vectorizer,
          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True,input_length=144),
          tf.keras.layers.GlobalAveragePooling1D(),
        ])

        self.release_vectorizer.adapt(np.concatenate(list(ratings.map(lambda x: x['release']).batch(4000))))

        self.artist_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(
            max_tokens=max_tokens)
        self.artist_text_embedding = tf.keras.Sequential([
          self.artist_vectorizer,
          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),
          tf.keras.layers.GlobalAveragePooling1D(),
        ])
        self.artist_vectorizer.adapt(np.concatenate(list(ratings.map(lambda x: x['artist_name']).batch(4000))))
        
        self.title_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(
            max_tokens=max_tokens)
        self.title_text_embedding = tf.keras.Sequential([
          self.title_vectorizer,
          tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),
          tf.keras.layers.GlobalAveragePooling1D(),
        ])
        self.title_vectorizer.adapt(np.concatenate(list(ratings.map(lambda x: x['title']).batch(4000))))
        
        self.year_embedding = tf.keras.Sequential([
              tf.keras.layers.Embedding(len(year_data) + 1, 32),
            ])

    def call(self, inputs):
      return tf.concat([
          self.user_embedding(inputs['user_id']),
          self.release_text_embedding(inputs['release'])
          ,
          self.year_embedding(inputs['year']), 
          self.artist_text_embedding(inputs['artist_name']),
          self.title_text_embedding(inputs['title']),
             ], axis=1)
</code></pre>
<p>Item model</p>
<pre><code>class ItemModel(tf.keras.Model):

    def __init__(self):
        super().__init__()

        max_tokens = 10_000

        embedding_dimension = 32

        ## embed title from unique_song_titles
        self.title_embedding = tf.keras.Sequential([
        tf.keras.layers.StringLookup(
            vocabulary=unique_song_titles, mask_token=None),
        tf.keras.layers.Embedding(len(unique_song_titles) + 1, embedding_dimension)
      ])

    def call(self, inputs):
      return self.title_embedding(inputs)
</code></pre>
<p>Query model . Creating Deep model</p>
<pre><code>class QueryModel(tf.keras.Model):
  &quot;&quot;&quot;Model for encoding user queries.&quot;&quot;&quot;

  def __init__(self, layer_sizes):
    &quot;&quot;&quot;Model for encoding user queries.

    Args:
      layer_sizes:
        A list of integers where the i-th entry represents the number of units
        the i-th layer contains.
    &quot;&quot;&quot;
    super().__init__()

    # We first use the user model for generating embeddings.
    self.embedding_model = UserModel()

    # Then construct the layers.
    self.dense_layers = tf.keras.Sequential()

    # Use the ReLU activation for all but the last layer.
    for layer_size in layer_sizes[:-1]:
      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=&quot;relu&quot;))

    # No activation for the last layer.
    for layer_size in layer_sizes[-1:]:
      self.dense_layers.add(tf.keras.layers.Dense(layer_size))

  def call(self, inputs):
    feature_embedding = self.embedding_model(inputs)
    return self.dense_layers(feature_embedding)
</code></pre>
<p>Creating deep model for the Item model</p>
<pre><code>class CandidateModel(tf.keras.Model):
  &quot;&quot;&quot;Model for encoding movies.&quot;&quot;&quot;

  def __init__(self, layer_sizes):
    &quot;&quot;&quot;Model for encoding movies.

    Args:
      layer_sizes:
        A list of integers where the i-th entry represents the number of units
        the i-th layer contains.
    &quot;&quot;&quot;
    super().__init__()

    self.embedding_model = ItemModel()

    # Then construct the layers.
    self.dense_layers = tf.keras.Sequential()

    # Use the ReLU activation for all but the last layer.
    for layer_size in layer_sizes[:-1]:
      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=&quot;relu&quot;))

    # No activation for the last layer.
    for layer_size in layer_sizes[-1:]:
      self.dense_layers.add(tf.keras.layers.Dense(layer_size))

  def call(self, inputs):
    feature_embedding = self.embedding_model(inputs)
    return self.dense_layers(feature_embedding)
</code></pre>
<p>Combining both query and candidate model</p>
<pre><code>class SongModel(tfrs.models.Model):

    def __init__(self, layer_sizes):
        super().__init__()
        self.query_model = QueryModel(layer_sizes)
        self.candidate_model = CandidateModel(layer_sizes)
        self.task = tfrs.tasks.Retrieval(
          metrics=tfrs.metrics.FactorizedTopK(
              candidates=songs.batch(128).map(self.candidate_model),
          ),
      )

    def compute_loss(self, features, training=False):
        print('type of feature ----',type(features))

        query_embeddings = self.query_model({
            &quot;user_id&quot;: features[&quot;user_id&quot;]
            ,
                &quot;release&quot; : features[&quot;release&quot;]
                ,
                &quot;artist_name&quot; : features[&quot;artist_name&quot;],
                &quot;title&quot;: features[&quot;title&quot;],
                &quot;year&quot; : features[&quot;year&quot;],
        })

        item_embeddings = self.candidate_model(features[&quot;song_id&quot;])

        return self.task(query_embeddings, item_embeddings)
</code></pre>
<p>training the model</p>
<pre><code>model = SongModel([32])
model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))
model_hist = model.fit(cached_train, epochs=9)
</code></pre>
<p>Below id the outout that I got</p>
<pre><code>WARNING:tensorflow:Failed to read source code from path: /content/&lt;ipython-input-26-fdc864fc30cf&gt;. Reason: Source path neither exists nor can be loaded as a .par file: /content/&lt;ipython-input-26-fdc864fc30cf&gt;
WARNING:tensorflow:Failed to read source code from path: /content/&lt;ipython-input-25-e3009db55439&gt;. Reason: Source path neither exists nor can be loaded as a .par file: /content/&lt;ipython-input-25-e3009db55439&gt;
Epoch 1/9
type of feature ---- &lt;class 'dict'&gt;
WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.float32, name='embedding_10_input'), name='embedding_10_input', description=&quot;created by layer 'embedding_10_input'&quot;), but it was called on an input with incompatible shape (None,).
type of feature ---- &lt;class 'dict'&gt;
WARNING:tensorflow:Model was constructed with shape (None, None) for input KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.float32, name='embedding_10_input'), name='embedding_10_input', description=&quot;created by layer 'embedding_10_input'&quot;), but it was called on an input with incompatible shape (None,).
10/10 [==============================] - 63s 1s/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0022 - factorized_top_k/top_10_categorical_accuracy: 0.0033 - factorized_top_k/top_50_categorical_accuracy: 0.0073 - factorized_top_k/top_100_categorical_accuracy: 0.0103 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 2/9
10/10 [==============================] - 9s 945ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 3/9
10/10 [==============================] - 10s 953ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 4/9
10/10 [==============================] - 9s 948ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 5/9
10/10 [==============================] - 10s 966ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 6/9
10/10 [==============================] - 10s 955ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 7/9
10/10 [==============================] - 10s 955ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 8/9
10/10 [==============================] - 10s 958ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
Epoch 9/9
10/10 [==============================] - 10s 971ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: nan - regularization_loss: 0.0000e+00 - total_loss: nan
</code></pre>
",1
71894965,Tensorflow for a specific cpu instruction set,"<p>When I run <code>classifier = tensorflow.keras.models.Sequential()</code> on my machine I get the following messages:</p>
<pre><code>2022-04-16 20:34:33.552104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-16 20:34:33.583160: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-16 20:34:33.583834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-16 20:34:33.584363: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-04-16 20:34:33.584756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-16 20:34:33.585071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-16 20:34:33.585411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-16 20:34:33.963910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-16 20:34:33.964159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-16 20:34:33.964360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-16 20:34:33.964555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3396 MB memory:  -&gt; device: 0, name: NVIDIA GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0
</code></pre>
<p>I don't understand what half of it means. I searched about  <code>This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags</code> part online and what I sort of figured is that my hardware is able to AVX2 FMA operations which is supposed to make tensorflow faster but to do that it needs to be compiled &quot;with appropirate compiler flags&quot;.</p>
<p>My question is how do I do just that because I want the most out of my hardware.
Here are my specs:</p>
<pre><code>CPU: Intel i7-8550U (8) @ 4.000GHz
Dedicated GPU: NVIDIA GeForce 940MX
Integrated GPU: Intel UHD Graphics 620
</code></pre>
<p>Runtime Environment:</p>
<pre><code>Python 3.10.4
tensorflow==2.8.0
tensorflow-io-gcs-filesystem==0.24.0
</code></pre>
",0
71933464,How to make true_fn of tf.cond skip a for loop in tensorflow v1.0/python?,"<p>I want to use <code>tf.cond</code> to mimic the python <code>if-else</code> logic in the <code>_preprocessing_fn</code> of <code>transform.py</code>.</p>
<p>Specifically, if the condition of <code>tf.cond</code> is true, I want to skip the current iteration of the for loop.</p>
<p>This seems problematic because <code>true_fn</code> and <code>false_fn</code> parameters of <code>tf.cond</code> are expected to return Tensors according to the documentation.</p>
<p>However, in my case, I want <code>true_fn</code> (aka <code>skip_feature_fn</code>)to simply &quot;continue&quot; to the next for loop iteration. Also, I want <code>false_fn</code> to take in two inputs (<code>feature</code> and <code>sp</code>) and simply feed them to some other API (e.g. <code>tft.vocabulary</code>).
I don't expect either of <code>true_fn</code> or <code>false_fn</code> to return anything.</p>
<p>Could someone help me accomplish my goal?</p>
<p>Here is the code snippet I'm working with:</p>
<pre><code>def _preprocessing_fn(inputs, category_features=features.STRING_FEATURES):
  outputs = transform_lib.preprocessing_helper_fn(
      inputs, used_features=category_features)

  for feature in category_features:
    if feature:
      sp = outputs[feature]
      tf.cond(
          tf.equal(sp.dense_shape[1], 0), skip_feature_fn, lambda: process_feature_further(
              feature,
              sp,
          ))

  return outputs
</code></pre>
<p>Thank you.</p>
",1
72041726,Is there a difference between creating tf.Variable and keras.layers.Layer.add_weight(),"<p>I've seen both approaches in Tensorflow documentation:</p>
<hr />
<p>#1</p>
<pre><code>class MyDenseLayer(tf.keras.layers.Layer):
  def __init__(self, num_outputs):
    super(MyDenseLayer, self).__init__()
    self.num_outputs = num_outputs

  def build(self, input_shape):
    self.kernel = self.add_weight(&quot;kernel&quot;,
                                  shape=[int(input_shape[-1]),
                                         self.num_outputs])

  def call(self, inputs):
    return tf.matmul(inputs, self.kernel)
</code></pre>
<hr />
<p>#2</p>
<pre><code>class Linear(keras.layers.Layer):
    def __init__(self, units=32, input_dim=32):
        super(Linear, self).__init__()
        w_init = tf.random_normal_initializer()
        self.w = tf.Variable(
            initial_value=w_init(shape=(input_dim, units), dtype=&quot;float32&quot;),
            trainable=True,
        )
        b_init = tf.zeros_initializer()
        self.b = tf.Variable(
            initial_value=b_init(shape=(units,), dtype=&quot;float32&quot;), trainable=True
        )

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b
</code></pre>
<hr />
<p>Is there a fundamental difference between creating a variable with self.add_weight() or tf.Variable() inside your custom layers? Examples are taken from
<a href=""https://www.tensorflow.org/tutorials/customization/custom_layers"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/customization/custom_layers</a> and <a href=""https://www.tensorflow.org/guide/keras/custom_layers_and_models"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/keras/custom_layers_and_models</a> respectively.</p>
",0
72165812,Connecting BatchDataset with Keras VGG16 preprocess_input,"<p>I am using <code>tf.keras.preprocessing.image_dataset_from_directory</code> to get a <code>BatchDataset</code>, where the dataset has 10 classes.</p>
<p>I am trying to integrate this <code>BatchDataset</code> with a Keras <code>VGG16</code> (<a href=""https://keras.io/api/applications/vgg/"" rel=""nofollow noreferrer"">docs</a>) network.  From the docs:</p>
<blockquote>
<p>Note: each Keras Application expects a specific kind of input preprocessing. For VGG16, call <code>tf.keras.applications.vgg16.preprocess_input</code> on your inputs before passing them to the model.</p>
</blockquote>
<p>However, I am struggling to get this <code>preprocess_input</code> working with a <code>BatchDataset</code>.  <strong>Can you please help me figure out how to connect these two dots?</strong></p>
<p>Please see the below code:</p>
<pre class=""lang-py prettyprint-override""><code>train_ds = tf.keras.preprocessing.image_dataset_from_directory(train_data_dir, image_size=(224, 224))
train_ds = tf.keras.applications.vgg16.preprocess_input(train_ds)
</code></pre>
<p>This will throw <code>TypeError: 'BatchDataset' object is not subscriptable</code>:</p>
<pre><code>Traceback (most recent call last):
  ...
  File &quot;/path/to/venv/lib/python3.10/site-packages/keras/applications/vgg16.py&quot;, line 232, in preprocess_input
    return imagenet_utils.preprocess_input(
  File &quot;/path/to/venv/lib/python3.10/site-packages/keras/applications/imagenet_utils.py&quot;, line 117, in preprocess_input
    return _preprocess_symbolic_input(
  File &quot;/path/to/venv/lib/python3.10/site-packages/keras/applications/imagenet_utils.py&quot;, line 278, in _preprocess_symbolic_input
    x = x[..., ::-1]
TypeError: 'BatchDataset' object is not subscriptable
</code></pre>
<p>From <a href=""https://github.com/tensorflow/tensorflow/issues/39338"" rel=""nofollow noreferrer"">TypeError: 'DatasetV1Adapter' object is not subscriptable</a> (from <a href=""https://stackoverflow.com/q/61642569/11163122"">BatchDataset not subscriptable when trying to format Python dictionary as table</a>) the suggestion was to use:</p>
<pre class=""lang-py prettyprint-override""><code>train_ds = tf.keras.applications.vgg16.preprocess_input(
    list(train_ds.as_numpy_iterator())
)
</code></pre>
<p>However, this also fails:</p>
<pre><code>Traceback (most recent call last):
  ...
  File &quot;/path/to/venv/lib/python3.10/site-packages/keras/applications/vgg16.py&quot;, line 232, in preprocess_input
    return imagenet_utils.preprocess_input(
  File &quot;/path/to/venv/lib/python3.10/site-packages/keras/applications/imagenet_utils.py&quot;, line 117, in preprocess_input
    return _preprocess_symbolic_input(
  File &quot;/path/to/venv/lib/python3.10/site-packages/keras/applications/imagenet_utils.py&quot;, line 278, in _preprocess_symbolic_input
    x = x[..., ::-1]
TypeError: list indices must be integers or slices, not tuple
</code></pre>
<p>This is all using <code>Python==3.10.3</code> with <code>tensorflow==2.8.0</code>.</p>
<p>How can I get this working?  Thank you in advance.</p>
",0
72184958,Pydantic: Type hinting tensorflow tensor,"<p>any idea of how to type-hint tf tensors using pydantic??. Tried default tf.Tensor</p>
<pre><code>RuntimeError: no validator found for &lt;class 'tensorflow.python.framework.ops.Tensor'&gt;, see `arbitrary_types_allowed` in Config
</code></pre>
<p>and tf.flaot32</p>
<pre><code>RuntimeError: error checking inheritance of tf.float32 (type: DType)
</code></pre>
<p>Looking at documentation in pydantic, i believe something like this arbitrary class need to be defined...</p>
<pre><code>class Tensor:
    def __init__(self, Tensor):

        self.Tensor = Union[
            tensorflow.python.framework.ops.Tensor,
            tensorflow.python.framework.sparse_tensor.SparseTensor,
            tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor,
            tensorflow.python.framework.ops.EagerTensor,
        ]
</code></pre>
<p>with following in main..</p>
<pre><code> class Main(BaseModel):
     tensor : Tensor


 class Config:
    arbitary_types_allowed = True
</code></pre>
",1
72241763,Why the result of categorical cross entropy in tensorflow different from the definition?,"<p>I am testing outcomes of <code>tf.keras.losses.CategoricalCrossEntropy</code>, and it gives me values different from the definition.
My understanding of cross entropy is:</p>
<pre><code>
def ce_loss_def(y_true, y_pred):
    return tf.reduce_sum(-tf.math.multiply(y_true, tf.math.log(y_pred)))
</code></pre>
<p>And lets say I have values like this:</p>
<pre><code>pred = [0.1, 0.1, 0.1, 0.7]
target = [0, 0, 0, 1]
pred = tf.constant(pred, dtype = tf.float32)
target = tf.constant(target, dtype = tf.float32)

pred_2 = [0.1, 0.3, 0.1, 0.7]
target = [0, 0, 0, 1]
pred_2 = tf.constant(pred_2, dtype = tf.float32)
target = tf.constant(target, dtype = tf.float32)
</code></pre>
<p>By the definition I think it should disregard the probabilities in the non-target classes, like this:</p>
<pre><code>ce_loss_def(y_true = target, y_pred = pred), ce_loss_def(y_true = target, y_pred = pred_2)

(&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.35667497&gt;,
 &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.35667497&gt;)
</code></pre>
<p>But <code>tf.keras.losses.CategoricalCrossEntropy</code> doesn't give me the same results:</p>
<pre><code>ce_loss_keras = tf.keras.losses.CategoricalCrossentropy()

ce_loss_keras(y_true = target, y_pred = pred), ce_loss_keras(y_true = target, y_pred = pred_2)

</code></pre>
<p>outputs:</p>
<pre><code>(&lt;tf.Tensor: shape=(), dtype=float32, numpy=0.35667497&gt;,
 &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.5389965&gt;)
</code></pre>
<p>What am I missing?</p>
<p>Here is the link to the notebook I used to get this result:
<a href=""https://colab.research.google.com/drive/1T69vn7MCGMSQ8hlRkyve6_EPxIZC1IKb#scrollTo=dHZruq-PGyzO"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1T69vn7MCGMSQ8hlRkyve6_EPxIZC1IKb#scrollTo=dHZruq-PGyzO</a></p>
",0
72314802,Tensor split to dynamic length tensors based on continuous mask values in tensorflow?,"<p>I'm trying to figure out how to split my tensor of sequential data into multiple parts based on partitioning continuous masks with value of binary number '1'.</p>
<p>I've read the official documentation.
Howerver I can't find any function that can handle this easy.
Any helpful ways for this in python?</p>
<p>I have tried with 'tf.ragged.boolean_mask' but it doesn't seem to fit in my case.</p>
<p>The visualized example of my explanation is:</p>
<p>inputs:</p>
<pre><code># both are tensors, NOT data.
data_tensor = ([3,5,6,2,6,1,3,9,5])
mask_tensor = ([0,1,1,1,0,0,1,1,0])
</code></pre>
<p>expected output:</p>
<pre><code>output_tensor = ([[3],[5,6,2],[6,1],[3,9],[5]])
</code></pre>
<p>Thank you.</p>
",1
72329108,Is there a simple way to know which Tensorflow ops have a registered GPU kernel?,"<p>I have been trying to optimize some Tensorflow code that was pretty memory inefficient (use of large dense tensors containing very sparse information), and would thus limit batch size and scalability, by trying to make use of SparseTensors.
After some struggle I finally come up with a decent solution with satisfactory speedup on CPU and very low memory usage, and when the time comes to use a GPU I realize that the previous memory inefficient is orders of magnitude faster...</p>
<p>Using tensorboard profiling I've discovered that two of the operations I have used in my &quot;&quot;optimized&quot;&quot; version only run on CPU (namely UniqueV2 and sparse_dense_matmul), but I could not see any hint of that in the documentation.</p>
<p>The only related piece of <a href=""https://www.tensorflow.org/guide/gpu#overview"" rel=""nofollow noreferrer"">documentation</a> states:</p>
<blockquote>
<p>If a TensorFlow operation has no corresponding GPU implementation,
then the operation falls back to the CPU device. For example, since
tf.cast only has a CPU kernel, on a system with devices CPU:0 and
GPU:0, the CPU:0 device is selected to run tf.cast, even if requested
to run on the GPU:0 device.</p>
</blockquote>
<p>In turn there is nothing in the tf.cast documentation hinting that the op has no GPU kernel.</p>
<p>Thus, is there a simple way to know whether a TF ops has a registered GPU kernel, without having to use a GPU to find it out?</p>
<p>The <a href=""https://www.tensorflow.org/guide/create_op#gpu_support"" rel=""nofollow noreferrer"">custom ops</a> guide suggest that this could be seen by looking at the ops C files, but this seems a rather cumbersome way to do it...</p>
<p>I'm using TF v2.8</p>
<p>Thanks!</p>
",1
72707453,How to save a tensorflow dataset to multiple shards without using enumerate,"<p>I have a tensorflow dataset with some elements in it, and I want to save it with <code>tf.data.Dataset.save</code> such that each element gets its own shard. Thus if the dataset contains 2,000 elements, it would be saved to 2,000 shards.</p>
<p>The documentation <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=nightly#save"" rel=""nofollow noreferrer"">here</a> specifies how to create 1 shard only, but not how to make a shard for each element.</p>
<p>Below, I am able to do it with enumerate, but is there another way to do it without also saving the index from <code>enumerate</code>?</p>
<pre><code>tuple_data = np.array([3, 4])
data = tf.data.Dataset.from_tensor_slices(tuple_data)
data = data.enumerate()
print(list(data.as_numpy_iterator()))
# [(0, 3), (1, 4)]

data.save(path='~/Desktop/1', shard_func=lambda i, x: i)
</code></pre>
",1
72720129,Understanding tf.keras.metrics.Precision and Recall for multiclass classification,"<p>I am building a model for a multiclass classification problem. So I want to evaluate the model performance using the Recall and Precision.
I have 4 classes in the dataset and it is provided in <code>one hot</code> representation.</p>
<p>I was reading the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision"" rel=""nofollow noreferrer"">Precision</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall"" rel=""nofollow noreferrer"">Recall</a> <code>tf.keras</code> documentation, and have some questions:</p>
<ol>
<li>When it calculating the Precision and Recall for the multi-class classification, how can we take the average of all of the labels, meaning the global precision &amp; Recall? is it calculated with <code>macro</code> or <code>micro</code> since it is not specified in the documentation as in the <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html"" rel=""nofollow noreferrer"">Sikit learn</a>.</li>
<li>If I want to calculate the precision &amp; Recall for each label separately, can I use the argument <code>class_id</code> for each label to do  <code>one_vs_rest</code> or <code>binary</code> classification. Like what I have done in the code below?</li>
<li>can I use the argument <code>top_k</code> with the value <code>top_k=2</code> would be helpful here or it is not suitable for my classification of 4 classes only?</li>
<li>While I am measuring the performance of each class, What could be the difference, when I set the <code>top_k=1</code> and not setting <code>top_k</code>overall?</li>
</ol>
<pre><code>model.compile(
      optimizer='sgd',
      loss=tf.keras.losses.CategoricalCrossentropy(),
      metrics=[tf.keras.metrics.CategoricalAccuracy(),
               ##class 0
               tf.keras.metrics.Precision(class_id=0,top_k=2), 
               tf.keras.metrics.Recall(class_id=0,top_k=2),
              ##class 1
               tf.keras.metrics.Precision(class_id=1,top_k=2), 
               tf.keras.metrics.Recall(class_id=1,top_k=2),
              ##class 2
               tf.keras.metrics.Precision(class_id=2,top_k=2), 
               tf.keras.metrics.Recall(class_id=2,top_k=2),
              ##class 3
               tf.keras.metrics.Precision(class_id=3,top_k=2), 
               tf.keras.metrics.Recall(class_id=3,top_k=2),
])
</code></pre>
<p>Any clarification of this function will be appreciated.
Thanks in advance</p>
",1
72749893,Optimizer.apply_gradients creating variables in tf.function,"<p>I have created a neural style transfer with Eager Execution, but it does not work when I  try to turn it into a tf.function.
The error message says:</p>
<pre><code>ValueError: tf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.
</code></pre>
<p>However, no variable is being created inside the function. Here is a simplified version of the code, which is just a neural style transfer with one image (the goal is to make the generated image look exactly like the content image):</p>
<pre><code>import tensorflow as tf
import numpy as np
from PIL import Image

#Get and process the images
image = np.array(Image.open(&quot;frame7766.jpg&quot;)).reshape(1, 720, 1280, 3)/255
content_image = tf.convert_to_tensor(image, dtype = tf.float32)
# variable is defined outside of tf.function
generated_image = tf.Variable(np.random.rand(1, 720, 1280, 3)/2 + content_image/2, dtype = tf.float32)

def clip_0_1(image): # keeps image values between 0 and 1
    return tf.clip_by_value(image, clip_value_min=0, clip_value_max=1)

@ tf.function
def train_step(generated_image, content_image): #turn generated image into tf variable
    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)
    with tf.GradientTape() as tape:
        cost = tf.reduce_sum(tf.square(generated_image - content_image))
    grad = tape.gradient(cost, generated_image) 
    optimizer.apply_gradients([(grad, generated_image)]) # More information below
    generated_image.assign(clip_0_1(generated_image))
    return generated_image

generated_image = train_step(generated_image, content_image)
</code></pre>
<p>The error message points to the line</p>
<pre><code>optimizer.apply_gradients([(grad, generated_image)]) 
</code></pre>
<p>I have tried to change the input of <code> optimizer.apply_gradients</code> to <code>zip([grad], [generated_image])</code>, and every combination of lists and tuples I can think of, but the error still remains. I have also looked through <a href=""https://www.tensorflow.org/guide/function#creating_tfvariables"" rel=""nofollow noreferrer"">https://www.tensorflow.org/guide/function#creating_tfvariables</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer</a>, but neither of them shows examples where the variable is not explicitly defined.
The only conclusion that I can come to is that one of my commands (most likely <code>optimizer.apply_gradients</code>) creates a variable because of an issue in my earlier code. Is that correct?</p>
",1
72833114,tf.keras.layers.MultiHeadAttention's argument key_dim sometimes not matches to paper's example,"<p>For example, I have input with shape (1, 1000, 10) (so, <code>src.shape</code> wil be <code>(1, 1000, 10)</code>, which means the sequence length is 1000, and the dimension is 10. Then:</p>
<ul>
<li>This works (random <code>num_head</code> and <code>key_dim</code>):</li>
</ul>
<pre><code>class Model(tf.keras.Model):
        def __init__(self):
            super(Model, self).__init__()
            self.attention1 = tf.keras.layers.MultiHeadAttention(num_heads=20, key_dim=9)
            self.dense = tf.keras.layers.Dense(10, activation=&quot;softmax&quot;)

        def call(self, src) :
            output = self.attention1(src, src)
            output = tf.reshape(output, [1, 10000])
            output = self.dense(output)
            return output
</code></pre>
<ul>
<li>And this works too (random <code>num_head</code> and <code>key_dim</code>) :</li>
</ul>
<pre><code>class Model(tf.keras.Model):
        def __init__(self):
            super(Model, self).__init__()
            self.attention1 = tf.keras.layers.MultiHeadAttention(num_heads=123, key_dim=17)
            self.dense = tf.keras.layers.Dense(10, activation=&quot;softmax&quot;)

        def call(self, src):
            output = self.attention1(src, src)
            output = tf.reshape(output, [1, 10000])
            output = self.dense(output)
            return output
</code></pre>
<p>So, this layer works with whatever <code>num_heads</code> and <code>key_dim</code>, which does not match the paper idea. (It works because no error report, and it able to train)</p>
<p>In the paper, 'attention is all you need', it says <code>key_dim</code> is the dimension of key for each head, not the original head dimension, and thus <code>key_dim</code> should equal to <code>embed_dim</code>/<code>head_num</code>. So, if we want to have a <code>head_num</code> of 5, the <code>key_dim</code> has to be 2, if <code>embedding_dim</code> is 10.</p>
<p><a href=""https://i.stack.imgur.com/4MUe3.png"" rel=""nofollow noreferrer"">the screen shot from the paper</a></p>
<p>Also, from the keras attention class discription, the <code>key_dim</code> is Size of each attention head for query and key, which matches to the paper idea.</p>
<p><a href=""https://i.stack.imgur.com/eRRGN.png"" rel=""nofollow noreferrer"">the screen shot from the class discription</a></p>
<p>Therefore, why <code>tf.keras.layers.MultiHeadAttention</code> able to take unmatched dimension. When it takes the unmatching dimension, how does it work internally with these extra weight parameters?</p>
<ul>
<li>Some of the question descriptions are cite from <a href=""https://stackoverflow.com/questions/70034327/understanding-key-dim-and-num-heads-in-tf-keras-layers-multiheadattention"">here</a>.</li>
</ul>
",0
72850120,"Keras - Specifying from_logits=False when using tf.keras.layers.Dense(1,activation='sigmoid')(x)","<p>I am working on a binary classification problem, using transfer learning and image inputs and have a question regarding the</p>
<p>I have been working through using the correct activation layers (e.g. Softmax or Sigmoid - sigmoid for binary softmax for multiclass) and noticed when I specify 'sigmoid' as part of the <code>Dense()</code> output layer, I no longer need to specify <code>from_logits=True</code> during <code>model.compile()</code>.</p>
<p>This means when I am obtaining predictions, I don't use the <code>tf.nn.sigmoid()</code> function and instead simply check if the value is greater than 0.5, then 1, else 0. Is this correct? Here is my code:</p>
<pre><code>i = keras.Input(shape=(150, 150, 3))
                scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)
                mt = scale_layer(i)
                mt = base_model(model_top, training=False)
                mt = keras.layers.GlobalAveragePooling2D()(mt)
                mt = keras.layers.Dropout(dropout)(mt)  # Regularize with dropout
                o = keras.layers.Dense(1,activation='sigmoid')(mt)
                model = keras.Model(i, o)

....

model.compile(optimizer=keras.optimizers.Adam(lr),loss=keras.losses.BinaryCrossentropy(from_logits=False)
                )
</code></pre>
<p>And then when I obtain predictions, I have the following:</p>
<pre><code>                pred = model.predict(test)
                pred = tf.where(pred &lt; 0.5, 0, 1)
                pred = pred.numpy()
</code></pre>
<p>My intuition is that as I am specifying the sigmoid activation function during the Dense layer build, I no longer work with 'logits' and therefore do not need to apply the sigmoid function later on. In the documentation, I've seen both examples used but it's quite sparse on information when working with <code>model.predict()</code>, would appreciate any guidance.</p>
",1
72927065,The CSR matrix representation in Tensorflow,"<p>My tutor wants me to implement the function <code>tf.raw_ops.SparseMatrixTranspose</code></p>
<p>So I came to the website <a href=""https://www.tensorflow.org/api_docs/python/tf/raw_ops/SparseMatrixTranspose"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/raw_ops/SparseMatrixTranspose</a> to do some research on it. And at the website, it said this function is used to transpose a <code>CSR sparse matrix</code>.</p>
<p>So I went here <a href=""https://www.geeksforgeeks.org/sparse-matrix-representations-set-3-csr/"" rel=""nofollow noreferrer"">https://www.geeksforgeeks.org/sparse-matrix-representations-set-3-csr/</a> to get some more information about it, and it told me that the CSR representation of a matrix is three arrays called <code>A, IA, JA</code>.</p>
<p>So I went to <code>tensorflow.org</code> to find some function that can generate a CSR representation of a matrix, and I found this function <a href=""https://www.tensorflow.org/api_docs/python/tf/raw_ops/DenseToCSRSparseMatrix"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/raw_ops/DenseToCSRSparseMatrix</a>.</p>
<p>So I used the script below to test this function</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np
# dense_input=np.array([[[1,2,0]],[[1,2,0]],[[1,2,0]]]).astype(np.float32)
dense_input=np.array([[1,2,0], [1,2,0],[1,2,0]]).astype(np.float32)
print(&quot;dense=&quot;,dense_input)
# csr=tf.raw_ops.DenseToCSRSparseMatrix(
#     dense_input=dense_input, indices=[[0, 0, 0],[0,0,1], [1,0,1],[1,0,1]], name=None
# )

csr=tf.raw_ops.DenseToCSRSparseMatrix(
    dense_input=dense_input, indices=[[0,0],[0,1],[1,0],[1,1],[2,0],[2,1]], name=None
)

tt=tf.raw_ops.SparseMatrixTranspose(
    input=csr, type=tf.float32, conjugate=False, name=None
)

tf.print(csr)
tf.print(tt)
</code></pre>
<p>But the output is quite confused to me because the output CSR representation is not the three arrays
<a href=""https://i.stack.imgur.com/qNCYt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qNCYt.png"" alt=""https://i.stack.imgur.com/qNCYt.png"" /></a>
Is the CSR representation in TensorFlow different from what I learned?  please help me.</p>
<p><a href=""https://colab.research.google.com/drive/160cmnJKbQFxzG2IPhYGCysPoaCCf7eQT?usp=sharing"" rel=""nofollow noreferrer"">colab notebook</a></p>
<p><a href=""https://codebeautify.org/alleditor/y22ae4dae"" rel=""nofollow noreferrer"">This is my basic CSR format transpose</a>
<a href=""https://i.stack.imgur.com/nshJd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nshJd.png"" alt=""works like this"" /></a></p>
",1
72927130,"When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received:","<p>I am trying to replicate listwise loss for deep ranking model, basically I have trying to combine below two documentations from tensorflow.org.</p>
<ol>
<li><a href=""https://www.tensorflow.org/recommenders/examples/listwise_ranking"" rel=""nofollow noreferrer"">https://www.tensorflow.org/recommenders/examples/listwise_ranking</a></li>
<li><a href=""https://www.tensorflow.org/recommenders/examples/deep_recommenders"" rel=""nofollow noreferrer"">https://www.tensorflow.org/recommenders/examples/deep_recommenders</a></li>
</ol>
<p>Below are the detailed code, but this is giving error as <code> When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, 5) with rank=2</code></p>
<p>How can I flatten the tensor data so <code>TextVectorization</code> can work? I have tried using tf.flatten() but no sucees.</p>
<pre><code>!pip install -q tensorflow-recommenders
!pip install -q --upgrade tensorflow-datasets
!pip install -q tensorflow-ranking

import pprint

import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds

import tensorflow_ranking as tfr
import tensorflow_recommenders as tfrs
from typing import Dict, Text
import os
import tempfile

ratings = tfds.load(&quot;movielens/100k-ratings&quot;, split=&quot;train&quot;)
movies = tfds.load(&quot;movielens/100k-movies&quot;, split=&quot;train&quot;)

ratings = ratings.map(lambda x: {
    &quot;movie_title&quot;: x[&quot;movie_title&quot;],
    &quot;user_id&quot;: x[&quot;user_id&quot;],
    &quot;user_rating&quot;: x[&quot;user_rating&quot;],
    # &quot;timestamp&quot;: x[&quot;timestamp&quot;],
})
movies = movies.map(lambda x: x[&quot;movie_title&quot;])

unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))
unique_user_ids = np.unique(np.concatenate(list(ratings.batch(1_000).map(
    lambda x: x[&quot;user_id&quot;]))))

tf.random.set_seed(42)

# Split between train and tests sets, as before.
shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)

train = shuffled.take(80_000)
test = shuffled.skip(80_000).take(20_000)

# We sample 50 lists for each user for the training data. For each list we
# sample 5 movies from the movies the user rated.
train = tfrs.examples.movielens.sample_listwise(
    train,
    num_list_per_user=50,
    num_examples_per_list=5,
    seed=42
)
test = tfrs.examples.movielens.sample_listwise(
    test,
    num_list_per_user=1,
    num_examples_per_list=5,
    seed=42
)

for example in train.take(1):
  pprint.pprint(example)

class UserModel(tf.keras.Model):

  def __init__(self):
    super().__init__()

    self.user_embedding = tf.keras.Sequential([
        tf.keras.layers.StringLookup(
            vocabulary=unique_user_ids, mask_token=None),
        tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),
    ])
    # self.timestamp_embedding = tf.keras.Sequential([
    #     tf.keras.layers.Discretization(timestamp_buckets.tolist()),
    #     tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),
    # ])
    # self.normalized_timestamp = tf.keras.layers.Normalization(
    #     axis=None
    # )

    # self.normalized_timestamp.adapt(timestamps)

  def call(self, inputs):
    # Take the input dictionary, pass it through each input layer,
    # and concatenate the result.
    # return tf.concat([
    #     self.user_embedding(inputs[&quot;user_id&quot;]),
    #     self.timestamp_embedding(inputs[&quot;timestamp&quot;]),
    #     tf.reshape(self.normalized_timestamp(inputs[&quot;timestamp&quot;]), (-1, 1)),
    # ], axis=1)
    return self.user_embedding(inputs[&quot;user_id&quot;])


class QueryModel(tf.keras.Model):
  &quot;&quot;&quot;Model for encoding user queries.&quot;&quot;&quot;

  def __init__(self, layer_sizes):
    &quot;&quot;&quot;Model for encoding user queries.

    Args:
      layer_sizes:
        A list of integers where the i-th entry represents the number of units
        the i-th layer contains.
    &quot;&quot;&quot;
    super().__init__()

    # We first use the user model for generating embeddings.
    self.embedding_model = UserModel()

    # Then construct the layers.
    self.dense_layers = tf.keras.Sequential()

    # Use the ReLU activation for all but the last layer.
    for layer_size in layer_sizes[:-1]:
      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=&quot;relu&quot;))

    # No activation for the last layer.
    for layer_size in layer_sizes[-1:]:
      self.dense_layers.add(tf.keras.layers.Dense(layer_size))

  def call(self, inputs):
    feature_embedding = self.embedding_model(inputs)
    return self.dense_layers(feature_embedding)


class MovieModel(tf.keras.Model):

  def __init__(self):
    super().__init__()

    max_tokens = 10_000

    self.title_embedding = tf.keras.Sequential([
      tf.keras.layers.StringLookup(
          vocabulary=unique_movie_titles,mask_token=None),
      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, 32)
    ])

    self.title_vectorizer = tf.keras.layers.TextVectorization(
        max_tokens=max_tokens)

    self.title_text_embedding = tf.keras.Sequential([
      self.title_vectorizer,
      tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),
      tf.keras.layers.GlobalAveragePooling1D(),
    ])

    self.title_vectorizer.adapt(movies)

  def call(self, titles):
    return tf.concat([
        self.title_embedding(titles),
        self.title_text_embedding(titles),
    ], axis=1)


class CandidateModel(tf.keras.Model):
  &quot;&quot;&quot;Model for encoding movies.&quot;&quot;&quot;

  def __init__(self, layer_sizes):
    &quot;&quot;&quot;Model for encoding movies.

    Args:
      layer_sizes:
        A list of integers where the i-th entry represents the number of units
        the i-th layer contains.
    &quot;&quot;&quot;
    super().__init__()

    self.embedding_model = MovieModel()

    # Then construct the layers.
    self.dense_layers = tf.keras.Sequential()

    # Use the ReLU activation for all but the last layer.
    for layer_size in layer_sizes[:-1]:
      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=&quot;relu&quot;))

    # No activation for the last layer.
    for layer_size in layer_sizes[-1:]:
      self.dense_layers.add(tf.keras.layers.Dense(layer_size))

  def call(self, inputs):
    feature_embedding = self.embedding_model(inputs)
    return self.dense_layers(feature_embedding)


class MovielensModel(tfrs.models.Model):

  def __init__(self, layer_sizes):
    super().__init__()
    self.query_model = QueryModel(layer_sizes)
    self.candidate_model = CandidateModel(layer_sizes)
    self.rating_model = tf.keras.Sequential([
        tf.keras.layers.Dense(256, activation=&quot;relu&quot;),
        tf.keras.layers.Dense(128, activation=&quot;relu&quot;),
        tf.keras.layers.Dense(1),
    ])
    self.rating_task: tf.keras.layers.Layer = tfrs.tasks.Ranking(
        loss=tf.keras.losses.MeanSquaredError(),
        metrics=[tf.keras.metrics.RootMeanSquaredError()],
    )

  def call(self, features: Dict[Text, tf.Tensor]) -&gt; tf.Tensor:
    # We pick out the user features and pass them into the user model.
    query_embeddings = self.query_model({
        &quot;user_id&quot;: features[&quot;user_id&quot;],
        # &quot;timestamp&quot;: features[&quot;timestamp&quot;],
    })
    movie_embeddings = self.candidate_model(features[&quot;movie_title&quot;])

    return (
        user_embeddings,
        movie_embeddings,
        # We apply the multi-layered rating model to a concatentation of
        # user and movie embeddings.
        self.rating_model(
            tf.concat([user_embeddings, movie_embeddings], axis=1)
        ),
    )
  
  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -&gt; tf.Tensor:
    # We only pass the user id and timestamp features into the query model. This
    # is to ensure that the training inputs would have the same keys as the
    # query inputs. Otherwise the discrepancy in input structure would cause an
    # error when loading the query model after saving it.
    ratings = features.pop(&quot;user_rating&quot;)

    user_embeddings, movie_embeddings, rating_predictions = self(features)

    rating_loss = self.rating_task(
        labels=ratings,
        predictions=rating_predictions,
    )
    retrieval_loss = self.retrieval_task(user_embeddings, movie_embeddings)

    return (retrieval_loss)

model = MovielensModel(layer_sizes=[32])
model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))


cached_train = train.shuffle(100_000).batch(8192).cache()
cached_test = test.batch(4096).cache()


model.fit(cached_train, epochs=3)
metrics = model.evaluate(cached_test, return_dict=True)

print(f&quot;Retrieval top-100 accuracy: {metrics['factorized_top_k/top_100_categorical_accuracy']:.3f}.&quot;)
print(f&quot;Ranking RMSE: {metrics['root_mean_squared_error']:.3f}.&quot;)
</code></pre>
",0
73049510,How to dynamically set pool size for AveragePooling2D layer/ How to pass external value to an sequential layer,"<p>Trying to understand <a href=""https://www.tensorflow.org/recommenders/examples/listwise_ranking"" rel=""nofollow noreferrer"">listwise documentation</a></p>
<p>while trying to replicate by mixing <a href=""https://www.tensorflow.org/recommenders/examples/deep_recommenders"" rel=""nofollow noreferrer"">deep model</a> to listwise I am stuck at point where I am not able to set the pool size inside the sequential layer in an dynamic manner. For example consider below code</p>
<pre><code>!pip install -q tensorflow-recommenders
!pip install -q --upgrade tensorflow-datasets
!pip install -q tensorflow-ranking
import pprint

import numpy as np
import tensorflow as tf
import tensorflow_datasets as tfds
import tensorflow_ranking as tfr
import tensorflow_recommenders as tfrs
from typing import Dict, Text
import os
import tempfile
import datetime
ratings = tfds.load(&quot;movielens/100k-ratings&quot;, split=&quot;train&quot;)
movies = tfds.load(&quot;movielens/100k-movies&quot;, split=&quot;train&quot;)

ratings = ratings.map(lambda x: {
    &quot;movie_title&quot;: x[&quot;movie_title&quot;],
    &quot;user_id&quot;: x[&quot;user_id&quot;],
    &quot;user_rating&quot;: x[&quot;user_rating&quot;],
    # &quot;timestamp&quot;: x[&quot;timestamp&quot;],
})
movies = movies.map(lambda x: x[&quot;movie_title&quot;])

unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))
unique_user_ids = np.unique(np.concatenate(list(ratings.batch(1_000).map(
    lambda x: x[&quot;user_id&quot;]))))


class MovieModel(tf.keras.Model):

  def __init__(self):
    super().__init__()

    max_tokens = 10_000_00

    self.title_vectorizer = tf.keras.layers.TextVectorization(
        max_tokens=max_tokens)

    self.title_text_embedding = tf.keras.Sequential([
      # tf.keras.layers.Flatten(),
      self.title_vectorizer,
      tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),
      tf.keras.layers.AveragePooling2D(pool_size=(1,4),strides=1,    padding='valid',),
    ])
    self.title_vectorizer.adapt(movies)

  def call(self, titles):
    return self.title_text_embedding(titles)
</code></pre>
<p>After we create movie model lets try to test it before we can use it on proper movie data</p>
<p>below is the test code</p>
<pre><code>test_movie_titles = [[&quot;M*A*S*H (1970)&quot;, &quot;Dances with Wolves (1990)&quot;, &quot;Speed (1994)&quot;,&quot;Dances with Wolves (1990)&quot;, &quot;Speed (1994)&quot;]]
md = MovieModel()
test_ratings = md(tf.constant(tf.reshape(test_movie_titles,[1,5,1])) )  
test_ratings
</code></pre>
<p>This now works perfect and I will get an output as below</p>
<pre><code>&lt;tf.Tensor: shape=(1, 5, 1, 32), dtype=float32, numpy=
array([[[[ 0.00778975, -0.00899004,  0.02926993, -0.00527342,
           0.00706512,  0.02012717,  0.03438753,  0.01971687,
          -0.00543808, -0.00754605, -0.02241766,  0.00045748,
          -0.00785657, -0.00291913,  0.00670988,  0.01176082,
          -0.02052191, -0.00751739, -0.01433057,  0.008
-----
----
</code></pre>
<p>Now if you notice in the code above I have hardcoded the pool_size as 1,4 ( <code>tf.keras.layers.AveragePooling2D(pool_size=(1,4),strides=1,    padding='valid',),</code>) because the test sample I had used above only have maximum 4 words, so the vectorization will produce vector of size 4, now problem is how to I ensure the right pool size when I pass the whole dataset (movies) to the model. How can I pass such external value (pool_size) to an sequential layer from outside?</p>
<p>The above code was run on google colab using tensorflow version 2.9.1</p>
",1
73213159,How to apply tf.data transformations to a DataFrame,"<p>I want to apply tf.data transformations to a panda  dataframe. According to the tensorflow docs <a href=""https://www.tensorflow.org/tutorials/load_data/pandas_dataframe"" rel=""nofollow noreferrer"">HERE</a> I can apply tf.data to a dataframe directly but the dtype of the dataframe should be uniform.</p>
<p>When I apply tf.data to my dataframe like below</p>
<pre><code>tf.data.Dataset.from_tensor_slices(df['reports'])
</code></pre>
<p>it generates this error</p>
<pre><code>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type float).
</code></pre>
<p>When I print <code>df['reports'].dtype</code> it is <code>dtype('O')</code> which seems to be not uniformed, if this is the case then how can I convert this dataframe to uniform <code>dtype</code>.</p>
",1
73224541,Tensorflow Recommender - Saving large model with ScaNN index - memory bottleneck,"<p>I have a relatively large TF retrieval model using the TFRS library. It uses a <a href=""https://github.com/google-research/google-research/tree/master/scann"" rel=""nofollow noreferrer"">ScaNN</a> layer for <a href=""https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/factorized_top_k/ScaNN"" rel=""nofollow noreferrer"">indexing the recommendations</a>. I am having a system host memory issue when I try to save this model via the <a href=""https://www.tensorflow.org/api_docs/python/tf/saved_model/save"" rel=""nofollow noreferrer"">tf.saved_model.save()</a> method. I am running the official TF 2.9.1 Docker Container with TFRS on a VM in the cloud. I have 28 GB of memory to try to save the model.</p>
<p><a href=""https://www.tensorflow.org/recommenders/examples/quickstart"" rel=""nofollow noreferrer"">Here is the quickstart example:</a></p>
<p>Basically we create the first embedding</p>
<pre><code>user_model = tf.keras.Sequential([
    tf.keras.layers.StringLookup(
    vocabulary=unique_user_ids, mask_token=None),
    # We add an additional embedding to account for unknown tokens.
    tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)
])
</code></pre>
<p>Then create the model</p>
<pre><code>class MovielensModel(tfrs.Model):

  def __init__(self, user_model, movie_model):
    super().__init__()
    self.movie_model: tf.keras.Model = movie_model
    self.user_model: tf.keras.Model = user_model
    self.task: tf.keras.layers.Layer = task

  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -&gt; tf.Tensor:
    # We pick out the user features and pass them into the user model.
    user_embeddings = self.user_model(features[&quot;user_id&quot;])
    # And pick out the movie features and pass them into the movie model,
    # getting embeddings back.
    positive_movie_embeddings = self.movie_model(features[&quot;movie_title&quot;])

    # The task computes the loss and the metrics.
    return self.task(user_embeddings, positive_movie_embeddings)
</code></pre>
<p>Next we create the ScaNN indexing layer</p>
<pre><code>scann_index = tfrs.layers.factorized_top_k.ScaNN(model.user_model)

scann_index.index_from_dataset(
  tf.data.Dataset.zip((movies.batch(100), movies.batch(100).map(model.movie_model)))
)

# Get recommendations.
_, titles = scann_index(tf.constant([&quot;42&quot;]))
print(f&quot;Recommendations for user 42: {titles[0, :3]}&quot;)
</code></pre>
<p>Finally the model is sent out to be saved</p>
<pre><code># Export the query model.
with tempfile.TemporaryDirectory() as tmp:
   path = os.path.join(tmp, &quot;model&quot;)

   # Save the index.
   tf.saved_model.save(
      index,
      path,
      options=tf.saved_model.SaveOptions(namespace_whitelist=[&quot;Scann&quot;])
   )

   # Load it back; can also be done in TensorFlow Serving.
   loaded = tf.saved_model.load(path)

   # Pass a user id in, get top predicted movie titles back.
   scores, titles = loaded([&quot;42&quot;])

   print(f&quot;Recommendations: {titles[0][:3]}&quot;)
</code></pre>
<p>This is the problem line:</p>
<pre><code>   # Save the index.
   tf.saved_model.save(
      index,
      path,
      options=tf.saved_model.SaveOptions(namespace_whitelist=[&quot;Scann&quot;])
   )
</code></pre>
<p>I'm not sure if there is a memory leak or what, but when I train my model on 5M+ records... I can watch the host system memory spike to 100% and the process is killed. If I train on a smaller dataset... there is no problem, so I know the code is okay.</p>
<p>Can anyone suggest how to get around the memory bottleneck when saving a large ScaNN retrieval model, so I can eventually load the model back in for inference?</p>
",0
73288781,Is advanced indexing available across n-dimensions in TensorFlow?,"<p>In PyTorch, we can use standard Pythonic indexing to apply advanced indexing across n-dimensions.</p>
<p><code>preds</code> is a <code>Tensor</code> of shape <code>[1, 3, 64, 64, 12]</code>.</p>
<p><code>a</code>, <code>b</code>, <code>c</code>, <code>d</code> are 1-dimensional <code>Tensor</code>s of the same length. In this instance that length is 9, but this is not always the case.</p>
<p>PyTorch example achieving the desired result:</p>
<pre><code>result = preds[a, b, c, d]

result.shape
&gt;&gt;&gt; [9, 12]
</code></pre>
<p>How can this be reproduced in TensorFlow, starting from the same 5 Tensors and creating the same output?</p>
<p>I have tried <code>tf.gather</code> whichs seem to be able to produce the same behaviour in a single dimension:</p>
<pre><code>tf.shape(tf.gather(preds, a))
&gt;&gt;&gt; [9, 3, 64, 64, 12]
</code></pre>
<p>Is it possible to extend this to eventually reach the desired output of shape <code>[9, 12]</code>?</p>
<p>I have also noted the presence of <code>tf.gather_nd</code> which seems like it may be relevant here but I cannot determine how I would employ it from the <a href=""https://www.tensorflow.org/api_docs/python/tf/gather_nd"" rel=""nofollow noreferrer"">documentation</a>.</p>
",1
73328337,Tensorflow 2 SSD MobileNet model breaks during conversion to tflite,"<p>I've been trying to follow this process to run an object detector (SSD MobileNet) on the Google Coral Edge TPU:
<a href=""https://i.stack.imgur.com/Hm22L.png"" rel=""nofollow noreferrer"">Edge TPU model workflow</a></p>
<p>I've successfully trained and evaluated my model with the Object Detection API. I have the model both in checkpoint format as well as tf SavedModel format. As per the documentation, the next step is to convert to .tflite format using post-training quantization.</p>
<p>I am to attempting to follow <a href=""https://colab.research.google.com/github/tensorflow/models/blob/master/research/object_detection/colab_tutorials/convert_odt_model_to_TFLite.ipynb"" rel=""nofollow noreferrer"">this</a> example. The export_tflite_graph_tf2.py script and the conversion code that comes after run without errors, but I see some weird behavior when I try to actually use the model to run inference.</p>
<ol>
<li>I am unable to use the saved_model generated by export_tflite_graph_tf2.py. When running the following code, I get an error:</li>
</ol>
<pre><code>print('loading model...')
model = tf.saved_model.load(tflite_base)
print('model loaded!')
results = model(image_np)
</code></pre>
<blockquote>
<p>TypeError: '_UserObject' object is not callable --&gt; results = model(image_np)</p>
</blockquote>
<p>As a result, I have no way to tell if the script broke my model or not before I even convert it to tflite. Why would model not be callable in this way? I have even verified that the type returned by tf.saved_model.load() is the same when I pass in a saved_model before it went through the export_tflite_graph_tf2.py script and after. The only possible explanation I can think of is that the script alters the object in some way that causes it to break.</p>
<ol start=""2"">
<li>I convert to tflite with post-training quantization with the following code:</li>
</ol>
<pre><code>def representative_data_gen():
  dataset_list = tf.data.Dataset.list_files(images_dir + '/*')
  for i in range(100):
    image = next(iter(dataset_list))
    image = tf.io.read_file(image)
    # supports PNG as well
    image = tf.io.decode_image(image, channels=3)
    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])
    image = tf.cast(image / 255., tf.float32) 
    image = tf.expand_dims(image, 0)
    if i == 0:
      print(image.dtype)
    yield [image]

# This enables quantization
# This sets the representative dataset for quantization
converter = tf.lite.TFLiteConverter.from_saved_model(base_saved_model)
# converter = tf.lite.TFLiteConverter.from_keras(model)

converter.optimizations = [tf.lite.Optimize.DEFAULT] # issue here?
converter.representative_dataset = representative_data_gen
converter.target_spec.supported_ops = [
  # tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  # tf.lite.OpsSet.SELECT_TF_OPS, # enable TensorFlow ops.
  tf.lite.OpsSet.TFLITE_BUILTINS_INT8 # This ensures that if any ops can't be quantized, the converter throws an error
]

# This ensures that if any ops can't be quantized, the converter throws an error
# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.
converter.target_spec.supported_types = [tf.int8]
converter.target_spec.supported_ops += [tf.lite.OpsSet.TFLITE_BUILTINS]
# These set the input and output tensors to uint8 (added in r2.3)
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
tflite_model_quantized = converter.convert()
</code></pre>
<p>Everything runs with no errors, but when I try to actually run an image through the model, it returns garbage. I tried removing the quantization to see if that was the issue, but even without quantization it returns seemingly random bounding boxes that are completely off from the model's performance prior to conversion. The shape of the output tensors look fine, it's just the content is all wrong.</p>
<p>What's the right way to get this model converted to a quantized tflite form? I should note that I can't use the tflite_convert utility because I need to quantize the model, and it appears according to the source code that the quantize_weights flag is deprecated? There are a bunch of conflicting resources I see from TF1 and TF2 about this conversion process so I'm pretty confused.</p>
<p>Note: I'm using a retrained SSD MobileNet from the model zoo. I have not made any changes to the architecture in my training workflow. I've confirmed that the errors persist even on the base model pulled directly from the object detection model zoo.</p>
",0
73370744,Why is seeding mandatory for stateless random flipping in tensorflow image operations?,"<p>I am implementing custom data augmentation layer in my model where I am trying to use the function
<code>tf.image.stateless_random_flip_left_right</code>.</p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/image/stateless_random_flip_left_right"" rel=""nofollow noreferrer"">Tensorflow Documentation</a> says this function can,</p>
<blockquote>
<p>Randomly flip an image horizontally (left to right) deterministically.</p>
</blockquote>
<p>Why do we need a deterministic flip? Also, for this function, <code>seed</code> is a <strong>mandatory</strong> argument.</p>
<p>In data augmentation, I understand that from epoch to epoch, a particular image is fed in different forms. Wouldn't this force images of the same kind to be supplied?</p>
",1
73440964,Model not improving with GradientTape but with model.fit(),"<p>I am currently trying to train a model using <code>tf.GradientTape</code>, as <code>model.fit(...)</code> from keras will not be able to handle my data input in the future. However, while a test run with <code>model.fit(...)</code> and my model works perfectly, <code>tf.GradientTape</code> does not.</p>
<p>During training, the loss using the <code>tf.GradientTape</code> custom workflow will first slightly decrease, but then become stuck and not improve any further, no matter how many epochs I run. The chosen metric will also not change after the first few batches. Additionally, the loss per batch is unstable and jumps between nearly zero to something very large. The running loss is more stable but shows the model not improving.
This is all in contrast to using <code>model.fit(...)</code>, where loss and metrics are improving immediately.</p>
<p>My code:</p>
<pre><code>def build_model(kernel_regularizer=l2(0.0001), dropout=0.001, recurrent_dropout=0.):
    x1 = Input(62)
    x2 = Input((62, 3))

    x = Embedding(30, 100, mask_zero=True)(x1)
    x = Concatenate()([x, x2])

    x = Bidirectional(LSTM(500,
                           return_sequences=True,
                           kernel_regularizer=kernel_regularizer,
                           dropout=dropout,
                           recurrent_dropout=recurrent_dropout))(x)

    x = Bidirectional(LSTM(500,
                           return_sequences=False,
                           kernel_regularizer=kernel_regularizer,
                           dropout=dropout,
                           recurrent_dropout=recurrent_dropout))(x)

    x = Activation('softmax')(x)

    x = Dense(1000)(x)
    x = Dense(500)(x)
    x = Dense(250)(x)
    x = Dense(1, bias_initializer='ones')(x)

    x = tf.math.abs(x)
    return Model(inputs=[x1, x2], outputs=x)


optimizer = Adam(learning_rate=0.0001)

model = build_model()
model.compile(optimizer=optimizer, loss='mse', metrics='mse')

options = tf.data.Options()
options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA 
dat_train = tf.data.Dataset.from_generator(
    generator= lambda: &lt;load_function()&gt; 
    output_types=((tf.int32, tf.float32), tf.float32)
) 
dat_train = dat_train.with_options(options) 

# keras training
model.fit(dat_train, epochs=50)


# custom training
for epoch in range(50):
    for (x1, x2), y in dat_train:
        with tf.GradientTape() as tape:
            y_pred = model((x1, x2), training=True)
            loss = model.loss(y, y_pred)
        grads = tape.gradient(loss, model.trainable_variables)
        model.optimizer.apply_gradients(zip(grads, model.trainable_variables))
</code></pre>
<p>I could use relu at the output layer, however, I found the abs to be more robust. Changing it does not change the outcome. The input <code>x1</code> of the model is a sequence, <code>x2</code> are some additional features, that are later concatenated to the embedded <code>x1</code> sequence. For my approach, I'm not using the MSE, but it works either way.</p>
<p>I could provide some data, however, my dataset is quite large, so I would need to extract a bit out of it.</p>
<p>All in all, my problem seems to be similar to:
<a href=""https://stackoverflow.com/questions/57189363/keras-model-doesnt-train-when-using-gradienttape"">Keras model doesn&#39;t train when using GradientTape</a></p>
<hr />
<p><strong>Edit 1</strong></p>
<p>The softmax activation is currently not necessary, but is relevant for my future goal of splitting the model.
Additionally, some things I noticed:</p>
<ol>
<li>The custom training takes roughly 2x the amount of time compared to <code>model.fit(...)</code>.</li>
<li>The gradients in the custom training seem very small and range from ±1e-3 to ±1e-9 inside the model. I don't know if that's normal and don't know how to compare it to the gradients provided by <code>model.fit(...)</code>.</li>
</ol>
<hr />
<p><strong>Edit 2</strong></p>
<p>I've added a Google Colab notebook to reproduce the issue:</p>
<p><a href=""https://colab.research.google.com/drive/1pk66rbiux5vHZcav9VNSBhdWWIhQM-nF?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1pk66rbiux5vHZcav9VNSBhdWWIhQM-nF?usp=sharing</a></p>
<p>The loss and MSE for 20 epochs is shown here:</p>
<p><a href=""https://i.stack.imgur.com/chyy6.png"" rel=""nofollow noreferrer"">custom training</a></p>
<p><a href=""https://i.stack.imgur.com/W8ZVu.png"" rel=""nofollow noreferrer"">keras training</a></p>
<p>While I only used a portion of my data in the notebook, it will still run for a very long time. For the custom training run, the loss for each batch is simply stored in losses. It matches the behavior in the custom training run image.
So far, I've noticed two ways of improving the performance of the custom training:</p>
<ol>
<li>The usage of custom layer initialization</li>
<li>Using MSE as a loss function</li>
</ol>
<p>Using the MSE, compared to my own loss function actually improves the custom training performance. Still, using MSE and/or different initialization won't come close to the performance of keras fit.</p>
",0
73554067,Conceptual difference between TensorFlow (graph-mode) and JAX,"<h1>TensorFlow</h1>
<p>TensorFlow (in graph-mode) generates a computation graph and <code>tf.gradients</code> is implemented as an operation on the graph, which outputs a new graph for the gradient.</p>
<p>For execution, there are different options: It could directly operate on the graph, or it could compile it to XLA, or you could also transform it to TFLite, etc.</p>
<h1>JAX</h1>
<p>JAX operates on functions. <code>jax.grad</code> gets a function and returns a function.</p>
<p>For execution, there are different options, but I think the most common is to compile to XLA.</p>
<h1>Question</h1>
<p>So, this both sounds very similar to me. A computation graph is just another way to represent a function. Is there any conceptual difference?</p>
<p>I often see that people say that <code>jax.vmap</code> is one big advantage of JAX, but you can do just the same in TensorFlow, e.g. <a href=""https://www.tensorflow.org/api_docs/python/tf/vectorized_map"" rel=""nofollow noreferrer""><code>tf.vectorized_map</code></a>.</p>
<p>Or phrased a bit different: Is there any algorithm which you could implement in JAX but not in TF in the same way, or vice versa? Or which would be efficient in one case but not in the other?</p>
<p>This question is really only about the conceptual aspect, which should be a purely objective thing to answer: Either the answer is yes, they are conceptually the same (equally powerful), maybe with a short explanation, or no, they are conceptually different, with an example what is possible in one framework but conceptually not possible on the other. I don't want to have any discussion here on any subjective aspect, e.g. that maybe <code>tf.vectorized_map</code> is a bit buggy, that TF documentation is worse, or whatever.</p>
",1
73585314,Replicating tensorflow bert model in R,"<p>I am just replicating <a href=""https://tensorflow.rstudio.com/tutorials/keras/text_classification.html"" rel=""nofollow noreferrer"">this code</a> based on Basic Text Classification. Until the below line, it seems alright. However, the following line:</p>
<pre><code>vectorize_layer %&gt;% adapt(train_text)
</code></pre>
<p>throws me an error as below. Any idea, how should I approach it in order to solve it ? My tensorflow version is <code>TensorFlow v2.8.2,</code> is it the main issue for this error ?</p>
<p>I assume that the train_text should not be like that as well, should it ?</p>
<pre><code>&gt; train_text
&lt;MapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)&gt;
</code></pre>
<p>The thrown error:</p>
<pre><code>Error in py_call_impl(callable, dots$args, dots$keywords) : 
RuntimeError: in user code:

File &quot;C:\ANACON~2\envs\R-TENS~1\lib\site-packages\keras\engine\base_preprocessing_layer.py&quot;, line 118, in adapt_step *
self.update_state(data)
File &quot;C:\ANACON~2\envs\R-TENS~1\lib\site-packages\keras\layers\preprocessing\text_vectorization.py&quot;, line 431, in update_state **
self._lookup_layer.update_state(self._preprocess(data))
File &quot;C:\ANACON~2\envs\R-TENS~1\lib\site-packages\keras\layers\preprocessing\text_vectorization.py&quot;, line 512, in _preprocess
inputs = self._standardize(inputs)
File &quot;C:\Users\xxxxx\AppData\Local\R\win-library\4.2\reticulate\python\rpytools\call.py&quot;, line 21, in python_function
raise RuntimeError(res[kErrorKey])

RuntimeError: Evaluation error: attempt to apply non-function.
</code></pre>
",0
73657854,Is SageMaker Distributed Data-Parallel (SMDDP) supported for keras models?,"<p>Is SageMaker Distributed Data-Parallel (SMDDP) supported for keras models?</p>
<p>In documentation it says &quot;SageMaker distributed data parallel is adaptable to TensorFlow training scripts composed of tf core modules except tf.keras modules. SageMaker distributed data parallel does not support TensorFlow with Keras implementation.&quot; <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-modify-sdp.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-modify-sdp.html</a></p>
<p>But inside the training script and how to modify it, I can see the tf.keras and tf.keras.model is used. <a href=""https://sagemaker.readthedocs.io/en/stable/api/training/sdp_versions/v1.0.0/smd_data_parallel_tensorflow.html"" rel=""nofollow noreferrer"">https://sagemaker.readthedocs.io/en/stable/api/training/sdp_versions/v1.0.0/smd_data_parallel_tensorflow.html</a></p>
",0
73660612,"Model was constructed with shape (None, 65536) but it was called on an input with incompatible shape (None, 65536, None)","<p>For reference the full error is here:</p>
<pre><code>WARNING:tensorflow:Model was constructed with shape (None, 65536) for input KerasTensor(type_spec=TensorSpec(shape=(None, 65536), dtype=tf.float32, name='input_1'), name='input_1', description=&quot;created by layer 'input_1'&quot;), but it was called on an input with incompatible shape (None, 65536, None).
</code></pre>
<p>I am using <a href=""https://www.kymat.io/index.html"" rel=""nofollow noreferrer""><code>kymatio</code></a> to classify audio signals. Before constructing the model I use tensorflow's <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/audio_dataset_from_directory"" rel=""nofollow noreferrer""><code>tf.keras.utils.audio_dataset_from_directory</code></a> to create the training and testing sets.</p>
<p>The audio samples are of shape <code>(65536,)</code> before the sets are created. To create the sets I use the following code:</p>
<pre><code>T = 2**16
J = 8
Q = 12
log_eps = 1e-6
SEED = 42

train_dataset = tf.keras.utils.audio_dataset_from_directory(
    '../train',
    labels='inferred',
    label_mode='int',
    class_names=['x', 'y', 'z', 'xy', 'xz', 'yz', 'xyz'],
    batch_size=32,
    output_sequence_length=T,
    ragged=False,
    shuffle=True,
    seed=SEED,
    follow_links=False
)
</code></pre>
<p>The <code>element_spec</code> of the <code>train_dataset</code> is <code>(TensorSpec(shape=(None, 65536, None), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))</code>.</p>
<p>So at some point the shape is changing in the <code>TensorSpec</code> to <code>(None, 65536, None)</code> for some reason...</p>
<p>The model is constructed as follows and the error points to <code>model.fit(...)</code>.</p>
<pre><code>x_in = layers.Input(shape=(T))
x = Scattering1D(J, Q=Q)(x_in)
x = layers.Lambda(lambda x: x[..., 1:, :])(x)
x = layers.Lambda(lambda x: tf.math.log(tf.abs(x) + log_eps))(x)
x = layers.GlobalAveragePooling1D(data_format='channels_first')(x)
x = layers.BatchNormalization(axis=1)(x)
x_out = layers.Dense(7, activation='softmax')(x)
model = tf.keras.models.Model(x_in, x_out)
model.summary()
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(train_dataset, epochs=50)
</code></pre>
",0
73793304,How to fourth layer to tensor to let convolution consume it,"<p>I am trying to add preprocessing layers to a tensorflow model but can't figure out how to use tf.keras.layers.Reshape correctly.</p>
<p>I have some previous preprocessing layers that bring a numpy image to a tensorflow tensor of shape <code>TensorShape([50, 50, 3])</code>.</p>
<p>The first layer of the model that I am trying to connect these preprocessing layers to is a convolutional layer which requires a four dimensional input. When I call the model on a tensor of dimensions <code>Tensorshape([50,50,3])</code>, with three dimensions, I get the error:</p>
<pre><code>Input 0 of layer &quot;Conv1&quot; is incompatible with the layer: expected min_ndim=4, found ndim=3. 
Full shape received: (50, 50, 3)

Call arguments received by layer &quot;model_1&quot; (type Functional):
  • inputs=tf.Tensor(shape=(50, 50, 3), dtype=float32)
  • training=False
  • mask=None
</code></pre>
<p>Changing the input tensor to a numpy array and sizing it to a (1,50,50,3) array, then inputting that into the model works fine. I want to do this with a tensorflow layer however so that I can save the preprocessing and model inference into a single Saved Model format tensorflow file without having to do python preprocessing.</p>
<p><code>tf.expand_dims(input_tensor, axis=0)</code> works, but it's not a layer object so I can't use it. It looks like <code>tf.keras.layers.Reshape((1,50,50,3), input_shape=(50,50,3))</code> or <code>tf.keras.layers.Reshape((1,50,50,3))</code> is the way to go then.</p>
<p>My silly problem is that I just can't figure out how to use <code>tf.keras.layers.Reshape</code>, even using the tf/keras documentation.
When I pass in a tensor of dimensions <code>TensorShape([50, 50, 3])</code>, to <code>tf.keras.layers.Reshape((1,50,50,3))</code> I get the error message:</p>
<pre><code>Input to reshape is a tensor with 7500 values, but the requested shape has 375000 [Op:Reshape]

Call arguments received by layer &quot;reshape_9&quot; (type Reshape):
  • inputs=tf.Tensor(shape=(50, 50, 3), dtype=float32) 
</code></pre>
<p>So, how do I get this to work? All I want is to end up with the same Tensor as inputted but with <code>TensorShape([1,50,50,3])</code> instead of <code>TensorShape([50,50,3])</code>, and I want this to happen with a tf.keras.layers object. So really I just want to perform an identity transformation that adds a fourth dimension of size one at the beginning of a tensor so that a convolutional layer can consume it, and I want this from a tf.keras.layers object.</p>
",1
73794766,what is the meaning of axis=-1 in tf.keras.layers.Normalization?,"<p>I'm trying to learn deep learning using keras and tensorflow and I came across a code explaining linear regression at <a href=""https://www.tensorflow.org/tutorials/keras/regression"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/keras/regression</a> wherein they have created a normalization layer using normalizer = tf.keras.layers.Normalization(axis=-1). Someone please explain the meaning of axis =-1 . I tried looking at the API documentation but I couldnt understand the explanation from there?I know that axis=0 represent rows and axis=1 columns, right?
Thanks in advance</p>
",1
73930132,Build an input pipeline with tf.data,"<p>I have two lists of paths
One call training and one called Test.</p>
<p>I want to Build two input pipelines with tf.data.
One called train_dataset and one called test_dataset
Here is my code :</p>
<pre><code>train_dataset = tf.data.Dataset.list_files(train_set)
train_dataset = train_set.map(load_image_train,
                              num_parallel_calls=tf.data.AUTOTUNE)
train_dataset = train_dataset.shuffle(BUFFER_SIZE)
train_dataset = train_dataset.batch(BATCH_SIZE)


def load_image_train(a_training_datapoint):
  real_image_path, drawing_path = a_training_datapoint.split('&amp;')
  real_image = convert_images_to_tensor(real_image_path)
  drawing_image = convert_images_to_tensor(drawing_path)
  real_image, drawing_image = random_jitter(real_image, drawing_image)
 real_image, drawing_image = normalize(real_image, drawing_image)
 return real_image, drawing_image
</code></pre>
<p>the error I am getting is saying :</p>
<pre><code>InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: gdrive/My Drive/DrawingDataSet/FacePictures/19/IMG_1835.JPG&amp;gdrive/My 
</code></pre>
<p>I am getting this error because I combined to images together with an @ as you can see.
I did that because I am trying to follow this <a href=""https://www.tensorflow.org/tutorials/generative/pix2pix"" rel=""nofollow noreferrer"">tutorial</a> but with my Dataset.</p>
<p>how do I make my train_dataset but keep my two images group together ?</p>
",0
74005009,How to create output_signature for tensorflow.dataset.from_generator,"<p>I have a generator yielding data and labels <code>yield data, labels</code> where the data is
an <code>numpy.ndarray</code> with variable rows and 500 columns of type <code>dtype=float32</code> and the labels are integers of <code>numpy.int64</code>.</p>
<p>I'm trying to pass this data into TensorFlow from_generator function to create a TensorFlow dataset: <code>tf.data.Dataset.from_generator</code></p>
<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator"" rel=""nofollow noreferrer"">docs</a> say that the from_generator function needs a parameter <code>output_signature</code> as an input. But I'm having trouble understanding how to build this output_signature.</p>
<p>How can I make the output_signature for the generator I described?</p>
<p>Thank you!</p>
<p>Edit:
I used <code>tf.type_spec_from_value</code> to get this:</p>
<pre><code>dataset = tf.data.Dataset.from_generator(
   datagen_row,
   output_signature=(
      tf.TensorSpec(shape=(None, 512), dtype=tf.float32, name=None),
      tf.TensorSpec(shape=(), dtype=tf.int64, name=None)
   )
)
</code></pre>
<p>But is it correct to use None when the number of rows is varying for the first data type?</p>
",1
74029376,Tensorflow custom reduction function with axis support,"<p>I would like to get the value with the maximum absolute value in a tensor, with respect to an axis. Note that I don't want the maximum absolute value, I want the <em>value that has the maximum absolute value</em> (so I need to keep the sign).</p>
<p>Ideally, I would like something similar to <code>reduce_max</code> or <code>reduce_min</code>:</p>
<pre class=""lang-py prettyprint-override""><code>tensor = tf.constant(
  [
    [[ 1,  5, -3],
     [ 2, -3,  1],
     [ 3, -6,  2]],

    [[-2,  3, -5],
     [-1,  4,  2],
     [ 4, -1,  0]]
   ]
)
# tensor.shape = (2, 3, 3)

tensor.reduce_maxamplitude(tensor, axis=0)
# Tensor(
#  [[-2,  5, -5],
#   [ 2,  4,  2],
#   [ 4, -6,  2]]
# )
# shape: (3, 3)

tensor.reduce_maxamplitude(tensor, axis=1)
# Tensor(
#  [[3, -6, -3],
#   [4,  4, -5]]
# )
# shape: (2, 3)

tensor.reduce_maxamplitude(tensor, axis=2)
# Tensor(
#  [[5, -3, -6],
#   [-5,  4, 4]]
# )
# shape: (2, 3)
</code></pre>
<p>but I did not find anything useful in tensorflow documentation.</p>
<p>With a flat tensor, I know that I could use <code>tf.foldl</code> or <code>tf.foldr</code>:</p>
<pre class=""lang-py prettyprint-override""><code>flat = tf.reshape(tensor, -1)
tf.foldr(lambda a, x: x if tf.abs(x) &gt; tf.abs(a) else a, flat)
# -6
</code></pre>
<p>However, I don't know how to handle an axis parameter in the case of multidimensional tensors.</p>
",1
74060508,How to Save a Tensorflow Dataset,"<p>As the title says I'm trying to save a <code>TensorSliceDataset</code> object to file. Viewing tensorflow's <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">website</a> it seems that the <code>tf.data.Dataset</code> class has a save function but it is not implemented for <code>TensorSliceDataset</code> objects. Pickling also did not work for me.</p>
<p>Example code</p>
<pre><code>import tensorflow as tf
t = tf.range(10)
ds = tf.data.Dataset.from_tensor_slices(t)
ds.save()
</code></pre>
<p>returns error: <code>AttributeError: 'TensorSliceDataset' object has no attribute 'save'</code></p>
",1
74172887,Is it better to create a Dockerfile based on tensorflow:latest-gpu-py3-jupyter or refactor for load_image() attribute?,"<p>I am developing image classification models in the Jupyter notebook environment. After getting my model to work with the CPU, I am trying to use the latest TensorFlow Docker image supported for Jupyter &amp; GPU (<a href=""https://www.tensorflow.org/install/docker"" rel=""nofollow noreferrer"">tensorflow/tensorflow:latest-gpu-py3-jupyter</a>) so I can take advantage of my GPU for training. The GPU configuration is not the problem (<code>nvidia-smi</code> command shows the GPU is available), but I'm now stuck on what I should do with my image data pipeline setup.</p>
<p>I have folders containing images with the following structure:</p>
<pre><code>my_folder
│
└───Training
│   │
│   └───Class_A
│   │       01234.jpg
│   │       56789.jpg
│   │       ...
│   │        
│   └───Class_B
│   │       01234.jpg
│   │       56789.jpg
│   │       ...
│   
└───Validation
│   │
│   └───Class_A
│   │       01234.jpg
│   │       56789.jpg
│   │       ...
│   │        
│   └───Class_B
│   │       01234.jpg
│   │       56789.jpg
│   │       ...
</code></pre>
<pre><code>path_training = 'my_folder/Training/'
path_validation = 'my_folder/Validation/'
image_size = (90, 90)
</code></pre>
<p>With tensorflow == 2.6.2, I can easily load in my training/validation image datasets with the following code:</p>
<pre><code>train_ds = tf.keras.preprocessing.image_dataset_from_directory(path_training,
                                                               seed=1993, 
                                                               image_size = image_size)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(path_validation,
                                                             seed=1993,
                                                             image_size = image_size)
</code></pre>
<p>However, it became apparent that this command does not work when using the Docker image:</p>
<blockquote>
<p>----&gt; 3 train_ds = tf.keras.preprocessing.image_dataset_from_directory(path_training,
4                                                                seed=1993,
5                                                                image_size = image_size)</p>
<p>AttributeError: module 'tensorflow_core.keras.preprocessing' has no
attribute 'image_dataset_from_directory'</p>
</blockquote>
<p>So I discovered the tensorflow version of the Docker image is 2.1.0, and that attribute is not listed in the API, which leaves me this option:</p>
<pre><code># Read in all image files and split into training/validation sets (tensorflow-gpu 2.1.0)
train_ds = tf.keras.preprocessing.image.load_img(path_training, target_size = image_size)
val_ds = tf.keras.preprocessing.image.load_img(path_validation, target_size = image_size)
</code></pre>
<p>As might be expected, the <a href=""https://www.tensorflow.org/versions/r2.1/api_docs/python/tf/keras/preprocessing/image/load_img"" rel=""nofollow noreferrer"">load_img()</a> command from TensorFlow 2.1.0 does not read in directories, like image_dataset_from_directory() does.</p>
<blockquote>
<p>IsADirectoryError: [Errno 21] Is a directory: 'my_folder/Training/'</p>
</blockquote>
<p>I'm not sure what the best or easiest path forward would be here, as I'm not very familiar with building Docker images. Would it be better to build a Dockerfile based on TensorFlow's latest official tensorflow-GPU &amp; Jupyter Docker image so I can utilize <code>tf.keras.preprocessing.image_dataset_from_directory()</code> or should I just make do with this pre-built Docker image and load my image data with <code>tf.keras.preprocessing.image.load_img()</code> by looping through files in the directory path and creating training/validation image datasets this way? For the latter approach, I searched and found <a href=""https://www.programcreek.com/python/example/89223/keras.preprocessing.image.load_img"" rel=""nofollow noreferrer"">some similar examples</a>, notably this example code:</p>
<pre><code>def get_data(dir):
    X_train, Y_train = [], []
    X_test, Y_test = [], []
    subfolders = sorted([file.path for file in os.scandir(dir) if file.is_dir()])
    for idx, folder in enumerate(subfolders):
        for file in sorted(os.listdir(folder)):
            img = load_img(folder+&quot;/&quot;+file, color_mode='grayscale')
            img = img_to_array(img).astype('float32')/255
            img = img.reshape(img.shape[0], img.shape[1],1)
            if idx &lt; 35:
                X_train.append(img)
                Y_train.append(idx)
            else:
                X_test.append(img)
                Y_test.append(idx-35)

    X_train = np.array(X_train)
    X_test = np.array(X_test)
    Y_train = np.array(Y_train)
    Y_test = np.array(Y_test)
    return (X_train, Y_train), (X_test, Y_test) 
</code></pre>
",0
74182037,"How to ""update"" from module tf.keras.preprocessing.image to tf.keras.utils.image_dataset_from_directory for features extraction","<p>This code part is common to both &quot;problematic&quot; codes below:</p>
<pre><code>BATCH_SIZE = 32
IM_DIR = '/content/drive/My Drive/101_ObjectCategories/'
IM_HEIGHT = 224
IM_WIDTH = 224
NUM_IM = 8686
NUM_EPOCHS = int(math.ceil(NUM_IM / BATCH_SIZE))

#load pre-trained base model
model = ResNet50(weights='imagenet',
                 include_top=False,
                 input_shape=(IM_WIDTH, IM_HEIGHT, CH),
                 pooling='max')
</code></pre>
<p>The following code I successfully use to extract features of a set of images using module <code>tf.keras.preprocessing.image</code>.</p>
<pre><code>datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)
dataset = datagen.flow_from_directory(IM_DIR,
                                      target_size=(IM_HEIGHT, IM_WIDTH),
                                      class_mode=None,
                                      shuffle=False)

feature_list = []
feature_list = model.predict(dataset, num_epochs)
</code></pre>
<p>Thereafter I train a simple nearest-neighbor model using brute-force algorithm and I'm able to find three other images that are really similar to the query image as you can see below:</p>
<p><a href=""https://i.stack.imgur.com/qPi7q.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qPi7q.png"" alt=""Right results"" /></a></p>
<p>But as pointed in the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image"" rel=""nofollow noreferrer"">documentation</a> this preprocessing module is deprecated.<br />
So, I would like to &quot;update&quot; the code as suggested in the documentation: &quot;Prefer loading data with <code>tf.keras.utils.image_dataset_from_directory</code>, and then transforming the output <code>tf.data.Dataset</code> with preprocessing layers&quot;.<br />
For that I'm trying the following:</p>
<pre><code>#load images
dataset = tf.keras.utils.image_dataset_from_directory(
  IM_DIR,
  labels='inferred', #'inferred', None
  label_mode='categorical',  #'int', 'categorical', 'binary' or None
  class_names=None,
  color_mode='rgb',  #'grayscale', 'rgb' or 'rgba'
  batch_size=BATCH_SIZE,
  image_size=(IM_HEIGHT, IM_WIDTH),
  shuffle=True,
  seed=51719,
  validation_split=None,
  subset=None,                #'training', 'validation' or 'both'
  interpolation='bilinear',   #'bilinear', 'nearest', 'bicubic', 'area', 'lanczos3', 'lanczos5', 'gaussian' or 'mitchellcubic'
  follow_links=False,
  crop_to_aspect_ratio=False
)

#&quot;transforming the output with preprocessing layers&quot;
#rescale (normalize) dataset
rescale_layer = tf.keras.layers.Rescaling(1./255)

rescaled_dataset = dataset.map(lambda x, y: (rescale_layer(x), y))
im_batch, labels_batch = next(iter(rescaled_dataset))


#configure dataset for performance
#https://www.tensorflow.org/tutorials/load_data/images#configure_the_dataset_for_performance

AUTOTUNE = tf.data.AUTOTUNE
tuned_dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)
</code></pre>
<p>And now I begin with the features extraction</p>
<pre><code>#features extraction
#https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict
feature_list = []

feature_list = model.predict(
    tuned_dataset,
    batch_size=None,
    verbose='auto',
    steps=None,
    callbacks=None,
    max_queue_size=10,
    workers=1,
    use_multiprocessing=False
)

#save features
pickle.dump(
    feature_list,
    open(DATA_DIR + 'features.pickle', 'wb'))
</code></pre>
<p>After that I do the same and train the nearest neighbor model with this features, but the results are catastrophic as you can see below:</p>
<p><a href=""https://i.stack.imgur.com/18Wsa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/18Wsa.png"" alt=""Bad results"" /></a></p>
<p>What I'm doing so wrong that I have such different results?</p>
<p><strong>== EDIT 1 ==</strong></p>
<p>Answering @DWKOT using the same image we have following results:</p>
<pre><code>#Query image with first code
im_idx = 75
distances, indices = neighbors.kneighbors([feature_list[im_idx]])
plt.imshow(mpimg.imread(filenames[im_idx]), interpolation='lanczos')
</code></pre>
<p><a href=""https://i.stack.imgur.com/V7EdQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V7EdQ.png"" alt=""Query image"" /></a></p>
<pre><code>#Similar image
plt.imshow(mpimg.imread(filenames[indices[0][1]]), interpolation='lanczos')
</code></pre>
<p><a href=""https://i.stack.imgur.com/UfUG8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UfUG8.png"" alt=""Similar image"" /></a></p>
<p>And the code that give us the distance to the 5 nearest neighbors:</p>
<pre><code>for i in range(5):
    print(distances[0][i])
</code></pre>
<p>With the following results:</p>
<pre><code>0.0
185.60701
185.75049
195.71657
196.4056
</code></pre>
<p>With the second code we have following result for query / similar image:</p>
<p><a href=""https://i.stack.imgur.com/V7EdQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V7EdQ.png"" alt=""Query image"" /></a> / <a href=""https://i.stack.imgur.com/lW4n1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lW4n1.png"" alt=""Similar image 2"" /></a></p>
<p>And following results for the first five &quot;similar&quot; images:</p>
<pre><code>0.0
0.81401
0.88622
0.92734
0.9346
</code></pre>
<p>What is also strange as I would expect similar images having values next to zero and different ones far from zero...</p>
",1
74293173,How to print a FlatMapDataset?,"<p>For debugging, I would like to <code>print</code> a tensorflow <code>FlatMapDataset</code>.</p>
<p>When trying to use the print method of a <code>tf.data.Dataset</code>, I get the error: <code>AttributeError: 'FlatMapDataset' object has no attribute 'print'</code>.</p>
<p>What I expected was some kind of print-out to assess whether the content of the dataset is what I expected.</p>
<p>Apparently a <code>FlatMapDataset</code> does not have the method:</p>
<pre class=""lang-python prettyprint-override""><code>['_GeneratorState', '__abstractmethods__', '__bool__', '__class__', '__class_getitem__', '__debug_string__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__nonzero__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__tf_tracing_type__', '__weakref__', '_abc_impl', '_add_trackable_child', '_add_variable_with_custom_getter', '_apply_debug_options', '_as_serialized_graph', '_checkpoint_dependencies', '_common_args', '_consumers', '_convert_variables_to_tensors', '_deferred_dependencies', '_deserialization_dependencies', '_deserialize_from_proto', '_export_to_saved_model_graph', '_flat_shapes', '_flat_structure', '_flat_types', '_functions', '_gather_saveables_for_checkpoint', '_graph', '_graph_attr', '_handle_deferred_dependencies', '_input_dataset', '_inputs', '_lookup_dependency', '_map_func', '_map_resources', '_maybe_initialize_trackable', '_maybe_track_assets', '_metadata', '_name', '_name_based_attribute_restore', '_name_based_restores', '_no_dependency', '_object_identifier', '_options', '_options_attr', '_options_tensor_to_options', '_preload_simple_restoration', '_restore_from_tensors', '_serialize_to_proto', '_serialize_to_tensors', '_setattr_tracking', '_shape_invariant_to_type_spec', '_structure', '_tf_api_names', '_tf_api_names_v1', '_trace_variant_creation', '_track_trackable', '_trackable_children', '_transformation_name', '_type_spec', '_unconditional_checkpoint_dependencies', '_unconditional_dependency_names', '_update_uid', '_variant_tensor', '_variant_tensor_attr', 'apply', 'as_numpy_iterator', 'batch', 'bucket_by_sequence_length', 'cache', 'cardinality', 'choose_from_datasets', 'concatenate', 'element_spec', 'enumerate', 'filter', 'flat_map', 'from_generator', 'from_tensor_slices', 'from_tensors', 'get_single_element', 'group_by_window', 'interleave', 'list_files', 'load', 'map', 'options', 'padded_batch', 'prefetch', 'random', 'range', 'reduce', 'rejection_resample', 'repeat', 'sample_from_datasets', 'save', 'scan', 'shard', 'shuffle', 'skip', 'snapshot', 'take', 'take_while', 'unbatch', 'unique', 'window', 'with_options', 'zip']
</code></pre>
<p>How can I print a <code>FlatMapDataSet</code> in some convenient way to review it's content?</p>
",0
74330500,Keras GradCam implementation that can process batches of images instead of a single image at a time,"<p>I'm following the GradCam example from the Keras documentation <a href=""https://keras.io/examples/vision/grad_cam/"" rel=""nofollow noreferrer"">https://keras.io/examples/vision/grad_cam/</a> and want to modify it so that it can process a batch of images instead of only a single image at a time.</p>
<p>I was already able to accomplish this but had to use a call to <code>tf.map_fn</code> which I would like to get rid of in the hopes of a performance improvement.</p>
<p>My progress so far (whole code at <a href=""https://colab.research.google.com/drive/1ptoRzS3WpThhHokX4p2nv-e2I-Oaq9bq?usp=sharing"" rel=""nofollow noreferrer"">Google Coolab</a>):</p>
<pre class=""lang-py prettyprint-override""><code>#https://keras.io/examples/vision/grad_cam/
def make_gradcam_heatmap(grad_model, images, pred_index=None):
    images = tf.cast(images, tf.float32)

    # Then, we compute the gradient of the top predicted class for our input image
    # with respect to the activations of the last conv layer
    with tf.GradientTape() as tape:
        tape.watch(images)
        last_conv_layer_output, preds = grad_model(images)
        if pred_index is None:
            pred_index = tf.argmax(preds[0])
        class_channel = preds[:, pred_index]

    # This is the gradient of the output neuron (top predicted or chosen)
    # with regard to the output feature map of the last conv layer
    grads = tape.gradient(class_channel, last_conv_layer_output)
    assert grads is not None, &quot;GradientTape returned gradients=None&quot;

    # This is a vector where each entry is the mean intensity of the gradient
    # over a specific feature map channel
    pooled_grads = tf.reduce_mean(grads, axis=(1, 2))

    # We multiply each channel in the feature map array
    # by &quot;how important this channel is&quot; with regard to the top predicted class
    # then sum all the channels to obtain the heatmap class activation
    def single_image(index):
        return last_conv_layer_output[index] @ pooled_grads[index][tf.newaxis, ..., tf.newaxis]

    heatmaps = tf.map_fn(single_image, tf.range(tf.shape(grads)[0]), dtype=tf.float32)

    # normalize the whole batch to [0, 1]
    heatmaps -= tf.math.reduce_min(heatmaps, axis=(0,1,2,3))
    heatmaps /= tf.math.reduce_max(heatmaps, axis=(0,1,2,3))

    return heatmaps
</code></pre>
<hr />
<p>Is there any way to rewrite this code in such a way that it doesn't use <code>tf.map_fn</code>?</p>
<pre class=""lang-py prettyprint-override""><code>    def single_image(index):
        return last_conv_layer_output[index] @ pooled_grads[index][tf.newaxis, ..., tf.newaxis]

heatmaps = tf.map_fn(single_image, tf.range(tf.shape(grads)[0]), dtype=tf.float32)
</code></pre>
",0
74382926,Tensorflow Keras resizing layer causing a shape error with raw image,"<p>I'am new to ML and I'am just trying to build a face recognition CNN. After some time I was able to build my model, using data-augmentation and pre-processing layers to improve it accuracy. I've read on this <a href=""https://www.tensorflow.org/guide/keras/preprocessing_layers?hl=en"" rel=""nofollow noreferrer"">guide</a> that I could add those layers direct inside my model, so that when I export the model, i would'nt need to do all of the pre-processing/normalization thing.</p>
<p>My problem is, after I trained and exported the model, I tried to predict a raw image of me, but I just got a shape error, because the the raw image size is different from the expcted input_shape of the model, that I dont even had especified.</p>
<p>This is the model that I built</p>
<pre><code>img_size = (196, 196)

model = tf.keras.Sequential([
  Resizing(img_size[0], img_size[1]),
  Rescaling(1.0 / 255),
  RandomZoom(0.2),
  RandomFlip('horizontal'),
  Conv2D(filters=32, kernel_size=(3,3), activation=&quot;relu&quot;),
  MaxPooling2D((2, 2)),
  Conv2D(filters=16, kernel_size=(3,3), activation=&quot;relu&quot;),
  MaxPooling2D((2, 2)),
  Flatten(),
  Dense(512, activation=&quot;relu&quot;),
  Dropout(0.4),
  Dense(128, activation=&quot;relu&quot;),
  Dropout(0.4),
  Dense(5, activation=&quot;softmax&quot;)
])
</code></pre>
<p>I've trained it with a dataset that I loaded using tf.image_dataset_from_directory</p>
<pre><code>def load_dataset(subset):
  dataset = tf.keras.utils.image_dataset_from_directory(
    database_dir,
    validation_split=0.2,
    subset=subset,
    seed=321,
    shuffle=True,
    batch_size=64,
    labels=&quot;inferred&quot;,
    label_mode='categorical')
  dataset.prefetch(buffer_size=tf.data.AUTOTUNE)
  return dataset

train_dataset = load_dataset('training')
test_dataset = load_dataset('validation')


model.compile(
  optimizer='adam',
  loss='categorical_crossentropy',
  metrics=['accuracy']
)

model.fit(
  train_dataset,
  validation_data=test_dataset,
  epochs=10)
</code></pre>
<p>Then I exported the model, using <code>model.save(&quot;model.h5&quot;)</code></p>
<p>And finally, I tried to predict a new raw image of me.</p>
<pre><code>new_model = tf.keras.models.load_model(&quot;./model.h5&quot;)

image_file = tf.io.read_file(&quot;new_image.jpg&quot;)
image = tf.io.decode_jpeg(image_file)
image = tf.keras.utils.img_to_array(image)
image = np.expand_dims(image, axis=0)
prediction = model.predict(image)
</code></pre>
<p>And got the following error, that dont even make sense to me, because I specifed in the Resizing layer a shape of (196, 196), but the error says that it was expecting a shape of (256, 256)</p>
<pre><code>ValueError: Input 0 of layer &quot;sequential_14&quot; is incompatible with the layer: expected shape=(None, 256, 256, 3), found shape=(None, 223, 223, 3)
</code></pre>
<p>Well, this is where I'm stuck, any help will be welcome. Btw, sorry for any english misstakes, I'm using Google Translate. Also, this is my first post on StackOverflow, so if I missed something just tell me.</p>
",0
74434308,Setting only global level seed gives same output in consecutive iterations of loop in Tensorflow,"<p>I am testing out the <code>tf.random.set_seed</code> according to the rules given at - <a href=""https://www.tensorflow.org/api_docs/python/tf/random/set_seed"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/random/set_seed</a></p>
<p>In particular I am testing the second rule - where we set only global level seed and no operation level seed.</p>
<p>According to the documentation (the link is mentioned above), the second rule is:</p>
<blockquote>
<p>If the global seed is set, but the operation seed is not: The system deterministically picks an operation seed in conjunction with the global seed so that it gets a unique random sequence.</p>
</blockquote>
<p>To explain the second rule, the documentation uses the following snippet:</p>
<pre><code>tf.random.set_seed(1234)
print(tf.random.uniform([1]))  # generates 'A1'
print(tf.random.uniform([1]))  # generates 'A2'
</code></pre>
<p>and states that</p>
<blockquote>
<p>The reason we get 'A2' instead 'A1' on the second call of tf.random.uniform above is because the second call uses a different operation seed.</p>
</blockquote>
<p>Now, I tested this rule on a 1D tensor of shape (3,) to check if the output of shuffling the tensor does not give the same sequence within consecutive iterations of the loop as follows:</p>
<pre><code>import tensorflow as tf


&quot;&quot;&quot;
Only global level seed
&quot;&quot;&quot;

tf.random.set_seed(1234)
   
constant_tensor = tf.constant([1,2,3])

for i in range(1, 15):
    shuffled_tensor = tf.random.shuffle(constant_tensor)
    print(shuffled_tensor)
</code></pre>
<p>I got the following output:</p>
<pre><code>tf.Tensor([3 1 2], shape=(3,), dtype=int32)
tf.Tensor([2 3 1], shape=(3,), dtype=int32)
tf.Tensor([2 1 3], shape=(3,), dtype=int32)
tf.Tensor([2 3 1], shape=(3,), dtype=int32)
tf.Tensor([1 3 2], shape=(3,), dtype=int32)
tf.Tensor([3 2 1], shape=(3,), dtype=int32)
tf.Tensor([2 1 3], shape=(3,), dtype=int32)
tf.Tensor([2 1 3], shape=(3,), dtype=int32)
tf.Tensor([2 3 1], shape=(3,), dtype=int32)
tf.Tensor([2 1 3], shape=(3,), dtype=int32)
tf.Tensor([3 2 1], shape=(3,), dtype=int32)
tf.Tensor([1 2 3], shape=(3,), dtype=int32)
tf.Tensor([3 2 1], shape=(3,), dtype=int32)
tf.Tensor([3 2 1], shape=(3,), dtype=int32)
</code></pre>
<p>From the output you can see that the sequence on line number 7 and 8 match.
Also the sequence on line number 13 and 14 match.</p>
<p>According to the documentation, tensorflow should not output the same sequence in a consecutive iteration.</p>
<p>Then why am I getting this kind of output? Have I misunderstood the concept?</p>
<p>To test this further, I also tested to following snippet which I used to generate 14 1-D tensors and check if any tensor is repeated within consecutive runs as follows:</p>
<pre><code>import tensorflow as tf
tf.random.set_seed(1234)
for i in range(1, 15):
    print(tf.random.uniform(shape=[1], minval=1, maxval=15, dtype=tf.int32))
</code></pre>
<p>And I got the following output:</p>
<pre><code>tf.Tensor([12], shape=(1,), dtype=int32)
tf.Tensor([8], shape=(1,), dtype=int32)
tf.Tensor([1], shape=(1,), dtype=int32)
tf.Tensor([2], shape=(1,), dtype=int32)
tf.Tensor([4], shape=(1,), dtype=int32)
tf.Tensor([3], shape=(1,), dtype=int32)
tf.Tensor([2], shape=(1,), dtype=int32)
tf.Tensor([7], shape=(1,), dtype=int32)
tf.Tensor([13], shape=(1,), dtype=int32)
tf.Tensor([11], shape=(1,), dtype=int32)
tf.Tensor([8], shape=(1,), dtype=int32)
tf.Tensor([3], shape=(1,), dtype=int32)
tf.Tensor([1], shape=(1,), dtype=int32)
tf.Tensor([4], shape=(1,), dtype=int32)
</code></pre>
<p>You can see that no two consecutive tensors are repeated. Why didn't I see this behaviour for my first snippet?</p>
",1
74560416,Tensorflow/Keras: Modify weights inside train_step function before gradient computation,"<p>In the <em>train_step</em> function of a Keras Functional Model, we have:</p>
<pre><code>def train_step(self, data):
    # Unpack the data. Its structure depends on your model and
    # on what you pass to `fit()`.
    x, y = data

    with tf.GradientTape() as tape:
        y_pred = self(x, training=True)  # Forward pass
        # Compute the loss value
        # (the loss function is configured in `compile()`)
        loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)

    # Compute gradients
    trainable_vars = self.trainable_variables
    gradients = tape.gradient(loss, trainable_vars)
    # Update weights
    self.optimizer.apply_gradients(zip(gradients, trainable_vars))
    # Update metrics (includes the metric that tracks the loss)
    self.compiled_metrics.update_state(y, y_pred)
    # Return a dict mapping metric names to current value
    return {m.name: m.result() for m in self.metrics}
</code></pre>
<p>I want to be able to change the self.weights inside the function. When it calls <code>y_pred = self(x, training=True)</code> it will use the original weights, but I want to change these weights before computing the gradient and applying <code>self.optimizer.apply_gradients(zip(gradients, trainable_vars))</code>. The idea is to apply the gradient based on the loss with previous weights to the new weights.</p>
<p>1- I first tried with <code>self.set_weights(x)</code> after computing the loss and before computing the gradients but this error arises: <code>RuntimeError: Cannot get value inside Tensorflow graph function.</code></p>
<p>2- I saw that this could be resolved with <code>run_eagerly=True</code> but then <code>WARNING:tensorflow:5 out of the last 5 calls to &lt;function pfor.&lt;locals&gt;.f at 0x29d89bf70&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing</code></p>
<p>3- Moreover, the results with <code>run_eagerly=True</code> are quite different, so I am not sure if this solution is working correctly.</p>
<p>Is it possible to change the weights before the gradient computation as I want with Keras?</p>
<p>Extra clarification: let's imagine we have two arrays of weights A and B, and these arrays are correlated. I want to get the LOSS with A weights but then optimize B weights with LOSS. Optimizing B from the LOSS of A makes sense because these weight arrays are correlated.</p>
",0
74702527,Tensorflow - constructing a tensor from particular values extracted from two different tensors,"<p>I'm trying to construct a single tensor using values from two different tensors and an array of two dimensional indices, in a manner compatible with TensorFlow autodiff.</p>
<p>In a first step I want to extract the elements of a tensor <code>D</code> of shape <code>(n,n)</code> whose values are the same as those in another tensor <code>a</code>. In particular, I'm looking for a better way to implement the following loop:</p>
<pre><code>a = []
for i in range(len(f)):
    a.append(tf.where(tf.experimental.numpy.isclose(f[I], D, atol=1e-6))[0])
P_x = tf.gather(D,a)
</code></pre>
<p>In the append step, I'm just using the first instance where the values are equal because the function I'm interested in is independent of this choice. I need to use isclose because the two arrays are float32 arrays and are not exactly equal to one another.</p>
<p>Then in a second step I want to combine <code>P_x</code> with <code>P_y = tf.gather(g, indices)</code> to construct a tensor <code>P</code>. Assume that <code>P_x</code> and <code>P_y</code> are both of shape <code>(n, )</code>. Then,</p>
<pre><code>P = [[P_x[0], P_y[0]],[P_x[1], P_y[1]], ..., [P_x[n], P_y[n]] ]
</code></pre>
<p>I'm pretty new to TensorFlow, so despite looking through the docs I don't see a way to do all of these operations using gather, scatter etc., which seems to be necessary to make autodiff work. When I use loops and other methods, I get gradients = none.</p>
",0
74734685,"how to fix this Value Error ' ValueError: decay is deprecated in the new Keras optimizer,'?","<p>I'm new at deep learning and i follow tutorial about face detection.</p>
<pre><code>model = canaro.models.createSimpsonsModel(IMG_SIZE=IMG_SIZE, channels=channels, output_dim=len(characters), 
                                         loss='binary_crossentropy', decay=1e-7, learning_rate=0.001, momentum=0.9,
                                         nesterov=True)


</code></pre>
<p>ValueError Traceback (most recent call last)
WARNING:absl:<code>lr</code> is deprecated, please use <code>learning_rate</code> instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.
Output exceeds the size limit. Open the full output data in a text editor
ValueError: decay is deprecated in the new Keras optimizer, pleasecheck the docstring for valid arguments, or use the legacy optimizer, e.g., tf.keras.optimizers.legacy.SGD.</p>
<p>I already tried follow some steps but i dont know how to fix it.</p>
",0
74872064,Does steps_per_epoch param of model.fit() function override the value of Sequence's __len__(),"<p>According to the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence"" rel=""nofollow noreferrer"">tf.keras.utils.Sequence</a> official documentation, the general convention to set the return value of <code>__len__()</code> function is <code>training size / batch size</code>.</p>
<p>This ensures that for a single epoch, your model is trained on the all the training data for once.</p>
<p><strong>My question -</strong> When we pass the value of <code>steps_per_epoch</code> in the <code>model.fit()</code> function, does it override the value returned by the <code>__len__()</code> function?</p>
<p>Suppose the value of the param <code>steps_per_epoch</code> is <code>x</code>. And I explicitly pass this param in the <code>model.fit()</code> function. Then regardless of what value is returned by the <code>__len__()</code> function, for a single epoch, will the model be trained on <code>x</code> * <code>batch_size</code> number of samples?</p>
",1
75397327,How to reset memory usage of GPU when training model on server,"<p>I am training an LSTM model, but the time taking by one epoch is too high. And if I check the memory usage by using nvidia-smi, I get the following where all the available memory is assigned.</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla P100-PCIE...  Off  | 00000000:02:00.0 Off |                    0 |
| N/A   38C    P0    58W / 250W |  15967MiB / 16280MiB |     71%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla P100-PCIE...  Off  | 00000000:82:00.0 Off |                    0 |
| N/A   30C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A     10871      C   ...tify/aidentify/bin/python    15965MiB |
+-----------------------------------------------------------------------------+ 
</code></pre>
<p>I tried to kill the process with PID ID, but then kernel restarts and if I start training the model again, it uses all the memory and the training is slow again.
Warning I get when I use &quot;tf.keras.utils.timeseries_dataset_from_array&quot;</p>
<pre><code>I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15389 MB memory:  -&gt; device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:02:00.0, compute capability: 6.0
</code></pre>
<p>Is it possible to reset the memory usage or is it possible to run on other GPU1 which is available. Kindly help to resolve this issue.</p>
",0
75401761,Change Verbosity of Keras train_on_batch()?,"<p>I am training a GAN using Keras's <code>train_on_batch()</code> command. This is very similar to Keras's <code>fit()</code>. However, in the documentation for <code>fit()</code>, there is a parameter for <code>verbose</code>, which changes how often a progress bar is printed to the console.</p>
<p>My model has many batches, and so it is printing tons of progress bars to the command line. Unfortunately, <code>train_on_batch()</code> does not have a <code>verbose</code> parameter. Is there a workaround for this? Is there a Keras global variable/environment variable that I can set? I don't want to disable my program from printing to the console, I just want to change the verbosity of specifically <code>train_on_batch()</code>.</p>
<p>For clarify, I am using Keras directly from the Keras package, I am not using tf.keras.</p>
",0
75572543,What to look out for when passing a generator into model.fit in tensorflow?,"<p>I want to replace the x and y training data parameters in tf.keras.Model.fit with a generator. However, some subtlety seems to escape me, as the model accuracy doesn't improve with the generator when training.</p>
<p>As far as I understand the documentation, the generator is supposed to yield tuples <code>(x_vals,y_vals)</code>, such that <code>x_vals</code> is a concatenation of <code>batch_size</code>-many training samples along a new 0th dimension, and 'v_vals' is the concatenation of their corresponding labels.</p>
<p>As long as the generator fulfills this, as I understand it, we can just replace the x parameter in tf.keras.Model.fit with the generator and omit the y parameter, though to define an epoch, we also need to specify 'steps_per_epoch' in fit.</p>
<p>There however seems to be something here I misunderstood or forgot, because starting with a model and input data that trains (i.e. its accuracy improves) and replacing the training data array with a generator as discussed, results in a model that doesn't train (i.e. its accuracy instead goes up a little, then however goes back down till its equal to chance).</p>
<p>The corresponding code:</p>
<pre><code>import numpy as np
import tensorflow as tf

BATCH_SIZE = 32

#Loading training data:
def load_cifar():
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
    assert x_train.shape == (50000, 32, 32, 3)
    assert x_test.shape == (10000, 32, 32, 3)
    assert y_train.shape == (50000, 1)
    assert y_test.shape == (10000, 1)

    #Normalize the data &amp; cast to fp32:
    x_train = np.true_divide(x_train,255,dtype=np.single)
    x_test =  np.true_divide(x_test,255,dtype=np.single)
    y_train = y_train.astype(np.single)
    y_test =  y_test.astype(np.single)

    return (x_train,y_train), (x_test,y_test)


(train_x, train_y) , (validation_x, validation_y) = load_cifar()
   


# Defining the generator:
def data_generator_dummy(input_data_x:np.ndarray,
                          input_data_y:np.ndarray,
                          batch_size=BATCH_SIZE,
                          ):
    &quot;&quot;&quot;
    Given the input_data's, generate infinitely by:
     1. Drawing batch_size-many vectors from input_data_x and input_data_y
     2. Turn the drawn vectors into a mini-batch (with shape  [None]+input_data.shape)

    :param batch_size:
    :param input_data_x, input_data_y: The data on which noise shall be added
    :return: A generator for the input data.
    &quot;&quot;&quot;
    index =0
    while True:
        # We start with a zero-vector of expected size and fill the drawn samples into it:
        samples_x = np.zeros( [batch_size] + list(input_data_x.shape[1:]),dtype=np.single)
        samples_y = np.zeros( [batch_size] + list(input_data_y.shape[1:]),dtype=np.single)
        for i in range(batch_size):
            samples_x[i] = input_data_x[index%50_000]
            samples_y[i] = input_data_y[index%50_000]
            index +=1

        yield samples_x,samples_y

# Basically a linear classifier:
def make_model():
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Flatten())
    model.add(tf.keras.layers.Dense(10,tf.nn.softmax))
    model.build([None] +list(train_x[0,:,:,:].shape))
    return model


#Training:
generator = data_generator_dummy(train_x,train_y,batch_size=BATCH_SIZE)
model = make_model()
model.summary()
optimizer_adam=tf.keras.optimizers.Adam(learning_rate=0.0005/32,beta_1=0.9,beta_2=0.999,epsilon=1e-07)
model.compile(optimizer_adam,loss=&quot;sparse_categorical_crossentropy&quot;,metrics=&quot;accuracy&quot;)
model.fit(generator, validation_data=(validation_x,validation_y),epochs=10,
          steps_per_epoch=train_x.shape[0]//BATCH_SIZE,
          )

# This one however works:
# model.fit(train_x, train_y, validation_data=(validation_x,validation_y),epochs=30,
#           steps_per_epoch=train_x.shape[0]//BATCH_SIZE,
#           shuffle=True
#           )
</code></pre>
<hr />
<p>The model also trains if one first let's the generator generate a long list of samples and then passes those into <code>fit</code> as <code>x</code>and <code>y</code>:</p>
<pre><code>#Training:
generator = data_generator_dummy(train_x,train_y,batch_size=50000)
model = make_model()
model.summary()
optimizer_adam=tf.keras.optimizers.Adam(learning_rate=0.0005/32,beta_1=0.9,beta_2=0.999,epsilon=1e-07)
model.compile(optimizer_adam,loss=&quot;sparse_categorical_crossentropy&quot;,metrics=&quot;accuracy&quot;)
while True:
    samples_x,samples_y = next(generator)
    model.fit(samples_x,samples_y, validation_data=(validation_x,validation_y),epochs=10,batch_size=BATCH_SIZE
          )
</code></pre>
",1
75609953,"TensorFlow sample_from_datasets, weights have to be probabilities?","<p>It is specified in the <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#sample_from_datasets"" rel=""nofollow noreferrer"">documentation</a> of <code>tf.data.Dataset.sample_from_datasets</code> that <code>weights</code> are the probabilities with which each respective dataset is sampled. What is the behavior in the case that <code>weights</code> do not represent a valid probability distribution, e.g. do not sum to 1? Are the weights turned into normalized probabilities somehow normalized under-the-hood by some undocumented behavior? Does it throw an error? Something else?</p>
",1
75639137,TF1 to TF2 migration,"<p>Hello I am new to tensorflow and I am working on a code that I would like to migrate from tensorflow 1 to 2. I have this line of code:</p>
<pre><code>x1 = tf.compat.v1.placeholder(tf.float32, [], name=&quot;x1&quot;)
</code></pre>
<p>As mentioned in <a href=""https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder</a>, I should use <code>keras.Input</code>. But even when specifying the shape, I can't have the same tensor as with compat.v1:</p>
<pre><code>x2 = tf.keras.Input(shape=[], dtype=tf.float32, name=&quot;x2&quot;)
</code></pre>
<p>To check the shape I use <code>tf.shape(x1)</code> or <code>tf.shape(x2)</code>, but the shapes are not the same. Could anyone explain to me how to have, in TF2, the same shape as in TF1 ?
Thanks and regards</p>
",1
75695322,"mypy: ""Untyped decorator makes function untyped"" error","<p>When using <code>@tf.function</code> as a decorator of a <code>call</code> method of a <code>tf.keras.Model</code>:</p>
<pre><code>@tf.function(input_signature=[INPUT_SIGNATURE_DICT])
def call(self, inputs: Dict[str, tf.Tensor], training: Any = None, mask: tf.Tensor = None) -&gt; tf.Tensor:
</code></pre>
<p><code>mypy</code> fails with the error message: <code>&quot;Untyped decorator makes function &quot;call&quot; untyped&quot;</code></p>
<p>I tried to decorate this decorator with a decorator that preserves the signature (as done <a href=""https://mypy.readthedocs.io/en/stable/generics.html#declaring-decorators"" rel=""nofollow noreferrer"">here</a>), but the error message remains.</p>
<p>How should I define my decorator to make sure this passes? I appreciate any help provided.</p>
",0
75776990,Error Training Tensorflow2 Custom Model for Object Detection,"<p>I receive the following error when trying to run a training script. I am using this google colab tutorial with my own dataset. Link to the tutorial is here &quot;https://colab.research.google.com/drive/1sLqFKVV94wm-lglFq_0kGo2ciM0kecWD?ref=roboflow-blog#scrollTo=HEsOLOMHzBqF&amp;uniqifier=1&quot;. I'm new to using Tensorflow and any help would be appreicated.</p>
<p>I get the following error:</p>
<pre><code>`2023-03-18 15:43:36.393544: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inEfficientDet-D0/model/stack_1/block_1/drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer
</code></pre>
<p>`</p>
<p>This is the lower half of my config file:</p>
<pre><code>train_config: {
  fine_tune_checkpoint: &quot;/content/models/research/deploy/efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0&quot;
  fine_tune_checkpoint_version: V2
  fine_tune_checkpoint_type: &quot;detection&quot;
  batch_size: 16
  sync_replicas: true
  startup_delay_steps: 0
  replicas_to_aggregate: 8
  use_bfloat16: true
  num_steps: 40000
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    random_scale_crop_and_pad_to_square {
      output_size: 512
      scale_min: 0.1
      scale_max: 2.0
    }
  }
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        cosine_decay_learning_rate {
          learning_rate_base: 8e-2
          total_steps: 300000
          warmup_learning_rate: .001
          warmup_steps: 2500
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  max_number_of_boxes: 100
  unpad_groundtruth_tensors: false
}

train_input_reader: {
  label_map_path: &quot;/content/drive/MyDrive/Image3tfrecord/train/weed_label_map.pbtxt&quot;
  tf_record_input_reader {
    input_path: &quot;/content/drive/MyDrive/Image3tfrecord/train/weed.tfrecord&quot;
  }
}

eval_config: {
  metrics_set: &quot;coco_detection_metrics&quot;
  use_moving_averages: false
  batch_size: 16;
}

eval_input_reader: {
  label_map_path: &quot;/content/drive/MyDrive/Image3tfrecord/train/weed_label_map.pbtxt&quot;
  shuffle: false
  num_epochs: 1
  tf_record_input_reader {
    input_path: &quot;/content/drive/MyDrive/Image3tfrecord/test/weed.tfrecord&quot;
  }
}

</code></pre>
<p>I tried changing eval steps and adding different load times but I haven't seen success.</p>
",0
75833344,How to build a dataset of fixed-length windows from distinct time-series to train a TensorFlow model,"<p>I have a training dataset made of several distinct csv annotated time-series, of course of different length.</p>
<p>The objective is to classify each row of new streaming data, with a model trained on the mentioned dataset.</p>
<p>I thought of using a <strong>CNN</strong> with as input a fixed-length window, but I'm struggling in building the training dataset correctly: <em>from each distinct time-series I need to extract the complete windows and then concat those coming from distinct time-series together</em>.</p>
<p>I tried building them explicitly with <strong>pandas</strong> but since they are very overlapped, the final df does not fit in <strong>memory</strong>.</p>
<p>So, I looked for <strong>generators</strong> and I found the function  <code>tf.keras.preprocessing.sequence.TimeseriesGenerator</code> that could work for windowing each single time-series but have two downsides:</p>
<ol>
<li>how to concat two different <code>TimeSeriesGenerator</code> objects?</li>
<li>in the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/TimeseriesGenerator"" rel=""nofollow noreferrer"">docs</a> it is deprecated in favor of <code>tf.data.Dataset</code> but from its <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""nofollow noreferrer"">docs</a> its pretty hard to tell how to obtain windows from each time-series and concat them into a single generator to feed a TensorFlow model</li>
</ol>
<p>Summarizing, I can't find a proper way of extracting fixed-length windows from many time-series and concat them into a single generator (building the dataset of the windows, which of course have overlaps don't fit in memory), can you help me?</p>
<p>I'm quite confused also because it sounds a quite common situation so it should have some dedicated documentation and examples, which I could not find.</p>
",1
75835179,Deploying Tensorflow 2.x model to Azure Function App,"<p>I am trying to deploy my Tensorflow model to an azure function app for predictions.</p>
<p>Here is my requirements.txt</p>
<pre><code># DO NOT include azure-functions-worker in this file
# The Python Worker is managed by Azure Functions platform
# Manually managing azure-functions-worker may cause unexpected issues

azure-functions
einops
tqdm
numpy
tensorflow
</code></pre>
<p>I also tried removing tensorflow from the list and uploading. Deployment went fine, but the code is obviously broken.</p>
<p>Function app has the python version 3.9 as mentioned in the settings on the portal.</p>
<pre><code> &quot;siteProperties&quot;: {
            &quot;metadata&quot;: null,
            &quot;properties&quot;: [
                {
                    &quot;name&quot;: &quot;LinuxFxVersion&quot;,
                    &quot;value&quot;: &quot;Python|3.9&quot;
                },
                {
                    &quot;name&quot;: &quot;WindowsFxVersion&quot;,
                    &quot;value&quot;: null
                }
            ],
</code></pre>
<p>Initially, the function was created locally and deployed with python version 3.9.7, however, the problem seems to be connected with installing Tensorflow.</p>
<pre><code>$ func azure functionapp publish server-func --build remote
Getting site publishing info...
Removing WEBSITE_CONTENTAZUREFILECONNECTIONSTRING app setting (DefaultEndpointsProtocol=https;AccountName=serverfunc22219e2;AccountKey=I/svo3fVFYcr9noxkNkPRq1pi191bhtGoVLhoYvJWTtmP0kQn9EsHDJhTh59ZhSK5fC3VgoOAmTN+ASt04e95w==;EndpointSuffix=core.windows.net)
Removing WEBSITE_CONTENTSHARE app setting (server-func0205de)
Creating archive for current directory...
Performing remote build for functions project.
Deleting the old .python_packages directory
Uploading 123.46 MB [#############################################################################]
Remote build in progress, please wait...
Updating submodules.
Preparing deployment for commit id '6762ff62-a'.
PreDeployment: context.CleanOutputPath False
PreDeployment: context.OutputPath /home/site/wwwroot
Repository path is /tmp/zipdeploy/extracted
Running oryx build...
Command: oryx build /tmp/zipdeploy/extracted -o /home/site/wwwroot --platform python --platform-version 3.9.7 -p packagedir=.python_packages/lib/site-packages
Operation performed by Microsoft Oryx, https://github.com/Microsoft/Oryx
You can report issues at https://github.com/Microsoft/Oryx/issues

Oryx Version: 0.2.20210120.1, Commit: 66c7820d7df527aaffabd2563a49ad57930999c9, ReleaseTagName: 20210120.1

Build Operation ID: |8/uRTiZVQ/A=.719d3f9e_
Repository Commit : 6762ff62-a88f-4b1f-9851-51e94dc6e634

Detecting platforms...
Detected following platforms:
  python: 3.9.7
Version '3.9.7' of platform 'python' is not installed. Generating script to install it...


Source directory     : /tmp/zipdeploy/extracted
Destination directory: /home/site/wwwroot


Downloading and extracting 'python' version '3.9.7' to '/tmp/oryx/platforms/python/3.9.7'...
Downloaded in 2 sec(s).
Verifying checksum...
Extracting contents...
Done in 5 sec(s).

Python Version: /tmp/oryx/platforms/python/3.9.7/bin/python3.9

Running pip install...
[14:40:16+0000] Collecting azure-functions
[14:40:16+0000]   Downloading azure_functions-1.13.3-py3-none-any.whl (163 kB)
[14:40:17+0000] Collecting einops
[14:40:17+0000]   Downloading einops-0.6.0-py3-none-any.whl (41 kB)
[14:40:17+0000] Collecting tqdm
[14:40:17+0000]   Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)
[14:40:18+0000] Collecting numpy
[14:40:18+0000]   Downloading numpy-1.24.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)
[14:40:18+0000] Collecting tensorflow
[14:40:18+0000]   Downloading tensorflow-2.12.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)
Done in 11 sec(s).
\n/opt/Kudu/Scripts/starter.sh oryx build /tmp/zipdeploy/extracted -o /home/site/wwwroot --platform python --platform-version 3.9.7 -p packagedir=.python_packages/lib/site-packages

Generating summary of Oryx build
Deployment Log file does not exist in /tmp/oryx-build.log
The logfile at /tmp/oryx-build.log is empty. Unable to fetch the summary of build
Deployment Failed. deployer = Push-Deployer deploymentPath = Functions App ZipDeploy. Extract zip. Remote build.
Remote build failed!
</code></pre>
<p>Also, the function works perfectly when run locally.</p>
<p>I tried both VS Code button to deploy and command mentioned above expecting to see the deployment successful, but got a build error without any logs btw.</p>
",0
75952109,Prefetch optimization of tf.data doesn't work,"<p>I am working with the <em>tf.data API</em> and am analyzing the various speed-ups obtained with optimizations written <a href=""https://www.tensorflow.org/guide/data_performance"" rel=""nofollow noreferrer"">here</a>.
I'm working with the tf.data API and I'm analyzing the various speed-ups obtained with the optimizations written in, but in all cases what I've noticed is that you don't optimize performance using the prefetch option.
It almost seems that no optimization is implemented and therefore there is no overlapping between CPU and GPU.
Currently I'm using TF 2.11.0 but I've used also TF 2.10.0 and TF 2.8.3 and the fact remains the same.
I tried also with different batch size.
I have used also different PC with different GPUs but the fact remains the same.
I worked with Cifar10 in which each image is RGB 32x32 and this training-set is made by 40.000 images.
A dummy code that I've used is this:</p>
<pre><code>import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.layers as layers
import time

def get_model_data_augmentation_CPU():
    &quot;&quot;&quot;Return the Keras model for data-augmentation on CPU&quot;&quot;&quot;
    # Define Keras Model
    model = tf.keras.Sequential([
      layers.Conv2D(64, 3, activation='relu'),
      layers.MaxPooling2D(),
      layers.Dropout(0.1),
      layers.Conv2D(128, 3, activation='relu'),
      layers.MaxPooling2D(),
      layers.Dropout(0.1),
      layers.Conv2D(128, 3, activation='relu'),
      layers.MaxPooling2D(),
      layers.Dropout(0.2),
      layers.Flatten(),
      layers.Dense(256, activation='relu'),
      layers.Dropout(0.3),
      layers.Dense(10)
    ])
    adam_opt = keras.optimizers.Adam(learning_rate=0.001)
    model.compile(optimizer = adam_opt,
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy'])
    return model

data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip(mode='horizontal'),
    tf.keras.layers.RandomRotation(0.1),
    tf.keras.layers.RandomZoom(0.2),
])

model = get_model_data_augmentation_CPU()

BATCH_SIZE = 32 
(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()
dataset_train = tf.data.Dataset.from_tensor_slices((X_train, y_train)) 
dataset_train = dataset_train.map(lambda x,y : (data_augmentation(x),y), num_parallel_calls=3) 
dataset_train = dataset_train.batch(BATCH_SIZE)
dataset_train = dataset_train.prefetch(1) # If I comment this line the perfomance remain the same


start_time = time.time()
history = model.fit(
    dataset_train,
    epochs=EPOCHS,
)
end_time = time.time()
</code></pre>
",0
75996642,Is there a good equivalent of pandas' `apply` for TensorFlow datasets?,"<p><strong>BACKGROUND</strong></p>
<p>The use of <a href=""https://www.tensorflow.org/guide/data"" rel=""nofollow noreferrer""><code>tf.data.Dataset</code></a> is promoted by TensorFlow as the best practice for implementing input pipelines due to their efficient implementation of common operations such as batching, shuffling, as well as their seamless integration with the Keras API.</p>
<p>I may just be lousy at looking up the documentation on the matter, but it seems to me that the major drawback of TensorFlow datasets is that they are quite unwieldy, if not impossible to work with, when trying to implement feature engineering tasks whereby a new column is created via the application of some generic Python function. This is in contrast to pandas' very nifty <a href=""https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html"" rel=""nofollow noreferrer""><code>apply()</code></a> function which can produce new columns from preexisting ones both efficiently (i.e., via vectorization) and in a pythonic manner.</p>
<p>To the best of my understanding, the closest thing to pandas' <code>apply()</code> is TensorFlow dataset's <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map"" rel=""nofollow noreferrer""><code>map()</code></a>. However, one can't simply use it with arbitrary Python functions since they'd first need to be converted to tensors. This becomes very difficult as one has to have arcane knowledge of the miscellaneous tensor analogues of arbitrary Python functions (e.g., <code>tf.strings.length()</code> instead of Python's <code>len()</code>). Even when one finds such functions, the idiosyncracies of tensor operations makes them very un-pythonic and prone to obscure dimensionality or type errors.</p>
<p>I've read about TensorFlow's <a href=""https://www.tensorflow.org/api_docs/python/tf/py_function"" rel=""nofollow noreferrer""><code>py_function</code></a> as some sort of wrapper that magically converts Pythonic code into a tensor representation, but judging from the documentation, it became clear to me that this is far from the case.</p>
<p><strong>QUESTION</strong></p>
<p>Is TensorFlow's <code>tf.data</code> just not mature yet to be able to handle feature engineering in the same way that pandas <code>apply()</code> does? If not, what am I missing in my understanding?</p>
<p><strong>MINIMUM WORKING EXAMPLE</strong></p>
<p>In the code below, I compare a pandas DataFrame <code>df</code> with the equivalent TensorFlow dataset <code>ds</code>. My goal is to engineer two extra features, namely</p>
<ol>
<li>adding a suffix to the string feature <code>my_string</code>, and</li>
<li>counting the number of time a certain letter appears in any given instance of <code>my_string</code>.</li>
</ol>
<p>As you can see for yourself, the first operation works intuitively for both pandas and TensorFlow, but the second only works easily for pandas. Getting it to work with TensorFlow is either extremely complex, or just plain impossible.</p>
<pre><code>from collections import Counter
import numpy as np
import pandas as pd
import tensorflow as tf

# Create the pandas DataFrame.
df = pd.DataFrame(
    {'index': list(range(5)),
     'my_string': ['Alondra', 'Spaanbiuk', 'Ibinth', 'Liefelle', 'Yoanda'], 
     'some_other_column': np.random.rand(5),
     }).set_index('index')
print('Original pandas DataFrame:')
print(df, '\n')

# Create the TensorFlow dataset and define a function to view it 
# as a pandas DataFrame.
ds = tf.data.Dataset.from_tensors(df.to_dict(orient='list'))
def view_ds(ds):
    data = pd.concat([pd.DataFrame(k) for k in ds.take(5)], axis=0)
    # Convert byte strings to Python strings.
    object_cols = data.select_dtypes([object])
    data[object_cols.columns] = object_cols.stack().str.decode('utf-8').unstack()
    print('TensorFlow dataset:')
    print(data, '\n')
view_ds(ds)

#### Add a suffix to `my_string` as `my_string_with_suffix`.
def add_a_suffix(x):
    x['my_string_with_suffix'] = x['my_string']+'.suffix'
    return x

# Apply to the pandas DataFrame.
print('DataFrame with the suffix:')
df = df.apply(add_a_suffix, axis=1)
print(df, '\n')

# Apply to the TensorFlow dataset.
print('TensorFlow dataset with the suffix:')
ds = ds.map(lambda x: add_a_suffix(x))
view_ds(ds)

#### Count they the number of `a`'s in `my_string`:
def count_letters(x, letter='a'):
    counter = Counter(x['my_string'].lower())
    x[f'{letter}_counts'] = counter[letter]
    return x

# Apply to the pandas DataFrame.
print('DataFrame with the letter count:')
df = df.apply(count_letters, axis=1)
print(df, '\n')
    
# Apply to the TensorFlow dataset.
# HOW BUT HOW?!!
# print('TensorFlow dataset with the letter count:')
# ds = ds.apply(lambda x: count_letters(x))
# view_ds(ds)
</code></pre>
<p>The output is of the above is as follows.</p>
<pre><code>Original pandas DataFrame:
       my_string  some_other_column
index                              
0        Alondra           0.209685
1      Spaanbiuk           0.972315
2         Ibinth           0.933700
3       Liefelle           0.186369
4         Yoanda           0.667436 

TensorFlow dataset:
   my_string  some_other_column
0    Alondra           0.209685
1  Spaanbiuk           0.972315
2     Ibinth           0.933700
3   Liefelle           0.186369
4     Yoanda           0.667436 

DataFrame with the suffix:
       my_string  some_other_column my_string_with_suffix
index                                                    
0        Alondra           0.209685        Alondra.suffix
1      Spaanbiuk           0.972315      Spaanbiuk.suffix
2         Ibinth           0.933700         Ibinth.suffix
3       Liefelle           0.186369       Liefelle.suffix
4         Yoanda           0.667436         Yoanda.suffix 

TensorFlow dataset with the suffix:
TensorFlow dataset:
   my_string  some_other_column my_string_with_suffix
0    Alondra           0.209685        Alondra.suffix
1  Spaanbiuk           0.972315      Spaanbiuk.suffix
2     Ibinth           0.933700         Ibinth.suffix
3   Liefelle           0.186369       Liefelle.suffix
4     Yoanda           0.667436         Yoanda.suffix 

DataFrame with the letter count:
       my_string  some_other_column my_string_with_suffix  a_counts
index                                                              
0        Alondra           0.209685        Alondra.suffix         2
1      Spaanbiuk           0.972315      Spaanbiuk.suffix         2
2         Ibinth           0.933700         Ibinth.suffix         0
3       Liefelle           0.186369       Liefelle.suffix         0
4         Yoanda           0.667436         Yoanda.suffix         2 
</code></pre>
",1
76040030,Problem using Huggingface imagenet-1k dataset in Keras / Tensorflow,"<p>I'm having a problem using the imagenet-1k dataset from Huggingface with a Keras model. I'm just experimenting with simple models, but am stuck trying to get the dataset to work with the model fit function.</p>
<p>Here is how I load the dataset:</p>
<pre><code>ds = load_dataset('imagenet-1k')  # loads a DatasetDict
ds_train = ds['train']  # get a Dataset
ds_train.set_format(type='tensorflow', columns=['image'])  # convert to tf tensor
ds_val = ds['validation']  # get a Dataset
ds_val.set_format(type='tensorflow', columns=['image'])  # convert to tf tensor
</code></pre>
<p>Here is the fit invocation:</p>
<pre><code># train the autoencoder
autoencoder.fit(ds_train, ds_train,
                epochs=10,
                shuffle=True,
                validation_data=(ds_val, ds_val))
</code></pre>
<p>I get the following error:</p>
<pre><code>ValueError: Failed to find data adapter that can handle input: &lt;class 'datasets.arrow_dataset.Dataset'&gt;, &lt;class 'datasets.arrow_dataset.Dataset'&gt;
</code></pre>
<p>When I inspect one of the elements of the datasets it looks like a tf.Tensor, so I don't understand why it can't be passed directly. None of the examples or docs I can find make it clear how to do this. Huggingface <a href=""https://huggingface.co/docs/datasets/v2.11.0/en/use_with_tensorflow"" rel=""nofollow noreferrer"">examples</a> for images produce the same format that I'm getting, but apparently there is a step I'm missing before it can be used with model.fit()</p>
",1
76078405,Modify tf.keras.metrics.MeanRelativeError,"<p>I want to use tf.keras.metrics.MeanRelativeError to implement the calculation of residual per unit observation, which is defined as abs(y_true - y_pred) / y_true. However, I found that I cannot set the normalizer to y_true when using tf.keras.metrics.MeanRelativeError. Therefore, I decided to copy the source code of tf.keras.metrics.MeanRelativeError, make some modifications to it, and create a custom metric named ResidualPerUnitObservation. Here are the modifications I made:</p>
<pre class=""lang-py prettyprint-override""><code>class ResidualPerUnitObservation(base_metric.Mean):
    @dtensor_utils.inject_mesh  # DTensor, Mesh, and Layout: https://www.tensorflow.org/guide/dtensor_overview
    def __init__(self, normalizer=None, name=None, dtype=None):
        super().__init__(name=name, dtype=dtype)
        if normalizer is not None:
            normalizer = tf.cast(normalizer, self._dtype)
        self.normalizer = normalizer

    def update_state(self, y_true, y_pred, sample_weight=None):
        &quot;&quot;&quot;Accumulates metric statistics.

        Args:
          y_true: The ground truth values.
          y_pred: The predicted values.
          sample_weight: Optional weighting of each example. Defaults to 1. Can
            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,
            and must be broadcastable to `y_true`.

        Returns:
          Update op.
        &quot;&quot;&quot;
        y_true = tf.cast(y_true, self._dtype)
        y_pred = tf.cast(y_pred, self._dtype)
        # ragged_tensor: https://www.tensorflow.org/guide/ragged_tensor
        [
            y_pred,
            y_true,
        ], sample_weight = metrics_utils.ragged_assert_compatible_and_get_flat_values(  # noqa: E501
            [y_pred, y_true], sample_weight
        )
        y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(
            y_pred, y_true
        )
        if self.normalizer is None:
            self.normalizer = y_true
        y_pred, self.normalizer = losses_utils.remove_squeezable_dimensions(
            y_pred, self.normalizer
        )
        y_pred.shape.assert_is_compatible_with(y_true.shape)
        relative_errors = tf.math.divide_no_nan(
            tf.abs(y_true - y_pred), self.normalizer
        )

        return super().update_state(
            relative_errors, sample_weight=sample_weight
        )

    def get_config(self):
        n = self.normalizer
        config = {
            &quot;normalizer&quot;: backend.eval(n) if is_tensor_or_variable(n) else n
        }
        base_config = super().get_config()
        return dict(list(base_config.items()) + list(config.items()))
</code></pre>
<p>However, when I specify metrics as ResidualPerUnitObservation() in model.compile and execute model.fit, I encounter an error message:</p>
<p><code>&quot;The tensor &lt;tf.Tensor 'IteratorGetNext:1' shape=(None, 1) dtype=float32&gt; cannot be accessed from here, because it was defined in FuncGraph(name=train_function, id=2877768837632), which is out of scope.&quot;</code></p>
<p>Can anyone help me solve this problem? Thank you in advance for your help.</p>
",0
76142861,Behavior of Tensorflow's GradientTape when target is not a scalar,"<p>Can somebody explain to me the shape and value of Tensorflow's <code>GradientTape</code> output when <code>target</code> is not a scalar value? For example, I had the following code:</p>
<pre><code>import tensorflow as tf

a = tf.Variable([[-1.], [0.], [1.]])
b = tf.Variable([[1.,2.,3.],[4.,5.,6.]])
with tf.GradientTape() as g:
    c = b @ a
grads = g.gradient(c, a)
print(c)
print(grads)
</code></pre>
<p>The value of c is <code>[[2.],[2.]]</code>. The value of <code>grads</code> is <code>[[5.],[7.],[9.]]</code>.</p>
<p>I expected the value of <code>grads</code> to have shape <code>(3,2)</code> or <code>(2,3)</code>, and contain values of partial derivatives of each entry of c with respect to a. I am not sure what the values of 5, 7, and 9 represent (interestingly, it seems to be the gradients as if <code>c</code> had been <code>tf.reduce_sum(b @ a)</code> instead)</p>
<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/GradientTape#:%7E:text=do%20not%20match.-,gradient,-View%20source"" rel=""nofollow noreferrer"">documentation</a> that I found doesn't really explain the output.</p>
",1
76153107,Difference between tf.Module and tf.keras.Model,"<p>I know both <code>tf.Module</code> and <code>tf.keras.Model</code> are used for building custom models.
But what's the difference between both of them?
Which one should be used when becuase there usage looks similar as shown in tensorflow docs?</p>
",1
76187146,Gather elements from tensor with an inner batch dimension in tensorflow,"<p>I'll present my actual problem, and then a simpler version that is more easily reproducible.</p>
<h2>My actual problem</h2>
<p>I have a tensor that represents a batch of images from my training data. Its shape is:</p>
<pre class=""lang-py prettyprint-override""><code>[domains, batch_size, image_size, image_size, channels] = [4 x 4 x 64 x 64 x 4]
</code></pre>
<p>The 64x64x4 images are pixel art character sprites, and the <code>domains</code> dimension represent images of them in: back, left, front, and right poses. The reason that the <code>batch_size</code> dim is not the outer-most is that the images are read from a <code>tf.data.Dataset</code> that produces a 4-tuple for each sample (character in the back, left, front, and right poses). And when I do <code>next(iter(dataset))</code>, what I get is a tensor with the mentioned shape (<code>[domains, batch_size, image_size, image_size, channels]</code>).</p>
<p>At each training step, I need to randomly pick a target pose for each image in the batch, so I can ask a generator to translate them from their source into a target pose. I am using <code>tf.gather</code> for this, but it does not correctly select images the way I need. My code:</p>
<pre class=""lang-py prettyprint-override""><code>batch_size = 4
domains = 4
batch = next(iter(dataset))  # shape 4 x 4 x 64 x 64 x 4
# back, left, front, right = batch

target_indices = tf.random.uniform(shape=[batch_size], dtype=&quot;int32&quot;, maxval=domains)
target_images = tf.gather(batch, target_indices, axis=0)

print(&quot;Shape of target_images&quot;, tf.shape(target_images))
# 4 x 4 x 64 x 64 x 4
</code></pre>
<p>I need the resulting <code>target_images</code> to have a shape of <code>[4, 64, 64, 4]</code>, which would be 1 image for each character sprite in the randomly picked target pose. But as per the documentation of <code>tf.gather</code>, <em>&quot;the output shape has the same shape as the input, with the indexed-axis replaced by the shape of the indices.&quot;</em></p>
<p>One option is to use the <code>batch_dims=1</code> argument of <code>tf.gather</code>, doing:</p>
<pre><code>batch_size = 4
domains = 4
batch = next(iter(dataset))  # shape 4 x 4 x 64 x 64 x 4
# back, left, front, right = batch

target_indices = tf.random.uniform(shape=[batch_size], dtype=&quot;int32&quot;, maxval=domains)
target_images = tf.gather(batch, target_indices, axis=1, batch_dims=1)

print(&quot;Shape of target_images&quot;, tf.shape(target_images))
# 4 x 64 x 64 x 4
</code></pre>
<p>...that does yield a tensor with the correct shape, but not with the correctly gathered subtensors: in the simplified example I show next it is easier to see what is going on. The reason is that although the <code>batch_dims</code> argument exists, it seems to require the batch dimension(s) to be the outer-most one(s). Let' move on to a simpler problem.</p>
<h2>Simplification (reproducible)</h2>
<p>Suppose we have a tensor with a shape of:</p>
<pre class=""lang-py prettyprint-override""><code>[domains, batch_size, content] = [2, 3, 1]
</code></pre>
<p>I want to be able to gather one element (<code>content</code>, <code>axis=2</code>) for each element in the batch (<code>axis=1</code>) according to some random domain (<code>axis=0</code>). Let's try some code:</p>
<pre class=""lang-py prettyprint-override""><code>batch_size = 3
domains = 2
batch = tf.constant([
  [ [1],  [2],  [3]],
  [[10], [20], [30]]
])
target_indices = tf.constant([0, 1, 1])  # randomly picked indices to gather

# the batch has 3 elements and I want to get the content of domain 0 for the first element,
# and domain 1 for the second and third: [[1], [20], [30]], with shape (3, 1)
#
# attempts:
tf.gather(batch, target_indices, axis=0)
# shape=(3, 3, 1) ❌
# [[[ 1], [ 2], [ 3]],
#  [[10], [20], [30]],
#  [[10], [20], [30]]] 

tf.gather(batch, target_indices, axis=1)
# shape=(2, 3, 1) ❌
# [[[ 1], [ 2], [ 2]],
#  [[10], [20], [20]]] 

tf.gather(batch, target_indices, axis=1, batch_dims=1)
# InvalidArgumentError: params.shape[0]: 2 should be equal to indices.shape[0]: 3 [Op:GatherV2] ❌


# last attempt: permute the batch tensor so the batch dim becomes the outer-most
permuted_batch = tf.transpose(batch, [1, 0, 2])
print(&quot;Shape of permuted_batch&quot;, tf.shape(permuted_batch)) # (3, 2, 1)
tf.gather(permuted_batch, target_indices, axis=1, batch_dims=1)
# shape=(3, 1)
# [[1], [20], [30]] ✅
</code></pre>
<p>As we can see, permuting the tensor to put the batch dimension as the outer-most allows me to use <code>batch_dims=1, axis=1</code> for <code>tf.gather</code> and yields the correct result. However, I'm a bit afraid of the performance of <code>tf.transpose</code>.</p>
<p><strong>My question</strong>: is there some way I can gather the desired elements of a tensor that has a batch dimension as an inner dimension?</p>
",0
76244268,Tensorflow: Build new model from input and middle layers of another model,"<p>I'm trying to build <code>new_model</code> from another model layers for class activation mapping purposes.</p>
<pre class=""lang-py prettyprint-override""><code>def vgg_sequential():
    input_shape = IMG_SIZE + (3,)
    model = Sequential()
    model.add(tf.keras.applications.vgg16.VGG16(input_shape=input_shape, include_top=False, weights='imagenet'))
    model.add(layers.GlobalAveragePooling2D())
    model.add(layers.Dense(1))
    return model
</code></pre>
<pre class=""lang-py prettyprint-override""><code>cam_model = tf.keras.Model(inputs=seq_vgg.layers[0].input, outputs=(seq_vgg.layers[-3].output, seq_vgg.layers[-1].output))
</code></pre>
<p>And with this code i get the following error:</p>
<pre><code>ValueError: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 480, 480, 3), dtype=tf.float32, name='vgg16_input'), name='vgg16_input', description=&quot;created by layer 'vgg16_input'&quot;) at layer &quot;vgg16&quot;. The following previous layers were accessed without issue: ['block1_conv1', 'block1_conv2', 'block1_pool', 'block2_conv1', 'block2_conv2', 'block2_pool', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block3_pool', 'block4_conv1', 'block4_conv2', 'block4_conv3', 'block4_pool', 'block5_conv1']
</code></pre>
<p>Already tried functional model API, providing <code>Input()</code> layer inside <code>vgg_sequential()</code> with the same error that my Input layer is disconected from the rest of my model. Beside this when using <code>tf.keras.applications.efficientnet_v2</code> that provides input layers for rescaling and resizing images i don't have any problem.</p>
<p>Any help, information, tips or links to docs that getas me to a solution will be very much appreciated.</p>
<p>Thanks in advance.</p>
",1
76333240,Saving symbolic Tensor for later use,"<p>I am following the tutorial from TensorFlow for a <a href=""https://www.kaggle.com/code/yening2000/chatbot-nlp-1/notebook"" rel=""nofollow noreferrer"">chatbot</a> and was wondering how you could save the output of the encoder later passes? I know the values of symbolic tensors can be printed out with tf.print and they can be written into tf.TensorArray's. Because of how the chatbot model is structured, global variables will not be affected by changes made to them during training, so how does one save these tensors in things like tf.TensorArray's for later iterations during training?</p>
",0
76341077,"What is the difference between MSE in loss=""mean_squared_error"" and metrics=[tf.keras.metrics.MeanSquaredError()]","<p>I am training a neural network model using:</p>
<p><code>model.compile(loss=&quot;mean_squared_error&quot;, optimizer=optimizer, metrics=[tf.keras.metrics.MeanSquaredError()]) </code></p>
<p>What troubles me, is the difference between value of loss and metric in training history, while at first glance both should calculate the same thing.</p>
<pre><code>Epoch 4/30
15484/15484 [==============================] - 135s 9ms/step - loss: 0.1212 - mean_squared_error: 0.1188 - val_loss: 0.1146 - val_mean_squared_error: 0.1120
Epoch 5/30
15484/15484 [==============================] - 135s 9ms/step - loss: 0.1198 - mean_squared_error: 0.1170 - val_loss: 0.1138 - val_mean_squared_error: 0.1109
Epoch 6/30
15484/15484 [==============================] - 131s 8ms/step - loss: 0.1187 - mean_squared_error: 0.1157 - val_loss: 0.1132 - val_mean_squared_error: 0.1101
</code></pre>
<p>Are they calculating MSE based on different things (batch vs whole epoch) or are implemented differently? I have searched through some documentation and did a little bit of googling but didn't find the answer.</p>
<p>What is the difference between these two?</p>
",0
76380927,Tensorflow decode image,"<p>I am a beginner in tensorflow and I am training a small cnn, I am using the tf.io.decode_image function but I can't figure out if this function does preprocess.
The tensorflow documentation about it doesn't say anything.
When I open images with this function the values are between 0 and 1.
The images are single channel grayscale.
This is the code.</p>
<pre><code>def decode_img(self, imgs, channels):
        # Convert the compressed string to a 3D uint8 tensor
        images = []
        for element in imgs:

            dec_image = tf.io.decode_image(element, channels=channels, dtype=tf.float32)
            try:
                img = keras.utils.img_to_array(dec_image)
            except AttributeError:
                img = keras.preprocessing.image.img_to_array(dec_image)
            images.append(img)
        images = np.array(images)
        return images
</code></pre>
<p>I would like to have more explanations</p>
",1
76391276,Custom gradient for broadcasting operation,"<p>I have an operation for which I want to define a custom gradient with <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer""><code>tf.custom_gradient</code></a>. The operation takes two broadcastable arguments and produces a result with the broadcasted shape. The problem is how to handle the broadcasting rules &quot;backwards&quot; in the custom gradient. Let's take the example for a multiplication operation from the documentation of <a href=""https://www.tensorflow.org/api_docs/python/tf/custom_gradient"" rel=""nofollow noreferrer""><code>tf.custom_gradient</code></a>:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

@tf.custom_gradient
def bar(x, y):
  def grad(upstream):
    dz_dx = y
    dz_dy = x
    return upstream * dz_dx, upstream * dz_dy
  z = x * y
  return z, grad
</code></pre>
<p>I can use this gradient alright for the non-broadcasting case:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

with tf.GradientTape() as tape:
    a = tf.ones([5])
    b = tf.ones([5])
    tape.watch([a, b])
    c = bar(a, b)
# Works fine
grad_a, grad_b = tape.gradient(c, [a, b])
</code></pre>
<p>However, when the inputs are broadcasted, the result is not correct:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

with tf.GradientTape() as tape:
    a = tf.ones([10, 1])
    b = tf.ones([5])
    tape.watch([a, b])
    c = bar(a, b)
grad_a, grad_b = tape.gradient(c, [a, b])
print(grad_a.shape, grad_b.shape)
# (10, 5) (10, 5)
</code></pre>
<p>In fact, trying to use it in graph mode fails:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

with tf.Graph().as_default():
    a = tf.ones([10, 1])
    b = tf.ones([5])
    c = bar(a, b)
    grad_a, grad_b = tf.gradients(c, [a, b])
# ValueError: Incompatible shapes between op input and calculated input gradient.
</code></pre>
<p>Is there a way to handle this &quot;unbroadcasting&quot; of the input gradients automatically?</p>
",1
76447508,How to retrain a model that was saved using the tf.saved_model.save() function in Tensorflow,"<p>I am building a Neural Machine Translator for English to Konkani (a local language) language using the Transformer architecture proposed by (Vaswani et, al. 2017). I am following the tutorial code from <a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer</a>. I have trained the model and used the <code>tf.saved_model.save()</code> method to save the model files locally.</p>
<p>I now want to retrain that saved model on a new dataset that I have gathered recently, but I've realised that after loading the model using the <code>tf.saved_model.load()</code> method, I am not able to train it again as the loaded model now lacks the necessary method <code>model.fit()</code> .</p>
<p>Here is a part of the model training code:</p>
<pre class=""lang-py prettyprint-override""><code>class Transformer(tf.keras.Model):
  def __init__(self, *, num_layers, d_model, num_heads, dff,
               input_vocab_size, target_vocab_size, dropout_rate=0.1):
    super().__init__()
    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,
                           num_heads=num_heads, dff=dff,
                           vocab_size=input_vocab_size,
                           dropout_rate=dropout_rate)

    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,
                           num_heads=num_heads, dff=dff,
                           vocab_size=target_vocab_size,
                           dropout_rate=dropout_rate)

    self.final_layer = tf.keras.layers.Dense(target_vocab_size)

  def call(self, inputs):
    # To use a Keras model with `.fit` you must pass all your inputs in the
    # first argument.
    context, x  = inputs

    context = self.encoder(context)  # (batch_size, context_len, d_model)

    x = self.decoder(x, context)  # (batch_size, target_len, d_model)

    # Final linear layer output.
    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)

    try:
      # Drop the keras mask, so it doesn't scale the losses/metrics.
      # b/250038731
      del logits._keras_mask
    except AttributeError:
      pass

    # Return the final output and the attention weights.
    return logits
#-----------------------------------------------------------------------

#...&lt;code to define optimizers and loss functions&gt;...

# This Class acts as an interface for the Transformer
class Translator(tf.Module):
  def __init__(self, context_tokenizers, target_tokenizers, transformer):
    self.context_tokenizers = context_tokenizers
    self.target_tokenizers = target_tokenizers
    self.transformer = transformer

  def __call__(self, sentence, max_length=MAX_TOKENS): #max_length=MAX_TOKENS
    assert isinstance(sentence, tf.Tensor)
    if len(sentence.shape) == 0:
      sentence = sentence[tf.newaxis]

    sentence = tokenize(sentence,self.context_tokenizers).to_tensor()

    encoder_input = sentence

    # As the output language is English, initialize the output with the
    # English `[START]` token.

    start_end = tokenize('',self.target_tokenizers)[0]
    start = start_end[0][tf.newaxis]
    end = start_end[-1][tf.newaxis]

    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)
    output_array = output_array.write(0, start)

    for i in tf.range(max_length):
      output = tf.transpose(output_array.stack())
      predictions = self.transformer([encoder_input, output], training=False)

      # Select the last token from the `seq_len` dimension.
      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.

      predicted_id = tf.argmax(predictions, axis=-1)

      # Concatenate the `predicted_id` to the output which is given to the
      # decoder as its input.
      output_array = output_array.write(i+1, predicted_id[0])

      if predicted_id == end:
        break

    output = tf.transpose(output_array.stack())
    # The output shape is `(1, tokens)`.

    text = self.target_tokenizers.detokenize(output)

    tokens = tf.gather(target_vocab, output)

    # `tf.function` prevents us from using the attention_weights that were
    # calculated on the last iteration of the loop.
    # So, recalculate them outside the loop.
    self.transformer([encoder_input, output[:,:-1]], training=False)
    attention_weights = self.transformer.decoder.last_attn_scores

    joined_text = tf.strings.reduce_join(text[0][1:-1], separator=' ', axis=-1)
    return joined_text, tokens, attention_weights
#-----------------------------------------------------------------------

class ExportTranslator(tf.Module):
  def __init__(self, translator):
    self.translator = translator

  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
  def __call__(self, sentence):
    (result,
     tokens,
     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)

    return result
#-----------------------------------------------------------------------

transformer = Transformer(
    num_layers=num_layers,
    d_model=d_model,
    num_heads=num_heads,
    dff=dff,
    input_vocab_size=context_vocab_size,
    target_vocab_size=target_vocab_size,
    dropout_rate=dropout_rate)

transformer.compile(
    loss=masked_loss,
    optimizer=optimizer,
    metrics=[masked_accuracy])

# training the model on the training data for some epochs
transformer.fit(train_batches,
                epochs=20,
                validation_data=val_batches,
                callbacks=[
                  tf.keras.callbacks.EarlyStopping(patience=3)],
                )

translator = Translator(context_tokenizer, target_tokenizer, transformer)

exp_translator = ExportTranslator(translator)

#saving the model
tf.saved_model.save(exp_translator, export_dir=MODEL_SAVED_FILES)

#-----------------------------------------------------------------------

#loading a saved model
reloaded = tf.saved_model.load(MODEL_SAVED_FILES)
</code></pre>
<p>Here's the error I get when I try to retrain the model using the following code:</p>
<pre class=""lang-py prettyprint-override""><code>reloaded = tf.saved_model.load(MODEL_SAVED_FILES)

#retraing the model on new dataset
reloaded.translator.transformer.fit(train_batches,
                epochs=20,
                validation_data=val_batches,
                callbacks=[
                  tf.keras.callbacks.EarlyStopping(patience=3)],
                )
</code></pre>
<p>The error:</p>
<pre class=""lang-py prettyprint-override""><code>
---------------------------------------------------------------------------

AttributeError                            Traceback (most recent call last)

&lt;ipython-input-41-ad1b625ff6c0&gt; in &lt;cell line: 2&gt;()
      1 #retraing the model on new dataset
----&gt; 2 reloaded.translator.transformer.fit(train_batches,
      3                 epochs=20,
      4                 validation_data=val_batches,
      5                 callbacks=[

AttributeError: '_UserObject' object has no attribute 'fit'
</code></pre>
<p>After reading the documentation I've realised that when saving the model in the above method, the <code>model.fit()</code> and other methods are not saved hence they are not callable.</p>
<p>I need help in finding a way to retrain my saved model, It is not feasible for me to train a new model on a combined dataset as It will take up lot of time and I have very limited resources. I have been looking up on the web for days but couldn't find a solution. Any help in this regards will be appreciated!</p>
",1
