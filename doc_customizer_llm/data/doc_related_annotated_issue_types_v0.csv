QuestionId,Title,Body,QuestionUrl,UserId,Issue Type
57995171,Need a clear simple approach to distributed learning in Tensorflow/Keras,"I have multiple machines, some with GPUs and some others not. I also have a keras model that works fine on a single machine but I want to train it in a distributed mode because I want to test it with a huge dataset and with a bigger number of layers. There is quite a lot of pages discussing the distribution strategy in tf.distribute, but at the same time there are a lot other pages showing how to do it with encapsulating keras model with an estimator, setting up the TF_CONFIG parameter and then call tf.estimator.train_and_evaluate. I used the second approach personally as it was more straightforward and am struggling to tune and debug it. It works anyway, but I am very confused what is the point in all of strategy-related stuff as I don't see any use them in the second approach, and the documentation is not helping to clear it. I also have some doubt if my file setting environment is correct: My understanding is that the PS server is going to hold the model parameters and the chief server is going to administer the whole training process, distributing data, and saving summaries and checkpoints. So I assume that: 0- I need only one chief server, at least one PS server, possibly some workers, and one evaluator. The data and parameter sharing and communication between all of these servers is done by system and I am not engaged in it. 1- The main python code for all machines should be exactly the same, except in TF_CONFIG definition that defines the task and index for that specific machine. 2- I should have one shared copy of data in a folder available to all chief and workers. 3- I should have one shared log directory accessible by all machines as defined in tf.estimator.RunConfig. 4- Having this setting then a piece of code such as below will do the job (assuming the model has been defined elsewhere and the read_datasets function returns features and labels for running the model): Although the above approach seems to work fine, I still have some difficulties understanding how the chief is partitioning the dataset among the workers and how to set the train_step and batch_size in this approach. Also I don't know how can I report accuracy and other metrics such as precision/recall/F1 in addition to loss when running the tf.estimator.evaluate without writing a custom model_fn for my encapsulated keras estimator.",https://stackoverflow.com/questions/57995171,6450489,Documentation Replication on Other Examples
57731214,What tf.keras.backend.clear_session actually do?,What is tf.keras.backend.clear_session actually do? https://www.tensorflow.org/api_docs/python/tf/keras/backend/clear_session How it's related to tf.reset_default_graph() and sess.close() ? https://www.tensorflow.org/api_docs/python/tf/reset_default_graph https://www.tensorflow.org/api_docs/python/tf/Session#close,https://stackoverflow.com/questions/57731214,1179925,Lack of Alternative Solutions/Documentation
57858794,"Why doesn't a Lambda function that returns the result of tf.py_func/tf.numpy_function have an output shape, and how can I correct this behavior?","For example purposes, I've created two Python functions. One takes a value and just returns it; the other is a lambda function that returns a Tensor to evaluate the function: When I run this code on Tensorflow 1.14.x + Keras 2.2.x + Python 3.6.x, and even if I set the output_shape parameter of the Lambda layer, I get a Lambda layer with a ""None"" output shape: When I test the code using a test tensor, I get the results I might expect: Output: For comparison purposes, I also created a neural network using the function found in this page of documentation: As somewhat expected, this prints: Now - if I understand correctly - both tf.py_func and K.concatenate return a TensorFlow Tensor object. So, the question is, Why does all the code I posted here work, EXCEPT my tf.numpy_function layer?",https://stackoverflow.com/questions/57858794,10083113,Documentation Replicability
57875937,Inconsistent tf.print results,"I was trying to understand the executing order in tensorflow graph. When I was running the following code. tf.print prints undetermined results. Here are the output of 20 repetitions. Sometimes, it prints ""msg1: 3"", sometimes it prints ""msg1: 1"". So what's going on here?",https://stackoverflow.com/questions/57875937,7212365,Documentation Replication on Other Examples
57929803,What is the proper way to convert Tracing Code using RunOptions to Tensorflow 2.0?,I'm having difficulty finding any documentation on how to migrate tracing code from 1.x to 2.0. In tensorflow 1.x you could do the following: How can you do a similar thing with a @tf.function call?,https://stackoverflow.com/questions/57929803,314864,Inadequate Examples
56693863,Why does model.losses return regularization losses?,"I have met a snippet of code of tensorflow 2.0, which is used for calculating the loss. The total loss is composed of two parts: 1) regularization loss, 2) prediction loss. My question is why model.losses is regularization loss? model here is an instance of tf.keras.Model. I'm kind of confused by the tensorflow official API documentation. tf.keras.Model, it says Why could we get regularization loss via accessing losses property? Also, what is eager safe? If losses property is returning regularization loss, why is it named losses instead of regularization_loss?",https://stackoverflow.com/questions/56693863,7212365,Documentation Ambiguity
40879504,How to apply Drop Out in Tensorflow to improve the accuracy of neural network?,"Drop-Out is regularization techniques. And I want to apply it to notMNIST data to reduce over-fitting to finish my Udacity Deep Learning Course Assignment.I have read the docs of tensorflow on how to call the tf.nn.dropout. And here is my code The tf.nn.dropout is called in function model(), but after I applied the DropOut technique to the neural network, the accuracy did seem any change, here is the result: How can I apply DropOut by Tensorflow to improve the accuracy of the network? Thank you!",https://stackoverflow.com/questions/40879504,5046896,Documentation Replication on Other Examples
59847045,Should I use @tf.function for all functions?,"An official tutorial on @tf.function says: It only mentions how to implement @tf.function annotated functions but not when to use it. Is there a heuristic on how to decide whether I should at least try to annotate a function with tf.function? It seems that there are no reasons not to do it, unless I am to lazy to remove side effects or change some things like range()-&gt; tf.range(). But if I am willing to do this... Is there any reason not to use @tf.function for all functions?",https://stackoverflow.com/questions/59847045,502727,Requesting (Additional) Documentation/Examples
41216322,How to write piecewise function in Tensorflow?,"I have tried to train only one Tensorflow variable theta by using tf.select as the code shown below. But I got So how to do this? If I really want use tf.cond or tf.where or tf.select or tf.case instead of something like sigmoid, I mean how to do condition braches (piecewise function) in Tensorflow in general?",https://stackoverflow.com/questions/41216322,2394303,Inadequate Examples
60222896,How can I calculate pos_weight variable of the tf.nn.weighted_cross_entropy_with_logits for multiple classes?,"I want to use tf.nn.weighted_cross_entropy_with_logits. There are three classes of the dataset. The number of images in class A is 5500, in class B is 2000, and in class C is 20000. How can I calculate and use pos_weight variable in tf.nn.weighted_cross_entropy_with_logits?",https://stackoverflow.com/questions/60222896,12594959,Documentation Replication on Other Examples
60237912,"Is a ""normal"" python object automatically a tensor? e.g. ""a"" or 1","I'm struggling to understand why this function works: img_path_list and label_list are lists of type string. The thing that I don't get is that apparently dataset = tf.data.Dataset.from_tensor_slices((img_path_list, label_list)) is a Tensor containing tuples (). So when I run the .map(map_func=get_pair, num_parallel_calls=None) two strings are passed, as a tuple, in the get_pair(path_to_img, label_string): function. One these strings is then passed in the get_img(path_to_img) function and finally to tf.io.read_file(path_to_img). The problem is that io.read_file() needs an input: ""Tensor of type string"" (see docs : https://www.tensorflow.org/api_docs/python/tf/io/read_file). But string != to Tensor of type string: isinstance(""hello"", tf.Tensor) == False as well as: isinstance([""hello""], tf.Tensor) == False Thanks for your help!",https://stackoverflow.com/questions/60237912,9621080,Documentation Ambiguity
41467115,"Using word2vec pretrained vectors, how to generate ids of a sentence as input to tf.nn.embedding_lookup function in tensorflow?","To extract the embedding representations of input data, the tensorflow documentation says we can use the following: Accdg to the TF documentation, the 2nd parameter of the function tf.nn.embedding_lookup is a tensor of ids: My question is: Given a sentence, say, how can I represent and transform it into ids? In the code below, how can I transform my sentence into input_data.",https://stackoverflow.com/questions/41467115,3009947,Documentation Replication on Other Examples
60525257,How to apply Non max suppression on batch of images in tensorflow 1.14?,"I have batch of cropped images from original image on which I have to perform object detection, I am trying to apply tensorflow NMS operation. I looked into tensorflow api docs, and found tf.image.combined_non_max_suppression(), but I am unable to understand it properly. The flow in my pipeline is of two step. For the first step, I use simple tf.image.non_max_suppression() followed by tf.gather(), but I am not able to understand, how to do it for second step. Please refer to code snippets below: But I got following error, when tried running above: How to solve it and apply NMS for batch of images in tensorflow ?",https://stackoverflow.com/questions/60525257,7112271,Documentation Replicability
41602374,tf.zeros doesn't return a 1D tensor?,"I'm trying to duplicate a tensor across a new axis, like this: However I'm getting this error: In the documentation it says the way I wrote tf.constant is supposed to have it return a 1D tensor but when I checked its shape with get_shape(), it has (5,) as its shape. I tried reshaping it but nothing changed. Why am I getting this error? Thanks.",https://stackoverflow.com/questions/41602374,3873000,Documentation Replicability
60761888,Chaining custom Keras layers in functional style,"I want to build a model using tf.keras' functional API. My model is quite large, hence I would like to create custom layers by inheriting from tf.keras.layers.Layer. Below is my attempt, inspired by TensorFlow's documentation. This code crashes with the following error. What's wrong with my approach?",https://stackoverflow.com/questions/60761888,5248987,Documentation Replication on Other Examples
60782077,How do you use tensorflow ctc_batch_cost function with keras?,"I have been trying to implement a CTC loss function in keras for several days now. Unfortunately, I have yet to find a simple way to do this that fits well with keras. I found tensorflow's tf.keras.backend.ctc_batch_cost function but there is not much documentation on it. I am confused about a few things. First, what are the input_length and label_length parameters? I am trying to make a handwriting recognition model and my images are 32x128, my RNN has 32 time steps, and my character list has a length of 80. I have tried to use 32 for both parameters and this gives me the error below. Shouldn't the function already know the input_length and label_length from the shape of the first two parameters (y_true and y_pred)? Secondly, do I need to encode my training data? Is this all done automatically? I know tensorflow also has a function called tf.keras.backend.ctc_decode. Is this only used when making predictions? Error: tensorflow.python.framework.errors_impl.InvalidArgumentError: squeeze_dims[0] not in [0,0). for 'loss/softmax_loss/Squeeze' (op: 'Squeeze') with input shapes: [] Model: Here is the tensorflow documentation I was referencing: https://www.tensorflow.org/api_docs/python/tf/keras/backend/ctc_batch_cost",https://stackoverflow.com/questions/60782077,11900339,Documentation Ambiguity
38601452,What is tf.nn.max_pool's ksize parameter used for?,"In the definition of tf.nn.max_pool, what is ksize used for? For instance, if an input value is of tensor : [1, 64, 64, 3] and ksize=3.what does that mean?",https://stackoverflow.com/questions/38601452,288609,Documentation Ambiguity
57609316,Does TFLiteConverter automatically quantize the Keras model?,I converted the trained Keras model using tf.lite.TFLiteConverter into tflite_model. Is the converted tflite_model quantized one? Here is the snippet to make the conversion.,https://stackoverflow.com/questions/57609316,7830286,Documentation Replication on Other Examples
38646996,tf.nn.dynamic_rnn() returning error when used in Google Cloud Datalab,"I'm trying to run an RNN on Google Cloud Datalab. The same network runs correctly on my computer, but when I run it on Datalab, I get the following error: TypeError: dynamic_rnn() takes at least 3 arguments (3 given) The use of dynamic_rnn() is as follows: rnn_outputs, state = tf.nn.dynamic_rnn(cells, inputs, initial_state = initial_state_placeholder) Is this a tensorflow version problem. On my computer I'm using tensorflow-0.9.0, from the latest GPU pip install for Ubuntu. Is Cloud Datalab perhaps using a different version, in which the use of dynamic_rnn() has changed slightly? UPDATE: Have checked version of tensorflow imported into Datalab - it's 0.7.1. I assume there's no way of importing a newer version if this is what's causing the problem? I'm following the directions for use here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard8/tf.nn.dynamic_rnn.md Many thanks.",https://stackoverflow.com/questions/38646996,6483227,Documentation Replication on Other Examples
57651287,How to swap tensor axes efficiently in tensorflow?,"I have to swap tensor's axes using tf.transpose to do the batch matrix multiplication (as the code shown below). tensor input_a: shape [10000, 10000] tensor input_b: shape [batch_size, 10000, 10] tensor output: shape [batch_size, 10000, 10] However, it seems very slow. I noticed this in the document page of tf.transpose: So, I think it might be the reason why my code run slowly? Is there any way to swap tensor's axes, or do the batch matrix multiplication efficiently?",https://stackoverflow.com/questions/57651287,9427280,Inadequate Examples
57878623,Error with tf.nn.sparse_softmax_cross_entropy_with_logits,"I am using tf.nn.sparse_softmax_cross_entropy_with_logits and when I pass through the labels and logits I get the following error I don't understnad how having a shape [50,1] is not the same as being 1D",https://stackoverflow.com/questions/57878623,9706484,Documentation Replication on Other Examples
57881799,How to define custom gradient for keras layer with tensorflow 1.14,I am trying to implement gradient reversal layer in tf.keras (tf ver 1.14). I defined a custom gradient using tf.custom_gradient but the grad function is never called and the result is wrong. I am using the following to test the layer,https://stackoverflow.com/questions/57881799,5026962,Documentation Replicability
38962308,Unclear behavior for sampler in Tensorflow,"For the samplers implemented in tensorflow, e.g. tf.nn.fixed_unigram_candidate_sampler. The behavior is not well-defined in the document. For instance, I would expect the labels specified in true_classes will be excluded from the sampling pool, and the sampling will be conducted for each batch. But according to my experiments, neither of above is true. Consider the following code: The output can be 3, which actually belongs to the set of true classes. - Also, the output has the dimension [1], which basically means that the sampling is only conducted once, not for each batch. Can someone help to clarify this?",https://stackoverflow.com/questions/38962308,4097290,Documentation Ambiguity
66146394,How do I retrieve the one-hot-encoded feature names in tensorflow.keras preprocessing layers,"What is the tf.keras equivalent of encoder.get_feature_names found in sklearn? As shown shown in this SO question Need this to get all the one-hot encoded feature names created ""from tensorflow.keras.layers.experimental import preprocessing"". I will appreciate any post with example.",https://stackoverflow.com/questions/66146394,8855052,Inadequate Examples
47319390,Why does this TensorFlow code behave differently when inside a test case?,"I have a function (foo below) which is behaving differently when it's run directly vs when it is run inside a tf.test.TestCase. The code is supposed to create a dataset with elems [1..5] and shuffle it. Then it repeats 3 times: create an iterator from the data and use that to print the 5 elements. When run on its own it gives output where all the lists are shuffled e.g.: but when run inside a test case they are always the same, even between runs: I imagine it's something to do with how test cases handle random seeds but I can't see anything about that in the TensorFlow docs. Thanks for any help! I am running it with Python 3.6 and TensorFlow 1.4. No other modules should be needed.",https://stackoverflow.com/questions/47319390,1015528,Documentation Replication on Other Examples
47327256,Understanding Tensor Inputs & Transformations for use in an LSTM (dynamic RNN),"I am building an LSTM style neural network in Tensorflow and am having some difficulty understanding exactly what input is needed and the subsequent transformations made by tf.nn.dynamic_rnn before it is passed to the sparse_softmax_cross_entropy_with_logits layer. https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn The input function is sending a feature tensor in the form [batch_size, max_time] However the manual states that input tensors must be in the form [batch_size, max_time, ...] I have therefore expanded the input with a 1d tensor to take the form [batch_size, max_time, 1] At this point the input does not break upon running, but I don't understand exactly what we have done here and suspect it may be causing the problems when calculating loss (see below). This expanded tensor is then the 'features' tensor used in the code below This throws a value error at loss dimensions must be equal, but are [max_time, num_classes] and [batch_size] from https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/classification - A common use case is to have logits of shape [batch_size, num_classes] and labels of shape [batch_size]. But higher dimensions are supported. At some point in the process max_time and batch_size have been mixed up and I'm uncertain if its at input or during the LSTM. I'm grateful for any advice!",https://stackoverflow.com/questions/47327256,7822780,Lack of Alternative Solutions/Documentation
66514664,Convert tf.Tensor to numpy array and than save it as image in without eager_execution,"My OC is big sur for apple M1, therefore my tensorflow version is 2.4 which has been installed from official apple github repo(https://github.com/apple/tensorflow_macos). When i use code bellow, i get tensor(&lt;tf.Tensor 'StatefulPartitionedCall:0' shape=(1, 2880, 4320, 3) dtype=float32&gt;) How to get image from sr Tensor?",https://stackoverflow.com/questions/66514664,14426701,Documentation Replication on Other Examples
47657835,"In Tensorflow, is there a direct way to construct an array tensor from a scalar tensor?","For example, something similar to: Is there a direct to construct the array tensor y given scalar tensor x in Tensorflow? Without using tf.expand_dims and tf.pad. Thank you very much.",https://stackoverflow.com/questions/47657835,7586189,Requesting (Additional) Documentation/Examples
47658691,Replace tf.squeeze using as tf.reshape,"I need to train a mobile net using tensorflow. The tf.squeeze layer is not supported. Can I replace it with the tf.reshape? Is the operation: the same as: where net has the shape [50,1,1,1000].",https://stackoverflow.com/questions/47658691,4888520,Documentation Replication on Other Examples
47665314,how can we get benefit from sharding the data to speed the training time?,"My main issue is : I have 204 GB training tfrecords for 2 million images, and 28GB for validation tf.records files, of 302900 images. it takes 8 hour to train one epoch and this will take 33 day for training. I want to speed that by using multiple threads and shards but I am little bit confused about couple of things. In tf.data.Dataset API there is shard function , So in the documentation they mentioned the following about shard function : So my question regarding the code above is when I try to makes d.shards of my data using shard function, if I set the number of shards (num_workers)to 10 , I will have 10 splits of my data , then should I set the num_reader in d.interleave function to 10 to guarantee that each reader take one split from the 10 split? and how I can control which split the function interleave will take? because if I set the shard_index (worker_index) in shard function to 1 it will give me the first split. Can anyone give me an idea how can I perform this distributed training using the above functions? then what about the num_parallel_call . should I set it to 10 as well? knowing that I have single tf.records file for training and another one for validation , I don't split the tf.records files into multiple files.",https://stackoverflow.com/questions/47665314,8262057,Documentation Replication on Other Examples
57570041,"Tensorflow 2.0 ""future proof"" way of using tf.feature_columns in tf.keras subclass api?","I see there are some ways of using features columns wrapped in a tf.keras.layers.DenseFeatures layer: https://www.tensorflow.org/beta/tutorials/keras/feature_columns#create_a_feature_layer This suggests passing datasets into @tf.function wrapped ""def call"" methods? I don't see any mention (yet) of the interaction between tf.data.DataSet and tf.function. Are tf.data.DataSet considered tensors yet for the purpose of tf.function graph cache or will they be?",https://stackoverflow.com/questions/57570041,287238,Documentation Ambiguity
57809049,"tf.while_loop -> Number of inputs and outputs of body must match loop_vars: 1, 2",I need to use a tf.while_loop but I got the following message :,https://stackoverflow.com/questions/57809049,11427310,Documentation Replicability
57823210,How to combine tf.data.Dataset and tf.estimator.DNNRegressor properly,"I am currently learning to use tensorflow and have troubles getting started. I would like to use the newest API, namely estimator and dataset. But if I run the code presented below I get an Error. On the tensorflow page https://www.tensorflow.org/api_docs/python/tf/estimator/DNNRegressor I found, that ""The function should construct and return one of the following: * A tf.data.Dataset object: Outputs of Dataset object must be a tuple (features, labels) with same constraints as below."" I thought my code would provide that, but there seems to be a problem and I am out of ideas. I expect to run a training session on a deep neural network with 3 hidden layers a' 100 neurons in order to approximate the 'constant 1' function; instead I get the Error ""ValueError: features should be a dictionary of 'Tensor's. Given type: class, 'tensorflow.python.framework.ops.Tensor'",https://stackoverflow.com/questions/57823210,12030773,Documentation Replication on Other Examples
57835609,pycharm says taht it cant find tensorflow.keras but it works,"it always says tat it tf.keras is an ""Unresolved References"" but it works. why and how can i get to reconize it? I tryed using and but then it just wont work at all",https://stackoverflow.com/questions/57835609,9560068,Documentation Replication on Other Examples
39008821,Tensorflow: When use tf.expand_dims?,"Tensorflow tutorials include the use of tf.expand_dims to add a ""batch dimension"" to a tensor. I have read the docs for this function but it still is rather mysterious to me. Does anyone know exactly under what circumstances this must be used? My code is below. My intent is to calculate a loss based on the distance between the predicted and actual bins. (E.g. if predictedBin = 10 and truthBin = 7 then binDistanceLoss = 3). In this case, do I need to apply tf.expand_dims to predictedBin and binDistanceLoss? Thanks in advance.",https://stackoverflow.com/questions/39008821,3444294,Documentation Replication on Other Examples
39133312,Why does setting an initialization value prevent placing a variable on a GPU in TensorFlow?,"I get an exception when I try to run the following very simple TensorFlow code, although I virtually copied it from the documentation: The exception is: If I change the variable's initial value to tf.zeros([1]) instead, everything works fine: Any idea what's going on?",https://stackoverflow.com/questions/39133312,38626,Documentation Replicability
58113387,What can be used to replace tf.train.GradientDescentOptimizer in TensorFlow 2,I'm trying to convert my TensorFlow 1.4 code to TensorFlow 2 but in there's no more tf.train.GradientDescentOptimizer in TF 2. What should be the replacement for tf.train.GradientDescentOptimizer? I found the same thing in tf.compat.v1.train. but I shouldn't use it as those stuff in tf.compat will be removed sooner or later.,https://stackoverflow.com/questions/58113387,5581893,Documentation Completeness
58176215,Tensorflow difference between tf.stop_gradient and feed variables to optimizer?,"I'm trying to train a model in self-supervised learning. The flow chart is something like the following: Let's assume that N1 is already trained and we want to train just N2. This is my current implementation: In this way, I should be optimizing only N2. What makes me confused is the fact that if I were to use the following code I would obtain very different results (much better than the above): I wonder what is the problem with the first implementation. What is exactly the behaviour of tf.stop_gradient (the documentation is a bit poor)? How does this differ from the second approach? From a practical perspective in semi-supervised learning: what is the difference between the two? Which one is the correct approach? Thank you :) I added a possible solution to the problem in the comments below. I would still be happy to receive any feedback from more experienced users and to share some opinions on the best approach to structure a self-supervised learning problem in tensorflow. Bye, G.",https://stackoverflow.com/questions/58176215,9710391,Documentation Replication on Other Examples
39493229,How to use tf.nn.max_pool_with_argmax correctly,"currently I play a little bit around with tensorflow to create a better understanding of machine learning an tensorflow itself. Therefore I want to visualize the methods (as much as possible) of tensorflow. To visualize max_pool I loaded an image and perform the method. After that I displayed both: input and output image. Everything works fine and I get this output (as expected) Now I wanted to test the method tf.nn.max_pool_with_argmax to get the argmax of the pooling operations. But if I uncomment the line output, argmax = tf.nn.max_pool_with_argmax(image_tensor, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1') Python crashes with I don't have an idea which argument is wrong because every argument should be correct (tensorflow docs) ... Does anyone know what went wrong?",https://stackoverflow.com/questions/39493229,3524844,Documentation Replicability
34870614,What does tf.nn.embedding_lookup function do?,"I cannot understand the duty of this function. Is it like a lookup table? Which means to return the parameters corresponding to each id (in ids)? For instance, in the skip-gram model if we use tf.nn.embedding_lookup(embeddings, train_inputs), then for each train_input it finds the correspond embedding?",https://stackoverflow.com/questions/34870614,5808490,Documentation Ambiguity
34931121,Can cond support TF ops with side effects?,"The (source code) documentation for tf.cond is unclear on whether the functions to be performed when the predicate is evaluated can have side effects or not. I've done some tests but I'm getting conflicting results. For example the code below does not work: I.e. no matter what value the predicate evaluates to, both functions are getting run, and so the net result is a subtraction of 1. On the other hand, this code snippet does work, where the only difference is that I add new ops to the graph every time my_op is called: Not sure why creating new ops every time works while the other case doesn't, but I'd obviously rather not be adding nodes as the graph will eventually become too big.",https://stackoverflow.com/questions/34931121,2615676,Documentation Replicability
69526735,Using tf.train.Feature and tf.train.Example with ragged tensors,How can we use tf.train.Feature and tf.train.Example to save Ragged Tensors to TFRecords? I have this piece of code: How can I write it to TFRecords along with other regular tensors? Normally I do something like: Thanks,https://stackoverflow.com/questions/69526735,11970084,Documentation Replication on Other Examples
50945733,why this Tensorflow code raises tf.errors.OutOfRangeError?,This Tensorflow code below raises a tf.errors.OutofRangeError: Why is tf.errors.OutofRangeError printed when all the samples are read? It seems unreasonable.,https://stackoverflow.com/questions/50945733,5030342,Documentation Replicability
50967885,"tf.gradients, how can I understand `grad_ys` and use it?","In tf.gradients, there is a keyword argument grad_ys Why is grads_ys needed here? The docs here is implicit. Could you please give some specific purpose and code? And my example code for tf.gradients is",https://stackoverflow.com/questions/50967885,5046896,Documentation Replicability
51507788,What are inputs and outputs in tf.saved_model.simple_save?,"In tf.saved_model.simple_save, there are 4 params: I've been reading how to simple_save but I haven't been able to figure out what to put in these two parameters (inputs and outputs). I know the model must have input values so that it can be either trained or predict. So I don't know what these two parameters should contain and wether they should map variables inside the model or what... The docs aren't that great so any explaining would be much appreciated.",https://stackoverflow.com/questions/51507788,1071459,Requesting (Additional) Documentation/Examples
51586459,Why the false_fn in tf.cond never called?,"It's so wired to use tf.while_loop with tf.cond together. why the false condition never met? it always fall in true condition. give me the result unexpected as below: [[0, 0], [6, 6], [6, 6], [6, 6]]",https://stackoverflow.com/questions/51586459,3331276,Documentation Replication on Other Examples
51586693,"Tensor has shape [?, 0] -- how to reshape to [?,]","When src has shape [?], tf.gather(src, tf.where(src != 0)) returns a tensor with shape [?, 0]. I'm not sure how a dimension can have size 0, and I'm especially unsure how to change the tensor back. I didn't find anything in the documentation to explain this, either. I tried to tf.transpose(tensor)[0], but the first dimension of the transposed tensor has size 0 and cannot be accessed! What's wrong?",https://stackoverflow.com/questions/51586693,6772171,Documentation Replication on Other Examples
51612489,tensorflow tf.edit_distance explanation required?,"How does tensorflow tf.edit_distance function works? How it compares string stored in two different sparse matrix equivalent of 2d or 3d dense matrix. Example given on tensorflow web page https://www.tensorflow.org/api_docs/python/tf/edit_distance is not so obvious. Please provide explanation using some other examples. Also this example is not clear. How output is of dimension [2,2]? What normalization is doing here?",https://stackoverflow.com/questions/51612489,7930290,Lack of Alternative Solutions/Documentation
51625529,How to use tf.data's initializable iterator and reinitializable interator and feed data to estimator api?,"All the official google tutorials use the one shot iterator for all the estimator api implementation, i couldnt find any documentation on how to use tf.data's initializable iterator and reinitializable interator instead of one shot iterator. Can someone kindly show me how to switch between train_data and test_data using tf.data's initializable iterator and reinitializable interator. We need to run a session to use feed dict and switch the dataset in the initializable iterator, its a low level api and its confusing how to use it part of estimator api architecture PS : I did find that google mentions ""Note: Currently, one-shot iterators are the only type that is easily usable with an Estimator."" But is there any work around within the community? or should we just stick with one shot iterator for some good reason",https://stackoverflow.com/questions/51625529,10163720,Lack of Alternative Solutions/Documentation
38893526,What's the meaning of tf.nn.embedding_lookup_sparse in TensorFlow?,"We spend a lot of time in reading the API document of tf.nn.embedding_lookup_sparse. The meaning of embedding_lookup_sparse is confusing and it seems quite different from embedding_lookup. Here's what I think and please correct me if I'm wrong. The example of wide and deep model uses contrib.layers APIs and call embedding_lookup_sparse for sparse feature colume. If it gets the SparseTensor(for example, country, which is sparse), it creates the embedding which is actually for one-host encoding. Then call to_weights_sum to return the result of embedding_lookup_sparse as prediction and the embedding as variable. The the result of embedding_lookup_sparse add bias and become the logits for loss function and training operation. That means the embedding_lookup_sparse do something like w * x(part of y = w * x + b) for dense tensor. Maybe for one-hot encoding or SparseTensor, the weight from embedding_lookup_sparse is actually the value of w * x because the look-up data is always 1 and no need to add other 0s. What I said is also confusing. Can anyone help to explain this in detail?",https://stackoverflow.com/questions/38893526,2502090,Documentation Replication on Other Examples
57857325,How to load custom op library when using c++?,"I have build a very simple custom op zero_out.dll with bazel, it works when using python. However I have to run the inference using C++, is there any c++ api which has the similar function as tf.load_op_library, as it seems that a lot of registeration work has been done in tf.load_op_library, TF has not counterpart c++ API?",https://stackoverflow.com/questions/57857325,9873377,Lack of Alternative Solutions/Documentation
57872334,parallel inference in tensorflow (CPUs),"this is a really basic question, but I can't get the answer anywhere. Using tensorflow, can I do inference (tf.keras.Model.predict()) on multiple CPUs in parallel? I am using tf.data.Dataset to represent my data, but from the documentation it seems multiprocessing can be used only with generators or tf.keras.Sequence objects. Why is that? Am I supposed to somehow create an ordinary python generator from the Dataset or to manage the parallelism on my own with multiprocessing package? What would be the standard way of doing this? Thanks for any hints.",https://stackoverflow.com/questions/57872334,3447343,Documentation Replication on Other Examples
57873334,Affine Transform in TensorFlow 2.0,How to code Affine Transform in TensorFlow 2.0 that can work for digital images? I've tried tf.keras.preprocessing.image.apply_affine_transform from TensorFlow 1.14 but TensorFlow 2.0 has no such transform. Now I need it for TensorFlow 2.0.,https://stackoverflow.com/questions/57873334,11160943,Documentation Replication on Other Examples
57888872,"how to fix ""OperatorNotAllowedInGraphError "" error in Tensorflow 2.0","I'm learn tensorflow2.0 from official tutorials.I can understand the result from below code. But if I change the inputs with tensor not python code, just like this I get below error!! I can't find any specifications about this error. I think the real reason is not ""iterating over tf.Tensor is not allowed"" . Becase I can write like this. I iterate over tensor just like above code. So my question is what's the real reason about this error? Any suggestions will help me. I really can't understand this error through I read a lot of materials.",https://stackoverflow.com/questions/57888872,6737375,Documentation Replication on Other Examples
58213561,"Under Tensorflow 2.0, how to use tf.summary?","It seems the use of tf.summary under TF2.0 is different from TF1.x. If I want to visualize multiple scalars (e.g., train_loss, val_loss, etc.) in TensorBoard, how to implement tf.summary in the code. I thought it might be: But it doesn't work. I think the problem may be from the use of write.flush().",https://stackoverflow.com/questions/58213561,9151176,Documentation Replicability
58248787,Reading TF2 summary file with tf.data.TFRecordDataset,"In TF1, i could use summary_iterator to read summary files. But now, it will throw a warning So i'm wondering how to use tf.data.TFRecordDataset(path) to read tfevent files generated by TF2.",https://stackoverflow.com/questions/58248787,4909165,Documentation Replication on Other Examples
58317413,how can we get cell state if we use tf.keras.layers.LSTM (tf version is 2.0)?,In tensorflow version 2 tf.keras.layers.LSTM returns only the hidden state but not the cell state when return_state is checked to be True,https://stackoverflow.com/questions/58317413,10187580,Documentation Ambiguity
74088086,Seed in tensorflow initializer (tf.keras.initializers) doesn't guarantees reproducible results,"looking at tensorflow documentation (see, e.g., https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotNormal) a seed should guarantee that ""multiple initializers will produce the same sequence when constructed with the same seed value"" The following easy experiment says otherwise Giving the output Interesting fact, if I run the python script multiple times I always get the same overall results. So the first seed somehow works, but when it is called a second time in the script it 'keeps advancing', although it should be fixed. Any opinion about that? Do you think it is a bug? Do you think it is the intended behaviour (if yes could you explain me why)? It may be a problem of my TF installation? I have python 3.7.9 on Windows and Tensorflow version is 2.7.0 Of course, the same behaviour applies when inserting an initializer in a tf.keras.layer giving Thanks in advance for your time!",https://stackoverflow.com/questions/74088086,17788510,Documentation Replication on Other Examples
57813806,Apply feature columns without tf.Estimator (Tensorflow 2.0.0-rc0),"In the Tensorflow tf.Estimator and tf.feature_column docs it is well documented, how to use feature columns together with an Estimator e.g. in order to one-hot encode the categorical features in the dataset being used. However, I want to ""apply"" my feature columns directly to a tf.dataset which I create from a .csv file (with two columns: UserID, MovieID), without even defining a model or an Estimator. (Reason: I want to check what's happening exactly in my datapipeline, i.e. I'd like to be able to run a batch of samples through my the pipeline, and then see in the output how the features got encoded.) This is what I have tried so far: However the last line with the map call raises the following error: I'm not sure what this error means... I also tried to define a tf.keras model with only the feature_layer I defined, and then run .predict() on my dataset - instead of using ds.map(lambda x: feature_layer(x)): However, this results exactly in the same error as above. Does anybody have an idea what is going wrong? Is there maybe an easier way to achieve this?",https://stackoverflow.com/questions/57813806,8725045,Documentation Replication on Other Examples
61692802,How to migrate from TensorFlow 1.x to TensorFlow 2.x,"i want to convert this code above to relevant TensorFlow 2.x without eager execution, anyone can help? I been trying to to change few things like: changing tf.compat.v1.nn.rnn_cell.LSTMCell to tf.keras.layers.LSTMCell and tf.compat.v1.nn.rnn_cell.MultiRNNCell to tf.keras.layers.StackedRNNCells also tf.compat.v1.nn.dynamic_rnn to tf.keras.layers.RNN how do i do this?",https://stackoverflow.com/questions/61692802,9359081,Documentation Replication on Other Examples
67561368,Difference between tf.nn.leaky_relu and tf.keras.layers.LeakyReLU?,I understand that tf.nn.leaky_relu is used to compute the function while tf.keras.layers.LeakyReLU is the layer version. But how would they differ while adding to a Functional API neural network model?,https://stackoverflow.com/questions/67561368,13316200,Documentation Ambiguity
69792031,Explanation of tf.keras.layers.CategoryEncoding output_mode='multi_hot' behavior,"Please help understand the definition of multi hot encoding of tf.keras.layers.CategoryEncoding and the behavior of output_mode='multi_hot'. According to What exactly is multi-hot encoding and how is it different from one-hot?: The document says num_tokens is the total number of tokens the layer should support. According to the definitions of multi hot encoding above, I expected tf.keras.layers.CategoryEncoding(num_tokens=5, output_mode=""multi_hot"") encodes 5 tokens into an array of size 3. However, the document says ""multi_hot"" encodes each sample into a single array of num_tokens size, containing a 1 for each vocabulary term present in the sample, and behaves as such. Which has no difference from One Hot Encoding. For multi value inputs, multi_hot only handles the first value. To handle multiple inputs, need to be 2D array. Apparently the definition of multi hot encoding of tf.keras.layers.CategoryEncoding is not the same with the one in What exactly is multi-hot encoding and how is it different from one-hot?.",https://stackoverflow.com/questions/69792031,4281353,Lack of Alternative Solutions/Documentation
51069173,What exactly qualifies as a 'Tensor' in TensorFlow?,"I am new to TensorFlow and just went through the eager execution tutorial and came across the tf.decode_csv function. Not knowing about it, I read the documentation. https://www.tensorflow.org/api_docs/python/tf/decode_csv I don't really understand it. The documentation says 'records: A Tensor of type string.' So, my question is: What qualifies as a 'Tensor'? I tried the following code: So the list dec_res contains tf.tensor objects. That seems reasonable to me. But is an ordinary string also a 'Tensor' according to the documentation? Then I tried something else with the tf.reshape function. In the documentation https://www.tensorflow.org/api_docs/python/tf/reshape it says that 'tensor: A Tensor.' So, l is supposed to be a tensor. But it is not of type tf.tensor but simply a python list. This is confusing. Then the documentation says But the type of l is list where the type of r is tensorflow.python.framework.ops.Tensor. So the types are not the same. Then I thought that TensorFlow is very generous with things being a tensor. So I tried: Now, the line in comments results in an error. So, can anyone help me to find out, what qualifies as a 'Tensor'? P.S.: I tried to read the source code of tf.reshape as given in the documentation But this file does not exist in the Github repo. Does anyone know how to read it?",https://stackoverflow.com/questions/51069173,6810233,Requesting (Additional) Documentation/Examples
51077930,tf.image.resize_bilinear()-when align_corners=False,"I am using Tensorflow 1.4.0 The Tensorflow tf.image.resize_bilinear() has an argument called 'align_corners' and I am confused with the behavior when we set it to be False. In the official document, it says: When I use tf.image.resize_bilinear() with align_corners=True in the following program: it outputs which corners are correctly aligned. However when I set the align_corners=False, I got the following weird outputs Is there anyone who understand why Tensorflow will use this weird implementation? I didn't find any explanation anywhere. Actually PyTorch's bilinear upsampling has the align_corner argument too, when you set it to True, it works well. But if you set it to False, it performs a differnet behaviour to Tensorflow's. I am totally confused with their implementations now (maybe just use align_corners=True will be fine).",https://stackoverflow.com/questions/51077930,9809485,Documentation Ambiguity
35565312,Is there a convolution function in Tensorflow to apply a Sobel filter?,Is there any convolution method in Tensorflow to apply a Sobel filter to an image img (tensor of type float32 and rank 2)? I've already seen tf.nn.conv2d but I can't see how to use it for this operation. Is there some way to use tf.nn.conv2d to solve my problem?,https://stackoverflow.com/questions/35565312,1112654,Documentation Replication on Other Examples
51089334,What is the difference between tf.keras.layers versus tf.layers?,"What is the difference between tf.keras.layers versus tf.layers? E.g. both of them have Conv2d, do they provide different outputs? Is there any benefits if you mix them (something like a tf.keras.layers.Conv2d in one hidden layer and in the next, tf.layers.max_pooling2d)?",https://stackoverflow.com/questions/51089334,9262788,Documentation Replication on Other Examples
69843239,How does tf.keras.util.array_to_image() work with regards to memory?,"I have image data that I want to use in a TensorFlow model, but I have to retrieve the image as an (Numpy) array of pixel values. From what I've read, TensorFlow has to read an image in as some image format and from some location. I know that tf.keras.util.array_to_image() can convert an array to a PIL instance of an image, and I know that there are several other libraries that have similar functionality, such as PIL.Image.fromarray(). My problem is that I don't want to duplicate the image data by copying it to a new format. The API documentation for the tf.keras.util.array_to_image() says that it returns a ""PIL image instance"". Does that mean that it is copying all array values to a new data structure and returning that, or is it creating an image data structure that references the original array pixel values? As a follow-up question, if the keras method does duplicate the data (by having both the original array and the image instance have independent values), is there a way to have TensorFlow accept an array representation of an image without needing to duplicate it as a separate image file?",https://stackoverflow.com/questions/69843239,11626498,Documentation Replication on Other Examples
51194912,How do I know which version of the Keras API is implemented in tf.keras?,"In the standard implementation of Keras, one can get the API version using keras.__version__. However, there is no tf.keras.__version__. So how do I check the version of the Keras API implemented in tf.keras?",https://stackoverflow.com/questions/51194912,1735003,Documentation Ambiguity
51248442,Behavior of the parameter 'throttle_secs' in tf.estimator.EvalSpec for use in tf.estimator.train_and_evaluate,"I am using tensorflow's train_and_eval function as in the example. Therefore i create an instance of tf.estimator.EvalSpec, according to According to its documentation the explanation of the parameter throttle_secs states that ""Of course, evaluation does not occur if no new checkpoints are available, hence, this is the minimum."" However, i observe a different behavior. If there is no new checkpoint and evaluation should be triggered according to the passed parameter a new checkpoint is created and evaluation is performed. Is this a bug or am i missing something here?",https://stackoverflow.com/questions/51248442,6456025,Documentation Replicability
51247229,"Cannot use custom dataset in official resnet model, raised tensorflow.python.framework.errors_impl.CancelledError","These days I modified the official cifar10_main.py , in order to train the kaggle dogs_cats_redux dataset. First, I created the tfrecord files, following the standard pipeline, you guys can download the tfrecord files here. And then, I wrote some tfrecord parsing functions and dogs_cats_model class, and the rest of the code remain the same as the original resnet repo, you guys can check my main.py here. But when I run the main.py, it raised the CancelledError: I've googled so many solutions, most of them say that's because somehow the data queue is stopped, or we didn't start the queue correctly, solutions like: But the official resnet_run_loop.resnet_main() using tf.estimator.Estimator to train the model, the source code doesn't need to start the queue like that, so how can we solve this problem? Any ideas would be appreciate. system info: ubuntu 16.04 LTS tensorflow-gpu v1.8.0 cuda 9.0 cudnn 7.1 The github issue is here.",https://stackoverflow.com/questions/51247229,8859796,Documentation Replication on Other Examples
51265030,Can tf.saved_model.simple_save be used to create a SavedModel for C++?,"I want to save a model for inference from C++ and I am looking into SavedModels as suggested by the documentation. Now tf.saved_model.simple_save seems to be a convenience function to easily create a SavedModel: from within a Session, provide the inputs, outputs and an export dir: When looking at this API, the function looks rather broad in application; however the documentation warns us that I am not familiar with Tensorflow serving, but it seems that these distinctions between Predict, Classify aso. relates to Tensorflow serving. Actually the documentaiton on tf.saved_model.simple_save says that it is a (emphasis mine). So tf.saved_model.simple_save can create SavedModels for serving, but my question is,",https://stackoverflow.com/questions/51265030,9973879,Documentation Replicability
51278422,Interpreting the FLOPs profile result of tensorflow,"I want to profile the FLOPs of a very simple neural network model, which is used to classify the MNIST dataset, and the batch size is 128. As I followed the official tutorials, I got the result of the following model, but I cannot understand some parts of the output. The images_iter and the labels_iter are the iterators of tf.data, which are similar to the placeholder. I used this code, which equals to scope -min_float_ops 1 -select float_ops -account_displayed_op_only in tfprof comments line tool, to profile the FLOPs and got the below result. My questions are I know it is discouraging to read a question so long, but a desperate boy who cannot find relating information from the official document needs your guys to help.",https://stackoverflow.com/questions/51278422,6156468,Lack of Alternative Solutions/Documentation
35689547,How to process single training file in parallel,"I have a file train.csv that contains paths to images and their labels. ie: After going through the reading data tutorial I came up with some code to go through each image, resize it and apply distortions: The problem now is that I'm confused how to do these operations across the file in parallel. The documentation suggests either using tf.train.batch_join or tf.train.batch with num_threads=N. I first tried following the example code using tf.train.batch_join but this seems to be intended for processing multiple files in parallel. In my case however I just have 1 file. I also tried setting tf.train.batch([example, label], batch_size, num_threads=8) but its not clear to me if this is doing the right thing (although I can see more cpu cores in use) Here is my code for executing the graph: Whats the best way to process this file in parallel?",https://stackoverflow.com/questions/35689547,2138200,Documentation Replication on Other Examples
69941348,does tf.data.Dataset .from_tensor_slices() preserve the order of examples?,"If I have a set of tfrecords, using .from_tensor_slices() here, will dataset created preserve the order of the data? For example, if I have 3 tfrecords (the first one contains 40 examples, the second one contains 30 examples, the third one contains 70 examples) called 1.tfrecord, 2.tfrecord, 3.tfrecord respectively, then I construct dataset = tf.data.Dataset.from_tensor_slices(['1.tfrecord', '2.tfrecord', '3.tfrecord']). During loading, will the order of these examples preserved?",https://stackoverflow.com/questions/69941348,11970084,Documentation Replicability
69942590,Load tf.data.Dataset with dynamic batch size,I am wondering if there is a way to load tf.data.Dataset with dynamic batch size instead of using fixed size like in here? Thanks,https://stackoverflow.com/questions/69942590,11970084,Documentation Replicability
51397198,Tensoflow Estimator: how to use tf.graph_util.convert_variables_to_constants,"I would like to know if it is possible to use the function tf.graph_util.convert_variables_to_constants (in order to store the frozen version of the graph) in a train/evaluation loop, while I'm using a custom estimators. For example:",https://stackoverflow.com/questions/51397198,6142101,Documentation Ambiguity
51407186,Invalid Syntax Error when creating estimator in Tensorflow,"I am attempting to use Tensorflow in a Colab Notebook to train identically structured neural networks with varying sizes of randomly chosen training examples from the MNIST data set. I have previously been successful in training neural networks in a similar fashion several times, but this time I am receiving an Invalid Syntax error that I have not been able to correct. The error appears for the following line of code: mnist_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn) I have used this same line of code successfully in a different notebook, and I have not been able to recognize any problems with the syntax (it's from the Tensorflow documentation found here: https://www.tensorflow.org/tutorials/layers) The code I am trying to execute can be found below (but I have omitted the definition of cnn_model_fn for the sake of brevity). Does anybody have any suggestions for why this error might be occurring? Thank you!",https://stackoverflow.com/questions/51407186,5910469,Documentation Replication on Other Examples
56166885,How to check evaluation auc after every epoch when using tf.estimator.EstimatorSpec?,"I defined my model using tf.estimator.EstimatorSpec. I know it has train, evaluation and prediction modes. But I want to check some metric scores such as auc after every epoch. Does this API support it like keras?",https://stackoverflow.com/questions/56166885,7185861,Inadequate Examples
56201490,What happens when @tf.function decorator compiles a function into graph? Why is it faster than that in eager mode?,"According to the TensorFlow document, @tf.function compiles a function into a graph, and ""makes you get the benefits of faster execution, running on GPU or TPU, or exporting to SavedModel."" The example in the document demonstrates such a benefit: output: What's the difference between a 'compiled graph' and a 'function in eager mode'? Why is the former faster when executed?",https://stackoverflow.com/questions/56201490,7238787,Documentation Replicability
56204555,Is there a simpler method to get slice of a tensors as shown in the following example?,I want to do slicing of tensors like the following slicing in numpy. How can I do that? Is there a simpler method to achieve this ? Can I use function tf.gather or tf.gather_nd to achieve this ? Many thanks!,https://stackoverflow.com/questions/56204555,8289138,Inadequate Examples
56212366,TensorFlow tf.data processing dev set after each epoch,"I am using tf.data with reinitializable iterator to handle training and dev set data. For each epoch, I initialize the training data set. The official documentation has similar structure. I think this is not efficient especially if the training set is large. Some of the resources I found online has sess.run(train_init_op, feed_dict={X: X_train, Y: Y_train}) before the for loop to avoid this issue. But then we can't process the dev set after each epoch; we can only process it after we are done iterating over epochs epochs. Is there a way to efficiently process the dev set after each epoch?",https://stackoverflow.com/questions/56212366,6217326,Documentation Replication on Other Examples
56213510,Is there an way to apply gradients for multiple output layers in tf.GradientTape?,"I'm trying to gradients apply over two output models, but the result indicates that the model does not learn and loss does not decrease, I need to your supported Thank you. @tf.function def train_step(inp, targ, intent, enc_hidden):",https://stackoverflow.com/questions/56213510,8053884,Documentation Replication on Other Examples
73645574,Why keras AUC returns zero when multi-label is set?,"I'm trying to understand how tf.keras.metrics.AUC(multi_label=True) works. From the docs, I'm led to understand that when working with multi-label vectors, each class is computed individually, then averaged. However, I can't seem to get the following trivial case to compute correctly. That is, if the prediction is the same as the expected vector, why is the output not 1.0?",https://stackoverflow.com/questions/73645574,774907,Documentation Ambiguity
56231695,When should tf.losses.add_loss() be used in TensorFlow?,I cannot find an answer to this question in the TensorFlow documentation. I once read that one should add losses from tf.nn functions but it isn't necessary for functions from tf.losses. Therefore: When should I use tf.losses.add_loss()? Example: Thank yoou.,https://stackoverflow.com/questions/56231695,7353970,Lack of Alternative Solutions/Documentation
73657854,Is SageMaker Distributed Data-Parallel (SMDDP) supported for keras models?,"Is SageMaker Distributed Data-Parallel (SMDDP) supported for keras models? In documentation it says ""SageMaker distributed data parallel is adaptable to TensorFlow training scripts composed of tf core modules except tf.keras modules. SageMaker distributed data parallel does not support TensorFlow with Keras implementation."" https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-modify-sdp.html But inside the training script and how to modify it, I can see the tf.keras and tf.keras.model is used. https://sagemaker.readthedocs.io/en/stable/api/training/sdp_versions/v1.0.0/smd_data_parallel_tensorflow.html",https://stackoverflow.com/questions/73657854,19947257,Documentation Replication on Other Examples
56247113,"why DNNClassifier doesn't accept Sparse features, but LinearClassifier does",tf.estimator.DNNClassifier doesn't accept sparse feature columns such as categorical_column_with_hash_bucket. It can only accept dense columns which always needed to be wrapped with indicator_column. tf.estimator.LinearClassifier on the other hand can accept categorical_column_with_hash_bucket directly. According to this tf source code Sparse features can be fed directly into linear models. They behave like an indicator column but with an efficient implementation. I wonder what such efficient implementation is there in Linear as against DNN which restricts DNN to only accept dense columns.,https://stackoverflow.com/questions/56247113,2277149,Documentation Ambiguity
56284927,tf.keras equivalent code block written in tf.contrib.slim,"I'm trying to re-implement a research paper code in tf.keras, in init block it was written as: I didn't find a equivalent in tf.keras.layer.Conv2D arguments for normalizer_fn=slim.batch_norm. How to achieve this in keras? I tried: Is this a valid equivalent to the above tf.contrib.slim code. With limited documentation of tf.contrib.slim, I'm really confused.",https://stackoverflow.com/questions/56284927,10544741,Documentation Replication on Other Examples
56286350,tf.keras.metrics.SpecificityAtSensitivity num_thresholds interpretation,"I'm trying to get my head around tf.keras.metrics.SensitivityAtSpecificity. I'm fine with the concept of sensity and specificity in isolation, but I'm unsure how the two are related in this single metric. More specifically, I'm unsure how to interpret the num_thresholds argument. The example in documentation has num_thresholds=1. Setting num_thresholds greater than 1 with the same input data seems to always return a metric value of 1.0.",https://stackoverflow.com/questions/56286350,3098092,Documentation Replication on Other Examples
73726034,Getting Attribute Error TensorDataset object has no attribute 'output_shapes' issue,Getting attribute error issue using tf.data.Dataset AttributeError: 'TensorDataset' object has no attribute 'output_shapes' Screenshot How to find output shapes?,https://stackoverflow.com/questions/73726034,13697791,Documentation Replicability
56312032,How to use Keras `add_loss` exclusively,"I am implementing a model that has a complicated loss term which requires the Keras add_loss function. I wish to implement my model as a tf.keras.Model (see documentation). Unfortunately, it seems like Keras cannot deal with the case where add_loss is used, but compile does not get a loss function specification. Here is a minimal example that ""works"" (doesn't do anything interesting, but no errors are raised): However, I don't need the ""external"" loss function, only one specified with add_loss, and this does not work. I had hoped that the following code does exactly the same thing as the one above: but it does not work. In particular, from m.fit(x) I get the error message One possible workaround is to add a trivial loss function but I had hoped for a more elegant solution. I am using Tensorflow 1.13.1.",https://stackoverflow.com/questions/56312032,6760298,Documentation Ambiguity
56321676,How to use tf.concat() function?,I found a bug in tf.concat. The problem is: I do not know how to correct it.,https://stackoverflow.com/questions/56321676,11559876,Documentation Replication on Other Examples
73744374,Does TensorFlow recalculate tensor if inputs do not change?,"Consider this simple example, where we execute the function get_d multiple times in a tf.scan call. get_d calls get_c, which depends on model parameters, which do not change during the tf.scan iteration. Do the operations in get_c get executed, even though the underlying tensors do not change? If so, what are options for optimizing the code? Is there something better/smarter than putting the tf.scan call inside Testclass?",https://stackoverflow.com/questions/73744374,7672162,Documentation Replicability
73752169,How to build custom model using tf.keras on TensorFlow 2.x that supports SageMaker distributed training?,"How to create custom models built using tf.keras on TensorFlow 2.x that support distributed training (multiple GPU instances) in Amazon SageMaker? E.g. using Distributed Data Parallel Library (DDPL)? The documentation mentioned that tf.keras is not supported by DDPL library, so that shouldn't be an option. I've seen examples of distributed training using Horovod: https://sagemaker-examples.readthedocs.io/en/latest/aws_sagemaker_studio/frameworks/keras_pipe_mode_horovod/keras_pipe_mode_horovod_cifar10.html",https://stackoverflow.com/questions/73752169,3203213,Documentation Replication on Other Examples
56344827,"in TF2, how do you save models/weights when not using the tf.keras API?","In the documentation it seems they focus on how to save and restore tf.keras.models, but i was wondering how do you save and restore models trained customly through some basic iteration loop? Now that there isnt a graph or a session, how do we save structure defined in a tf function that is customly built without using layer abstractions?",https://stackoverflow.com/questions/56344827,10038330,Lack of Alternative Solutions/Documentation
73778590,Eager execution inside lambda layer in Tensorflow,"I have Tensorflow 2.9.1 installed at my laptop, and according to the documentation the eager execution should be enabled by default. I have a problem while trying to convert Tensor object to Numpy Array inside a model. I keep getting 'Tensor' object has no attribute 'numpy' I wanted to have some Lambda Layers inside my model and do some operations using Numpy, but the eager execution seems to be disabled inside the model. I tried to run tf.executing_eagerly() inside model and it returned false. On the otherhand, when I tried to run tf.executing_eagerly() outside the mode, I got true. Could someone clear my confusion here?",https://stackoverflow.com/questions/73778590,20036706,Documentation Replication on Other Examples
73794766,what is the meaning of axis=-1 in tf.keras.layers.Normalization?,"I'm trying to learn deep learning using keras and tensorflow and I came across a code explaining linear regression at https://www.tensorflow.org/tutorials/keras/regression wherein they have created a normalization layer using normalizer = tf.keras.layers.Normalization(axis=-1). Someone please explain the meaning of axis =-1 . I tried looking at the API documentation but I couldnt understand the explanation from there?I know that axis=0 represent rows and axis=1 columns, right? Thanks in advance",https://stackoverflow.com/questions/73794766,19986715,Documentation Completeness
56385622,Keras model.get_config() returns list instead of dictionary,"I am using tensorflow-gpu==1.10.0 and keras from tensorflow as tf.keras. I am trying to use source code written by someone else to implement it on my network. I saved my network using save_model and load it using load_model. when I use model.get_config(), I expect a dictionary, but i""m getting a list. Keras source documentation also says that get_config returns a dictionary (https://keras.io/models/about-keras-models/). I tried to check if it has to do with saving type : save_model or model.save that makes the difference in how it is saved, but both give me this error: my code block : my pip freeze :",https://stackoverflow.com/questions/56385622,8668650,Documentation Replication on Other Examples
56386901,Example for tf. group_by_reducer?,Can someone show me an example of tf.data.experimental.group_by_reducer? I find the documentation tricky and couldn't understand fully. How can I use it for calculating average?,https://stackoverflow.com/questions/56386901,7418127,Documentation Replicability
56419668,What's the difference between tf.random.normal and tf.distributions.Normal?,What's the difference between tf.random.normal and tf.distributions.Normal? Or the difference between tf.distributions.Multinomial and tf.random.multinomial or anything similar? Is tf.distributions.Normal used as the backend for tf.random.normal?,https://stackoverflow.com/questions/56419668,4918159,Documentation Replication on Other Examples
56452714,"replacement of ""tf.gather_nd""","I am doing a project but their tensorflow version does not support tf.gather_nd. I am asking if possible that use tf.gather, tf.slice or tf.strided_slice to rewrite a function of tf.gather_nd? tf.gather_nd is used to gather slices from a tensor into a Tensor with shape specified by indices. details can be found in https://www.tensorflow.org/api_docs/python/tf/gather_nd Thanks,",https://stackoverflow.com/questions/56452714,11601605,Documentation Replicability
56458133,Tensorflow error when adding writing summaries 'Tensor' object has no attribute 'value',"this is my code: It runs fine when the commented parts are commented out. However, I don't get why whenwriter.add_summary() is uncommented out I get the following error: It doesn't make sense to me. The docs for the FileWriter class clearly states that it takes in a list of String tensors, and the docs for the tf.summary.scalar says that it returns a tensor of type string with summary protocol, so it seems like it should work. Here's what I've tried, by people who seemed to have the same issue as me, but nothing worked: Tensorboard expects a string for summary, adding eval() didn't work. The value error is raised because you have to evaluate the summary node within a session. I am already running it in session. Merges all summaries in the default graph. Initially, I was using merging all summaries, but now I switched to tf.summary.merge. Coming from improperly connected tensors. I am using tf.matmul and tf.add here instead of regular operands. Another thing I tried was putting tensor objects into the second argument of tf.summary.scalar, something like replacing the commented part of the code above like this: But event that didn't seem to work. Why doesn't my above code work, while this code here works? I don't see a difference. Also, I am using Google Colab, which is why I'm importing tensorboardcolab.",https://stackoverflow.com/questions/56458133,9721336,Documentation Ambiguity
56465794,Re-write TensorFlow into Keras with tf.keras,I wan to re-write TensorFlow code into Keras. I just wonder if you can use for this purpose the tf.keras.layers to just replace the tf.layers? Like to: Can I re-write TensorFlow to Keras in this way? Does this define a proper Keras model where you can use the model.fit method?,https://stackoverflow.com/questions/56465794,9451356,Documentation Replication on Other Examples
73900721,Confusion regarding num_heads & key_dim keras.layers.MultiHeadAttention in the transformer tutorial,"In the tf.keras tutorial: https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb, According to the code comment and its documentation, the relation between d_model and key_dim should have been such that d_model/key_dim = num_heads. This convention is also used in the original paper https://arxiv.org/pdf/1706.03762.pdf (on top of page 5): So is the configuration ""wrong"" in this tutorial, and it should have been:",https://stackoverflow.com/questions/73900721,1762295,Documentation Replication on Other Examples
56547737,Nested tf.function is horribly slow,"Within a function decorated with tf.function, I try to call another function decorated with tf.function. The result is horribly slow. Is that because I am not suppose to use python native types in the function? Tensorflow 2.0 model using tf.function very slow and is recompiling every time the train count changes. Eager runs about 4x faster Test:",https://stackoverflow.com/questions/56547737,8704463,Documentation Replicability
56553579,How to export Estimator's best model?,"I am training a simple CNN based on a Custom Estimator with TF Records. I am trying to export the best model in terms of validation loss during the train_and_evaluate phase. According to the documentation of the tf.estimator.BestExporter, I should feed a function that returns a ServingInputReceiver but after doing so, the train_and_evaluate phase crashes with a NotFoundError: model/m01/eval; No such file or directory. Seems like if the BestExporter does not permit saving the evaluation results as it would do without the exporter. I tried with different ServingInputReceiver but I keep getting the same error. As defined here: and here Here are my exporter and training procedure: This is a gist with the output. What's the correct way to define a ServingInputReceiver for the BestExporter?",https://stackoverflow.com/questions/56553579,3674176,Documentation Ambiguity
56552397,Custom metric: Using scikit learn's AucRoc Calculator with tf.keras,"I'm training a multilabel classifier using tf.keras and horovod that has 14 classes. AucRoc is used as the metric to evaluate the performance of the classifier. I want to be able to use scikit learn's AucRoc calculator as mentioned here: How to compute Receiving Operating Characteristic (ROC) and AUC in keras?. If I feed the tensors as is for the following function: I get an error that looks like this: I'm trying to convert tf tensors into a numpy array and then feed them to the roc_auc_score method like so: I get the following error: I've also tried tensorflow's https://www.tensorflow.org/api_docs/python/tf/metrics/auc like so: It works just fine. However, it gives me a single number for aucroc. I wonder what that number represents, is it an average aucroc value for all the 14 classes? or max aucscores of all the classes? or how does it get to a single number? 1) How do I fix the error with roc_auc_score? 2) What does that single number represent?",https://stackoverflow.com/questions/56552397,10287380,Lack of Alternative Solutions/Documentation
74005009,How to create output_signature for tensorflow.dataset.from_generator,"I have a generator yielding data and labels yield data, labels where the data is an numpy.ndarray with variable rows and 500 columns of type dtype=float32 and the labels are integers of numpy.int64. I'm trying to pass this data into TensorFlow from_generator function to create a TensorFlow dataset: tf.data.Dataset.from_generator The docs say that the from_generator function needs a parameter output_signature as an input. But I'm having trouble understanding how to build this output_signature. How can I make the output_signature for the generator I described? Thank you! Edit: I used tf.type_spec_from_value to get this: But is it correct to use None when the number of rows is varying for the first data type?",https://stackoverflow.com/questions/74005009,2300622,Documentation Replicability
56606757,Tensorflow: output of multi-step decay function returns a TypeError,"We are trying to write a multi-step decay function in Tensorflow using tf.train.piecewise_constant() as suggested here. Tensorflow documentation here states that: ""When eager execution is enabled, this function returns a function which in turn returns the decayed learning rate Tensor"" However, when we tried running the code, it returned a TypeError. It returns the same error even when lr() is used. The code works as expected when we provide a constant learning rate. Is there something that we are missing?",https://stackoverflow.com/questions/56606757,5079359,Documentation Ambiguity
74029376,Tensorflow custom reduction function with axis support,"I would like to get the value with the maximum absolute value in a tensor, with respect to an axis. Note that I don't want the maximum absolute value, I want the value that has the maximum absolute value (so I need to keep the sign). Ideally, I would like something similar to reduce_max or reduce_min: but I did not find anything useful in tensorflow documentation. With a flat tensor, I know that I could use tf.foldl or tf.foldr: However, I don't know how to handle an axis parameter in the case of multidimensional tensors.",https://stackoverflow.com/questions/74029376,18159603,Lack of Alternative Solutions/Documentation
56621039,Understanding the graph of tf.cond,"I am trying to understand the inner workings of tf.cond by looking at the graph of low-level ops that are used to build it. This is the code in question And the resulting graph. What I am wondering is the following. Thank you, S",https://stackoverflow.com/questions/56621039,1620643,Documentation Replicability
76153107,Difference between tf.Module and tf.keras.Model,I know both tf.Module and tf.keras.Model are used for building custom models. But what's the difference between both of them? Which one should be used when becuase there usage looks similar as shown in tensorflow docs?,https://stackoverflow.com/questions/76153107,21760922,Documentation Ambiguity
76222858,TypeError: Failed to convert elements of <keras.losses.BinaryCrossentropy object at 0x7f1f6c0a4460> to Tensor,"I am trying to make a custom loss function, which doesn't exactly look like this, but I am trying a simplified version first, in effect . The following works: model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(14,)), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dense(1, activation='sigmoid')]) model.compile(optimizer=Adam(),loss=keras.losses.BinaryCrossentropy(), metrics=['accuracy']) But, if I try to make my own custom loss function, I get an error: TypeError: Failed to convert elements of &lt;keras.losses.BinaryCrossentropy object at 0x7f1f6c0a4460&gt; to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes. Eventually I want to add a loss term to cat_loss, but I can't even return cat_loss here. The shapes of my data are: print(X_train.shape, X_test.shape, y_train.shape, y_test.shape) (326080, 14) (166106, 14) (326080, 1) (166106, 1) Thank you! I tried to make my own custom loss function.",https://stackoverflow.com/questions/76222858,13826445,Documentation Replication on Other Examples
76227668,How can I combine a py_function inside a map function?,"I wanted to combine a py_function inside a map function, which took me a day, despite chatGPT's assistance. Since resizing an image with tf.image has implementation differences in relate to openCVs, I wanted to keep using the optimized tf.Dataset with the .map API, but also combine the opencv.resize API.",https://stackoverflow.com/questions/76227668,13805655,Documentation Replicability
76244271,How to connect multiple instances of computers and run TensorFlow distributed strategy on it?,"There is a very little idea about the type of hardware/software that can be used with Distributed training on TensorFlow. I've access to some supercomputer instances (pardon me for ignorance, this isn't the field where I usually work, so bit daunting for me), the problem is to train some very large model on it. The documentation featuring distribution strategy: https://www.tensorflow.org/guide/distributed_training doesn't gives much idea about the devices that are supported on TensorFlow: CPU clusters/GPU clusters or TPU Clusters. I hope it supports multiple machines with no GPU/TPU access. Second, it doesn't give much idea about configuration of machines for the use either - I suppose if they are connected on Ethernet or Kubernetes (or something like that), they should show up in mirrored_strategy = tf.distribute.MirroredStrategy() as a list? Right? So, basically no third party installation is needed in the cluster nodes or anything like that, one ethernet cable should suffice? (I guess). Thank You",https://stackoverflow.com/questions/76244271,14515133,Requesting (Additional) Documentation/Examples
76319875,Tensorflow 2.0 how to get value of a tensor in a tf.function,"I know that we can use .numpy() to get value of a tensor. But it's not allowed in tf.function since it's in graph mode. See this github issue. Is there any workaround that I can keep the tf.function and still able to get value of a tensor? I tried tf.Eager_execution(), and it's not allowed either.",https://stackoverflow.com/questions/76319875,7400194,Documentation Replicability
76324368,Understanding tf.keras.layers.Dense(),"I am trying to understand why there is a difference between calculating a dense layer operation directly and using the keras implementation. Following the documentation (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) tf.keras.layers.Dense() should implement the operation output = activation(dot(input, kernel) + bias) but result and result1 below are not the same. output Using test.get_weights() I can see that the kernel and bias (b) are getting set to the correct values. I am using TF version 2.12.0.",https://stackoverflow.com/questions/76324368,18338104,Documentation Replication on Other Examples
76334449,"Custom Tensorflow transformer only produces beginning of sentence token, despite being trained on 200,000 examples","I have been facing this problem for a few weeks now, and I think it is time to ask the community for help. I am training a custom transformer model to perform the task of guided summarization. I am using tensorflow and the xsum summarization dataset. Briefly put, I have created a modified x sum dataset by using a textrank algorithm to find keywords in the 'document' feature of the xsum dataset and include them as a new feature 'guidance'. Therefore, the custom dataset I have has the following features: 'document', 'summary', and 'guidance'. I am using the BertTokenizerFast to tokenize the data, and generally follow the function process_data_to_model_inputs in this tutorial BERT2BERT for CNN/Dailymail found in this google Colab. After mapping this to a huggingface dataset, I use the .to_tf_dataset()tensorflow method to transform the huggingface datasets to tensforflow datasets, which all works correctly. The tensors output correctly and are shaped well. In regards to my tensorflow transformer from scratch I follow this tensorflow tutorial on Neural Machine translation. My model is based on this tutorial, but it has two encoders one for the guidance feature and one for the document feature, and one decoder. I use the attention mechanism provided by tensorflow tf.keras.layers.MultiHeadAttention for the computation of attention throughout the model. To view the output, I developed a summarization class that decodes the predicted ID's into English summaries. I also changed the padding token to the length of the tokenizer, rather than the value -100 or the Bert tokenizer's pad token due to issues with -100. The modified xsum train dataset has 204045 total examples, the validation set has 11360 total examples. Originally, to test the functionality of the model I only used 20 examples to ensure that the model was able to process the data and produce an output. The model was able to successfully produce summaries, although the summaries did not make grammatical sense. I could see that it was learning the structure of a sentence and words. with 20 examples the model would overfit and plateau around 0.78 val_accuracy. The interesting issue was that as I increased the epochs to 40 with the 20 examples and the Training Loss and Validation accuracy would converge the model would stop producing any output. Not sure why, so I decided to use the entire train and validation dataset mentioned above. This is the training output, with a batch size of 8: And the model produces no output aside from the beginning of sentence token [CLS]. This is the code I am using to produce the summaries This is the output: These are all my hyperparemeters model and optimizers, I have experimented with 1-8 layers, 1-8 number of attention heads, vocab sizes from 50,000 - 200, 000 and dropout rates from 0.3-0.6. Finally, here is a picture of my model.summary() Model Summary Can anyone help me understand why my model seems to be overfitting somewhere? I changed the pad token in the tokenization process from -100 to the length of the BERTTokenizerFast: len(tokenizer) token because my thinking is that the model will not predict such a high number. When I decode the pad token in my [label] feature it equates to ' '. This could be what my model is predicting, which is why it looks like nothing is being predicting but the beginning of sentence token. When I had the actual pad token provided by huggingface, and trained with 20 examples with over 40 epochs the model predicted the pad token consistently. I have experimented with a batch size of 8 and 32, I want to use a smaller batch size for the model to learn more intricate patterns from the training data. Any help would be much appreciated. Thank you :)",https://stackoverflow.com/questions/76334449,10107984,Documentation Replication on Other Examples
76380927,Tensorflow decode image,"I am a beginner in tensorflow and I am training a small cnn, I am using the tf.io.decode_image function but I can't figure out if this function does preprocess. The tensorflow documentation about it doesn't say anything. When I open images with this function the values are between 0 and 1. The images are single channel grayscale. This is the code. I would like to have more explanations",https://stackoverflow.com/questions/76380927,15460221,Requesting (Additional) Documentation/Examples
76391276,Custom gradient for broadcasting operation,"I have an operation for which I want to define a custom gradient with tf.custom_gradient. The operation takes two broadcastable arguments and produces a result with the broadcasted shape. The problem is how to handle the broadcasting rules ""backwards"" in the custom gradient. Let's take the example for a multiplication operation from the documentation of tf.custom_gradient: I can use this gradient alright for the non-broadcasting case: However, when the inputs are broadcasted, the result is not correct: In fact, trying to use it in graph mode fails: Is there a way to handle this ""unbroadcasting"" of the input gradients automatically?",https://stackoverflow.com/questions/76391276,1782792,Documentation Ambiguity
76396532,"Ragged tensors in dataset, tensorflow, how do I train the model","I have in my model, for fitting, and my dataset contains ragged tensors, basically context and x are ragged tensor, of variable length, everything I try gives me some sort of error, for example first I tried [ [ context, x], ...] where all the arrays were np.ndarray, but it said something along the lines that np.ndarray is an unrecognised datatype and cannot be converted to a tf.Tensor.Then when I tried putting this in a tf.data.Dataset, it says ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type tensorflow.python.framework.ops.EagerTensor). I am totally lost on how to train my model I tried several different data types from list, to tf.data.Dataset, but none of them were working, and now I am totally in the dark. Consulting the documentation has not helped me figure out how does the fit function actually treat data",https://stackoverflow.com/questions/76396532,13154958,Documentation Replicability
76444107,Are 'validation_steps' used if the validation_dataset is 'DirectoryIterator'?,"I was trying to use the Keras API to train a given model by using its fit function. In its documentation we can see the following: By reading the above text I understand that if the validation_data is not a tf.data dataset the validation_steps is ignored. However I am not sure if in my case this principle is applied. I am using an ImageDataGenerator to then use its flow_from_directory function that returns a DirectoryIterator. In its documentation we can see the following: So, by reading this I am convinced that the whole validation dataset is going to be used despite the fact that validation_steps might be set to a given number. Is there a way I can check whether it is using the whole dataset or only the given validation_steps? Does it simply ignore the validation_steps because the validation_data is not a tf.data and therefore there is no need to check it? Thanks! Code example of usage: PS: I know that we can let validation_steps be None to use it fully for validating the model in each epoch. My question is especifically if the validation_steps parameter is ignored or not if provided while using data from a ImageDataGenerator.",https://stackoverflow.com/questions/76444107,11875606,Documentation Replication on Other Examples
42334855,state output from tf.nn.dynamic_rnn operation,"For this code snippet: I was expected the last time index from rnn_out to be equal to state. Or, perhaps the tanh of the state. But this isn't what I am seeing - they don't match. In the context of this RNN recurrence relation, what value does state contain? h(t) = tanh[b + Wh(t-1) + Ux(t)] The answer here, implies the last time index of rnn_out and state should be equal (but they are not): for the tf.nn.rnn_cell.BasicRNN,what's the difference between the state and output The TF documentation isn't clear to me on this point.",https://stackoverflow.com/questions/42334855,7590388,Documentation Ambiguity
67557800,How to increase the rank (ndim) of input of BERT keras hub layer for learning-to-rank,"I am trying to implement a learning-to-rank model using a pre-trained BERT available on tensorflow hub. I am using a variation of ListNet loss function, which requires each training instance to be a list of several ranked documents in relation to a query. I need the model to be able to accept data in a shape (batch_size, list_size, sentence_length), where the model loops over the 'list_size' axis in each training instance, returns the ranks and passes them to the loss function. In a simple model that only consists of dense layers, this is easily done by augmenting the dimensions of the input layer. For example: ...now the model will perform 6 forward passes over vectors of length 10 before calculating the loss and updating gradients. I am trying to do the same with the BERT model and its preprocessing layer: But when I try to change the shape of 'text_input' to, say, (6), or meddle with it in any way really, it always results in the same type of error: As per https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer, it seems like you can configure the input shape of hub.KerasLayer via tf.keras.layers.InputSpec. In my case, I guess it would be something like this: When I run the above code, the attributes indeed get changed, but when trying to build the model, the same exact error appears. Is there any way to easily resolve this without the necessity to create a custom training loop?",https://stackoverflow.com/questions/67557800,14913631,Documentation Replication on Other Examples
57711285,How can I get global step in tensorflow.estimator?,"Does anyone know, how can I get the global step count in estimator definition? I need it to adjust the learning rate, when the optimizer created. like the example below: And does tf.train.get_global_step work?",https://stackoverflow.com/questions/57711285,10846167,Documentation Replication on Other Examples
59729634,"In tensorflow 2.0, how to compute gradients of loss to input variables?","In tensorflow 2.0, tf.gradients is not supported, and GradientTape only compute gradients to trainable weights, so how to get gradients to input as TF1.0 can do? thanks, correct me if i'm wrong.",https://stackoverflow.com/questions/59729634,12708383,Documentation Ambiguity
65525687,My model.fit and model.evaluate are not working properly and I am getting an error,"InvalidArgumentError: indices[1] = [0,40295] is out of order. Many sparse ops require sorted indices. Use tf.sparse.reorder to create a correctly ordered copy.",https://stackoverflow.com/questions/65525687,14920348,Documentation Replicability
47775244,Difference between tf.nn.convolution and tf.nn.conv2d,Could someone explain me the difference between tf.nn.convolution and tf.nn.conv2d ?,https://stackoverflow.com/questions/47775244,6627095,Documentation Replication on Other Examples
64081367,Slicing a tensor with a tensor of indices and tf.gather,"I am trying to slice a tensor with a indices tensor. For this purpose I am trying to use tf.gather. However, I am having a hard time understanding the documentation and don't get it to work as I would expect it to: I have two tensors. An activations tensor with a shape of [1,240,4] and an ids tensor with the shape [1,1,120]. I want to slice the second dimension of the activations tensor with the indices provided in the third dimension of the ids tensor: I have given it the axis=1 option since that is the axis in the activations tensor I want to slice. However, this does not render the expected result and only gives me the following error: I have tried various combinations of the axis and batch_dims options, but to no avail so far and the documentation doesn't really help me on my path. Anybody care to explain the parameters in more detail or on the example above would be very helpful! Edit: The IDs are precomputed before runtime and come in through an input pipeline as such: They are then reshaped into the previous format: Afterwards they have the previously mentioned shape (batch_size = 1 and num_neighbours = 30 -&gt; [1,1,120]) and I want to use them to slice the activations tensor. Edit2: I would like the output to be [1,120,4]. (So I would like to gather the entries along the second dimension of the activations tensor in accordance with the IDs stored in my ids tensor.)",https://stackoverflow.com/questions/64081367,8183516,Documentation Ambiguity
45077445,How to use method recover_last_checkpoints of tf.train.Saver()?,"The documentation writes that a list of checkpoint paths should be passed to it, but how to get the list? By hard coding? No, it's a silly practice. By parsing the protocol buffer file (a file named as checkpoint in your model directory)? But tensorflow does not implement a parser, does it? So do I have to implement one by myself? Do you have a good practice to get the checkpoint paths list? I raise this question because these days I am troubled by one thing. As you know, a days-long training may crash for some reason, and I have to recover it from the latest checkpoint. Recovering training is easy, since I just need to write the following code: I can hard code latest_checkpoint, or somewhat wiser, use tf.train.latest_checkpoint(). However, a problem arises after I recover the training. Those old checkpoints files that are created before crash are left there. The Saver only manages the checkpoint files created in one run. I hope it could also manage the previously created checkpoints files so they would be automatically deleted, and I don't have to manually delete them every time. I think such repeating work is really silly. Then I find the recover_last_checkpoints method in class tf.train.Saver(), which allows Saver to manage old checkpoints. But it's not handy to use. So is there any good solution?",https://stackoverflow.com/questions/45077445,8269216,Lack of Alternative Solutions/Documentation
64093750,Example of inferencing a Tensorflow lite model with parsing_serving_input_receiver_fn using C++ API,"I have followed the Tensorflow2 documentation to convert my trained tf.estimator model to tflite model; in order to convert my model, first I had to save my model in saved_model format with an input_receiver_fn and then convert it with SELECT_OPS flag: I wanted to run my tflite model on an ARM device without python support so I built the C++ interpreter shared libs with Bazel as it is explained in the documentation : Cross-compile for armhf with Bazel Select TensorFlow operators C++ My model has 3 input features but when I try to use the following guide for inferencing I get a segmentation fault. I used the following code to extract my model details: I got the following output: first few lines of the output of tflite::PrintInterpreterState(interpreter.get()) are: The output illustrates that the input shape is not the same as the original model, also the input type is &lt;class 'numpy.bytes_'&gt; but the Tensorflow 2 model inputs are [numpy.float32, numpy.float32, numpy.float32]. my input dictionary for prediction in TF2 model is something like : {'feature0' : data0, 'feature1' : data1, 'feature2' : data2} here is the Google Colab link to the Tensorflow model I didn't have previous experience with inferencing TensorFlow Lite models so I searched first and found out these related questions that helped me write below C++ code: TensorFlow Lite C++ API example for inference How to give multi-dimensional inputs to tflite via C++ API I tried to fill the input buffer with a vector of zeros but it was without success. Here is my C++ code to load a tflite model and feed it inputs for prediction. can someone please point me to the right direction since I could not find any examples or related documentation for feeding inputs to converted tf.estimator with a serving_input_fn. I also asked this question in Tensorflow's GitHub and got a comment mentioning that I have to feed my inputs in the form of an ""example proto"", now the problem is reduced to what is an ""example proto"" and how can one feed inputs to a tflite model in from of an example proto? Github issue link",https://stackoverflow.com/questions/64093750,11358743,Inadequate Examples
64095538,Can I replace tf.keras.backend with tf?,"Implementations in tf.keras.backend have duplicates in pure tensorflow. For example: tf.keras.backend.ones vs tf.ones. My question: Can I use tensorflow instead of tf.keras.backend, by just replacing it? Both are same API?",https://stackoverflow.com/questions/64095538,5102533,Documentation Replicability
64096624,what is the difference between using softmax as a sequential layer in tf.keras and softmax as an activation function for a dense layer?,what is the difference between using softmax as a sequential layer in tf.keras and softmax as an activation function for a dense layer? and,https://stackoverflow.com/questions/64096624,11995630,Documentation Replication on Other Examples
64100466,How to use tf.Dataset in Keras model.fit without specifying targets?,"I want to use an AutoEncoder model with Keras functional API. Also I want to use tf.data.Dataset as an input pipeline for the model. However, there is limitation that I can pass the dataset to the keras.model.fit only with tuple (inputs, targets) accroding to the docs: So here is the question: can I pass the tf.data.Dataset without repeating inputs like that (inputs, inputs) and more like (inputs, None). And if I can't, will the repeated inputs double the GPU memory for my model?",https://stackoverflow.com/questions/64100466,9048235,Documentation Replication on Other Examples
45090843,Does sequence_length help performance of dynamic_rnn?,"In Google's recent nmt tutorial, they say this: with this code: encoder_outputs, encoder_state = tf.nn.dynamic_rnn( encoder_cell, encoder_emb_inp, sequence_length=source_seqence_length, time_major=True) However, I was reading dynamic_rnn's documentation and it says: I'm just wondering if sequence_length really helps performance of dynamic_rnn, e.g. they do some kind of dynamic bucketing? If they do, is there any place where I can read more about it? Thanks a lot.",https://stackoverflow.com/questions/45090843,5029595,Requesting (Additional) Documentation/Examples
51687832,Probability Distribution for tf.nn.softmax_cross_entropy_with_logits_v2,"I am trying to understand the Tensorflow documentation better for tf.nn.softmax_cross_entropy_with_logits_v2(). In the documentation, it states: While the classes are mutually exclusive, their probabilities need not be. All that is required is that each row of labels is a valid probability distribution. If they are not, the computation of the gradient will be incorrect. Does this mean that, for my labels, I shouldn't be simply using one-hot encoding, but should also account for the number of instances of each label? For example, if I have 2 classes, and there are 90 examples for class ""A"" and only 10 examples for class ""B"", should my label for a class A be [0.9, 0.1], instead of just [1, 0]? I hope this makes sense. Thanks!",https://stackoverflow.com/questions/51687832,10012553,Documentation Replication on Other Examples
51690095,how to gather element with index in tensorflow,"For example, if I use extract = tf.gather_nd(values, index) the return is However, I want the result is where the index is along axis = 1, however, there is no axis parameter setting in tf.gather_nd. What should I do? Thanks!",https://stackoverflow.com/questions/51690095,5020881,Documentation Ambiguity
51706848,How does tf.reshape() work internally ?,"I'm trying to understand how tf.reshape works. Let's have an example: Here I have a 2D tensor M_2D whose columns represent coefficients for the N0 embeddings of dimension N1. I want to create a 3D tensor where each column of M_2D is placed in the first dimension of M_3D, and columns are keep in the same order. My final goal is to create a 3D tensor of 2D embeddings, each weighted by the columns of M_2D. How can I be sure that reshape actually place each column in the new dimension of M_3D. Is it possible that it places the rows instead ? Is there somewhere in tensorflow documentation a clear explanation on the internal working process of tf.reshape, particularly when -1 is provided ?",https://stackoverflow.com/questions/51706848,6084245,Documentation Completeness
45107068,How to fine-tune model using `MonitoredTrainingSession` / `Scaffold`,"I want to restore model parameters of VGG_19 which is used as feature extractor for an appended newly initialised graph and train everything in a distributed setup. Everything works if I use slim.learning.train, but I am not able to get it to work with the Scaffold required by tf.train.MonitoredTrainingSession. If I pass a restore_fn (created using tf.contrib.framework.assign_from_checkpoint_fn as in documentaiton)as init_fn into the Scaffold I am getting TypeError: callback() takes 1 positional argument but 2 were given I tried ""fixing"" it by passing a lambda scaffold, sess: restore_fn(sess). If I try to create a restore operator and pass it in as init_op (created with tf.contrib.slim.assign_from_checkpoint I am getting I tried using a local_init_op, too, which did not work. My code:",https://stackoverflow.com/questions/45107068,2156909,Documentation Replication on Other Examples
45115650,"How to find Tensorflow max value index, but the value is repeat","A tensor array is: array = [1, 1, 0, 1, 1, 0] If I use tf.argmax(), its can find only the first index. output =&gt; ""0"" But I want find the max value at last index. output would be ""4""",https://stackoverflow.com/questions/45115650,7625321,Documentation Replicability
45401311,What are channels in tf.nn.conv2D?,"I've looked through some great explanations on what different arguments of tf.nn.conv2D represent, but I still can't understand what exactly in_channels and out_channels represent. Could someone please clarify this for me?",https://stackoverflow.com/questions/45401311,4588128,Documentation Replicability
51762406,"What is the Tensorflow loss equivalent of ""Binary Cross Entropy""?","I'm trying to rewrite a Keras graph into a Tensorflow graph, but wonder which loss function is the equivalent of ""Binary Cross Entropy"". Is it tf.nn.softmax_cross_entropy_with_logits_v2? Thanks a lot!",https://stackoverflow.com/questions/51762406,3337758,Documentation Replication on Other Examples
36570729,tf.IndexedSlicesValue when returned from tf.gradients(),"I'm having the following problem, I have four embedding matrices and want to get the gradients of my loss function with respect to those matrices. When I run the session to return the values for the gradients, two of those returned objects are of type tensorflow.python.framework.ops.IndexedSlicesValue, the other two are numpy arrays. Now for the numpy arrays, their shape corresponds to the shape of their corresponding embedding matrix, but I'm having problems with the IndexedSlicesValue objects. If I call .values on one of those objects, I get an array whose shape does not match that of the gradient, the shape of the embedding matrix is [22,30], but calling .values on the IndexedSlicesValue object I get an array with shape [4200,30] ( The shape of my input tensor had dimensions of [30,20,7], the product of those dimensions equals 4200, not sure if this is relevant). The IndexedSlicesValue object has an attribute called dense_shape, which is an array that holds the dimensions the gradient should have, i.e. array([22,30]) is value returned by .dense_shape. I don't really understand the docs here: https://www.tensorflow.org/versions/r0.7/api_docs/python/state_ops.html#IndexedSlices It says: So this array of shape (4200,30) is extracted from an array corresponding to an even larger, dense tensor? What exactly is the gradient in this IndexedSlicesValue object and why does tensorflow automatically use this type for some gradients returned by tf.gradients()? Here is my code:",https://stackoverflow.com/questions/36570729,3042790,Documentation Replication on Other Examples
64424397,Keras - Custom layer with multiple inputs,"I would like to implement a custom tf.keras layer called MyLayer which has three inputs and contains a sub layer which in turn has three inputs, like in the figure below: I assume that the right thing to do would be to create a MyLayer class that extends tf.keras.layers.Layer and implement the __init__, build and call methods, as mentioned in the official documentation. Now, the examples provided in the documentation are relative to pretty simple layers that are composed of several sublayers connected in a sequential manner, that is one after the other. For instance, the MLPBlock layer consists of 3 linear layers ordered sequentially. In general, however, sublayers are not ordered sequentially, but can form branches. This suggests that those layers could be run in parallel, since they are not connected to one another. Going back to the custom layer I would like to implement, you can see that Layer1, Layer2 and Layer3 could be run in parallel. Once their outputs are computed, they can be fed to Layer4. The point is: how do I run them in parallel? I couldn't find any ""ParallelCombinator"" or things like that among the available Keras layers. If I were to follow the examples provided in the documentation, I would write something along these lines: This, however, would imply that Layer1, Layer2 and Layer3 are run sequentially, not in parallel. One possible solution that I came up with involves structuring MyLayer as a tf.keras.Model built with Keras's functional API rather than as a subclass of tf.keras.Layer, like so: The reason why I think this would work is that I assume that when I feed some inputs to a tf.keras.Model, as in output = my_layer([input_1, input_2, input_3]), the layers that can be run in parallel are effectively run in parallel (or are they?). This solution, however, feels like a hack to me, as MyLayer is supposed to be a layer, not a model. In fact, a tf.keras.Model instance exposes methods like fit(...) that aren't meant to be called on a layer. Does anybody know what's the best approach to implement MyLayer?",https://stackoverflow.com/questions/64424397,9079812,Documentation Replication on Other Examples
45428557,Tensorflow: How to make return value of tf.unique same size as input,"According to https://www.tensorflow.org/api_docs/python/tf/unique, tf.unique(x) returns a tuple (y, idx), The shape of y is (?, ) is not known during build time. Is there anyway I can pad y to match the input size x?. For example, I wanna make y = [1, 2, 4, 7, 8, 0, 0, 0, 0]",https://stackoverflow.com/questions/45428557,7765065,Inadequate Examples
64474463,Custom f1_score metric in tensorflow,I want to implement the f1_score metric for tf.keras. I got an error:,https://stackoverflow.com/questions/64474463,2130515,Documentation Replicability
36631868,Tensorflow: Noise contrastive estimation language model,"I want to change the loss function in the ptb_word_lm.py example to tf.nn.nce_loss. Looking at the tf.nn.nce_loss implementation: I think But I do not know what are the first two parameters, weights and biases. How could I adapt tf.nn.nce_loss to language model? Thanks. @Aaron: Thanks, I have tried the following: According to the document at here: So, My PTBModel model looks like However, I got an error Did I miss anything here? Thanks again.",https://stackoverflow.com/questions/36631868,200340,Documentation Replication on Other Examples
64496955,"Using albumentation's augmentation in tensorflow dataset API is giving this error : Incompatible shapes expected [?,224,224,3] but got [8,1,224,224,3]",I am getting this error while trying to augment the images using the albumentations library which uses tf.numpy_function to wrap the python function for augmentation in tensorflow from this link :https://albumentations.ai/docs/examples/tensorflow-example/ I have loaded my dataset of images and target label using tensorflow dataset API. The code : The full error message: Can someone can at least tell me why this error is happening? Thanks in advance!,https://stackoverflow.com/questions/64496955,10432635,Documentation Replicability
52447384,Estimator API: AttributeError: 'NoneType' object has no attribute 'dtype',"I have already looked up the previous answers to this problem but it has not been resolved yet. I am implementing a YOLO algorithm (for object detection) from scratch and am having problem in training part. For training, I am tf.estimator API and am using a code similar to CNN MNIST code in tensorflow example. I am getting the following error: The code related to loss function in the main file is as shown(similar to official CNN MNIST example): Previous answers to similar problem suggested that the loss function is returning nothing. However, when I try the loss function with randomly generated arrays, it works fine and yields normal values. Also, if I return a constant like 10.0 from loss function, I still get the same error. I am not sure how to proceed now. Also, is there any way I could print the loss returned by the loss function. Apparently, tf.estimator API start a tensorflow session by itself, and if I try to create another session (in order to print the value returned by loss function), I get other errors.",https://stackoverflow.com/questions/52447384,10397210,Documentation Replication on Other Examples
36693740,What's the difference between tf.placeholder and tf.Variable?,"I'm a newbie to TensorFlow. I'm confused about the difference between tf.placeholder and tf.Variable. In my view, tf.placeholder is used for input data, and tf.Variable is used to store the state of data. This is all what I know. Could someone explain to me more in detail about their differences? In particular, when to use tf.Variable and when to use tf.placeholder?",https://stackoverflow.com/questions/36693740,6176753,Lack of Alternative Solutions/Documentation
45869131,All Tensorflow outputs are nan,"On their website, tf gives model code to perform linear regression. However, I wanted to play around to see if I could also get it to do quadratic regression. To do so, I added a tf.Variable A, put it into the model and then modified the output to tell me what it got as the value. Here are the results: What do y'all think is the issue here? Is it between the chair and the keyboard?",https://stackoverflow.com/questions/45869131,5183434,Documentation Replication on Other Examples
52471921,Why there is a need of using tf.Variable?,In the following code I am unable to understand the need of using tf.Variable? I get the same value whether I use tf.Variable or omit it.,https://stackoverflow.com/questions/52471921,10333833,Documentation Replicability
52471675,Check if Tensor is Placeholder?,"Placeholders are recognized as Tensors in TensorFlow. isinstance(tf.placeholder(""float"", []), tf.Tensor) returns True Is there a way to check if a Tensor is a placeholder specifically? Something like: Unfortunately for the API I'm building, tf.Placeholder is not an actual instance type in TensorFlow.",https://stackoverflow.com/questions/52471675,10210261,Documentation Replication on Other Examples
52473088,How tf.Variable maintains state of the graph?,I am trying to learn tensorflow. I am really confused with the usage of tf.Variable . I know that in machine learning we have to randomly assign weights to the filter. But this can be done with tf.truncated_normal function. Then what is the role of tf.Variable here? Documentation states that tf.Variable maintains the state of graph. What does it mean? If I omit tf.Variable result is same. So what is the role of tf.Variable? Can someone please help me to understand this?,https://stackoverflow.com/questions/52473088,10333833,Lack of Alternative Solutions/Documentation
45879776,TensorFlow how to make results reproducible for `tf.nn.sampled_softmax_loss`,"I would like to get reproducible results for my tensorflow runs. The way I'm trying to make this happen is to set up the numpy and tensorflow seeds: As well as make sure that the weights of the neural network, that I initialized with tf.truncated_normal also use that seed: tf.truncated_normal(..., seed=rnd_seed) For reasons that are beyond the scope of this question, I'm using the sampled softmax loss function, tf.nn.sampled_softmax_loss, and unfortunately, I'm not able to control the stochasticity of this function with a random seed. By a look at the TensorFlow documentation of this function (https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss), I can see that parameter sampled_values should be the only parameter that affects randomization, but I'm not able to understand how to actually use a seed. [EDITED] This is (part of) my script",https://stackoverflow.com/questions/45879776,863713,Documentation Replicability
45886201,Tensorflow: Can't use tf.case with input argument,"I need to create a variable epsilon_n that changes definition (and value) based on the current step. Since I have more than two cases, it seems that I can't use tf.cond . I am trying to use tf.case as follows: However, I keep getting this error message: I tried the following: Still I would the same error. The examples in Tensorflow documentation weigh in on cases where no input argument is passed to the callable functions. I couldn't find enough info about tf.case on the internet! Please any help?",https://stackoverflow.com/questions/45886201,5650892,Lack of Alternative Solutions/Documentation
45900233,tf.slice and tf.strided_slice,"trying to comprehend tensorflow strided_slice and slice output for tf.slice i understand we have to mentions slice sizes in each dimension and hence out of range values makes sense. but in strided slice the end is a tensor index in the tensor itself, how come out of size value is valid. Example is taken from https://www.tensorflow.org/api_docs/python/tf/strided_slice Trying to implement folding layer part from paper A Convolutional Neural Network for Modelling Sentences",https://stackoverflow.com/questions/45900233,471384,Documentation Replicability
45917464,Tensorflow: What's the difference between tf.nn.dropout and tf.contrib.rnn.DropoutWrapper?,How's the following codes different? It seems that there is a difference in the number of states we get from tf.nn.dynamic_rnn. len(encoding state) is greater with tf.nn.dropout. An explanation will be highly appreciated. Thank you.,https://stackoverflow.com/questions/45917464,8409009,Requesting (Additional) Documentation/Examples
45955241,How do I create padded batches in Tensorflow for tf.train.SequenceExample data using the DataSet API?,"For training an LSTM model in Tensorflow, I have structured my data into a tf.train.SequenceExample format and stored it into a TFRecord file. I would now like to use the new DataSet API to generate padded batches for training. In the documentation there is an example for using padded_batch, but for my data I can't figure out what the value of padded_shapes should be. For reading the TFrecord file into the batches I have written the following Python code: The code works well if I use dataset = dataset.batch(1) (no padding needed in that case), but when I use the padded_batch variant, I get the following error: Can you help me figuring out what I should pass for the padded_shapes parameter? (I know there is lots of example code using threading and queues for this, but I'd rather use the new DataSet API for this project)",https://stackoverflow.com/questions/45955241,8536501,Documentation Replication on Other Examples
64807842,Using tf.data.experimental.make_csv_dataset for time series data,How do I use tf.data.experimental.make_csv_dataset with CSV files containing time series data?,https://stackoverflow.com/questions/64807842,7656080,Documentation Replicability
61564748,What is the difference in purpose between tf.py_function and tf.function?,"The difference between the two is muddled in my head, notwithstanding the nuances of what is eager and what isn't. From what I gather, the @tf.function decorator has two benefits in that From the definition of tf.py_function, it seems that it does just #2 above. Hence, why bother with tf.py_function when tf.function does the job with a performance improvement to boot and without the inability of the former to serialize?",https://stackoverflow.com/questions/61564748,5640161,Documentation Ambiguity
42818819,What is the difference between tensorflow conv2d_transpose and conv2d_backprop_filter?,Can someone please explain in simple terms and examples on how these work after performing the conv2d forward pass. Let me add to this question - What is the difference between conv2d_backprop_filter and tf.nn.conv2d_backprop_input?,https://stackoverflow.com/questions/42818819,7716958,Documentation Ambiguity
61743921,can we build object detection model using Tensorflow or it is only possible with the help f tf.keras,Is there any way to build object detection model using Tensorflow without any help of tf.keras module? From Tensorflow documentation I'm not able to find any example which helps to create model without Keras.,https://stackoverflow.com/questions/61743921,1490940,Lack of Alternative Solutions/Documentation
61762324,why tf.divide does not return a tensor,If I run the following code using tf2 I get this as an ouput Why only tf.divide does not return a tensor?,https://stackoverflow.com/questions/61762324,3926152,Documentation Replicability
61767803,Tensorflow 1.x to Tensorflow 2.1.0,"I am trying to update code written in Tensorflow 1.x to code in Tensorflow 2.1.0. I have been converting codes using Tensorflow 2.1.0 documentation, and I had no problems until this code. Above code is Tensorflow 1.x version, and I think, according to Tensorflow 2.1.0 documentation, the properly updated code is Then, when I run I get the following error. So, I am guessing in Tensorflow 1.x version, the loss was passed as 'tensor' to tf.estimator.EstimatorSpec, but in Tensorflow 2.1.0, the loss has to be passed as scalar to tf.estimator.EstimatorSpec? Loss (the way it is defined here) in both Tensorflow 1.x and 2.1.0 is tensor if I remember it correctly. So, does anyone know how to convert tensor to scalar (which I don't think will be sufficient nor efficient in building the CNN model) or better yet, how to solve this dilemma? Or did I convert the original code the wrong way? I would very much appreciate if compat.v1. is not used unless absolutely necessary (i.e. no other way to use the code in Tensorflow 2.1.0 than compat.v1.)",https://stackoverflow.com/questions/61767803,12997689,Documentation Replication on Other Examples
61946509,tf.estimator input_fn and eager mode,"I tried to use numpy inside cnn_model.evaluate(), but it gave AttributeError: 'Tensor' object has no attribute 'numpy'. I used numpy to calculate accuracy and mean squared error using tf.keras.metrics.Accuracy() and tf.keras.metrics.MeanSquaredError() inside cnn_model.evaluate() I googled it, and in tensorflow documentation, it said ""Calling methods of Estimator will work while eager execution is enabled. However, the model_fn and input_fn is not executed eagerly, Estimator will switch to graph mode before calling all user-provided functions (incl. hooks), so their code has to be compatible with graph mode execution."" So, I was wondering how I can update the current tf 1.x code to tf 2.1.0 code, while also using above information. My current code is: What I have tried so far is add tf.compat.v1.enable_eager_execution() to the 1) beginning of the code after all the imports, 2) next line right after importing tf, 3) line right before declaring eval_input_fn, 4) line right before calling eval_results, 5) inside CNN model definition. It all failed to turn on the eager mode. One other option that I found was remove @tf.function decorator, but I have no idea what that means and how to pass input_fn if @tf.function is removed.",https://stackoverflow.com/questions/61946509,12997689,Documentation Replication on Other Examples
61986166,How can I save a Tensorflow 2.2.0 model with a custom training loop?,"I am struggling to save a tf.keras model to easily load and be able to use it. I have used the tf.keras.Model subclass method to construct a MLP model with a custom loss function, as you can see below: I then make an instance of the model and proceed with a standard training loop: I have tried to follow the steps outlined here. I call the model on a random input in order to trigger model.build internally, and then I attempt to save the model using the following: I then get this following error: I don't understand this issue as I specify the same TensorSpec as in the tf.function above the model.call attribute. I attempted this without including the tf.function above the model call, which leads to an error relating to the input dimensions having to be set. I am able to address this by calling the model on an arbitrary input, which does allow me to save the model but I have to compile it prior to using it and I get the following warning: My question is whether or not I am completely missing something or if it is usual to have to compile loaded custom models? I am running TensorFlow 2.2.0 with Python 3.8.2, and based on the documentation for saving models this should really be quite simple. I am new to TensorFlow so it may well be that it's a silly mistake, but ultimately it's still a basic model with 5 inputs and a single output. Any help would be greatly appreciated.",https://stackoverflow.com/questions/61986166,12860541,Documentation Replication on Other Examples
61988657,Why does tensorflow.rank always return shape with null value,Being a beginner to TensorFlow I couldn't get why does tensorflow.rank always return shape with null value? This is what I am working on: and the output is So my question is what is this shape=() from tf.rank output? I couldn't get much from here - https://www.tensorflow.org/api_docs/python/tf/rank,https://stackoverflow.com/questions/61988657,5176364,Documentation Replicability
61994285,TensorFlow: Error using weighted_categorical_column,"I work on a binary classification problem containing a field STREET. In a first step I used the Tokenization to the get a word list (frequency of how often one word appears in the different datasets). Then I used this information to create two columns in my Dataframe describing the word and how often it was used: After I converted the Dataframe to a TensorFlow Dataset I have used the following code to add it to my future columns: As you can see I used the function tf.feature_column.weighted_categorical_column. Unfortunately I get the following error when I try to train my model: Furthermore I get the following warning: Now I have two questions: First: does it make sense to use this function for my described problem? Unfortunately, I couldn’t find a detailed description how this function works (only this short documentations: https://www.tensorflow.org/api_docs/python/tf/feature_column/weighted_categorical_column) Second: How can I fix the described error?",https://stackoverflow.com/questions/61994285,11986067,Lack of Alternative Solutions/Documentation
62211822,TF2 Keras - Feature Engineering in Keras saved model via Tensorflow Serving,"The Tensorflow 2 documentation for preprocessing / feature engineering over a Keras model seems to be quite confusing and isn't very friendly. Currently I have a simple Keras N-layer model with TF feature columns feeding as dense layer. For training I have CSV files read using tf.dataset API and I have written a feature engineering function that creates new features using dataset.map function. I can save the model easily using tf.keras.models.save_model method. However I am having trouble figuring out how to attach the feature_engineering steps in the serving function. Requirement: Now I want to take the same feature engineering function above and attach it to my serving function so that in JSON input via tensorflow_model_server the same feature engineering steps are applied. I know about the lambda Layer option in Keras but I want to do this via saved_model method but there are a lot of difficulties here. For Example, below code gives error: Error: The above error is because I have not provided InputSignature of my Keras model but I am not able to understand that I have 13 input fields, what is expected as input signature. So I wanted to know if anyone knows the shortest way of solving this out. This is a very basic requirement and Tensorflow seems to have kept this quite complicated for Keras Tensorflow model serving. GIST: https://colab.research.google.com/gist/rafiqhasan/6abe93ac454e942317005febef59a459/copy-of-dl-e2e-structured-mixed-data-tf-2-keras-estimator.ipynb EDIT: I fixed it, so TensorSpec has to be generated and passed for each feature and also model( ) has to be called in serving function.",https://stackoverflow.com/questions/62211822,9334184,Documentation Replication on Other Examples
62395187,"In tensorflow graph mode, why doesn't tensor.shape work, but tf.shape(tensor) does?","In tensorflow, when running something in graph mode, tensor.shape does not work, but tf.shape(Tensor) does. I am wondering if there's any reason for this.",https://stackoverflow.com/questions/62395187,3259896,Documentation Ambiguity
62405592,Tensorflow 2 Metrics produce wrong results with 2 GPUs,"I took this piece of code from tensorflow documentation about distributed training with custom loop https://www.tensorflow.org/tutorials/distribute/custom_training and I just fixed it to work with the tf.keras.metrics.AUC and run it with 2 GPUS (2 Nvidia V100 from a DGX machine). The problem is that AUC's evaluation is definitely wrong cause it exceeds its range (should be from 0-100) and i get theese results by running the above code for one time: Accuracy is ok but it seems that it's the only one metric that behaves nice. I tried other metrics too but they are not evaluated correctly. It seems that the problems come when using more than one GPU, cause when I run this code with one GPU it produce the right results.",https://stackoverflow.com/questions/62405592,10687511,Documentation Replication on Other Examples
62411242,tf.keras.backend.clip not giving correct results,"tf.keras.backend.clip is not clipping the tensors When I use tf.keras.backend.clip inside this function THe value of the gradients remain the same (unclipped). When I use tf.keras.backend.clip inside the custom training loop, same way it doesn't work. The gradient applied to the variables are not clipped. However, if I print g within the loop, then it shows the clipped value. Can't understand where the problem is.",https://stackoverflow.com/questions/62411242,5337505,Documentation Replicability
62413757,Loading a saved BertClassifer model,I am using the following example from this colab notebook: https://github.com/tensorflow/models/blob/master/official/colab/fine_tuning_bert.ipynb It saves a fine-tuned model using the model.save() functionality. I am trying to load that same model using tf.keras.models.load_model() but am getting the following error: Can someone please advise what I am doing wrong or if I should be doing something else to load this model? Thanks!,https://stackoverflow.com/questions/62413757,11392855,Documentation Replicability
62418856,The differences between tf.nest.map_structure vs tf.map_fn in speed and in results,"My question may be summarized as follows: Now, let me explain the problem that I had in more detail. I need to apply a certain function to each example in a batch. At first, I tried the tf.map_fn method described in: https://www.tensorflow.org/api_docs/python/tf/map_fn. After implementing using tf.map_fn, I realize that the code is terribly slow. I did some search, and it seems that many people have experienced similar issues. (e.g. https://github.com/tensorflow/tensorflow/issues/24774) When I replaced tf.map_fn with tf.nest.map_structure, the speed was much better. However, the results were slightly different. I made the following toy example to check this case. y1 and y3 are exactly the same but y2 and y3 are slightly different. y2 and y3 are not totally different, but still similar. The following is the result:",https://stackoverflow.com/questions/62418856,735008,Documentation Replication on Other Examples
62620694,What is difference between tf.keras.models.sequential vs tf.keras.sequential?,What is difference between tf.keras.models.Sequential() vs tf.keras.Sequential()? I don't understand differences between them quite well. Can somebody explain it to me? I am new to TensorFlow but have some basic understanding on machine learning.,https://stackoverflow.com/questions/62620694,9685065,Documentation Ambiguity
62643330,Tensorflow: '+' operator,"I'd like to ask: is the symbol '+' used in Tensorflow? I mean. in Pytorch I can sum/subtract tensors using + and - operators, while in Tensorflow is good use to exploit tf.add for example. Could you please help me figure it out? Thanks",https://stackoverflow.com/questions/62643330,13835960,Documentation Replication on Other Examples
62668694,"How to create an _Arg, _Retval, _If operation in tensorflow?","I'm trying to test all the operations available in tensorflow. For example, we can find 'Conv2d' in tf.nn module. There are some operations started with an '_', e.g, '_Arg', '_ArrayToList', '_Retval'. I looked into the tensorflow source code, but still can't find how to create an operation '_Arg'. Please give me some instructions of how to find these operations, or what does these operations do?",https://stackoverflow.com/questions/62668694,2525479,Requesting (Additional) Documentation/Examples
62670041,batch_size in tf model.fit() vs. batch_size in tf.data.Dataset,"I have a large dataset that can fit in host memory. However, when I use tf.keras to train the model, it yields GPU out-of-memory problem. Then I look into tf.data.Dataset and want to use its batch() method to batch the training dataset so that it can execute the model.fit() in GPU. According to its documentation, an example is as follows: Is the BATCH_SIZE in dataset.from_tensor_slices().batch() the same as the batch_size in the tf.keras modelt.fit()? How should I choose BATCH_SIZE so that GPU has sufficient data to run efficiently and yet its memory is not overflown?",https://stackoverflow.com/questions/62670041,6227592,Documentation Replication on Other Examples
62877768,Input shape of tf.data.Dataset not accepted by model.fit(),"I would like to feed with data my model by applying a tf.data.Dataset. Having checked the documentation of TF 2.0 I found that the .fit() function (https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) accepts: So, I wrote the following minial proof of concept code: As mentioned in the second comment, the code that does not apply a tf.data.Dataset works fine. However, when applying the Dataset object I get the following error message: From my understanding of the documentation, the dataset I have constructed should be exactly the tuple object the fit method expects. I do not understand this error message. What am I doing wrong here?",https://stackoverflow.com/questions/62877768,1020704,Documentation Ambiguity
50560013,"Tensorflow, multi-label confusion matrix","I am trying to figure out how to the generate a confusion matrix for a multi-label classification task using neural networks. I previously managed to calculate the accuracy using the function ""intersection"", since for that I did not care about any ordering. However, in order to calculate the confusion matrix, I do care about the indexing order of the predictions/labels. And since the labels have always the same value (1,1 or 0.5,0.5) there is no possible sorting according to higher/lower value. I wonder: 1) Is it possible to calculate a confusion matrix for the multi-label classification task? 2) How would that be implemented ? 3) How can you handle the case of failure in predicting both labels? Since it is not possible to know which confusion belongs to which prediction. 4) What is the logic behind the sorting of the function tf.nn.top_k() Below I show an example of the code that I was trying to use. I don't really get why the output of predicted_softmax is: I was expecting [5] [3] for the last two terms. There is no any logic to this output. In the documentation they don't specify anything about the ordering in the case that sorted = False thought, but I was expecting some consistent behavior. Thanks for any help!",https://stackoverflow.com/questions/50560013,9527947,Documentation Completeness
69458522,What does tf.squeeze does to the audio and how can I load an mp3?,"I'm using TensorFlow and I would like to be able to load audio and generate a spectrogram from it. I have little knowledge of how audio internally works. Currently, this is the code I'm using: I have been reading the documentation and in order to create a tensor I need to either use the tf.squeeze method or audio.to_tensor(). I have no clue what the tf.squeeze method does, but when I use it I get the error: If I instead use the method audio.to_tensor(), I'm unable to display the created spectrogram on the plt and instead I get the following error:",https://stackoverflow.com/questions/69458522,4999534,Documentation Replication on Other Examples
50724495,Train and validate using tensorflow estimator,"I have created a shallow NN using tf.estimator API. I would like to something similar to the hyperparameter search explained in here https://www.youtube.com/watch?time_continue=948&amp;v=eBbEDRsCmv4 at TensorFlow Dev Summit. I could not find any updated documentation about how can you do this. I have the following code (I will try to simplify as much as possible): Executing this code I can obtain the summary for the loss and observe it in Tensorboard. But imagine I want to obtain different curves. Let's say that I want to see how the loss evolves with the number of samples, so I would train two models with different sample size. Or two models with a different architecture... whatever. How can I get these two curves in Tensorboard?",https://stackoverflow.com/questions/50724495,8380638,Documentation Replication on Other Examples
63140320,How to use sequence/generator on tf.data.Dataset object to fit partial data into memory?,"I am doing image classification with Keras on Google Colab. I load images with the tf.keras.preprocessing.image_dataset_from_directory() function (https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory) which returns a tf.data.Dataset object: I found that when the data contains thousands of images, model.fit() will use all memory after training a number of batches (I am using Google Colab and can see RAM usage grow during the first epoch). Then I try to use Keras Sequence, which is a suggested solution of loading partial data into RAM (https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence): And I train the model with: history = model.fit(DatasetGenerator(train_ds), ...) The problem is that getitem() must return a batch of data with index. However, the list() function I use has to put the whole dataset into RAM and thus hit memory limit when a DatasetGenerator object instantiates (tf.data.Dataset object does not support indexing with []). My questions: Thanks in advance!",https://stackoverflow.com/questions/63140320,5222932,Documentation Replication on Other Examples
63146831,What is the analytic interpretation for Tensorflow custom gradient?,"In the official tf.custom_gradient documentation it shows how to define custom gradients for log(1 + exp(x)) When y = log(1 + exp(x)), analytically the derivative comes out to be dy/dx = (1 - 1 / (1 + exp(x))). However in the code def grad says its dy * (1 - 1 / (1 + exp(x))). dy/dx = dy * (1 - 1 / (1 + exp(x))) is not a valid equation. While dx = dy * (1 - 1 / (1 + exp(x))) is wrong as it should be the reciprocal. What does the grad function equate to?",https://stackoverflow.com/questions/63146831,1217998,Documentation Replication on Other Examples
63158314,Tensorflow 2.3.0 - Warning: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version,"I've just updated to TF-2.3. In a model using tf.data.Dataset.from_tensor_slices as data source, I get the folowing warning: I didn't find the instructions on the documentation on how to use the updated methods. Training: Thanks in advance.",https://stackoverflow.com/questions/63158314,11524722,Lack of Alternative Solutions/Documentation
63376657,Should I be using tf.keras or keras in 2020?,A while back I read this post on PyImageSearch and was satisfied enough to switch completely to tf.keras. But since then I noticed the Keras website got an overhaul and seems to be expanding on its ambitions. So I'm a bit confused.,https://stackoverflow.com/questions/63376657,4391249,Documentation Ambiguity
63379008,How to make a diagonal tensor and why doesn't Tensorflow linalg.tensor_diag do that?,"What I would consider a diagonal tensor is a tensor t of shape (d1, ..., dr) which is all zero except when the components are equal. So t[i,j,k,l] = 0 unless i == j == k == l. A function to create such a tensor should take in a shape (d1, ..., dr) and a vector [a1, ..., ak] of length min(d1, ..., dr), placing these values along the diagonal. I would like to do this in Tensorflow, and the most relevant function I could find was tf.linalg.tensor_diag, but it doesn't do what I want. For instance, the diagonal input is a tensor, and the output tensor always has twice the rank, and so it can never output tensors of odd rank. The documentation says ""Given a diagonal, this operation returns a tensor with the diagonal and everything else padded with zeros"", but I don't know how to square that with its actual behavior. My question is two parts: Here is an example output:",https://stackoverflow.com/questions/63379008,6667924,Documentation Ambiguity
63383594,How does Tensorflow build() work from tf.keras.layers.Layer,"I was wondering if anyone knew how the build() function works from the tf.keras.layers.Layer class under the hood. According to the documentation: so to me it seems like the class is behaving similar to this: I can't imagine build() would be called for ever __call__, but it is the only place where the input is passed in. Does anyone know how exactly this works under the hood?",https://stackoverflow.com/questions/63383594,11065415,Documentation Replication on Other Examples
44526763,How to perform tf.image.per_image_standardization on a batch of images in tensorflow,"I would like to know how to perform image whitening on a batch of images. According to the documentation in https://www.tensorflow.org/api_docs/python/tf/image/per_image_standardization, it is said that tf.image.per_image_standardization takes as input a 3D tensor, that is an image, of shape: [height, width, channels]. Is it a missing feature or there is a different method? Any help is much appreciated.",https://stackoverflow.com/questions/44526763,7886651,Documentation Replicability
63399368,Error with exporting TF2.2.0 model with tf.lookup.StaticHashTable for Serving,"I'm using StaticHashTable as in one Lambda layer after the output layer of my tf.keras model. It's quite simple actually: I've a text classification models and I'm adding a simple lambda layer that takes the model.output and convert the model_id to more general labels. I can save this version of model with model.save(... as H5 format..) without any issue, and can load it back and use it without any problem. Issue is, when I try to export my TF2.2.0 model for TF-Serving, I can't find how I can export it. Here is what I can do with TF1.X or with TF2.X + tf.compat.v1.disable_eager_execution() This will save my models with TF1.X format for serving and I can use it without any issue. Things is, I'm using LSTM layer and I want to use my model on GPU. By the documentation, if I disable the eager mode, I can't use the GPU-version of LSTM with TF2.2. And without going through above mentioned code, I can't save my model for serving wrt TF2.2 standard and StaticHashTables. Here is how I'm trying to export my TF2.2 model which is using StaticHashTables in final layer; and which is giving error as below: Error: Any suggestion or am I missing anything on exporting TF2.2 model which is using the StaticHashTables in final Lambda layer for TensorFlow Serving? More info here: https://github.com/tensorflow/serving/issues/1719 Thanks!",https://stackoverflow.com/questions/63399368,4496896,Documentation Replication on Other Examples
44540673,TypeError('The value of a feed cannot be a tf.Tensor object....) though I am providing it a numpy array,I am providing to my feed_dict a numpy array but it still gives this error that the feed need to be a tf.Tensor object. Is this a bug in the tensorflow library since I wanted to post as an issue but wasn't sure. Any help is highly appreciated. Thanks,https://stackoverflow.com/questions/44540673,7289393,Documentation Replicability
63851431,How to Augment the Training Set using the tf.keras.utils.Sequence API?,"TensorFlow documentation have the following example that can illustrate how to create a batch generator to feed a training set in batches to a model when the training set is too large to fit in memory: My intention is to further increase the diversity of the training set by rotating each image 3x by 90º. In each Epoch of the training process, the model would first be fed with the ""0º training set"" and next with the 90º, 180º and 270º rotating sets, respectively. How can I modify the previous piece of code to perform this operation inside the CIFAR10Sequence() data generator? Please don't use tf.keras.preprocessing.image.ImageDataGenerator() so that the answer does not lose its generality for another type of similar problems that are of a different nature. NB: The idea would be to create the new data ""in real time"" as the model is fed instead of creating (in advance) and storing on disk a new and augmented training set bigger than the original one to be used later (also in batches) during the training process of the model. Thx in advance",https://stackoverflow.com/questions/63851431,14230555,Documentation Replication on Other Examples
63882987,Is there a tf.keras.optimizers implementation for L-BFGS?,"Does anybody have a Tensorflow 2 tf.keras subclass for the L-BFGS algorithm? If one wants to use L-BFGS, one has currently two (official) options: These two options are quite cumbersome to use, especially when using custom models. So I am planning to implement a custom subclass of tf.keras.optimizers to use L-BFGS. But before I start, I was curious, whether somebody already tackled this task?",https://stackoverflow.com/questions/63882987,11908003,Documentation Replication on Other Examples
63894153,TypeError: 'Tensor' object cannot be interpreted as an integer when using tf.map_fn(),"I am trying to construct label-dependent convolutional filters in keras/tensorflow. Therefore, the convolutional filter(s) depend on each example in the batch. When I run this code snippet, I get a TypeError: 'Tensor' object cannot be interpreted as an integer for the last line. Since the last line uses tf.map_fn, I saw that tf.map_fn results in a TypeError if either the function used (single_conv in this case) is not callable or the structure of the output of function and fn_output_signature do not match: https://www.tensorflow.org/api_docs/python/tf/map_fn#raises. However, I'm still not sure why this is happening? I feel like both of those reasons should not be an issue?",https://stackoverflow.com/questions/63894153,7375754,Documentation Replication on Other Examples
44871420,TensorFlow dynamic_rnn input for regression,"I'm stuck trying to convert an existing tensorflow sequence to sequence classifier to a regressor. Currently I'm stuck in handling the input for tf.nn.dynamic_rnn(). According to the documentation and other answers, input should be in the shape of (batch_size, sequence_length, input_size). However my input data has only two dimensions: (sequence_length, batch_size). The original solution uses tf.nn.embedding_lookup() as an intermediate step before feeding input to dynamic_rnn(). If I understand correctly, I believe I don't need this step since I'm working on a regression problem, not a classification problem. Do I need the embedding_lookup step? If so, why? If not, how can I fit my encoder_inputs directly into dynamic_rnn()? Below is a working minimalized example of the general idea: I have read similar questions here on stackoverflow but find my self still puzzled as to how to solve this. EDIT: I think I should clarify that the code above works well, however the real desired output should mimic a noisy signal (text to speech for example) which is why I think I need continuous output values instead of words or letters.",https://stackoverflow.com/questions/44871420,1145023,Documentation Replication on Other Examples
63905839,Incompatible shapes between op input and calculated input gradient. conv2d_transpose any idea how to solve it?,"w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]], initializer=tf.random_normal_initializer(stddev=stddev))",https://stackoverflow.com/questions/63905839,8343298,Documentation Replication on Other Examples
45151015,How does tf.gradients behave when passed a list of `ys` tensors?,"How exactly does tf.gradients behave when passed a list of tensors as its first argument? Take this very small example: If I compute the gradients of a single tensor, c, with respect to [a,b], I get the expected answer: According to the Tensorflow documentation, if you pass in a list of tensors as your first argument ys, tf.gradients will sum the gradients over that list, returning sum_over_ys(dy/dx) for each x in your second argument. So I would expect: to behave the same way as: Am I reading the docs wrong? When I test this code, I get the expected result [2, 3] for the second expression (explicitly summing a + b + c), but [2, 1] for the first. Where is this [2, 1] coming from?",https://stackoverflow.com/questions/45151015,1931098,Documentation Ambiguity
45172725,Tensorflow - Why are there so many similar or even duplicate functions in tf.nn and tf.layers / tf.losses / tf.contrib.layers etc?,"In Tensorflow (as of v1.2.1), it seems that there are (at least) two parallel APIs to construct computational graphs. There are functions in tf.nn, like conv2d, avg_pool, relu, dropout and then there are similar functions in tf.layers, tf.losses and elsewhere, like tf.layers.conv2d, tf.layers.dense, tf.layers.dropout. Superficially, it seems that this situation only serves to confuse: for example, tf.nn.dropout uses a 'keep rate' while tf.layers.dropout uses a 'drop rate' as an argument. Does this distinction have any practical purpose for the end-user / developer? If not, is there any plan to cleanup the API?",https://stackoverflow.com/questions/45172725,5615276,Documentation Replicability
45202404,TensorFlow Slim Pre-trained models Negative Dimensions,"I've been trying to design a wrapper to use the pre-made tensorflow slim models for a custom dataset. the dataset is 1000 images of squares and triangles, 32x32 grayscale. They are organized as dataset/shapes/triangles/ and dataset/shapes/squares/. Using the following code, I am able to train the inception_v2 model without errors. The tf.reshape will be replaced with the correct variable parameters later. The .tfrecords files are created using this script from google that creates the records from the above mentioned dataset structure. The issue I'm having is with other models. Using inception_v1, with the same arguments, I get the following error: I get a similar error using inception_v3. With vgg_16 and vgg_19, I get: Can anyone give insight into these errors? What could be the difference between inception_v1 and inception_v2 that would cause it to crash, and how are the inception models this different? I haven't tried this dataset with ResNet yet, but I suspect a similar error will happen with that as well. For reference, this example code is based on the 'working example' provided with the tf slim documentation, located here The system it is running on is using Python 2.7.10 with Tensorflow-GPU 1.2.0. It's a Xeon system with 4 Nvidia Titan X GPUs, on Ubuntu 14.10. Thanks! If you need any additional system configurations or the getImage function I can provide those as well!",https://stackoverflow.com/questions/45202404,8309477,Documentation Replication on Other Examples
45203872,How tf.train.shuffle_batch works?,"Does it do one shuffling in one epoch, or else? What is the difference of tf.train.shuffle_batch and tf.train.batch? Could someone explain it? Thanks.",https://stackoverflow.com/questions/45203872,7424342,Requesting (Additional) Documentation/Examples
45217998,Tensorflow: Weighted sparse softmax with cross entropy loss,"I am doing image segmentation using fully convolutional neural networks (link to the paper): https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf This can be considered as pixel classification (in the end each pixel is getting a label) I am using the tf.nn.sparse_softmax_cross_entropy_with_logits loss function. Everything is going well. However, I saw that one class occurs in the vast majority of pixels (95%+), call this class 0. Lets say that we have another three classes, 1, 2 and 3. What would be the easiest way to put weights to the classes? Essentially, I would like to have very low weight for class 0 (like 0.1) compared to the other three classes who should have normal weight 1. I know that this function exists: https://www.tensorflow.org/api_docs/python/tf/losses/sparse_softmax_cross_entropy It just looks to me that it does something totally different and I do not understand how the weights should have the same rank as labels. I mean, in my case, weights should be something like Tensor([0.1, 1, 1, 1]) so shape (4,) and rank 1, while labels have shape (batch_size, width, height) and so rank 3. Am I missing something? The equivalent on PyTorch would be where weight is a torch tensor [0.1, 1, 1, 1] Thanks!",https://stackoverflow.com/questions/45217998,1549979,Documentation Ambiguity
64552543,Tensorflow 2.2.0 :- WARNING:tensorflow:Gradients do not exist for variables when minimizing the loss,"After implementing Custom Loss class as per the tensorflow api documentation and when invoking model.fit , facing this warning alongwith below error:- This is reference link on github and they have asked to raise here in stack overflow.https://github.com/tensorflow/tensorflow/issues/42542# TypeError: An op outside of the function building code is being passed a ""Graph"" tensor. It is possible to have Graph tensors leak out of the function building context by including a tf.init_scope in your function building code. For example, the following function will fail:",https://stackoverflow.com/questions/64552543,6926418,Documentation Replication on Other Examples
45549251,Tensorflow Dataset API doubles graph protobuff filesize,"Summary: Using the new tf.contrib.data.Dataset doubles the size of my graph protobuff file and I'm unable to visualize the graph in Tensorboard. The details: I'm trying out the new TensorFlow tf.contrib.data.Dataset functionality together with the tf.contrib.learn.Experiment framework. My input data is defined as input functions which return tensors of features and labels. If I create my input function with the tf.train.slice_input_producer function like in the following codeblock (full code here), then my resulting graph.pbtxt file is 620M and the .meta files are around 165M in size. Now if I create my input function with the new tf.contrib.data.Dataset.from_tensor_slices like in the following codeblock (full code here), then my resulting graph.pbtxt file doubles in size to 1.3G and the .meta files double in size to 330M. Now because the graph.pbtxt file is so big TensorBoard takes ages to parse this file, and I'm unable to debug my model graph visually. I found in the Dataset documentation that this increase in size comes from: ""the contents of the array will be copied multiple times"" and the solution would be to use placeholders. However, in this case, I would need to feed in the numpy arrays into the placeholders with an active session to initialize the iterator: This seems, however, to be out of my control when using the tf.contrib.learn.Experiment framework. How can I initialize the iterator's initialiser with the Experiment framework? Or find a workaround to using the Dataset API without increasing my graph size?",https://stackoverflow.com/questions/45549251,919431,Documentation Replication on Other Examples
45553038,How to compile custom ops in tensorflow without having to dynamically import them in python?,"I checked through tensorflow documentation and they seem to only give information about compiling a custom op through a bazel rule: Once bazel builds it, you get a zero_out.so file which you can import into python like below: Is there anyway you can link custom_ops during the bazel build of tensorflow so that you don't need to manually import custom ops through tf.load_op_library?",https://stackoverflow.com/questions/45553038,7922336,Documentation Completeness
45553280,TensorArray Initialization from another tensor,"What is the right way to initialize a tensorarray from another tensor in tensorflow. Suppose I have a tensor What is way to say that this tensorarray depends on T1? Looking at the documentation I cant figure out how to initialize this. Correct me if my understanding is wrong, T1 is a nested tensor and I want to loop over a dimension using tf.while_loop and hence I want to initialize the TensorArray with it.",https://stackoverflow.com/questions/45553280,4040998,Documentation Replication on Other Examples
64578310,tfrecordswriter does not write,"I am trying to create a tf.data.Dataset from a generator I wrote, and following this great answer: Split .tfrecords file into many .tfrecords files This is supposed to be a straight-forward use of tfrecords writer. However, It does not write any files at all. Does anyone understand why this doesn't work?",https://stackoverflow.com/questions/64578310,13608754,Documentation Replication on Other Examples
45595419,Is it possible to have multiple conditions defined in tf.while_loop,"Is it possible to define to multiple conditions for termination of a tf.while_loop in tensorflow? For example depending on the two tensor values achieving two specific values. eg. i==2 and j==3 ? Also can I have several blocks of code in the body? In all the examples in the documentation, it seems that the body is more like a single statement returning a value or a tuple. I want to execute a set of several ""sequential"" statements in the body.",https://stackoverflow.com/questions/45595419,8348464,Inadequate Examples
64611137,port TensorFlow 1 code to TensorFlow 2 (model learning process without sess.run),"I have this piece of tf1 code, which was taken from nice book ""Deep Learning"" by S. Nikolenko. It's a simple linear regression that learns k and b to 2 and 1 respectively. I'm striving to port it on TensorFlow 2 And for long time I can't wrap my head what should I use instead of sess.run() and feed_dict, which doing magic behind the scenes, official documentation go into to details with writing model class and so on, but I'm want to keep this as flat as possible. Also it's suggested to calculate derivatives with tf.GradientTape, but I'm struggling with applying it right to my example I need SGD optimizer minimize that given loss function and learn k and b values, how can I achieve it from this point?",https://stackoverflow.com/questions/64611137,6634635,Documentation Replicability
64612657,Manually change weights of Keras convolutional layer,"There is a way to change manually the weights for the tf.layers.Conv2d (https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers/Conv2D)? Because this class takes in only the input, the number of kernels to use, etc... and the weights are automatically stored and computed by Tensorflow, but I would a way (like in tf.nn.conv2d - https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) to pass directly the weights to the class. Anyone have a suggestion? Maybe one could be to load and change manually the value for the variable associate at that layer? I found this solution very bad but it could work. Thank you.",https://stackoverflow.com/questions/64612657,14548310,Documentation Replication on Other Examples
58069572,Workaround for lack of broadcast in TFLite,"I would like to run a TFLite model that requires me to produce a 3d output (the sample code is a minimum example generating the error). Is there a tensorflow equivalent to gather_nd that does not reduce the dimension by one? I've tried looking through the documentation for related functions that I can think of and haven't found a good option. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: GATHER_ND, TRANSPOSE. Here is a list of operators for which you will need custom implementations: BroadcastTo.",https://stackoverflow.com/questions/58069572,12092080,Documentation Replication on Other Examples
58096095,How does tf.audio.decode_wav get its contents?,"I'm trying to pull some audio files into Tensorflow by using tf.audio.decode_wav. I can see someone is looking into providing more info in the docs, but does anyone have any examples of how this should work? Args: I'm guessing the contents is a tensor which has already been pulled from a file rather than a path?",https://stackoverflow.com/questions/58096095,12118244,Inadequate Examples
39210093,regarding the correct way to understand the result of tf.pad,"When reading the document for tf.pad, I feel quite confusing about the example given in the tutorial. For instance, padding is [[1,1,],[2,2]], how does it cause the resulting tensor has the shape as shown in the figure. Besides, what's the mechanism to generate those padded values, e.g., the ones marked in red circle. It is not very clear how to connect the explanation with the example.",https://stackoverflow.com/questions/39210093,785099,Lack of Alternative Solutions/Documentation
39211332,Custom initializer for get_variable,"How can one specify a custom initializer as the third argument for tf.get_variable()? Specifically, I have a variable y which I want to initialize using another (already initialized) variable x. This is easy to do using tf.Variable(), just say, y = tf.Variable(x.initialized_value()). But I couldn't find an analog in the documentation for tf.get_variable().",https://stackoverflow.com/questions/39211332,1994648,Lack of Alternative Solutions/Documentation
58126494,How to Translate CSV Data into TFRecord Files,"Currently I am working on a system that can take data from a CSV file and import it into a TFRecord file, However I have a few questions. For starters, I need to know what type a TFRecord file can take, when using CSV types are removed. Secondly, How can I convert data type:object into a type that a TFRecord can take? I have two columns (will post example below) of two objects types that are strings, How can I convert that data to the correct type for TFRecords? When importing Im hoping to append data from each row at a time into the TFRecord file, any advice or documentation would be great, I have been looking for some time at this problem and it seems there can only be ints,floats inputted into a TFRecord but what about a list/array of Integers? Thankyou for reading! Quick Note, I am using PANDAS to create a dataframe of the CSV file Some Example Code Im using Error Message: CSV Data Update: Got Two Columns of dating working using the following function... However when trying to include my two other column object types (What data looks like in both those columns) ""3,9,11,16,25,26,28,29,36,40,41,46,63,66,67,69,72,73,78,80"" I get an error, here is the function I tried for that This Error Appears: Should I try to Cast those two types outside the function and try combining it later into the TFRecord file alongside the tf.data from the make_csv_dataset function?",https://stackoverflow.com/questions/58126494,9873122,Requesting (Additional) Documentation/Examples
58186564,TensorFlow multi class training and prediction,"The following code (working), train a model to recognize cats and make a prediction on the selected picture. (Code TensorFlowJS but the question is generally TensorFlow) So far it is only predicting one class (""cat""), so that a car or a dog would be for example 80% a cat. Question: How do i add other classes (like ""dog"") ? Should it look like that (abstracted): model.fit([img1, img2, img3], [label1, label2, label3] ...) ? I don't get it: What is the relation between the labels and the training set. Here is the code (please ignore the ""Predict"" part for now): The code is based on the TFJS documentation and a comment on the github : https://github.com/tensorflow/tfjs/issues/1288 UPDATE : So I need X and Y to be the same length for X:images and Y:labels, with Y1 being the label for X1 and so on... I tried: One image + all labels -&gt; with ""model.fit(images[i], ys, {epochs: 100})..."", I get: Error: ""Input Tensors should have the same number of samples as target Tensors. Found 1 input sample(s) and 10 target sample(s)."" One image + one label -&gt; with ""model.fit(images[i], ys[i], {epochs: 100})..."", I get: Error: ""Cannot read property 'shape' of null"", i guess ys is a tensor but y[i] is not. All images + all labels -&gt; with ""model.fit(images, ys, {epochs: 100})..."", I get: Error: ""when checking model input: the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see 1 Tensor(s), but instead got the following list of Tensor(s): Tensor ..."" Guess: I need to put all images in one tensor with the same structure as ys. SOLVED : After solving the problem with the labels thanks to Rishabh Sahrawat, I had to merge all tensor(images) in to one with the help of tf.concat(...). Updated code :",https://stackoverflow.com/questions/58186564,6258907,Documentation Replication on Other Examples
58225926,Tensorflow Gradient Tape returning None,"I'm using TensorFlow 1.14.0 with Python(3.6.8). I'm trying to use tensorflow_probability's lbfgs optimizer implementation(documentation/example). If I run the example code provided in the documentation it works fine. I tried to follow the same procedure for my own code which uses the tf.GradientTape() approach for computing the objective function. When doing it that way, the gradients come back as None type. I'm not seeing why one is working, but the other is not. Edit: I realized that running eager execution using the gradients wouldn't work, so I adjusted the example to be able to be run with eager execution. Non-working example(using GradientTape) with eager execution",https://stackoverflow.com/questions/58225926,3667142,Documentation Replication on Other Examples
58412668,Hparams plugin with tf.keras (tensorflow 2.0),"I try to follow the example from the tensorflow docs and setup hyperparameter logging. It also mentions that, if you use tf.keras, you can just use the callback hp.KerasCallback(logdir, hparams). However, if I use the callback I don't get my metrics (only the outcome).",https://stackoverflow.com/questions/58412668,7441757,Documentation Ambiguity
58499116,How to raise an error based on condition in Tensorflow?,"I am writing a function my_function to be used as tf.data.Dataset.map argument, using tf.py_function, as recommended in the doc. How can I raise a tf.errors.InvalidArgumentError in my_function ? my_function In my data pipeline:",https://stackoverflow.com/questions/58499116,326849,Documentation Replicability
58550146,How to use the tf.keras.layers.BatchNormalization() in custom training loop?,"I went back to tensorflow after quite a while and it seems the landscape is completely changed. However, previously I used to use tf.contrib....batch_normalization with the following in the training loop: But it seems, contrib is nowhere to be found and tf.keras.layers.BatchNormalization does not work the same way. Also, I couldn't find any training instruction in their documentation. So, any information of help is appreciated.",https://stackoverflow.com/questions/58550146,817824,Lack of Alternative Solutions/Documentation
58599039,Understand how to use tf.nn.conv2d function,"I'm trying to understand how does tf.nn.conv2d() function work. Thus, I created a simple tensorflow program for that: The results that I get are: The ofmaps value are not correct! Any one can help me in getting what I'm missing? Thanks in advance.",https://stackoverflow.com/questions/58599039,5925476,Documentation Replication on Other Examples
58843164,Use `tf.contrib.predictor` to predict on batches from `tf.estimator.export_savedmodel` for TF 1.13,"I found several examples to load my saved estimator, my_estimator.export_savedmodel(export_dir, export_input_fn) model, as a predictor like so, predictor = tf.contrib.predictor.from_saved_model(export_dir). This works great when my tf.train.Example has only one item. How can I make it work for a batch for TF 1.13? call fails when there are multiple inputs per feature.",https://stackoverflow.com/questions/58843164,1585523,Documentation Replication on Other Examples
40090943,How can the tf.Variable be shared in between-graph replication training?,"I read the document of Distributed TensorFlow and have a question about between-graph replication. https://www.tensorflow.org/versions/master/how_tos/distributed/index.html In my understanding, between-graph replication training creates same number of graphs as workers and the graphs share tf.Variables on parameter servers. That is, one worker creates one session and one graph, and all graphs share same tf.Variable. However, I just thought two different sessions can not share the same tf.Variable. Is it misunderstanding?",https://stackoverflow.com/questions/40090943,6472530,Documentation Replication on Other Examples
58986077,AttributeError: module 'tensorflow._api.v1.compat.v1.nn' has no attribute 'avg_pool2d',can some help me with this? here's the code it should be tf.nn.avg_pool2d(...) like this is it? i did search for solution. but i dont really understand it.,https://stackoverflow.com/questions/58986077,12212094,Documentation Replicability
59074659,Best practice for allocating GPU and CPU resources in TensorFlow,"I'm wondering what is the correct way to set devices for creating/training a model in order to optimize resource usage for speedy training in TensorFlow with the Keras API? I have 1 CPU and 2 GPUs at my disposal. I was initially using a tf.device context to create my model and train on GPUs only, but then I saw in the TensorFlow documentation for tf.keras.utils.multi_gpu_model, they suggest explicitly instantiating the model on the CPU: I did this, and now when I train I see my CPU usage go way up with all 8 cores at about 70% usage each, and my GPU memory is maxed out. Would things go faster if the model were created on one of the GPUs? Even if I have just 1 GPU, is it still better to create model on CPU and use tf.device context to train the model on the GPU?",https://stackoverflow.com/questions/59074659,3711266,Documentation Replication on Other Examples
59166678,"In tf.function, how to convert dynamic tensor shape getted by tf.shape() to python values but not tensors itself?","I have a padded batch from tf.dataset, because every padded batch's shape is not fixed.So I have to use tf.shape method to get the dynamic shape of padded batch.The question is how can I convert the tensor shape getted by tf.shape to python values under tf.function? As the above code, I want to create a numpy array as the same shape of padded_batch，but 'shape' is a tensor, it can't be used directly in numpy.If there is someway to convert tensor to python values under tf.function. The tensorflow version I use is tf2.0",https://stackoverflow.com/questions/59166678,12476733,Documentation Replication on Other Examples
40394910,What do classes tf.train.Coordinator and class tf.train.QueueRunner do in tensorflow?,"I understand that both classes deal with threads. According to the documentation, tf.train.Coordinator coordinates the termination of a set of threads and tf.train.QueueRunner holds a list of enqueue operations for a queue, each to be run in a thread. However, what is their role in simple words? When are they necessary during the training?",https://stackoverflow.com/questions/40394910,1279459,Documentation Replicability
59260563,"TypeError from ""tf.keras.layers.concatenate"": The added layer must be an instance of class Layer. Found: Tensor","I'm trying to merge three layers and add it to model, but I got a Tensor from tf.keras.layers.concatenate, not a layer? How to solve this?",https://stackoverflow.com/questions/59260563,7815609,Documentation Replication on Other Examples
59422616,Get ValueError when calling tf.image.non_max_suppression,"I want to use tensorflow tf.image.non_max_suppression function. I tried both snippets below: where anchors_fit.shape is [36864,4] and rpn_cls_prob.shape is [36864] Both calls raise: What should i do?",https://stackoverflow.com/questions/59422616,11919966,Documentation Replication on Other Examples
59432671,Issue with tf.keras.backend.random_normal?,"I am very new to TF2 and tried to customize the example code on the tensorflow guide documentation: https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example but my code requires the latent dimension to be =1, and my code returned the: ValueError: The last dimension of the inputs to Dense should be defined. Found None. error. After trouble shooting I think the error is in the: where tf.keras.backend.random_normal sets epsilon always to dimensions [None,None] Then I just copied the example from the guide (reference above) and set the latent dimension to 1. For training i used the given code: same error: ValueError: The last dimension of the inputs to Dense should be defined. Found None. The code works fine if latent dimension is more than 1! Could somebody help me?",https://stackoverflow.com/questions/59432671,12573439,Documentation Replication on Other Examples
59528975,tf.estimator.predict slow with tensorflow ranking module,"I'm currently using the TensorFlow Ranking module for a recommendation task. Specifically I'm modifying this tutorial file for my own purpose. I can't say the tutorial was very friendly to new users. As this is the first time I've interacted with TensorFlow, I'm just trying to make it run. As you may notice, the example file doesn't say how to make predictions, so after I finished training the model, I modified its train_and_eval() function to predict. Here's my code. The predict_ function takes dictionary and returns a list a scores for all docs in that order. (or at least that's what I think it should do) The prediction results are very good. Problem is, it's really slow. Prediction for 400 docs takes more than 1 second (I only have 4 features). Is this a normal speed or there's space for optimization in the code? I heard tf.estimator reloads the graph everytime it makes a prediction, but I can't tell if it's the problem here.",https://stackoverflow.com/questions/59528975,10289249,Documentation Replication on Other Examples
59531864,Why does TensorFlow calculate 2D convolutions when 1D convolution is called?,"In the documentation of tf.nn.conv1d, it is stated that I get that the operations are equivalent, but I am a bit confused about the implications of this implementation detail. Does the reshaping create some computational overhead? The 3D convolution has its own implementation, so why not the 1D convolution? Thanks for any explanation that helps me and others to understand this implementation detail of TensorFlow!",https://stackoverflow.com/questions/59531864,6251391,Documentation Replicability
39282060,Nonlinear regression on tensorflow,"What activation and cost functions on tensorflow could be suitable below for tf.nn to learn a simple single variate nonlinear relationship f(x) = x * x that is a priori unknown? Certainly, this impractical model is used for the sole purpose of understanding tf.nn mechanics 101.",https://stackoverflow.com/questions/39282060,363663,Documentation Replication on Other Examples
58162110,How to port a tf.Session to a tf.train.MonitoredSession call while allowing graph modifications,"The code I'm working on is this. The code uses tf.session call to take in a graph for object detection tasks. Link My aim here is to profile this code for Nvidia GPUs using the nvtx-plugins-tf to analyze the time taken for different ops. Link to docs The plugin library provides a function hook for a tf.train.MonitoredSession as given in their example code here. The code linked above uses tf.session along with a tf.config and when I try to modify the tf.session call to a tf.train.MonitoredSession call, I can't get my code to work and it fails with an error that graph can't be modified. I went through the tensorflow APIs and it turns out that tf.session doesn't support hook callbacks and tf.train.MonitoredSession doesn't support tf_config as a function argument. Any directions to go in would be appreciated. If there are ways in tensorflow to use hooks in conjunction with tf.session, that will also work for me.",https://stackoverflow.com/questions/58162110,5232265,Documentation Replication on Other Examples
58338310,Predefined layers inside Custom layers,I want to use predefined layers from tf.keras.layers inside a custom layer. I want to create a custom layer that is a combination of dense and 1D Convolution layers. Is it possible to do something like that? I could not find an example in the tensorflow pages.,https://stackoverflow.com/questions/58338310,11698102,Lack of Alternative Solutions/Documentation
58384884,tensorflow.python.keras.api._v1.keras.losses' has no attribute 'Reduction',"I am using Huber loss implementation in tf.keras in tensorflow 1.14.0 as follows: I am getting the error AttributeError: module 'tensorflow.python.keras.api._v1.keras.losses' has no attribute 'Reduction' I have tried using tf.losses.Reduction, tf.compat.v2.losses.Reduction nothing seems to work. Did tensorflow remove Reduction from tf.keras.losses, it is strange if they did so because their documentation still shows: https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/keras/losses/Huber#args",https://stackoverflow.com/questions/58384884,11986203,Documentation Ambiguity
39627140,Why would the naive definition of moving average cause unnecessary locking in TensorFlow?,"The TensorFlow docs for tf.train.ExponentialMovingAverage say, Why does the first formula permit more concurrency than the second formula? How can I know if my own code is incurring unnecessary locking because of some subtle locking issue?",https://stackoverflow.com/questions/39627140,4943253,Documentation Replicability
39681026,Tensorflow: How to pass output from previous time-step as input to next timestep,"It is a duplicate of this question How can I feed last output y(t-1) as input for generating y(t) in tensorflow RNN? I want to pass the output of RNN at time-step T as the input at time-step T+1. input_RNN(T+1) = output_RNN(T) As per the documentation, the tf.nn.rnn as well as tf.nn.dynamic_rnn functions explicitly take the complete input to all time-steps. I checked the seq2seq example at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py It uses a loop and calls the cell(input,state) function. The cell can be lstm or gru or any other rnn cell. I checked the documentation to find the data type and shape of the arguments to cell(), but I found only the contructor of the form cell(num_neurons). I would like to know the correct way of passing output to input. I don't want to use other libraries/wrappers like keras built over tensorflow. Any suggestions?",https://stackoverflow.com/questions/39681026,2531306,Documentation Completeness
58561632,"Keras model_to_estimator doesn't log metrics on estimator.train, but logs on estimator.evaluate","I tried the keras model_to_estimator to build an estimator, but found that it wasn't logging metrics during training, although it does during evaluation. If that doesn't work out of the box, how can I use tf.summary.scalar or hooks in an estimator that consumes a tf.keras model?",https://stackoverflow.com/questions/58561632,3211422,Documentation Replicability
58728086,Passing `training=true` when using Tensorflow 2's Keras Functional API,"When operating in graph mode in TF1, I believe I needed to wire up training=True and training=False via feeddicts when I was using the functional-style API. What is the proper way to do this in TF2? I believe this is automatically handled when using tf.keras.Sequential. For example, I don't need to specify training in the following example from the docs: Can I also assume that keras automagically handles this when training with the functional api? Here is the same model, rewritten using the function api: I'm unsure if hid = tf.keras.layers.BatchNormalization()(hid) needs to be hid = tf.keras.layers.BatchNormalization()(hid, training)? A colab for these models can be found here.",https://stackoverflow.com/questions/58728086,12250264,Documentation Ambiguity
58730460,Freeze sublayers in tensorflow 2,"I have a model which is composed of custom layers. Each custom layer contains many tf.keras.layers. The problem is that if I want to freeze those layers after defining my model, the loop: only prints the ""outer"" custom layers and not those who exist inside. Is there any way to access the inner layers so I can freeze them? an example of a custom layer from the official tf docs:",https://stackoverflow.com/questions/58730460,7248145,Documentation Replication on Other Examples
39971297,Tensorflow: python tf.gradients equivalent in C++,"What is the equivalent of Python function tf.gradients(loss, [var]) in C++? Thanks!",https://stackoverflow.com/questions/39971297,6166308,Documentation Replication on Other Examples
58867494,"tf2.0: tf.image.resize_with_pad fails with ""using a `tf.Tensor` as a Python `bool"" with tf.keras.Input","With tensorflow 2.0, resize_with_pad does not seem to work when tf.keras.Input is given as an input, but resize works nicely. For example, throws an error",https://stackoverflow.com/questions/58867494,6476080,Documentation Replicability
58867635,Build tensorflow dataset with datasource as database,I have to create a data input pipeline with tensorflow tf.data. The datasource is a mongodb and sql server. How can I create a tf.data object from a database. All the articles I see have .tfrecords or .csv as datasource for tensorflow. Thank you. Appreciate your inputs,https://stackoverflow.com/questions/58867635,11093016,Documentation Replication on Other Examples
59174710,default tf.gradients in TensorFlow - total or partial derivatives?,"so I'm reading about tf.gradients() in the documentation (https://www.tensorflow.org/api_docs/python/tf/gradients) and I'm a bit confused. I've seen people stating that the results of tf.gradients() are This is also what I was thinking first. But then the documentation describes one optional arguments of this function as follows: So is it only possible to calculate the partial derivatives if I use 'stop_gradient' and otherwise the default values returned in a vector with len(xs) are total derivatives? Probably it's just my misunderstanding, it would be much appreciated if someone could elaborate on this a bit. Thanks a lot!",https://stackoverflow.com/questions/59174710,10274289,Requesting (Additional) Documentation/Examples
59177677,Custom aggregation for tf.GradientTape().gradient? (TF2.0),"As far as I know, the tf.gradients function provides option to choose the aggregation method for summarizing the gradients from multiple sources. https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/gradients However according to the Tensorflow API documentation, the tf.GradientTape().gradient method has no such option. https://www.tensorflow.org/api_docs/python/tf/GradientTape So my questions are as follows:",https://stackoverflow.com/questions/59177677,7386954,Documentation Ambiguity
59268676,"How to implement a tensorflow2 layer, tf.nn.conv1d_transpose inside a keras model architecture?","I need to use Transpose Conv1D layer which keras don't have yet , but tensorfow2 does . Till now i can only code in keras. Is there any way to implement a tf.nn.conv1d_transpose layer directly in a keras model along with other keras layers? Please provide some sample code.",https://stackoverflow.com/questions/59268676,8525083,Inadequate Examples
59359113,"What is the use of tf.keras.backend nowadays, is it safer/more future proof to code w/ or w/o it?","I understand the historical need for keras.backend in the long gone days of multiframework support. But now that we are talking about tf.keras, and since Keras is scheduled to support this toolkit only, I am wondering what is today's use for tf.keras.backend. From what I can see, it exposes only a fraction of the functions available in tf.*, and evolves more slowly. So, is tf.keras.backend",https://stackoverflow.com/questions/59359113,9973879,Documentation Replication on Other Examples
59361689,Redundancies in tf.keras.backend and tensorflow libraries,"I have been working in TensorFlow for about a year now, and I am transitioning from TF 1.x to TF 2.0, and I am looking for some guidance on how to use the tf.keras.backend library in TF 2.0. I understand that the transition to TF 2.0 is supposed to remove a lot of redundancies in modeling and building graphs, since there were many ways to create equivalent layers in earlier TensorFlow versions (and I'm insanely grateful for that change!), but I'm getting stuck on understanding when to use tf.keras.backend, because the operations appear redundant with other TensorFlow libraries. I see that some of the functions in tf.keras.backend are redundant with other TensorFlow libraries. For instance, tf.keras.backend.abs and tf.math.abs are not aliases (or at least, they're not listed as aliases in the documentation), but both take the absolute value of a tensor. After examining the source code, it looks like tf.keras.backend.abs calls the tf.math.abs function, and so I really do not understand why they are not aliases. Other tf.keras.backend operations don't appear to be duplicated in TensorFlow libraries, but it looks like there are TensorFlow functions that can do equivalent things. For instance, tf.keras.backend.cast_to_floatx can be substituted with tf.dtypes.cast as long as you explicitly specify the dtype. I am wondering two things:",https://stackoverflow.com/questions/59361689,4982425,Documentation Replication on Other Examples
59370662,RuntimeError: Invalid quantization params for op GATHER at index 2 in subgraph 0,"I use tflite convert tf.keras h5 model to tflite, and quantized model. than cause the error: RuntimeError: Invalid quantization params for op GATHER at index 2 in subgraph 0 does tflite not support GATHER op? how to fix it?",https://stackoverflow.com/questions/59370662,7707994,Documentation Replicability
40518132,regarding the usage of tf.tile and tf.pack,"I saw a program including the following two lines of codes The output_map is of shape (1,255,255,2). I can see sum_exp generally add the two channels for the exponential_map into one. So sum_exp should be of shape (1,255,255,1). But I am confused about what does tensor_sum_exp = tf.tile(sum_exp, tf.pack([1, 1, 1, tf.shape(output)[3]])) aim to do?",https://stackoverflow.com/questions/40518132,785099,Documentation Replicability
59458980,How to print the tensorflow objects?,"I want to print label_ids, predicted_labels and log_probs but I am not able achieve it using tf.print. My model is having a very poor accuracy so i need to check it step by step. Hence I want to print the tensor objects step by step when I cal the function. I am using tf 1.15.",https://stackoverflow.com/questions/59458980,10900497,Documentation Replicability
59555206,keras to tf.keras Conversion: Dense layer dimensions not defined?,"So I've built a convnet using pure keras. It compiles and operates exactly as intended, but I need to convert it to use tf.keras so that I can make use of tfmot. Having read documentation, I attempted to convert it, only to get the following error: The last dimension of the inputs to Dense should be defined. Found None. Any idea what I'm doing wrong? Thanks! Original keras model: Converted tf.keras model: EDIT 1: I thought maybe I could get around the issue by saving the keras model after creation and loading it as a tf.keras model immediately before compilation / training. That throws the same error!",https://stackoverflow.com/questions/59555206,6639365,Documentation Replication on Other Examples
40891533,"ValueError: Variable weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?",I can run my tensorflow code using tf.Variable but tf.get_variable is much more efficient. The above error is produced by this code: and I can't understand the reason. Any ideas?,https://stackoverflow.com/questions/40891533,6631259,Documentation Replicability
69672777,Compute Hessian of lossfunction in Tensorflow,"I would like to compute the hessian of a loss function of a neural network in Tensorflow with respect to all the parameters (or trainable variables). By modifying the example code from the Tensorflow documentation (https://www.tensorflow.org/api_docs/python/tf/GradientTape) I managed to compute the hessian w.r.t the weight matrix for the first layer (if I'm not mistaken): If I try to compute it w.r.t model.trainable_variables instead the tape.jacobian complains that 'list object has no attribute shape'. I instead tried to flatten the model.trainable_variables and compute it w.r.t the flattened vector: The problem now is that g is empty (NoneType) for some reason. I noticed that source is tf.Tensor-type but model.trainable_variables[0] was of type tf.ResourceVariable so I tried changing this by declaring source as This didn't change anything though, so I'm guessing that this is not the issue. I also thought that the problem might be that the source-variable is not watched, but it seems that it is set to trainable and even if i do tape.watch(source), g is still empty. Does anybody know how I can solve this?",https://stackoverflow.com/questions/69672777,9163968,Documentation Replication on Other Examples
50606178,TensorFlow tf.data.Dataset and bucketing,"For an LSTM network, I've seen great improvements with bucketing. I've come across the bucketing section in the TensorFlow docs which (tf.contrib). Though in my network, I am using the tf.data.Dataset API, specifically I'm working with TFRecords, so my input pipeline looks something like this How can I incorporate the bucketing method into a the tf.data.Dataset pipeline? If it matters, in every record in the TFRecords file I have the sequence length saved as an integer.",https://stackoverflow.com/questions/50606178,5368083,Documentation Replicability
69344858,How do I get the first 3 max numbers from tensor,"How do i get first 3 max number from y_classe = tf.argmax(preds, axis=1, output_type=tf.int32)?",https://stackoverflow.com/questions/69344858,9591542,Documentation Replicability
34589335,How does the distorted_inputs() function in the TensorFlow CIFAR-10 example tutorial get 128 images per batch?,"I was going through the CIFAR-10 example at TensorFlow getting started guide for CNN Now in the train function in cifar10_train.py we get images as In the distorted_inputs() function we generate the filenames in a queue and then read a single record as When I add debugging code, the read_input variable contains only 1 record with an image and its height, width, and label name. The example then applies some distortion to the read image/record and then passes it to the _generate_image_and_label_batch() function. This function then returns a 4D Tensor of shape [batch_size, 32, 32, 3] where batch_size = 128. The above function utilizes the tf.train.shuffle_batch() function when returns the batch. My question is where do the extra records come from in the tf.train.shuffle_batch() function? We are not passing it any filename or reader object. Can someone shed some light on how we go from 1 record to 128 records? I looked into the documentation but didn't understand.",https://stackoverflow.com/questions/34589335,628096,Documentation Replication on Other Examples
34619177,What does tf.nn.conv2d do in tensorflow?,"I was looking at the docs of tensorflow about tf.nn.conv2d here. But I can't understand what it does or what it is trying to achieve. It says on the docs, Now what does that do? Is that element-wise multiplication or just plain matrix multiplication? I also could not understand the other two points mentioned in the docs. I have written them below : It would be really helpful if anyone could give an example, a piece of code (extremely helpful) maybe and explain what is going on there and why the operation is like this. I've tried coding a small portion and printing out the shape of the operation. Still, I can't understand. I tried something like this: I understand bits and pieces of convolutional neural networks. I studied them here. But the implementation on tensorflow is not what I expected. So it raised the question. EDIT: So, I implemented a much simpler code. But I can't figure out what's going on. I mean how the results are like this. It would be extremely helpful if anyone could tell me what process yields this output. output",https://stackoverflow.com/questions/34619177,4341948,Documentation Replication on Other Examples
34801342,tensorflow: how to rotate an image for data augmentation?,"In tensorflow, I would like to rotate an image from a random angle, for data augmentation. But I don't find this transformation in the tf.image module.",https://stackoverflow.com/questions/34801342,3988198,Inadequate Examples
50763281,Does tensorflow only optimize/update tf.get_variable and tf.Variable?,Does TensorFlow only optimize tf.Variable and tf.get_variable tensors in the computational graph?,https://stackoverflow.com/questions/50763281,7887773,Documentation Replicability
39650169,SparseTensor equivalent of tf.tile?,"There's a tf.tile function, which takes a tensor and copies it a given number of times. How to achieve something similar with SparseTensors: ?",https://stackoverflow.com/questions/39650169,1951176,Documentation Replication on Other Examples
58699961,Problem of predicting with a loaded tensorflow estimator trained beforehand,"I had trained a tensorflow NN estimator to predict something in Python. And I saved the model in my Google drive via Google colab. Today, I loaded the model and it was a pretty hard work. I finally succeeded to load the estimator using tf.compat.v2.saved_model.load and .signature method. it seems WrappedFunction. This is my code till this step. But still I fail to put a test data to model to make a prediction. first, I tried: And it return an error Secondly, looked for some tensorflow document, I wrote this code This time it return I found dynamic/static shape of tensorflow. But I couldn't fully understand those concept and failed to reshaping. How can I get the result? Thanks.",https://stackoverflow.com/questions/58699961,10541065,Documentation Ambiguity
58822319,How to use tfa.seq2seq.BahdanauAttention with tf.keras functional API?,"I want to use tfa.seq2seq.BahdanauAttention with functional API of tf.keras. I have looked at the example given at tensorflow/nmt/attention_model.py. But I couldn't figure out how to use it with tf.keras's functional API. So I would like to use tfa.seq2seq.BahdanauAttention for a lipreading task, something like this: Thanks in advance.",https://stackoverflow.com/questions/58822319,7618706,Documentation Ambiguity
58828768,Do I need to switch between tensorflow and numpy?,"Data set is numpy set. Some tutorial said: because it is needed to in advantage of GPU, we should change numpy array to tensorflow tensor. And then use tensorflow model. But after training, some code use numpy function to test and interactive. But the code in tensorflow official tutorial still use the same tensorflow model and tf.dataset to test. I want to know: When testing or real time apply, should I use numpy or tensorflow tensor and model? In other words, is there some bad influences using tensorflow tensor and function if not traing? eg.: we use selected_words =tf.argsort(o_j) in stead of selected_words = np.argsort(o_j)",https://stackoverflow.com/questions/58828768,7241796,Documentation Replicability
58897066,How to print one example of a dataset from tf.data?,I have a dataset in tf.data. How can I easily print (or grab) one element in my dataset? Similar to:,https://stackoverflow.com/questions/58897066,4313927,Documentation Replication on Other Examples
58912135,Performance drawback of tf.numpy_function?,"I'm using a tf.numpy_function to load a file in my tensorflow program. I can't find sufficient information about the drawbacks of using numpy_function, would there be enough that it's worth the trouble of passing this function to tensorflow compatible code ? Thanks a lot",https://stackoverflow.com/questions/58912135,4064248,Documentation Replicability
59056872,Why class name change after saving a keras model?,i wrote a basic keras model (tf.keras.__version = 2.2.4-tf) using tensorflow (2.0.0) : Results are Where can I find the difference between tensorflow.python.keras.saving.saved_model.load.Sequential and tensorflow.python.keras.engine.sequential.Sequential in tensorflow or keras documentation?,https://stackoverflow.com/questions/59056872,5128148,Lack of Alternative Solutions/Documentation
59408065,tf.print from TF2 not printing to output,"I'm using TF2 and I would like to print the tensor inside a function running in a tf.data.Dataset pipeline. This is my code: But as result, I'm getting only this: How can I use tf.print to print the tensors to the console ?",https://stackoverflow.com/questions/59408065,2135819,Documentation Replicability
59607363,Tensorflow 2.0 dataset batching not working properly,"Tensorflow 2.0 dataset api's batch is not working as I expected it to work. I've made a dataset like this. This yields DatasetV1Adapter shapes: ((6,), ()), types: (tf.float32, tf.float32), and to this dataset I applied batch function from tf.data.Dataset. yields DatasetV1Adapter shapes: ((None, 6), (None,)), types: (tf.float32, tf.float32), and changing the batch size doesn't help at all. From official description of the batch, The way I thought this function would work, was to make [batch, 6], [batch,] but didn't work out well. I originally used pytorch, and started using TF 2.0 recently, and need some help on proper batching. Thanks in advance.",https://stackoverflow.com/questions/59607363,10690874,Documentation Replication on Other Examples
40742947,How can I change the output of tf.cond to tuple in Tensorflow?,"When I feed tuple of tensors into tf.cond, the output becomes a list not a tuple. How can I change it to the list to the tuple?",https://stackoverflow.com/questions/40742947,5522711,Documentation Replicability
59709349,How does tf.dataset interact with keras.conv1D?,"I'm using tf 1.15, i'm trying to make a regression task using a signal. First of all i load my signals into the pipeline, i have several files, here i simulate the loading using a np.zeros to make the code usable by you. Every file has this shape (?, 75000, 3), where ? is a random number of elements, 75000 is the number of samples in each element and 3 is the number of signals. Using the tf.data i unpack them and i get a dataset who output signals with this shape (75000,), and i use them in my keras model. Everything should be fine until i create the keras model, i copied my input pipeline because during my tests i got different errors using a generic tf.data.dataset or using the dataset built in this way. I get this error: Why? And how can i make it works? Also the input size of my net should be this one according the documentation: What is the steps? In my case i assume it should be (batch_size, 1, 75000), right?",https://stackoverflow.com/questions/59709349,12425142,Documentation Replication on Other Examples
40846881,"How do I finde the Code of ""tf.nn.dynamic_rnn"" in the tensorflow repository?","I'm trying to understand the structure and the coding of tensorflow. While going through this tutorial ""https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/"" I have searched for the code that is used in the functions. For example the line uses the function GRUCell, which I can find in the file ""rnn_cell.py"" in the tensorflow repository. Furthermore the GRUCell is wrapped by a function called ""tf.nn.dynamic_rnn"" as follows: Unfortunately I am not able to find the code for this function. Where do I find it? Everything I find is this documentation: https://github.com/tensorflow/tensorflow/blob/eb56a8af24695bf8258addf28b0c53fbabff72e6/tensorflow/g3doc/api_docs/python/functions_and_classes/shard8/tf.nn.dynamic_rnn.md Thanks for helping!",https://stackoverflow.com/questions/40846881,6716760,Lack of Alternative Solutions/Documentation
59723003,From .tfrecord to tf.data.Dataset to tf.keras.model.fit,"I am attemping to use Tensorflow (v2.0)'s Datasets API to pass large amounts of data to a tf.keras.model. Here is a simplified version of my dataset: ...and so on. Each record in the my_dataset object is a dictionary with the features' (and target's) names as the keys and associated tensors as the values. I created the dataset from several .tfrecord files, so I'm constrained in the sense that each tensor corresponds to a tf.train.Example (wrapper) object. The dataset precisely matches the format seen in tensorflow documentation (see, for example, the last code example in https://www.tensorflow.org/tutorials/load_data/tfrecord#reading_a_tfrecord_file). I would like to use this dataset with keras. The tf.keras.model objects I'm working with all seem, for their fit function, to take as input a tuple representing the feature vector (X) and the target (y). I think I could figure out how to transform the tensors from my dataset into numpy arrays and pass them into the model that way, or iterate over the dataset using an iterator, but if I understand correctly that seems to defeat the whole purpose of using the Datasets API to begin with (see, for example, https://www.tensorflow.org/guide/keras/overview#train_from_tfdata_datasets). My question: what is the appropriate way to transform my_dataset into some form that tf.keras.model.fit() will receive? Or if this is the wrong question, what fundamental concepts am I missing that keep me from asking the right one? (For example, should the .tfrecord Examples be structured differently? Or, am I required to use an iterator instead of directly passing my_dataset to the model as I'd prefer?)",https://stackoverflow.com/questions/59723003,3397173,Documentation Replicability
59727776,[Tensorflow 2.0][Tensor value printing],"We recently ported our Tensorflow1.12.0 source code to Tensorflow2.0. During debugging, we wanted to print the value of the tensors. For example, consider the following code snippet: In order to print the value of the tensor input, we just printed the variable using the tf.print(input) function according to [1] (previously just print(input)). So, the above code snippet can be re-written as follows: However, the output that we observed is the tensor object, instead of the exact value of the input tensor. Can anyone help me with the syntax to print the exact tensor values, instead of the tensor object. [1] https://www.tensorflow.org/api_docs/python/tf/print?version=stable Thanks.",https://stackoverflow.com/questions/59727776,7881422,Documentation Replicability
59962253,How do I use hparams with estimators?,"To log hparams without using Keras, I'm doing the following as suggested in the tf code here: Note that params is a dictionary object here that I'll pass to the estimator. Then I train the estimator as usual, After training, when I launch tensorboard, I do have the hparams logged, but I do not see any metrics logged against them I further confirmed that they show up in the scalars page with the same tag name for both train and validation i.e. . and ./eval, but the hparams page doesn't see those logged tensors. How do I use hparams with estimators? I'm using on Python 3.7.5 Attempt 1: After some googling, I saw some older tf code where they passed hparams to params argument of Estimator, so just to make sure if tf2 logs those hparams by itself when given, I checked the Estimator docs and it says: So using hparams as params will not be useful. Attempt 2: I doubt that since estimators use tensorflow.python.summary instead of tf.summary which is the default in v2, tensors logged by v1 was probably not accessible and so, I also tried to use However that failed with RuntimeError: tf.summary.FileWriter is not compatible with eager execution. Use tf.contrib.summary instead. Update: I ran it with eager execution disabled. Now, even the hparam initial logging did not happen. There was no hparams tab in tensorboard as it failed with error Is there a way to make tensorboard read already logged metric tensors and link them with hparams?",https://stackoverflow.com/questions/59962253,3211422,Documentation Replication on Other Examples
60067415,What is the correct usage of tf.image.yuv_to_rgb? Output is distorted,"I need to pass YUV images to my TF model. tf.image.yuv_to_rgb seems like the way to go, however it expects the YUV input to be of size [H,W,3] with Y normalized to [0,1] and UV to [-0.5,0.5] https://www.tensorflow.org/api_docs/python/tf/image/yuv_to_rgb. I'm new to the YUV format but this seems strange &amp; non-conventional to me based on other YUV-RGB conversion scripts I've found on the web: My question is: what is the correct usage of tf.image.yuv_to_rgb? Is there a better documentation or a pre-processing method provided by TF? I've tried modifying the above solutions to fit the range expected by tf.image.yuv_to_rgb but was getting distorted outputs. Below is my code (ipynb) and attached are my outputs: outputs",https://stackoverflow.com/questions/60067415,6567283,Requesting (Additional) Documentation/Examples
41283115,"Tensorflow, difference between tf.nn.softmax_cross_entropy_with_logits and tf.nn.sparse_softmax_cross_entropy_with_logits","I have read the docs of both functions, but as far as I know, for function tf.nn.softmax_cross_entropy_with_logits(logits, labels, dim=-1, name=None), the result is the cross entropy loss, in which the dimensions of logits and labels are the same. But, for function tf.nn.sparse_softmax_cross_entropy_with_logits, the dimensions of logits and labels are not the same? Could you give a more detail example of tf.nn.sparse_softmax_cross_entropy_with_logits?",https://stackoverflow.com/questions/41283115,5046896,Requesting (Additional) Documentation/Examples
41294094,Is there a difference between a placeholder and variable when not building a model?,"I am trying to understand the difference between a placeholder and a variable in TensorFlow: I also read the Stack Overflow's question below. I understand the difference when they are the input of a model. However, in general, if we are not building a model, is there still a difference between tf.placeholder() and tf.Variable()?",https://stackoverflow.com/questions/41294094,3993270,Documentation Ambiguity
60213882,Using Tensorflow Interleave to Improve Performance,"I have an input pipe that is performing poorly with low CPU, GPU, and disk utilization. I've been reading the tensorflow ""Better performance with tf.data API"" doc and the Dataset docs, but I don't understand what's going on well enough to apply it to my situation. Here's my current setup: Should I interleave the whole pipe right at the end (POINT2), before the prefetch? Or interleave imgd and outd separately, after each FixedLengthRecordDataset (POINT1A, POINT1B), and parallelize the maps? (need to keep the imgd and outd synced up!) What's up with Dataset.range(rvalue)---seems it's necessary but not obvious what rvalue to use? Is there a better overall plan? Note that the datasets are very large and do not fit in RAM.",https://stackoverflow.com/questions/60213882,2076669,Documentation Replicability
60215970,What's the cleanest and most efficient way to pass two stereo images to a loss function in Keras?,"First off, why am I using Keras? I'm trying to stay as high level as possible, which doesn't mean I'm scared of low-level Tensorflow; I just want to see how far I can go while keeping my code as simple and readable as possible. I need my Keras model (custom-built using the Keras functional API) to read the left image from a stereo pair and minimize a loss function that needs to access both the right and left images. I want to store the data in a tf.data.Dataset. What I tried: So, is there anything I did not consider? I read the documentation and found nothing about what gets considered as y_pred and what as y_true, nor about how to convert a dataset into a tensor smartly and without loading it all in memory. My model is designed as such: And my dataset is built as such (in case 2, while in case 1 only the function read_stereo_pair_from_line() changes):",https://stackoverflow.com/questions/60215970,5623016,Lack of Alternative Solutions/Documentation
41437483,How does tf.nn.conv2d calculate its values?,"I've looked at the documentation for tf.nn.conv2d but it didn't really help much. So I tried to multiply the first 4 values of my input array that form a square (0, 1, 2, 2.5) with the first column of the filter_weights array (0.19041163, -0.36705261, 0.69018674, 1.7655524). But regardless of how I multiply these values I'm not getting 1.13938534, I don't know what I'm doing wrong. Below I have the code that I used. Given an array: And weights: Which prints: How does: Give:",https://stackoverflow.com/questions/41437483,3529361,Lack of Alternative Solutions/Documentation
41534593,Why would I ever use tf.concat instead of tf.stack?,Is there a good reason to use tf.concat instead of tf.stack? They seem very similar. Is it just to guarantee that the resulting tensor will have the same number of dimensions as the input list of tensors?,https://stackoverflow.com/questions/41534593,7287271,Documentation Ambiguity
45634450,What are the advantages of using tf.train.SequenceExample over tf.train.Example for variable length features?,"Recently I read this guide on undocumented featuers in TensorFlow, as I needed to pass variable length sequences as input. However, I found the protocol for tf.train.SequenceExample relatively confusing (especially due to lack of documentation), and managed to build an input pipe using tf.train.Example just fine instead. Are there any advantages to using tf.train.SequenceExample? Using the standard example protocol when there is a dedicated one for variable length sequences seems like a cheat, but does it bear any consequence?",https://stackoverflow.com/questions/45634450,7000919,Documentation Replicability
64642944,Steps of tf.summary.* operations in TensorBoard are always 0,"When I'm training my model with TensorFlow 2.3, I want to visualize some intermediate tensors calculated using the weight in the computation graph of my customized tf.keras.layers.Layer. So I use tf.summary.image() to record these tensors and visualize them as images like this: But in TensorBoard, no matter how many steps passed, there is only one image of step 0 shown. And I tried to set the parameter step of tf.summary.image() to the value obtained from tf.summary.experimental.get_step(): And update the step by calling tf.summary.experimental.set_step from a customized Callback using a tf.Variable like codes shown below: This Callback's instance is passed in the argument callbacks in model.fit() function. But the value tf.summary.experimental.get_step() returned is still 0. The TensorFlow document of ""tf.summary.experimental.set_step()"" says: Accroding to the document, I am already using a Variable to store the steps, but it's changes are still not reflected inside the function (or keras.Model). Note: My code produces expected results in TensorFlow 1.x with just a simple line of tf.summary.image() before I migrate it to TensorFlow 2. So I want to know if my approach is wrong in TensorFlow 2? In TF2, how can I get training steps inside the computation graph? Or there is other solution to summarize tensors (as scalar, image, etc.) inside a model in TensorFlow 2?",https://stackoverflow.com/questions/64642944,14562728,Documentation Replication on Other Examples
64652064,Finding intersection of two tensors,"I have used that the dice coefficient is calculated by 2xintersection/union in semantic segmentation neural networks. And the intersection of y_true and y_pred is found by tf.math.reduce_sum(y_pred*y_true). Please someone can help me figure out, how the multiplication of two tensors are equal to the intersection?",https://stackoverflow.com/questions/64652064,14042046,Documentation Replication on Other Examples
45678931,tensorflow input pipeline returns multiple values,"I'm trying to make an input pipeline in tensorflow for image classification, therefore I want to make batches of images and corresponding labels. The Tensorflow document suggests that we can use tf.train.batch to make batches of inputs: However, I'm thinking would it be a problem if I feed in the graph like this: The question is does the operation in the cost function dequeues images and their corresponding labels, or it returns them separately? Therefore causing the training with wrong images and labels.",https://stackoverflow.com/questions/45678931,8075619,Documentation Replicability
64687375,Get labels from dataset when using tensorflow image_dataset_from_directory,"I wrote a simple CNN using tensorflow (v2.4) + keras in python (v3.8.3). I am trying to optimize the network, and I want more info on what it is failing to predict. I am trying to add a confusion matrix, and I need to feed tensorflow.math.confusion_matrix() the test labels. My problem is that I cannot figure out how to access the labels from the dataset object created by tf.keras.preprocessing.image_dataset_from_directory() My images are organized in directories having the label as the name. The documentation says the function returns a tf.data.Dataset object. Here is the code: I have tried using (foo, foo1) = tf.keras.preprocessing.image_dataset_from_directory(dataDirectory, etc), but I get (trainData, trainLabels) = tf.keras.preprocessing.image_dataset_from_directory( ValueError: too many values to unpack (expected 2) And if I try to return as one variable and then split it as so: I get TypeError: 'BatchDataset' object is not subscriptable I can access the labels via testClasses = testData.class_names, but I get: I am open to any method to get those labels into the confusion matrix. Any ideas as to why what I am doing is not working would also be appreciated. UPDATE: I tried the method proposed by Alexandre Catalano, and I get the following error I printed the first element of the labels array, and it is zero",https://stackoverflow.com/questions/64687375,6419985,Documentation Replication on Other Examples
46062649,tensorflow slim concurrent train and evaluation loops; single device,"I am interested in using the tensorflow slim library (tf.contrib.slim) to do evaluation of a model performance on a(n) (entire) test set periodically during training. The documentation is pretty clear that slim.evaluation.evaluation_loop is the way to go, and it looks promising. The issue is that I don't have a second gpu to spare, this model parameters take up an entire gpu's worth of memory, and I would like to do concurrent evaluation. For example, if I had 2 GPUs, I could run a python script that terminated with ""slim.learning.train()"" on the first gpu, and another that terminated with ""slim.evaluation.evaluation_loop()"" on the second gpu. Is there an approach that can manage 1 gpu's resources for both tasks? tf.train.Supervisor comes to mind, but I don't honestly know.",https://stackoverflow.com/questions/46062649,3391229,Documentation Replication on Other Examples
64947184,"converting very large raw file to csv file, tf.data.Dataset","I have a 200GB .raw file with tab as column separator (It has a tab before first column too). I want to convert this file to .csv file with comma as column separator. I used the following commands: However, when I want to read this csv file (File2.csv) with tf.data.Dataset (https://colab.research.google.com/github/adammichaelwood/tf-docs/blob/csv-feature-columns/site/en/r2/tutorials/load_data/csv.ipynb#scrollTo=sUtoed20cRJJ), then I get this error: I am sure my .raw file is fine. How can I convert File1.raw file to File2.csv file with comma as column separator without producing duplicated columns?",https://stackoverflow.com/questions/64947184,8155727,Documentation Replication on Other Examples
46139202,Tensorflow: TypeError with numpy_input_fn,I am coding a Convolutional Neural Network to classify images in TensorFlow but there is a problem: When I try to feed my NumPy array of flattened images (3 channels with RGB values from 0 to 255) to a tf.estimator.inputs.numpy_input_fn I get the following error: My numpy_imput_fn looks like this: In the documentation for the function it is said that x should be a dict of NumPy array:,https://stackoverflow.com/questions/46139202,5423940,Documentation Replicability
46418686,tf.nn.dynamic_rnn shape error in seq2seq,"I am attempting to write my own basic seq2seq classifier. Im doing this by using tf.nn.dynamic_rnn and the code is shown below. However, there seems to be a problem with the shape of the tensor I'm sending to tf.nn.dynamic_rnn. The reason I'm doing this is because tensorflow's documentation when it comes to seq2seq is very much all over the place. Running gives me the error: ValueError: Cannot feed value of shape (128, 10) for Tensor 'decoding/rnn/transpose:0', which has shape '(128, 10, 32)'. The graph is shown below:",https://stackoverflow.com/questions/46418686,2530674,Documentation Replicability
65260831,Why does tf.image.ssim always return AttributeError: 'numpy.ndarray' object has no attribute 'get_shape',"I am trying to use tf.image.ssim to get the similarity between 2 images, however, it returns an attribute error. Since I am just directly using the TensorFlow code, I don't see any way to debug this issue.",https://stackoverflow.com/questions/65260831,11628437,Documentation Replicability
65277703,image normalization and TPU,"I'm trying to incorporate image normalization in my keras model to run on Google's cloud TPU. Therefore I inserted a line into my code: There was nor error thrown, but according the documentation of google tf.image.per_image_standardization is not a supported function. Does anybody know if it works anyhow, or does anybody have an idea how to check if it works?",https://stackoverflow.com/questions/65277703,14818604,Documentation Replicability
46484373,Return a tf.Variable from an Estimator,"I have an Tensorflow Estimator defined by a model function in the usual way. I want to determine which of my (zscore normalised) inputs are significant to the result, and which can be eliminated. I have altered the model to introduce two changes: (1) A new layer weight_layer which is randomly intialized and elementwise multiplied with input_layer. (2) A penalty sparsity which is added to the loss function to penalize the loss by the sum of the weights in weight_layer The trouble comes at prediction time, when I try to return the values of weight_layer, as follows: I get the following error: This seems odd, since although predictions[sparsity] is not a Tensor, it is a tf.Variable, and the tf.Variable documentation suggests I can treat a tf.Variable 'like a normal tf.Tensor'. How can I fix the above to return the weight_layer, or if I there is a more fundamental mistake, please recommend a way for me to determine which of my input variables are significant.",https://stackoverflow.com/questions/46484373,1536634,Documentation Replication on Other Examples
65634172,What's the difference between tf.math.reduce_mean and tf.keras.metrics.Mean?,"I'm confused what function to use. They look the same and do the same work. I understand that tf.keras.metrics.Mean is metric, but can't I use it somewhere else?",https://stackoverflow.com/questions/65634172,13781254,Documentation Ambiguity
65640885,CRNN tf.keras.backend.ctc_decode. What is log probability?,"Based on the documentation, the function tf.keras.backend.ctc_decode is supposed to return a Tuple. Its first field contains the best path (let's assume we use greedy search), whereas the second one contains its log probability. Is this probability actually the accuracy of the prediction? If not how am I supposed to calculate it? I've tried on some test images and this was my output: During the training part, the final CTC loss was around 0.1 and the prediction is always correct. However what I think it's the probability seems not to be what I expect. They looks like completely random numbers, even grater than 1 or 2! What am I doing wrong?",https://stackoverflow.com/questions/65640885,14967854,Documentation Replication on Other Examples
65649660,Combine arbitrary shaped tensors,"I'd like to combine two variable length tensors. Since they don't match in shape I can't use tf.concat or tf.stack. So I thought I'd flatten one and then append it to each element of the other - but I don't see how to do that. For example, Is there a method like this?",https://stackoverflow.com/questions/65649660,5239473,Inadequate Examples
65696549,How to clean \xc2\xa0 using tf.strings.regex_replace?,"I tried the the following but does not work: or But both don't work. the python string replace method should work but as part of tf.data.Dataset pipeline that can potentially part of a model layer, i will have to use tf.string.replace(...). I also could do the string processing outside of tf pipeline but it is not elegant/robust solution.",https://stackoverflow.com/questions/65696549,1762295,Documentation Replication on Other Examples
65712409,How to convert tf2 model so it will run on tflite interpreter,"Background: I am trying to convert the tf2 model for SSD MobileNet V2 FPNLite 320x320 (for example) from the official tf zoo. The model should run eventually on raspberry pi, so I would like it to run on the tflite interpreter (without full tf). The docs imply that ssd model conversion is supported. Whats happening: the process is detailed in this colab notebook. It is failing with the error: if I add the flag tf.lite.OpsSet.SELECT_TF_OPS, it works but wont run on the rpi, as it does not have the ops. Can this be done? Has anyone succeeded?",https://stackoverflow.com/questions/65712409,9694304,Documentation Replication on Other Examples
47167409,Using weights initializer with tf.nn.conv2d,"When using tf.layers.conv2d, setting the initializer is easy, it can be done through its parameter. But what if I use tf.nn.conv2d? I use this code. Is this equivalent to setting the kernel_initializer parameter in tf.layers.conv2d? Although the program runs without errors, I don't know how to verify whether it does what it is expected do.",https://stackoverflow.com/questions/47167409,6204637,Documentation Replicability
66019998,"How to get a processed dataset, if the processing steps are not tensor operations?","I have an instance of tf.data.Dataset(), of images, basically, acquired this way: So, this dataset has (data, label) where the data is a tensor of shape (batch_size, image_height, image_width, channels) [I don't really need the labels it assigns]. So far so good. The problem is, I need to process this dataset, applying certain operations to the images, and, this dataset is too big to load everything in memory (that's why batch_size is there). According to the tensorflow documentation, tf.data.Dataset.map() is the function I need (or so I assume....). First of all, the shape returned by the print: (None, 200, 200, 3) instead of (32, 200, 200, 3), or, instead of (200, 200, 3) [which is what I'd expect from reading the documentation] [let's assume batch of 32, and images 200x200], and this is messing my code, because, I need to do assigments, like, take the ith image, and change a couple pixels: data[i][12:15,40:50] = np.array([1,2,3]) and things like that. Basically, that's the error message: TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'. In summary, my question: How can I get a processed_dataset, where the processing steps will not be whole tensor operations, but instead, will be changing individual values in the data (say, individual pixels), for certain images (say, the ith image, jth image, etc)? If you must know, I am running this in Ubuntu. Tensorflow version is:",https://stackoverflow.com/questions/66019998,15049194,Documentation Replication on Other Examples
66030439,TensorFlow profiler using tf.profiler.experimental.client.trace gives empty trace data,"I'm unable to collect trace data using tf.profiler.experimental.client.trace Please can someone help? I'm following the (CPU/GPU) example usage here https://www.tensorflow.org/api_docs/python/tf/profiler/experimental/client/trace which looks simple enough. I have a very simple model, and I'm able to collect trace data from it using tf.profiler.experimental.start and tf.profiler.experimental.stop. But tf.profiler.experimental.client.trace gives me empty trace data. My code is as follows: The code runs through the epochs, and then outputs I've tried locating tf.profiler.experimental.server.start and tf.profiler.experimental.client.trace in other locations in the code, but with no success.",https://stackoverflow.com/questions/66030439,15131641,Documentation Ambiguity
66038861,Why are both branches in tf.cond being executed? And why does tf.while_loop finish the loop even though the condition still true?,"I am using keras for a while now, but usually I don't have to use customized layers or perform some more complex flow control, so I'm struggling trying to understand somethings. I am modeling a neural network with a customized layer on the top. This customized layer calls another function (search_sigma) and inside this function I execute tf.while_loop and inside of tf.while_loop I execute tf.cond. I cannot understand why the conditions are not working. Could someone help me understand what I am missing? I already tried to change both tf.cond and tf.while_loop conditions for true tensors, just to see what would happen. The behavior (exactly same errors) remained the same. I also tried to write this code without implementing a class (using just functions). Nothing changed. I tried to find solutions looking at tensorflow documentation, other stack overflow doubts and websites talking about tf.while_loop and tf.cond. I left some print()s in the body of the code to try to track what was happening. The piece of code that calls the above code is: 'inputs' is a (None, 10) size tensor 'self.sigma' is a (10,) size tensor 'self.clusters' is a (N, 10) size tensor",https://stackoverflow.com/questions/66038861,15141021,Inadequate Examples
66049816,Custom layer in sequential model tensorflow,"I'm trying to create a custom layer for my model, which can be used the classic Dense layer of Keras. Here my custom layer: It does not do anything 'custom' for now. But when I add it to my model I get this: Because probably I'm creating directly the object of class Custom Layer. But I do not find in the tf documentation how to add other properties to make it work as a normal layer, i.e. as something like layers.Dense(100, activation=tf.nn.relu) Is there a way to make it work like that ?",https://stackoverflow.com/questions/66049816,12338521,Lack of Alternative Solutions/Documentation
47205160,Tensorflow v1.4: Layer.input not supported in Eager mode,"I understand that Eager mode is a new alpha feature on the nightly builds and that it is not perfect yet, but I do not know if there are any tf.keras workarounds for this problem. The error Layer.input not supported in Eager mode. triggers on the block I do not know anything about keras or the keras tensorflow API and I was wondering if there was a way to avoid Layer.input with keras techniques so as to stay within Eager mode. Following a tutorial in the tf.Eager docs I have confirmed that model = tf.layers.Dense(1) works but I don't know how to add another layer. Any help is very much appreciated. EDIT As of tensorflow v1.10, keras is supported in eager mode.",https://stackoverflow.com/questions/47205160,5496253,Documentation Replication on Other Examples
66062973,TensorFlow custom loss function error: No gradients provided for any variable,"I am creating a custom loss function using tf.raw_ops namespace to train my model using keras. Here is my loss function: Loss(pred,label)= { 0.0 if pred−label&lt;=0.1, 1.0 elsewhere And then I am using this in my I am getting an error I know this may be because some of the tf.operations that I am using are not differentiable, or doesn't have a gradient. However, I have checked this page https://docs.w3cub.com/tensorflow~2.3/raw_ops It shows which operations are differentiable and which are not. All of my operations are differentiable. I am not sure what am I missing. Any help is appreciated.",https://stackoverflow.com/questions/66062973,14408155,Documentation Ambiguity
58520594,"tf.Data.Dataset - On each Epoch, only train with a sub sample of the full dataset","I have an image dataset with a large imbalance of positive and negatives samples (many more negatives). I would like to create a tf.data.Dataset where each epoch it will train with all of the positive samples but only (ratio * len(positive) ) of the negative samples. I am currently using a datagen inherited from keras.util.Sequence to achieve this and using this subsampling policy is performing much better than training on all data. However reading the docs on Dataset, I cannot seem to find a way to do it, is it possible? In my existing data generator, I am doing this:",https://stackoverflow.com/questions/58520594,2252698,Lack of Alternative Solutions/Documentation
58630393,Does tf.keras.metrics.AUC work on multi-class problems?,"I have a multi-class classification problem and I want to measure AUC on training and test data. tf.keras has implemented AUC metric (tf.keras.metrics.AUC), but I'm not be able to see whether this metric could safely be used in multi-class problems. Even, the example ""Classification on imbalanced data"" on the official Web page is dedicated to a binary classification problem. I have implemented a CNN model that predicts six classes, having a softmax layer that gives the probabilities of all the classes. I used this metric as follows and the code was executed without any problem. However, sometimes I see some results that are quite strange for me. For example, the model reported an accuracy of 0.78333336 and AUC equal to 0.97327775, Is this possible? Can a model have a low accuracy and an AUC so high? I wonder that, although the code does not give any error, the AUC metric is computing wrong. Somebody may confirm me whether or not this metrics support multi-class classification problems?",https://stackoverflow.com/questions/58630393,9176854,Documentation Replication on Other Examples
58631390,What is the purpose of tf.compat?,"What's the purpose of tf.compat module? It looks like just the entire Tensorflow API is replicated inside this module. The documentation states So why there is a ""v1"" and a ""v2"" submodule? What are the compatibility problems address by tf.compat specifically?",https://stackoverflow.com/questions/58631390,1719931,Documentation Completeness
58680578,Return from tf.map_fn,"Two types of returns from bbox_organize(). The first one from if statement is [5] and second one from else statement is (2,5). How can I put it into boxes_ from tf.map_fn?",https://stackoverflow.com/questions/58680578,2467772,Documentation Replicability
58748202,Difference between feature_column.embedding_column and keras.layers.Embedding in TensorFlow,"I have been using keras.layers.Embedding for almost all of my projects. But, recently I wanted to fiddle around with tf.data and found feature_column.embedding_column. From the documentation: feature_column.embedding_column - DenseColumn that converts from sparse, categorical input. Use this when your inputs are sparse, but you want to convert them to a dense representation (e.g., to feed to a DNN). keras.layers.Embedding - Turns positive integers (indexes) into dense vectors of fixed size. e.g. [[4], [20]] -&gt; [[0.25, 0.1], [0.6, -0.2]] This layer can only be used as the first layer in a model. My question is, is both of the api doing similar thing on different type of input data(for ex. input - [0,1,2] for keras.layers.Embedding and its one-hot-encoded rep. [[1,0,0],[0,1,0],[0,0,1] for feature_column.embedding_column)?",https://stackoverflow.com/questions/58748202,4402524,Documentation Replication on Other Examples
58877056,why not use tensor or tf.data in tensorflow official tutorial?,"I fond the example just use numpy array as input(https://www.tensorflow.org/tutorials/keras/classification). Why not change numpy array to tensorflow tensor or tf.data? If both are ok, why need tensorflow tensor?",https://stackoverflow.com/questions/58877056,7241796,Documentation Replication on Other Examples
58963043,What is the difference between tf.keras.metrics.Accuracy and tf.keras.metrics.BinaryAccuracy?,It seems I can't use tf.keras.metrics.Accuracy instead of tf.keras.metrics.BinaryAccuracy. Why is that? I have a trainer based on this tutorial: I'm also able to train with metrics=['accuracy']. What class does tf use in this case? By the way tf.keras.metrics.Accuracy has an attribute name which is accuracy.,https://stackoverflow.com/questions/58963043,2047442,Documentation Replicability
58963513,TypeError: Cannot convert provided value to EagerTensor. Provided value: 0.0 Requested dtype: int64,I am trying to train the transformer model available from the tensorflow official models. I am able to train in cpu without any error but when I try gpu I get the following error: I tried tf.cast but it doesn't seem to help.,https://stackoverflow.com/questions/58963513,8258005,Documentation Replicability
58963793,ValueError: Shapes must be equal rank in assign_add(),"I am reading tf.Variable in Tensorflow r2.0 in TF2: And also how come the following returns false? My other question is that when we are in TF 2, we should not use tf.Session() anymore, correct? It seems we should never run session.run(), but the API document keys doing it with tf.compat.v1, etc. So why they are using it in TF2 docs? Any help would be appreciated. CS",https://stackoverflow.com/questions/58963793,2277812,Documentation Completeness
40195549,tf.rank function in Tensorflow,"I ma trying to understand tf.rank function in tensorflow. From the documentation here, I understood that rank should return the number of distinct elements in the tensor. Here x and weights are 2 distinct 2*2 tensors with 4 distinct elemnts in each of them. However, rank() function outputs are: Also, for the tensor x, I used tf.constant() with dtype = float to convert ndarray into float32 tensor but the rank() still outputs as int32. How should I interpret the output.",https://stackoverflow.com/questions/40195549,2289031,Documentation Replication on Other Examples
40347742,Closed Tensorflow FIFO queue not throwing exception,"I'm trying to run the following code on a CPU machine: I'm basically creating some dummy features and targets (for a classification task), push the data to a TF FIFO queue and close the queue to simulate some behavior I will need in the fully-fledged system code. Since the FIFO queue is closed, I'd expect the dequeue() function to throw an tf.errors.OutOfRangeError exception, once the queue gets empty, as explained in https://www.tensorflow.org/versions/r0.11/api_docs/python/io_ops.html#queues This should happen after iteration 7, since we have 8 elements in the features/target data. However, the program just keeps hanging - I guess due to some blocking call. What am I doing wrong here? RCB",https://stackoverflow.com/questions/40347742,7096427,Documentation Ambiguity
59299060,TF 2.0 while_loop and parallel_iterations,"I am trying to use tf.while_loop to run loops in parallel. However, in the following toy examples,loops don't appear to be running in parallel. Or What is preventing the tf.while_loop from parallelizing the loop? In addition, if anyone who maintain the Tensorflow documentation see this page, he/she should fix the bug in the first example. See the discussion here. Thanks.",https://stackoverflow.com/questions/59299060,12483081,Requesting (Additional) Documentation/Examples
59493127,Does tf.keras.losses.categorical_crossentropy return an array or a single value?,"I'm using a custom training loop. The loss that is returned by tf.keras.losses.categorical_crossentropy is an array of I'm assuming (1,batch_size). Is this what it is supposed to return or a single value? In the latter case, any idea what I could be doing wrong?",https://stackoverflow.com/questions/59493127,7694977,Documentation Replicability
60047705,Repeated use of GradientTape for multiple Jacobian calculations,"I am attempting to compute the Jacobian of a TensorFlow neural network's outputs with respect to its inputs. This is easily achieved with the tf.GradientTape.jacobian method. The trivial example provided in the TensorFlow documentation is as follows: This is fine if I want only want to compute the Jacobian of a single instance of the input tensor x. However, I need to repeatedly evaluate this Jacobian many, many times for various instances of x. For a non-trivial Jacobian calculation (e.g. for a deep convolutional neural network with non-linear activation functions), this is incredibly expensive to repeatedly rerun the GradientTape calculation and evaluate the jacobian method. I know from the TensorFlow documentation that the gradients (and hence the Jacobian) are computed via automatic differentiation. I have to imagine there is some internal storage of the analytical gradient of the network (computed by automatic differentiation) which is evaluated at the given inputs. My question: am I correct in assuming that TensorFlow builds and stores (at least parts of) the analytical gradients needed to compute the Jacobian? And if so, is there a way to save this analytical gradient and re-evaluate the Jacobian with new inputs without having to reconstruct it via the GradientTape method? A ""persistent"" GradientTape does not seem to solve this issue: it only allows for the repeated evaluation of a single GradientTape instance with respect to multiple internal arguments of the computation.",https://stackoverflow.com/questions/60047705,11737392,Documentation Replication on Other Examples
66084126,how to convert grayscale image to rgb RGB image?,"I'm trying to convert a grayscale image to rgb image,does tensorflow.js have an function like tf.image.grayscale_to_rgb in tensorflow to do this?",https://stackoverflow.com/questions/66084126,15136301,Inadequate Examples
47254265,dynamic batch size with tf.data or tf.contrib.datta,"I can use a placeholder as the batch_size with tf.train.batch_join() (Which is queue based,) so I can dynamically change the batch size in the training loop. But when I use placeholder (or a nontrainable variable) as the batch_size for tf.data.Dataset.batch(), I got this error, The whole error stack trace is very long. I traced the error to v1.4 tensorflow/python/data/ops/dataset_ops.py:108 in make_one_shot_iterator() Full stack trace attached. I was trying the official tf resnet model. Thanks!!",https://stackoverflow.com/questions/47254265,1702121,Documentation Replicability
47558813,How does tensorflow tf.contrib.image.rotate works?,I'm new in using tensorflow and also python and play little bit with it and can't figure out how tf.contrib.image.rotate works that i can use it as tensor for example in tf.cast operation. Maybe someone can help me out to figure out how it can be used? Greetz,https://stackoverflow.com/questions/47558813,9027227,Documentation Replication on Other Examples
47568998,Tensorflow: Load data in multiple threads on cpu,"I have a python class SceneGenerator which has multiple member functions for preprocessing and a generator function generate_data(). The basic structure is like this: I used the class member function sceneGenerator.generate_data() in keras model.fit_generator() function to read the data from disk, preprocess it and yield it. In keras, this is done on multiple CPU threads, if the workers parameter of model.fit_generator() is set to something &gt; 1. I now want to use the same SceneGenerator class in tensorflow. My current approach is this: This, however, is slow and does not use multiple threads. I found the tf.data.Dataset api with some documentation, but I fail to implement the methods. Edit: Notice that I do not work with images so that the image loading mechanisms with file paths etc. do not work here. My SceneGenerator loads data from hdf5 files. But not complete datasets but - depending on the initialization parameters - only parts of a dataset. I would love to keep the generator function as it is and learn how this generator can be directly used as input for tensorflow and runs on multiple threads on the CPU. Rewriting the data from the hdf5 files to csv is not a good option because it duplicated lots of data. Edit 2:: I think something similar to this could help: parallelising tf.data.Dataset.from_generator",https://stackoverflow.com/questions/47568998,3971621,Documentation Replication on Other Examples
66467746,How to use sklearn.preprocessing in tf.data.Dataset.map?,"I would like to find a way to use sklearn.preprocessing inside tf.data.Dataset.map() in TF2. Let's say I have a dataset generated from Then calculate the QuantileTransformer However, I was not able to use QuantileTransformer in tf.data.Dataset.map. For example, gives error",https://stackoverflow.com/questions/66467746,5310949,Documentation Replicability
66478545,Keras nomalise axis argument,"x_train = tf.keras.utils.normalize(x_train,axis = 1) Guys what's the axis argument here really means?? My image shape is (60000,28,28)",https://stackoverflow.com/questions/66478545,14187980,Documentation Ambiguity
44753916,How to slice a part of tensor?,"I want to slice [3.0 ,33.0].I have tried to access this slice by following code. I'm not so clear about tf.slice command. I'm not so clear about begin and size mentioned in documentaion about this command. Can someone please make it easy to understand.",https://stackoverflow.com/questions/44753916,8214056,Documentation Ambiguity
63624699,how to transform a model built with tensorflow to keras api?,"So in a current projet i have to modify the official code from this paper , and i'm pretty new to tensorflow and all his concepts, i worked with tf.keras but the actual code is written with tensorflow (version 1.7 which is old...), i wonder if someone can explain how the model works or had an idea for an equivalent to write it with keras and a recent version of tensorflow(tf version 2) the original code is in this repo(ConvNCF.py)",https://stackoverflow.com/questions/63624699,12987394,Documentation Replicability
44796793,Difference between tf.clip_by_value and tf.clip_by_global_norm for RNN's and how to decide max value to clip on?,Want to understand the difference in roles of tf.clip_by_value and tf.clip_by_global_norm during the implementation of Gradient Clipping in TensorFlow. Which one is preferred and how to decide the max value to clip on?,https://stackoverflow.com/questions/44796793,7730199,Documentation Replicability
44887367,"TensorFlow, tf.one_hot why the shape of output is defined by the value of axis?","I read the docs of tf.one_hot and found that What is The new axis? If indices is a matrix (batch) with shape [batch, features], the output shape will be: Why the shape of output is defined by axis?",https://stackoverflow.com/questions/44887367,5046896,Lack of Alternative Solutions/Documentation
63919438,TensorFlow keras model fit() parameters steps_per_epoch and epochs behavior on train set,"I'm using a tf.data dataset containing my training data consisting of (lets say) 100k images. I'm also using a tf.data dataset containing my validation set. Since an epoch of all 100k images takes quite long (in my case approximately one hour) before I get any feedback on performance on the validation set, I set the steps_per_epoch parameter in tf.keras.Model fit() to 10000. Using a batch size of 1 this results into having 10 validation scores when reaching 100k of images. In order to complete one epoch of 100k images of my entire training dataset, I set the epochs parameter to 10 However, I'm not sure if using steps_per_epoch and epochs this way has any other consequences. Is it correct to use these parameters in order to get more frequent feedback on performance? And also a more specific question, does it use all 100k images or does it use the same first 10k images of my training set at every 'epoch'? I already dug into the TensorFlow docs and read several different stack overflow questions, but I couldn't find anything conclusive to answer my own question. Hope you can help! Tensorflow version I'm using is 2.2.0.",https://stackoverflow.com/questions/63919438,3908025,Inadequate Examples
44905344,tensorflow.ones_like on a SparseTensor,"In tensorflow, I want to do tf.ones_like on a SparseTensor; however, it seems it only works for normal (dense) tensors. Do you know any function or workaround? As an example, I want to go from [['aa','ab','ac'],['ba','bb', UND],['ca',UND,UND] to [[1,1,1],[1,1,UND],[1,UND,UND]], where UND = undefined. Thanks!",https://stackoverflow.com/questions/44905344,4189580,Documentation Replicability
63953040,How to use a TFRecord file for batch prediction on GCP AI Platform?,"TL;DR How does Google Cloud AI Platform unpack TFRecord files when doing batch predictions? I have deployed a trained Keras model to Google Cloud AI Platform, but I'm having trouble with the file format for batch predictions. For training I'm using a tf.data.TFRecordDataset to read a list of TFRecord like the following which all works fine. I upload the saved model to Cloud Storage and create a new model in AI Platform. AI Platform documentation states that ""Batch with gcloud tool [supports] Text file with JSON instance strings or TFRecord file (may be compressed)"" (https://cloud.google.com/ai-platform/prediction/docs/overview#prediction_input_data). But when I provide a TFRecord file i get the error: My TFRecord file contains a bunch of Protobuf encoded tf.train.Example. I'm not providing the unpack_tfrecord function to AI Platform, so I guess it makes sense that it can not unpack it properly, but I have node idea where to go from here. I'm not interested in using the JSON format as the data is too large.",https://stackoverflow.com/questions/63953040,417385,Documentation Replication on Other Examples
44936825,use variational_recurrent in tf.contrib.rnn.DropoutWrapper,"In the api of tf.contrib.rnn.DropoutWrapper, I am trying to set variational_recurrent=True, in which case, input_size is mandatory. As explained, input_size is TensorShape objects containing the depth(s) of the input tensors. depth(s) is confusing, what is it please? Is it just the shape of the tensor as we can get by tf.shape()? Or the number of channels for the special case of images? But my input tensor is not an image. And I don't understand why dtype is demanded when variational_recurrent=True. Thanks!",https://stackoverflow.com/questions/44936825,6005072,Documentation Ambiguity
63958039,How do I interpret tf.keras.Model.predict() output?,"I am having trouble finding the documentation I need on this. To summarize the issue, I have trained a tf.keras model using two classes of images, labeled as '0' or '1'. I now want to use this model to predict whether new images are a '0' or '1'. My question is as follows: model.predict() returns a number between 1 and 0, but I can't seem to find what exactly this is. Is it correct to say that this is it's prediction (ie, closer to 1 means the image is likely a 1, and closer to 0 means the image is likely a 0)? Or is there something else going on here. I have included the code, and some output, below. In this case, is pred the probability the image is a 1, and 1 - pred the probability the image is a 0? Thanks for any and all help. Returns",https://stackoverflow.com/questions/63958039,14301246,Lack of Alternative Solutions/Documentation
63958998,Tensorflow/AI Cloud Platform: HyperTune trials failed to report the hyperparameter tuning metric,"I'm using the tf.estimator API with TensorFlow 2.1 on Google AI Platform to build a DNN Regressor. To use AI Platform Training hyperparameter tuning, I followed Google's docs. I used the following configuration parameters: config.yaml: And to add the metric to my summary, I used the following code for my DNNRegressor: According to Google's documentation, the add_metric function creates a new estimator with the metric specified, which is then used as the hyperparameter metric. However, the AI Platform Training service doesn't recognise this metric: Job details on AI Platform On running the code locally, the rmse metric does get outputted in the logs. So, how do I make the metric available to the Training job on AI Platform using Estimators? Additionally, there is an option of reporting the metrics through the cloudml-hypertune Python package. But it requires the value of the metric as one of the input arguments. How do I extract the metric from tf.estimator.train_and_evaluate function (since that's the function I use to train/evaluate my estimator) to input into the report_hyperparameter_tuning_metric function? ETA: Logs show no error. It says that the job completed successfully even though it fails.",https://stackoverflow.com/questions/63958998,14301413,Documentation Replication on Other Examples
44939540,How to get tensorflow to do a convolution on a 2 x 2 matrix with a 1 x 2 kernel?,"I have the following matrix: and the following kernel: If I do a convolution with no padding and slide by 1 row, I should get the following answer: Because: Based the documentation of tf.nn.conv2d, I thought this code expresses what I just described above: But it produces this output: And I have no clue how that is computed. I've tried experimenting with different values for the strides padding parameter but still am not able to produce the result I expected.",https://stackoverflow.com/questions/44939540,3775778,Documentation Ambiguity
44946189,TypeError: __init__() got an unexpected keyword argument 'shape',"I am new to Tensorflow and I met an error while trying to run some sample codes. Running the code above gives the error: TypeError: __init__() got an unexpected keyword argument 'shape'. The comment below says that tf.zeros_initializer does not accept 'shape' argument according to the documentation. I tried and it says ValueError: Shape of a new variable (v) must be fully defined, but instead was . So, what kind of argument/expression should I use to define the shape without causing a type error? I cannot find how to solve it online. Please help. Thank you",https://stackoverflow.com/questions/44946189,2504541,Documentation Replication on Other Examples
60801403,NotFoundError: No registered 'PyFunc' OpKernel for 'CPU' devices compatible with node {{node PyFunc}} . Registered: <no registered kernels>,I am getting an error while trying to access data from a tf.data.Dataset object. The dataset object is built from a generator. Any help will be appreciated. I'm using TensorFlow 2 and trying to run the example from https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator The error is :,https://stackoverflow.com/questions/60801403,7849569,Documentation Replicability
59998335,Constantly update tf.cond based on bool value,"I am using tf.cond for controlling the flow of the Tensorflow graph. I went through the documentation and was able to implement tf.cond based branching successfully. But my concern is that while the graph is being loaded the value of the bool variable is checked and the branching decision is made at the initialization step itself. Any further changes in the bool is not tracked. Following is the MWE that better describes the problem: This prints only 32s. I tried with eager_execution too with the following code: Still the same result. So my question is how can I write code such that one part of the graph is chosen dynamically, based on the updates to the bool variable (if possible)? Thanks. I am using Tensorflow v1.14.",https://stackoverflow.com/questions/59998335,6997665,Documentation Replication on Other Examples
41109652,How to read Ogg or MP3 audio files in a TensorFlow graph?,"I've seen image decoders like tf.image.decode_png in TensorFlow, but how about reading audio files (WAV, Ogg, MP3, etc.)? Is it possible without TFRecord? E.g. something like this:",https://stackoverflow.com/questions/41109652,7287271,Documentation Replicability
60977051,Split a tf.data.Dataset in to two distincts Input and Target tf.data.Dataset,How can I create two distincts Input and Target tf.data.Dataset from a tf.data.Dataset containing both Input and Target data so that I can uses them as x and y parameters of the model.fit function? I use the code below to load the dataset,https://stackoverflow.com/questions/60977051,9560337,Documentation Replicability
60013980,tf.nn.embedding_lookup_sparse 3D sparse tensor input,"I have an embedding matrix and there is a 3D sparse tensor which is used to get the embedding output, after reading the docs of tf.nn.embedding_lookup_sparse I found it only supports 2D sparse tensors, My example code here But the expected output is a matrix with a dimension of 3x2x4, not 3x4. Does tf.nn.embedding_lookup_sparse support this operation?",https://stackoverflow.com/questions/60013980,5046896,Documentation Replicability
41857602,sorting of 2d array min to max in tensorflow,"I have an array I want to sort the elements in each row from min to max. Is there any function for doing such ? In the example here they are using tf.nn.top_k2d array,using this I can loop to create the max to min. Is there any thing similar for finding the min to max or how to reverse the array in each row ?",https://stackoverflow.com/questions/41857602,7457101,Inadequate Examples
41125728,"When should tf.add(t, 0) be used instead of tensor t directly?","In tensorflow.contrib.slim.batch_norm, math_ops.add(moving_mean, 0) is used to copy the value of moving_mean, which is passed to nn.moments subsequently. Would it be a problem if we just pass moving_mean to nn.moments directly? Are there any guidelines on the use of copy operation (tf.add(t, 0))?",https://stackoverflow.com/questions/41125728,6770703,Documentation Completeness
60130582,How to convert images to TFRecords with tf.data.Dataset in most efficient way possible,"I am absolutely baffled by how many unhelpful error messages I've received while trying to use this supposedly simple API to write TFRecords in a manner that doesn't take 30 minutes every time I have a new dataset. I'd like to feed a list of image paths and a list of labels to a tf.data.Dataset, parse them in parallel to read the images and encode as tf.train.Examples, use tf.data.Dataset.shard to distribute them into different TFRecord shards (e.g. train-001-of-010.tfrecord, train-002-of-010.tfrecord, etc.), and for each shard finally write them to the corresponding file. Since I've been debugging this for hours I haven't gotten any single particular error to fix, otherwise I would provide it. I've struggled to find any up to date tutorial that doesn't either (a) come from 2017 and use queue runners, (b) use a tf.Session (I'm using tensorflow 1.15 but official docs keep telling me to phase out sessions), (c) Conveniently do the record creating in pure python, which makes a simple tutorial but is too slow for any actual application, or (d) use already created TFRecords and just skip the whole process. If necessary, I can put together an example of what I'm talking about. But since I'm getting stuck at every level of the process, at the moment it seems unhelpful. If anyone has utilized tf.data.Dataset to create TFRecord shards in parallel please point me in a better direction than google has.",https://stackoverflow.com/questions/60130582,7590318,Documentation Replication on Other Examples
61136605,Why tf2 can't save a tf_function model as .pb file?,"I tried to saved a model like the official code of transformer on official website, but when i want to save the train_step graph or trace on it with tf.summary.trace_on ,it errors.the error is as I supposed it was some error on tensor operation and write an other demo to confirm my idea: the error occurs as supposed. But how can i fixed it? the positional_encoding requires and i have no idea about how to replace this operation.",https://stackoverflow.com/questions/61136605,10796214,Documentation Replicability
60143153,Is there a way in tensorflow to load batches of data each time?,"So I'm running tensorflow 2+ python in google colab. Each of my data file is a 3d image with shape [563, 563, 563, 1], so loading all of them throws a resource exhaustion error. I've spent days and hours searching for a way to load only a batch of my dataset as tensor and unloading/loading new batch each iteration. I'm guessing there might be a way using tf.data.Dataset.list_files, but I can't find the exact way. Is there any good suggestions on a way to do it or any documents I could try to read? I've read the tf.data document from tensorflow, but couldn't find the information I needed. Thank you! so this is the function I want to use to load my image and this was the way I was loading the dataset before, which exhausted the resource;",https://stackoverflow.com/questions/60143153,12869645,Requesting (Additional) Documentation/Examples
42022950,Which seeds have to be set where to realize 100% reproducibility of training results in tensorflow?,"In a general tensorflow setup like Where construct_model() contains the model definition including random initialization of weights (tf.truncated_normal) and train_model(sess) executes the training of the model - Which seeds do I have to set where to ensure 100% reproducibility between repeated runs of the code snippet above? The documentation for tf.random.set_random_seed may be concise, but left me a bit confused. I tried: But got different results each time.",https://stackoverflow.com/questions/42022950,1934212,Documentation Replicability
41353079,Tensorflow image.central_crop (mis)behavior,In the Tensorflow documentation for tf.image.central_crop function: Consider the following code: Original image size is 456x450 Crop size is 228x226 Which gives area ratio of: Not 0.5 as I expected. Can someone help to clarify this?,https://stackoverflow.com/questions/41353079,6996238,Documentation Ambiguity
60398554,"Should we apply repeat, batch shuffle to tf.data.Dataset when passing it to fit function?","I still don't after having read documentation about tf.keras.Model.fit and tf.data.Dataset, when passing tf.data.Dataset to fit function, should I call repeat and batch on the dataset object or should I provide the batch_size and epochs arguments to fit instead? or both? Should I apply the same treatment to the validation set? And while I'm here, can I shuffle the dataset before the fit? (seems like it's an obvious yes) If so, before, after calling Dataset.batch and Dataset.repeat (if calling them)? Edit: When using batch_size argument, and without having called Dataset.batch(batch_size) previously, I am getting the following error: Thanks",https://stackoverflow.com/questions/60398554,7483509,Documentation Replication on Other Examples
42338981,It seems inconsistent the ways tensorflow allows me to specify variable length dimension,"I'm a novice to tensorflow. I was practicing coding with this tutorial code. Most of all the code made sense to me but at some points I got stuck. With tf.placholder function I had to specify variable length dimesion with None. But with tf.reshape I had to use -1, not None. In documentation for the two functions, both of the pertaining arguments have the name shape. So I am feeling lost here. Do they really have different meanings? Or is it just a small design mistake of the tensorflow developers?",https://stackoverflow.com/questions/42338981,,Documentation Ambiguity
61461381,The well-defined dimension of a tf.tensor is inexplicably `None`,"The example below is extracted from the official TensorFlow tutorial on data pipelines. Basically, one resizes a bunch of JPGs to be (128, 128, 3). For some reason, when applying the map() operation, the colour dimension, namely 3, is turned into a None when examining the shape of the dataset. Why is that third dimension singled out? (I checked to see if there were any images that weren't (128, 128, 3) but didn't fid any.) If anything, None should only show up for the very first dimension, i.e., that which counts the number of examples, and should not affect the individual dimensions of the examples, since---as nested structures---they're supposed to have the same shape anyway so as to be stored as tf.data.Datasets. The code in TensorFlow 2.1 is and yields Why None in that last line?",https://stackoverflow.com/questions/61461381,5640161,Documentation Replication on Other Examples
61480051,How to make Google Cloud AI Platform detect `tf.summary.scalar` calls during training?,"(Note: I have also asked this question here) I have been trying to get Google Cloud's AI platform to display the accuracy of a Keras model, trained on the AI platform. I configured the hyperparameter tuning with hptuning_config.yaml and it works. However I can't get AI platform to pick up tf.summary.scalar calls during training. I have been following the following documentation pages: 1. Overview of hyperparameter tuning 2. Using hyperparameter tuning According to [1]: And according to [2], one way of generating such a Tensorflow summary event is by creating a callback class as so: So in my code I included: I even tried Which successfully saved the 'val_accuracy' metric to Google storage (I can also see this with TensorBoard). But this does not get picked up by the AI platform, despite the claim made in [1]. Using the Cloud ML Hypertune package, I created the following class: which works! But I don't see how, since it all it seems to do is write to a file on the AI platform worker at /tmp/hypertune/*. There is nothing in the Google Cloud documentation that explains how this is getting picked up by the AI platform... Am I missing something in order to get tf.summary.scalar events to be displayed?",https://stackoverflow.com/questions/61480051,10143615,Documentation Replication on Other Examples
60287388,Error loading .npz files in tensorflow dataset,"I'm trying to create a data pipeline in tensorflow, but my data is in .npz files. Following the documentation at https://www.tensorflow.org/guide/data#consuming_sets_of_files, for consuming sets of files, combined with using tf.py_function() for using numpy ops, I wrote a code, a subsection of which is: I'm getting an error in np.load(filename), which goes like: Fixed this by using np.load(filename.numpy()), as stated by @jdehesa. Now I run into some shape issues, although I've already reshaped the input in the load_data_wrapper() function. Can anyone help me with this? I am attaching the entire code and entire error message below. CODE: ERROR:",https://stackoverflow.com/questions/60287388,12920968,Documentation Replicability
60311184,how to loop over a tensor object until a condition met,"I have a tensor like this: I want to loop through this tensor untill all elements get True. So I have another function, which will update this tensor, lets call it uniqueness. I looked at the documentation and got to know that I can do that using tf.while_loop. Although, I could not find any example working on boolean stuff. This is what I have done so far: It is obviously incorrect, but don't know how to use each element of masked_bad_col as a condition to continue looping through uniqueness function. Update 1 This is the method I am trying to call in the loop: And this is the way I called this method in the while_loop",https://stackoverflow.com/questions/60311184,7934786,Inadequate Examples
60314717,How does shuffle and batch work in tf.data.dataset?,I'm working on a large dataset with around 10million datapoints so I've decided to use tf.data.dataset api for fetching dataset. I've few doubts which isn't clear from tensorflow docs. I hope someone can address them. How does the shuffle work in my case? Because I have 10 million datapoints should I shuffle all 10 million (or) will 100k be enough? Will it have any performance impact choosing a large shuffle? Will the batch is considered only from shuffled dataset (or) the original dataset?,https://stackoverflow.com/questions/60314717,11816060,Documentation Replication on Other Examples
60444268,How to save model by training steps using tf.keras.callbacks.ModelCheckpoint,"My training data is very large, which means it have many training steps each epoch. I want to use tf.keras.callbacks.ModelCheckpoint to save a model every 100,000 steps, and save all these models in disk, not rewrite the model files when saving a new one. But I have no idea how to do this after i read the documentation. It seems that this callback is designed to save a model by epoch. So, do i have to implement a custom callback to save the model by steps?",https://stackoverflow.com/questions/60444268,9109641,Documentation Ambiguity
60453533,Tensorflow what is the tf.contrib.nccl.allsum in new version?,"It seems that from tensorflow 1.13, there is no api such as tf.contrib.nccl.allsum. However, in the Nvidia official GitHub https://github.com/tkarras/progressive_growing_of_gans, which uses this old API to reduce sum from different gpu devices as the following. I am not sure if there is similar api which can achieve the same collective operation cross different devices. I have checked the Tensorflow official website and it seems that programmers prefer to use tf.distribute.MirroredStrategy which hides the raw operation of NCCL. Thanks a lot.",https://stackoverflow.com/questions/60453533,9881203,Documentation Replication on Other Examples
60590333,Increasing each element of a tensor by the predecessor in Tensorflow 2.0,"I'm new to tensorflow 2.0, and haven't done much except designing and training some artificial neural networks from boilerplate code. I'm trying to solve an exercise for newcomers into the new tensorflow. I created some code, but it doesn't work. Below is the problem definition: Assuming we have tensor M of rational numbers in shape of (a, b, c) and scalar p ∈ (0, 1) (memory factor), let’s create a function that will return tensor N in shape of (a, b, c). Each element of N tensors moving along axis c should be increased by the value of predecessor multiplied by p. Assuming we have tensor: in shape of (1, 1, 4), we would like to get vector: Solution should be created in Tensorflow 2.0 and should be focused on delivering the shortest execution time on CPU. Created graph should allow to efficiently calculate derivative both on tensor M and value p. This is the code I created till now: But it throws a TypeError. I looked around documentation, I've seen functions like cumsum and polyeval, but I'm not sure they fit my needs. To my understanding, I need to write my own customer function annotated with @tf.function. I'm also not sure how to handle 3-dimension tensors properly according to the problem definition (adding the predecessor should happen on the last (""c"") axis). I've seen in documentation (here: https://www.tensorflow.org/tutorials/customization/performance) that there are ways to measure size of the produced graph. Although, I'm not sure how ""graph"" allows to efficiently calculate derivative both on tensor M and value p. ELI5 answers appreciated, or at least some materials I can read to educate myself better. Thanks a lot!",https://stackoverflow.com/questions/60590333,1554153,Lack of Alternative Solutions/Documentation
60610007,"Build tf,estimator.DNNClassifier from tf.data.Datasets","I am new to tensorflow in ML, and thought I could build the model from tf.data.Datasets directly. Here is my code, could not figure out why it did not work. Can someone please advise if it's possible to make it work?",https://stackoverflow.com/questions/60610007,1741590,Documentation Replicability
47814401,How does tf.layers.batch_normalization calculate mean and variance during test time? (test data has machine-generated samples),"I am trying to implement batch-normalization on my CNN that currently applies dropout. One problem is that I do not know how the mean and variance are calculated during test time. On the documentation it says that if training=False is set then the normalization is done with moving statistics. What does this mean? In addition, since my test data has lots of machine-generated samples I cannot use population mean and variance and just apply tf.nn.batch_normalization(). These samples are used to prevent hand labeling and are excluded when scoring my model",https://stackoverflow.com/questions/47814401,8805495,Documentation Completeness
47947629,"Tensorflow: Keras, Estimators and custom input function","TF1.4 made Keras an integral part. When trying to create Estimators from Keras models with propratery input function (I.e., not using the tf.estimator.inputs.numpy_input_fn) things are not working as Tensorflow can not fuse the model with the Input function. I am using tf.keras.estimator.model_to_estimator and I get the following error message: I found some reference for this topic here (strangely enough its hidden in the TF docs in the master branch - compare to this) If you have the same issue - see my answer below. Might save you several hours.",https://stackoverflow.com/questions/47947629,8096451,Documentation Replicability
44963306,Cannot printout concatenated tensor by tf.concat() (tensorflow 1.2.1 - gpu / py36),"Learning Tensorflow (Python bindings) since the last month. I've been reading the docs on tf.concat(), but cannot resolve the problem as shown below, so I'm asking for your help! What I want to do is to see the contents of the concatenated tensor. I tried Tensor.eval(). Output: tf.concat() supposed to return Tensor and looks like it does. But why aren't T.eval() and sess.run() not working?",https://stackoverflow.com/questions/44963306,8063281,Documentation Replication on Other Examples
45237900,Dynamic RNN padding and indexing to match ground truth,"I am running an RNN (many-to-many). I have a different time dimension for each user. E.g. user 1 has 5-time stamps and user 2 has 8-time stamps, etc. I think the RNN accepts only tensors with constant dimensions, so I currently pad (not using tensorflow) the time dimension with zeros till it reaches the maximum time stamps (max_user_time) among all users in the batch. For example, if user #1 has 2 time stamps and 3 features I get a tensor with dimensions [1,2,3]: If user 3 in the batch has 3 time stamps, then we need to add paading such that user 1 has a tensor with dimensions [1,3,3]: The padding for each user will be of different length. Is there a way to get this using tf.pad or something similar for all users at once? After padding, I pass these tensors as input to the RNN and reshape the output: For the sequence length parameter, I pass a vector with the timestamps for each user, so expecting a zero output if a user has passed its max time - according to tf.dynamic_rnn documentation. So, I get from reshaping a tensor sized [batch_sizeXmax_user_time,n_hidden]. This is a bigger tensor than the ground truth tesnor, which is smaller and has rows for each user according to its timestamps. Is there an easy way to use tensorflow to pick only the rows that are observed in order to calculate the loss?",https://stackoverflow.com/questions/45237900,7764047,Documentation Replication on Other Examples
45247909,Tensorflow - How to get the gradients of the output w.r.t the model parameters,"I would like to know if it is possible to compute the gradients of the output of a model with respect to the model parameters. In other words I would like to compute dy / d theta. Here is a short example of what I mean: I have looked at the documentation of tf.gradients() and it states So I do understand that both args need to be a tensor. However, when I try model_parameters = tf.trainable_variables() model_parameters is a list of elements of type tensorflow.python.ops.variables.Variable Is there a way to get the parameters of the model as a tensor to use for differentiation?",https://stackoverflow.com/questions/45247909,7476324,Documentation Replicability
64297691,Keras model evaluate returns triggered tf.function retracing warning,"I am training the following model using Keras as shown: When running the following to check the accuracy on the test set I get the following warning: WARNING:tensorflow:5 out of the last 13 calls to &lt;function Model.make_test_function..test_function at 0x000001E51AA92AE8&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details. 2/2 [==============================] - 0s 94ms/step - loss: 0.6660 - accuracy: 0.5909 Can you please help me understand why?",https://stackoverflow.com/questions/64297691,1525553,Documentation Replicability
45290607,Tensorflow tf_strided_slice elaboration,"I'm trying to understand how tf.strided_slice works. In order to do this, I've written the following code: When I execute the above example, I get the following error: I simply cannot understand why the number of elements is 840 since I start at [0] and end at [record_size]?",https://stackoverflow.com/questions/45290607,8312790,Documentation Replicability
45698621,TensorFlow embedding_rnn_decoder 'Tensor' object is not iterable,"I am trying to construct a custom estimator for my ML Engine package and I seem to be having trouble properly constructing my decoder input sequence in the correct format. Consider the following where label1, label2 is supposed to be a sequence of labels. these ""features"" are then passed as the decoder input as below. When I go to use the decoder_input as input into my custom estimator, I run into the an error 'TypeError: 'Tensor' object is not iterable.' here: The full stack trace (below) suggests that the portion of code causing the issue is for 'for i in decoder_inputs' from line 296 so it seems pretty clear to me that the issue is in how I construct my decoder_input in the input_fn(). However, I can't seem to figure out how to make the Tensor object an iterable list of sequences. Stacktrace: Can anybody help spot how I should correctly format my labels so that they are iterable? The documentation says that decoder_inputs should be ""A list of 1D batch-sized int32 Tensors (decoder inputs)."" so I thought that by staIs there a more appropriate way to generate the sequence of labels than tf.stack()?",https://stackoverflow.com/questions/45698621,4086968,Documentation Replication on Other Examples
45701681,Determining the Epoch Number with tf.train.string_input_producer in tensorflow,"I have some doubts on how tf.train.string_input_producer works. So suppose I fed filename_list as an input parameter to the string_input_producer. Then, according to the documentation https://www.tensorflow.org/programmers_guide/reading_data, this will create a FIFOQueue, where I can set epoch number, shuffle the file names and so on. Therefore, in my case, I have 4 file names (""db1.tfrecords"", ""db2.tfrecords""...). And I used tf.train.batch to feed the network batch of images. In addition, each file_name/database, contain a set of images for one person. The second database is for the second person and so on. So far I have the following code: Finally, when trying to view out the reconstructed image at the output of the autoencoder, I got the first the images from the 1st database, then I start viewing images from the second database and so on. My question: How can i know if I'm within the same epoch? And if I'm within the sane epoch, how can i merge a batch of images from all the file_names that I have? Finally, I tried to print out the value of the epoch by evaluating the local variable within the Session as follows: Then: Any help is much appreciated!!",https://stackoverflow.com/questions/45701681,7886651,Documentation Replicability
65408472,Tensorflow: How to handle preprocessing.Normalization returning NaN,"I am building a keras model and trying to use the tf.keras.layers.experimental.preprocessing.Normalization from https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Normalization to normalize the input training data as such: The goal is to save the normalization within the saved model. I am running into the issue where the normalize(trainX) is normalizing some of the inputs to nan due to most likely a division of zero in the variance. E.g. a row of the output: [ 0.00000000e+00, nan, 9.40457404e-01, 9.40672755e-01, 9.40672755e-01, 9.40672755e-01, 9.40672755e-01, nan, nan, nan, nan, nan] Is there a way to handle division of zero in preprocessing.Normalization() or is this there another way of normalization I should consider that I can save within the model?",https://stackoverflow.com/questions/65408472,6372640,Documentation Replicability
65413136,Tensorflow — Cannot call `tf.keras.Model.add_metric` when `tf.distribute.MirroredStrategy` is used,"I have a model class that inherits from tf.keras.Model. I can train, evaluate, and export it using 8 GPUs, distributing it with tf.distribute.MirroredStrategy. However, I need custom metrics, and when I call the add_metric method, it throws an error when trying to export. I have created a simple reproduction which shows this error here: I apologize for so much code, but most of it isn't important. The important part is that this model is instantiated and compiled inside the tf.distribute.MirroredStrategy scope. There is also a self.add_metric([0.], name=""foo"") in the model. If you remove that add_metric call, then it works. It will export correctly. Therefore, using the tf.keras.Model.add_metric method with tf.distribute.MirroredStrategy. I need to be able to add my custom metrics with a distributed model. Note: Metrics are supposed to be calculated in the strategy scope, as mentioned in the docs As for versions, I'm using the Google AI platform runtime version 2.3",https://stackoverflow.com/questions/65413136,6078821,Documentation Replicability
46885191,tf.nn.conv2d_transpose output_shape dynamic batch_size,"The documentation of tf.nn.conv2d_transpose says: The output_shape argument requires a 1D tensor specifying the shape of the tensor output by this op. Here, since my conv-net part has been built entirely on dynamic batch_length placeholders, I can't seem to device a workaround to the static batch_size requirement of the output_shape for this op. There are many discussions around the web for this, however, I couldn't find any solid solution to this issue. Most of them are hacky ones with a global_batch_size variable defined. I wish to know the best possible solution to this problem. This trained model is going be shipped as a deployed service.",https://stackoverflow.com/questions/46885191,4341842,Inadequate Examples
65745053,Tensorflow softmax does not ignore masking value,"I am reviving this github issue because I believe it is valid and needs to be explained. tf.keras has a masking layer with docs that reads The two results are virtually identical I expected to just take softmax of the valid entries, similiar to padded at the correct positions To ignore 0's in a softmax function, we could switch out massively negative numbers? Related: tensorflow - softmax ignore negative labels (just like caffe)",https://stackoverflow.com/questions/65745053,2984051,Documentation Replication on Other Examples
46904972,How to create a tf.feature_column by multiplying two other tf.feature_columns?,"In Tensorflow there is already a function to create feature by crossing columns tf.feature_column.crossed_column , but it is more for category data. How about numeric data? For example, there are 2 columns already if i want to create a third and fourth feature columns base on age and education_num like this How can it be done?",https://stackoverflow.com/questions/46904972,4049331,Documentation Replicability
65754675,tf.sparse.reshape(tf.sparse.split()) : TypeError: Input must be a SparseTensor,I am trying to convert a dense matrix to sparse matrix calcualtion on tensorflow. There is an error when trying to reshape after using tf.sparse.split(. Following is a toy example to demonstrate the issue. Tensorflow dense matrix Tensorflow sparse matrix can you please help me to overcome this issue?,https://stackoverflow.com/questions/65754675,6830120,Documentation Replicability
46920307,"Is tf.contrib.layers.flatten(x) the same as tf.reshape(x, [n, 1])?","I was just wondering whether tf.contrib.layers.flatten(x) is the same as tf.reshape(x, [n, 1])? Anyone has ideas about this?",https://stackoverflow.com/questions/46920307,1272683,Documentation Replicability
66136537,Tensorflow training with variable batch size,is it possible to use Tensorflow/Keras and train a model with variable batch sizes? So I have batches of different sizes in each epoch. I think using high level tf.keras API this is not possible?,https://stackoverflow.com/questions/66136537,,Documentation Replication on Other Examples
60469970,How does tf.function compile a python function with autograph?,"How does tf.function compile a python function operating on tensors into a graph, especially wrt autograph? The docs don't go into detail Does it use the special methods called by conditionals (__bool__) and loops (__iter__) to 'trace' the function's implementation? For example could use the fact that the if results in Tensor.__bool__(...) and for _ in t results in Tensor.__iter__(...)",https://stackoverflow.com/questions/60469970,5986907,Documentation Replicability
41559723,Does the tf.nn.conv2d_transpose transpose the filter?,"I have a trained feedforward CNN. The shape of filter is [height, width, in_channels, out_channels]. And i want to use those filter to do deconv, we know that the deconv process needs the transpose of the filter.Do i need to transpose the filter mannuly, or the TF will do it inside the tf.nn.conv2d_transposeand all we need to do is pass the trained filter to tf.nn.conv2d_transpose?",https://stackoverflow.com/questions/41559723,7345996,Documentation Replication on Other Examples
60639731,Tensorboard for custom training loop in Tensorflow 2,"I want to create a custom training loop in tensorflow 2 and use tensorboard for visualization. Here is an example I've created based on tensorflow documentation: I am accessing tensorboard with the following command on terminal: The code above produce summaries for losses and metrics. My question is: I've tried to use the recommended commands from tensorflow: tf.summary.trace_on() and tf.summary.trace_export(), but I haven't managed to plot the graph. Maybe I am using them wrong. I whould really appreciate any suggestion on how to do this.",https://stackoverflow.com/questions/60639731,10687511,Documentation Replication on Other Examples
60684241,How to shape TFRecordDataset to meet Model API?,"I am building a model based on this code for noise suppression. My problem with the vanilla implementation is that it loads all data at once, which is not the best idea when the training data gets really large; my input file, denoted in the linked code as training.h5, is over 30 GB. I decided to instead go with tf.data interface that should allow me to work with large data sets; my problem here is that I don't know how to properly shape TFRecordDataset so that it meets what's required by the Model API. If you check model.fit(x_train, [y_train, vad_train], it essentially requires the following: window one typically fixes (in the code: 2000), so the only variable nb_sequences that stems from how large is your data set. However, with tf.data, we don't supply x and y, but only x (see Model API docs). In an effort to make the code reproducible, I created the input file with the following code: data is our training data with shape [10000, 65]. My example.tfrecord is available here. It's 3 MB, in reality it would be 30 GB+. You might notice that in the linked code, numpy array has shape [x, 87], while mine is [x, 65]. That's OK - the remainder is not used anywhere. I would like to use tf.data to load ""on demand"" the data with some prefetching, there's no need to keep it all in memory. My attempt: My dataset has now the following shape: which I thought is what Model API expects (spoiler: it doesn't). gives following error: Makes sense, I don't have the window here. At the same time it seems like it's not getting correct shape expected by Model(inputs=main_input, outputs=[denoise_output, vad_output]). How to modify load_dataset so that it matches what's expected by the Model API for the tf.data?",https://stackoverflow.com/questions/60684241,1397946,Documentation Replication on Other Examples
60692861,How can I get trainable_variables from model subclassing the tf.keras.Model?,"I want to get trainable_variables from model which is created by subclassing tf.keras.Model, but it return [].I know it works when using tf.keras.Sequential. I tried feedding some values to the model but it still returns [].",https://stackoverflow.com/questions/60692861,11828768,Documentation Replication on Other Examples
41673889,TensorFlow: does tf.train.batch automatically load the next batch when the batch has finished training?,"For instance, after I have created my operations, fed the batch data through the operation and run the operation, does tf.train.batch automatically feed in another batch of data to the session? I ask this because tf.train.batch has an attribute of allow_smaller_final_batch which makes it possible for the final batch to be loaded as a size lesser than the indicated batch size. Does this mean even without a loop, the next batch could be automatically fed? From the tutorial codes I am rather confused. When I load a single batch, I get literally a single batch size of shape [batch_size, height, width, num_channels], but the documentation says it Creates batches of tensors in tensors. Also, when I read the tutorial code in the tf-slim walkthrough tutorial, where there is a function called load_batch, there are only 3 tensors returned: images, images_raw, labels. Where are 'batches' of data as explained in the documentation? Thank you for your help.",https://stackoverflow.com/questions/41673889,5107084,Lack of Alternative Solutions/Documentation
60708695,"How can I make ""element wise"" comparsion inside of the tf.function?","I try to make my own activation function in TensorFlow 2 and the function looks like this: The problem is that it cant take as argument tf.constant([2.0, 3.0])because there is an issue with conditions. I have tried tf.math.qreater_equal(x, 0) which lead to same output also tf.cond(). I have had no luck with documentation examples either. It returns error: Thanks!",https://stackoverflow.com/questions/60708695,10962934,Documentation Ambiguity
41685279,Distributed TensorFlow: about the using of tf.train.Supervisor.start_queue_runners,"I'm looking into the code for distributed inception model in TF, in which I have below questions about the use of tf.train.Supervisor.start_queue_runners in inception_distributed_train.py: Thanks for your time!",https://stackoverflow.com/questions/41685279,4394807,Documentation Replicability
60744247,Augmentation of a tf.data.Dataset,Following this guide here i stumbled across this:In order to augment a tf.data Dataset we manualy use the map function to map image transformations in each image of our original dataset: From what i can understand what this does is this:It takes the original train_dataset and creates a new augmented_train_batches dataset which has the same number of images altered by maps transformations.After that what this does is feeding this dataset into .fit like this: So what i can't seem to grasp is this:Aren't the data supposed to be altered after every epoch so that(according to documentation)our model won't see the same image more than once and moreover make our overfitting chances lower? In this tutorial isn't augmented_train_batches just a slightly altered dataset which is fed over and over to our model? Or is the augmentaion somehow being applied after each epoch in a way i can't understand? P.S.I suppose augmentation(if done correctly) must alter the pre-transformed data in a same manner after every epoch and not keep applying transformations to the same altered dataset.,https://stackoverflow.com/questions/60744247,,Documentation Replication on Other Examples
41780344,Gradient of tf.floor is None,"tf.floor return None gradient it means the weights before floor operation won't be update, right? but I still need the gradient to update weights Isn't it weird? sometimes we use floor, ceil... but they can't deliver gradient or this is right action as well as tf.cast, the gradient return none Here is an issue #897 to discuss the gradient, but I don't understand why return None is good Can I modify math_grad.py directly? need other action? something like delete math_grad.pyc?",https://stackoverflow.com/questions/41780344,6306884,Documentation Replication on Other Examples
41780655,What is the difference between tf.group and tf.control_dependencies?,"Aside from tf.control_dependencies being a context manager (i.e. used with Python with), what's the difference between tf.group and tf.control_dependencies? When should which be used? Is it that tf.group doesn't have any particular order of operations? I'd assume tf.group([op_1, op_2, op_3]) executes ops in the list's order, but maybe that's not the case? The docstring doesn't specify a behaviour.",https://stackoverflow.com/questions/41780655,7287271,Documentation Replicability
60919434,String to one_hot tensor in Tensorflow,I have found in tensorflow doc the following function to compute and apply a vocabulary onto a string tensor but it was still using tf.session and I can't make it work with tf.function: I have been able to build such a function with direct use of the hash facilities. However I have had to use a hard-coded bucket_size/depth param. Any ideas?,https://stackoverflow.com/questions/60919434,4444546,Documentation Replicability
41789133,What are c_state and m_state in Tensorflow LSTM?,Tensorflow r0.12's documentation for tf.nn.rnn_cell.LSTMCell describes this as the init: where state is as follows: What aare c_state and m_state and how do they fit into LSTMs? I cannot find reference to them anywhere in the documentation. Here is a link to that page in the documentation.,https://stackoverflow.com/questions/41789133,5299052,Lack of Alternative Solutions/Documentation
41953678,Documentation for Inference from saved model in Tensorflow,"As explained at Multilayer CNN for MNIST Digit Recognition I created a CNN for MNIST image recognition dataset. I don't want to test it in the same script so I trained the model, saved the checkpoint files and graph structure and then using freeze_graph.py script. I froze it into one single .pb file. Now using this .pb file I want to infer for MNIST test dataset and check the accuracy of the model but there is no clear documentation as to how to do it. There is one example written in examples/label_images directory but that too is in C++ and for inception network. I have already parsed the .pb file and made a tf.Graph object. And also why can't a single inference script be provided since whole graph structure can be read from graph saved in the .pb file.",https://stackoverflow.com/questions/41953678,5182224,Documentation Replication on Other Examples
61245158,Failed to apply vectorizing mapping for tf.data in Tensorflow 2.1.0,I wrote a program to load data by tf.data with Tensorflow 2.1.0. I wanted to speed up data pipeline and I studied the document in https://www.tensorflow.org/guide/data_performance#vectorizing_mapping. I would like to apply vectorizing mapping for tf.data and the code snippet is listed as below: The errors are listed as below: How should I fix it? Thanks,https://stackoverflow.com/questions/61245158,6244978,Documentation Replicability
42133661,Tensorflow - LSTM state reuse within batch,"I am working on a Tensorflow NN which uses an LSTM to track a parameter (time series data regression problem). A batch of training data contains a batch_size of consecutive observations. I would like to use the LSTM state as input to the next sample. So, if I have a batch of data observations, I would like to feed the state of the first observation as input to the second observation and so on. Below I define the lstm state as a tensor of size = batch_size. I would like to reuse the state within a batch: In the API there is a tf.nn.state_saving_rnn but the documentation is kinda vague. My question: How to reuse curr_state within a training batch.",https://stackoverflow.com/questions/42133661,2061800,Documentation Ambiguity
61273445,Tensorflow MapDataset iterator fails,"I am trying to implement the method suggested by the tensorflow documentation over here (https://www.tensorflow.org/tutorials/load_data/images) to load images from local directory as a tensorflow dataset. Especially I am interested in loading using tf.data as a tf.data.Dataset object as it is suggested that the performance is better that way. I pretty much took the exact code from the documentation page and also made sure that the tensorflow version matches to the one in the documentation The problem happens when I try to iterate over the MapDataset object using take(). I get the following error and have no idea how to go about resolving this By some random coincidence I found that when CLASS_NAMES is set to None, the code runs and the lebel object of labeled_ds has a value 'False' See output below",https://stackoverflow.com/questions/61273445,3357373,Documentation Replication on Other Examples
61411259,Which one should I ues for preprocessing image?,which one should I use for preprocessing image. using ImageDataGenerator from keras or using tf.data from tensorflow? Which one has better performance?,https://stackoverflow.com/questions/61411259,13399061,Documentation Replicability
61428918,tensorflow2: keras: model.fit() callbacks and eager mode,"I am running Tensorflow 2.1 with keras API. I am following the following coding style: Now, I would like to save some intermediate layer tensor value as image summary (as a sample what is happening at n-th training step). In order to do this, I've implemented my own callback class. I've also learned how keras.callbacks.TensorBoard is implemented, since it can save layer weights as image summaries. I do the following in my on_epoch_end: Unfortunately, I am still getting issue related to eager/graph modes: Unfortunately, there is a little to no documentation on how to correctly combine keras callbacks and tf.summary.image. How could I overcome this issue? upd: tf_nightly-2.2.0.dev20200427 has the same behaviour.",https://stackoverflow.com/questions/61428918,1879547,Lack of Alternative Solutions/Documentation
42333101,Predicting Next Word of LSTM Model from Tensorflow Example,"My buddy and I are trying to utilize the trained model from the LSTM tensorflow example here. We've been able to train our model, save the model, and then import the model. We've just used tensorflow's Supervisor. It was in the tutorial, but you can read more about it here. It's weird because there's not a whole lot of clear documentation for this. I understand that tensorflow is an API that's going through a lot of changes and adaptations right now, but it's hard to find clear answers. For example, we want to use tf.train.Saver(), but we aren't sure if there is anything comparable to tf.train.Supervisor()'s managed_session. More to the point, however, we just want to use our model. We want to be able to map a string using tensorflow.models.rnn.ptb.reader. We're not sure how to do this. We pass in a string, and we want to do a simple prediction in terms of like predicting the next word in a string. So, something similar to this: But again, my buddy and I are pretty new to this, so we're not sure about where to go. I know this is probably too broad of a question for stack, but we've been pouring over the documentation and haven't been able to make much progress. ANY help would be appreciated so much! We've already found these other Stack links. Check them out here and here. We are not sure how to associate the logits probability list with any meaningful words.",https://stackoverflow.com/questions/42333101,6347839,Documentation Replication on Other Examples
42728235,Tensorflow: Why is tf.case giving me the wrong result?,"I'm trying to use tf.case (https://www.tensorflow.org/api_docs/python/tf/case) to conditionally update a Tensor. As shown, I'm trying to update learning_rate to 0.01 when global_step == 2, and to 0.001 when global_step == 4. However, when global_step == 2, I already get learning_rate = 0.001. Upon further inspection, it looks like tf.case is giving me the wrong result when global_step == 2 (I get 0.001 instead of 0.01). This is happening even though the predicate for 0.01 is evaluating to True, and the predicate for 0.001 is evaluating to False. Am I doing something wrong, or is this a bug? TF Version: 1.0.0 Code: Results:",https://stackoverflow.com/questions/42728235,1232944,Documentation Replicability
42734106,How to restore variable object,"I want to restore a variable object. That is, I want to have an object of type tensorflow.Variables after deserialization. I try to use MetaGraph. Here is a minimal example. Serialization: Deserialization The issue is that tf.get_collection return tf.Tensor object, not tf.Variable. But I can see tf.Variable objects in variables collection. What is the correct way of restoring Variable object?",https://stackoverflow.com/questions/42734106,1292688,Documentation Replication on Other Examples
42754259,Sampled softmax loss over variable sequence batches?,"Background info: I'm working on sequence-to-sequence models, and right now my model accepts variable-length input tensors (not lists) with input shapes corresponding to [batch size, sequence length]. However, in my implementation, sequence length is unspecified (set to None) to allow for variable length inputs. Specifically, input sequence batches are padded only to the length of the longest sequence in that batch. This has sped up my training time considerably, so I'd prefer to keep it this way, as opposed to going back to bucketed models and/or padded all sequences in the training data to the same length. I'm using TensorFlow 1.0.0. Problem: I'm currently using the following to compute the loss (which runs just fine). where vocab size is typically about 40,000. I'd like to use a sampled softmax, but I've ran into an issue that's due to the unspecified nature of the input shape. According to the documentation for tf.nn.sampled_softmax_loss, it requires the inputs to be fed separately for each timestep. However, I can't call, for example, since the axis is unknown beforehand.Does anyone know how I might go about implementing this? One would assume that since both dynamic_rnn and tf.losses.sparse_softmax_cross_entropy seem to have no issue doing this, that a workaround could be implemented with the sampled softmax loss somehow. After digging around in the source code and even models repository, I've come up empty handed. Any help/suggestions would be greatly appreciated.",https://stackoverflow.com/questions/42754259,6706630,Documentation Replicability
42933599,Slice a tensor in half in tensorflow,"I have a tensor of shape (32, 32, 32, 1) and I want to slice it into two tensors, along the first dimension, containing the first and second halves like so I am trying to use tf.slice but I don't know how to use the begin and end indices, and the documentation is anything but clear.",https://stackoverflow.com/questions/42933599,5016028,Documentation Ambiguity
42940451,How can I get the file name of a tf.summary.FileWriter in TensorFlow?,How can I get the file name of a tf.summary.FileWriter (mirror) in TensorFlow? I am aware that I can use get_logdir() but I don't see any similar method to access the file name.,https://stackoverflow.com/questions/42940451,395857,Documentation Replicability
61875170,tf.keras custom metric is giving incorrect results,I have implemented a custom metric in tf.keras for a multi label classification problem. count_zero function produces integer results but while running the model it gives me float values. The custom function gives me correct results when tried outside the scope of the keras model. Why is this happenning?,https://stackoverflow.com/questions/61875170,8410166,Documentation Replication on Other Examples
61879049,tf.keras.Model doesn't find assigned variables,"Look at this code (assume Tensorflow 2.1 or 2.2): Why is model.variables empy? I would expect to contain var. As I understant the tf.Module, it works as a namescope and also records tf.Variable assigned to their attributes. Since a tf.keras.Model is a tf.Module, it should do the same recursively, shouldn't it?",https://stackoverflow.com/questions/61879049,6788571,Documentation Replicability
61884176,Understanding tf.name_scope,"I am trying to understand tf.name_scope. The documentation mentions the following: ""This context manager pushes a name scope, which will make the name of all operations added within it have a prefix. For example, to define a new Python op called my_op: When executed, the Tensors a, b, c, will have names MyOp/a, MyOp/b, and MyOp/c."" My understanding is that the with block does not introduce a new local scope in Python. Under normal situation, the tensor variable a will also refer to the local parameter a of function my_op. How is the name prefixing with ""MyOp/"" implemented using Python context? In the source code link for tf.name_scope (https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/framework/ops.py#L6423-L6442) there is an invocation of but I could not find the semantics of context.context(). Most context manager discussion talk about enter and exit, but no mention of variable renaming with some prefix. Is this some introspective mechanism in Python that allows the manipulation of Python variable scopes? Many thanks for any insights.",https://stackoverflow.com/questions/61884176,13572195,Documentation Replicability
61885570,Reading a tfrecord: DecodeError: Error parsing message,I am using colab to run a tutorial on tensorflow ranking. It uses wget to fetch the tfrecord: I am using this code to try to look at the structure of the tfrecord: And I am getting: How to generally look at the structure of tfrecords instead? A second question: Where to find documentation on classes like tf.train.Example? I just find this empty page.,https://stackoverflow.com/questions/61885570,8183621,Requesting (Additional) Documentation/Examples
43175272,check if tensorflow placeholder is filled,"Suppose I have two placeholder quantities in tensorflow: placeholder_1 and placeholder_2. Essentially I would like the following computational functionality: ""if placeholder_1 is defined (ie is given a value in the feed_dict of sess.run()), compute X as f(placeholder_1), otherwise, compute X as g(placeholder_2)."" Think of X as being a hidden layer in a neural network that can optionally be computed in these two different ways. Eventually I would use X to produce an output, and I'd like to backpropagate error to the parameters of f or g depending on which placeholder I used. One could accomplish this using the tf.where(condition, x, y) function if there was a way to make the condition ""placeholder_1 has a value"", but after looking through the tensorflow documentation on booleans and asserts I couldn't find anything that looked applicable. Any ideas? I have a vague idea of how I could accomplish this basically by copying part of the network, sharing parameters and syncing the networks after updates, but I'm hoping for a cleaner way to do it.",https://stackoverflow.com/questions/43175272,4938706,Lack of Alternative Solutions/Documentation
62086633,Conditional branches using tf.function,"I am having problems calculating the gradient using the gradient tape when using a tf.function with conditional branches. Inside a gradient tape scope, I am trying to calculate the gradient of z w.r.t self.LMn. This works perfectly fine when I do not annotate the function with @tf.function. The error originates in a subclassed tf.keras.layers.Layer call function: The actual error is given as follows: More specifically, and I can assume that each of these outputs corresponds to the cases for some set of conditional branches outputs that are then fed into the tf.einsum function. I have read over all of the edge cases and precautions in the gradient tape documentation as this seems to the problem. Just a note that I am only performing conditional computation using hyperparameters (passed into tf.function as pythonic variables, such as basis_filters). There are also some conditional branches using (tensorflow ops) functions of these hyperparameters, is this allowed? or do I need to compute these values outside tf.function and pass these in as pythonic variables too? I know the question is not completely clear and I can provide any extra information if needed. It would very helpful to have some guidance on what to look for with this kind of problem. Thanks!",https://stackoverflow.com/questions/62086633,1582331,Documentation Replication on Other Examples
62284095,What are the parameters to tf.GradientTape()'s __exit__ function?,"According to the documentation for tf.GradientTape, its __exit__() method takes three positional arguments: typ, value, traceback. What exactly are these parameters? How does the with statement infer them? What values should I give them in the code below (where I'm not using a with statement):",https://stackoverflow.com/questions/62284095,1403340,Documentation Replicability
43396525,How to display the code of an image after tf.decode_image,"I'm trying to see how the tf decode images so I try but it raise the error Also I've tried tf.image.decode_image, but it didn't work either. What's wrong ? How can I fix it? Thank u",https://stackoverflow.com/questions/43396525,7765466,Documentation Replication on Other Examples
43411738,tf.image.pad_to_bounding_box VS tf.pad and tf.image.crop_to_bounding_box VS tf.slice,"I'd like to understand why does the two functions tf.image.crop_to_bounding_box and tf.image.pad_to_bounding_box exists, since the behaviour of these two functions can be done really simply with respectively tf.slice and tf.pad. They are not so much easier to understand, and their scope is narrow since they accept only 3D and 4D tensors. Furthermore, they tend to be slower in terms of time of execution. Is there something I miss here ?",https://stackoverflow.com/questions/43411738,7370153,Documentation Replicability
43422949,CTC Loss InvalidArgumentError: sequence_length(b) <= time,"I am running into this error while trying to use tf.nn.ctc_loss through keras (ctc_batch_cost): According to the documentation for tf.nn.ctc_loss, Input requirements are: I am having a hard time understanding what this means-- what is b and what is sequence_length(b)?",https://stackoverflow.com/questions/43422949,740857,Documentation Ambiguity
62318212,Is it possible to create an Estimator with arbitrarily many input Tensors for Predict SignatureDef without placeholders in TensorFlow 2.X?,"The Problem I am converting my Tensorflow 1.14 estimator to TensorFlow 2.1. My current workflow involves training my tensorflow model on gcloud's ai-platform (training on gcloud) and using their model service to deploy my model for online predictions (model service). The issue when upgrading to TensorFlow 2 is that they have done away with placeholders, which is affecting my serving_input_fn and how I export my estimator model. With tensorflow 2, if I export a model without the use of placeholders, my model's ""predict"" SignatureDef only has a single ""examples"" tensor whereas previously it had many inputs named appropriately through my serving_input_fn. The previous set up for my estimator was as follows: And this has worked fine in the past, it has allowed me to have a multi-input ""predict"" SignatureDef where I can send a json of the inputs to ai-platforms model service and get predictions back. But since I am trying to not rely on the tf.compat.v1 library, I want to avoid using placeholders. What I've tried Following the documentation linked here I've replaced my serving_input_fn with the tf.estimator.export.build_parsing_serving_input_receiver_fn method: However, this gives me the following ""predict"" SignatureDef: whereas before my ""predict"" SignatureDef was as follows: I've also tried using the tf.estimator.export.build_raw_serving_input_receiver_fn, but my understanding is that this method requires actual Tensors in order to be used instead of a feature spec. Unless I use placeholders, I don't really understand where to grab these serving Tensors from. So my main questions are: Thanks!",https://stackoverflow.com/questions/62318212,10438511,Documentation Replication on Other Examples
43623121,Shape assertions and declarations in tensroflow,"I use tf.strided_slice to get one value out of the 1d tensor. Unfortunately, inferred shape is ?. How can I assert/declare that it has shape [1]? P.S. I used reshape, but it might have performance implications in some cases",https://stackoverflow.com/questions/43623121,250560,Documentation Replicability
62530401,How to set the min_frequency while using tf.keras.preprocessing.text.tokenizer and ignore the words which are less then min_frequency?,"Is there any way to set the 'min_frequency' in tf.keras.preprocessing.text.tokenizer, just like the 'min_frequency' in tf.contrib.learn.preprocessing.VocabularyProcessor?",https://stackoverflow.com/questions/62530401,12746574,Documentation Replicability
42569921,tf.contrib.slim.get_variables_to_restore() does not return value,"Running below code tf.contrib.slim.get_variables_to_restore() return empty value [] for all_vars, and then causing failure when calling tf.train.Saver. Detail error message shows below. Am I missing anything?",https://stackoverflow.com/questions/42569921,7397552,Documentation Replicability
61513032,Another use case of funtools.partial in T5 tutorial code,"I was reading this code. From T5 tutorial.. I found this snippet. What did I understand reading this? Well, functools.partial is used for creating a wrapper around some known to manipulate the arguments or to reduce the number of arguments, but what is the use case here? It is confusing, given that we could simply call tf.io.decode_csv?",https://stackoverflow.com/questions/61513032,13132388,Documentation Ambiguity
61713523,does tf.function create graph for other functions that it called as well?,"I dont know how exactly the tf.function traces the function that is calling other decorated functions. What is the difference between calling, from within a decorated function, the functions that are decorated vs the functions that are not decorated What happens when the function y is traced by tf.function? If this is the only use x1 and x2 are ever going to serve then does it make sense to decorate x1 with @tf.function?",https://stackoverflow.com/questions/61713523,6546694,Documentation Replicability
61717694,Embed trainable bijector into Keras model,"I am trying to implement normalizing flows embedded in a Keras model. In all examples I can find, such as the documentation of MAF, the bijectors which constitute the normalizing flows are embedded into a TransformedDistribution and exposed directly for training etc. I am trying to embed this TransformedDistribution in a keras Model to match the architecture of other models I have which are inheriting from keras Model. Unfortunately all my attempts (see code) so far fail at transferring the trainable variables inside the transformed distribution to the keras Model. I have tried to make the bijector inherit from tf.keras.layers.Layer, which did not change anything. Any idea if this is even possible? Thanks!",https://stackoverflow.com/questions/61717694,11552392,Documentation Replicability
61720708,How do you save a Tensorflow dataset to a file?,"There are at least two more questions like this on SO but not a single one has been answered. I have a dataset of the form: and another of the form: I have looked and looked but I can't find the code to save these datasets to files that can be loaded later. The closest I got was this page in the TensorFlow docs, which suggests serializing the tensors using tf.io.serialize_tensor and then writing them to a file using tf.data.experimental.TFRecordWriter. However, when I tried this using the code: I get an error on the first line: How can I modify the above (or do something else) to accomplish my goal?",https://stackoverflow.com/questions/61720708,424306,Documentation Replication on Other Examples
61925035,TensorflowException: Invalid GraphDef (TensorFlow 2.0),"I'm building a model using tf.keras.models.Sequential and saving it as a SavedModel object which contains a saved_model.pb file. The model is then going to be used in a C# service using ML.net. Here is the code (pulled and adapted from docs) When loading the saved_model.pb file in ML.NET I get the following exception. When I search for this error - it references freezing weights on model, but the solutions are for TF1. TF2 seems to have a more streamlined method of saving model, but I cannot understand what is wrong. Does anyone know what I'm missing?",https://stackoverflow.com/questions/61925035,7953944,Documentation Replicability
43443205,Tensorflow tf.nn.conv2d clarification,"In reading through the Tensorflow tutorial and API documentation, I do not understand how they defined the shape of the convolution input and filter arguments. The method is: tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None), where the input is shape: [batch, in_height, in_width, in_channels] and the filter is shape: [filter_height, filter_width, in_channels, out_channels]. If anyone could shed light on how to properly define the ""in_channel"" and ""out_channel"" sizes, that would be very helpful.",https://stackoverflow.com/questions/43443205,6749990,Documentation Ambiguity
62348605,How to parse tensor without giving out_type in tensorflow?,"When parsing a serialized tensor, the tf.io.parse_tensor has a required kwarg ""out_type"". However it seems that tf does not need this to know the type of the serialized tensor because when one gives a wrong type, it manages to print out the good one. How could then I parse without this arg? MWE:",https://stackoverflow.com/questions/62348605,4444546,Documentation Replicability
43460838,tensorflow tfrecord storage for large datasets,"I'm trying to understand the ""proper"" method of storage for large datasets for tensorflow ingestion. The documentation seems relatively clear that no matter what, tfrecord files are preferred. Large is a subjective measure, but the examples below are randomly generated regression datasets from sklearn.datasets.make_regression() of 10,000 rows and between 1 and 5,000 features, all float64. I've experimented with two different methods of writing tfrecord files with dramatically different performance. For numpy arrays, X, y (X.shape=(10000, n_features), y.shape=(10000,) I construct a tf.train.Example in the way that tensorflow developers seem to prefer, at least judging by tensorflow example code at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py. For each observation or row in X, I create a dictionary keyed with feature names (f_0, f_1, ...) whose values are tf.train.Feature objects with the feature's observation data as a single element of its float_list. I construct a dictionary with one feature (really two counting the label) whose value is a tf.train.Feature with the entire feature row in as its float_list As the number of features in the dataset grows, the second option gets considerably faster than the first, as shown in the following graph. Note the log scale 10,000 rows: It makes intuitive sense to me that creating 5,000 tf.train.Feature objects is significantly slower than creating one object with a float_list of 5,000 elements, but it's not clear that this is the ""intended"" method for feeding large numbers of features into a tensorflow model. Is there something inherently wrong with doing this the faster way?",https://stackoverflow.com/questions/43460838,306537,Documentation Replication on Other Examples
43472077,How to use tf.summary.text?,TensorFlow 1.1.0rc2 has support for Text in its dashboard but how do I actually log something that will show up there? TensorFlow master branch has a reference to tf.summary.text but nothing called that is available in 1.1.0rc2.,https://stackoverflow.com/questions/43472077,3504575,Lack of Alternative Solutions/Documentation
43681154,Shapes in tf.test.compute_gradient_error,"In addition to 2 tensors, tf.test.compute_gradient_error takes two shapes. Why are they required? Why can't we just evaluate the tensors and calculate their shapes?",https://stackoverflow.com/questions/43681154,250560,Documentation Replicability
62571896,question about tf.data.Dataset.from_generator for bert,"I searched regarding this, but I cant find how to do. usually, they use tf.constant or tf.data.TFrecord. I want to make bert input dataset api using generator from reading csv file. I try several hours, but various error happened. I confused hot to use output_types, and output_shape below is my code. what is the problem?? feed.py",https://stackoverflow.com/questions/62571896,10748392,Documentation Replication on Other Examples
62611459,How to support mixed precision in custom Tensorflow layers?,"When developing my own custom layers for tf.keras: how am I supposed to support mixed precision? The documentation of mixed precision - a feature which is currently marked as experimental in Tensorflow 2.2 - only explains how to use it from a consumers perspective with predefined layers such as the tf.keras.layers.Dense one. I already tried to guess it myself and found two - maybe relevant - details: Am I supposed to use the above get_layer_policy-method and just cast my variables into compute_dtype within the call(...) method of my layer? (And pass variable_dtype in my layers build(...) method to add_weight(...) when creating variables?) For example, here is naive sample implementation of a standard dense neuron layer: Sure, nobody would implement such basic stuff themselves, that one is just for demonstration. But, would this be the way the Tensorflow team expects us to implement the call(...) methods of our custom layers?",https://stackoverflow.com/questions/62611459,1037200,Documentation Replication on Other Examples
43916019,Control dependencies and order of evaluation,"Please consider the following code: The output is: My question is: why does f(1).eval() return 0 even if there is a control dependency on the whileOp that modifies the returned variable acc? After reading the documentation, I was expecting whileOp to be evaluated before returning acc. How should I write the function f(.) in order to force the evaluation of whileOp? In f(.), if I return tf.identity(acc) instead of acc, it works as I expect.",https://stackoverflow.com/questions/43916019,774133,Inadequate Examples
62818943,Tensorflow pretrained models input channel range,"I came across this example which implements a pretrained model. It says: I was wondering about this. What I understand is that image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE)) resizes the images (which can have any size) to one consistent size. I understand that image = (image/127.5) - 1 does not change the actual size of the images, but changes the values (pixels) (which are between 0 to 255) to a range of [-1,1]. In other examples I saw normalization/standardization being done to a range of [0,1], so rescaling by 1.0/255. I do not understand when I have to use which. If I use my own model, it is up to me to scale to a range of [-1,1] or [0,1]? However, when I use a pretrained model I need to know what is required. I googled the mobilenetv2 model, but could not find any documentation telling me that the required input channel is [-1,1]. In this comment it says all pretrained tensorflow models require an input channel of [-1,1]. Is that true? Especially, is that true that all models in the tensorflow hub (if about images) require a range of [-1,1]? Finally, how do I find out what the required range is for a pretrained model? I would not have figured out the [-1,1] in case of MobileNetv2 by my own. On the tensorflow MobileNetv2 page I could not find this information. Furthermore: Is there a way to basically have this done automatically? So that I use a function and it automatically checks the pretrained tensorflow dataset (which has an object storing that information) and applies it (assuming 0-255 is my input)? I think tf.keras.applications.mobilenet_v2.preprocess_input is doing something else (I am not really understanding what it does)? And it is also just for mobilenetv2.",https://stackoverflow.com/questions/62818943,2165335,Documentation Replication on Other Examples
62850250,Understanding the input_shape parameter of hub.KerasLayer,"When transfer learning is done, one could use a model from the tf hub. Like MobilNetV2 or Inception. These models expects the inputs, the images in a certain size. So one has to resize the images into this size before applying the models. In this tutorial the following is used: In this example the images were already resized to 224,224 before. I am wondering about the input_shape=(224,224,3). In this tutorial the pretrained model is not loaded with the hub-KerasLayer, but instead using Where IMG_SHAPE is and img_size is 160. So here the input_shape is input_shape=(160,160,3). Now coming back to the: I was wondering what exactly the input_shape parameter tells me or does? So I do not need to enter 224,224 here, right? I could enter another size, like 160, because my images were resized to this size? So MobilNetV2 does expect 224,224, but with this option I can specify something else? For tf.keras.applications.MobileNetV2 I found the documentation where it exactly explains it: So when I resized my images to 300,300 and I want to use MobileNetV2, can I use the following code: Or do I have to resize to 224,224 and enter the 224,224 here? When I check an implementation for inception the images are resized to 299,299 and then the following code is used: Is it necessary to do this exactly to 299? Or could I also resize to another size, like 250 and give this as an input: So the pretrained models do expect a certain fixed size and this input_shape parameter exists in order to make it flexible in case the user wants to use another size, right? But then why all these examples resize to exactly the size the models assume? I could also have done this to another size, right? So in all the examples it says like the models expects this and I understand it in the way that therefore we do have to resize to exactly what the model expects. But the input_shape parameter is exactly existing for this to make it flexible so that I do not have to resize to exactly what the model expects, but instead just resize to whatever I want and with the input_shape parameter I tell this to the model? As in the mentioned example with 160 image size. Or is this just possible in case I use tf.keras.applications.MobileNetV2 loading the pretrained models, but when using hub.KerasLayer I cannot do it?",https://stackoverflow.com/questions/62850250,2165335,Documentation Ambiguity
44206534,Why is tf.transpose so important in a RNN?,I've been reading the docs to learn TensorFlow and have been struggling on when to use the following functions and their purpose. My guess so far is that: tf.split() is used because inputs must be a sequence. tf.reshape() is used to make the shapes compatible (Incorrect shapes tends to be a common problem / mistake for me). I used numpy for this before. I'll probably stick to tf.reshape() now. I am not sure if there is a difference between the two. tf.transpose() swaps the rows and columns from my understanding. If I don't use tf.transpose() my loss doesn't go down. If the parameter values are incorrect the loss doesn't go down. So the purpose of me using tf.transpose() is so that my loss goes down and my predictions become more accurate. This bothers me tremendously because I'm using tf.transpose() because I have to and have no understanding why it's such an important factor. I'm assuming if it's not used correctly the inputs and labels can be in the wrong position. Making it impossible for the model to learn. If this is true how can I go about using tf.transpose() so that I am not so reliant on figuring out the parameter values via trial and error?,https://stackoverflow.com/questions/44206534,4005959,Documentation Replicability
63096162,How to expand_dims an unknown number of times in TensorFlow?,If the number of dimensions of an element is known then I can call tf.expand_dims. How do I put tf.expand_dims in a loop? The following code works in eager but not in graph mode.,https://stackoverflow.com/questions/63096162,1217998,Documentation Replicability
44226932,Difference between tf.nn_conv2d and tf.nn.depthwise_conv2d,What is the difference between tf.nn_conv2d and tf.nn.depthwise_conv2d in Tensorflow?,https://stackoverflow.com/questions/44226932,7994456,Lack of Alternative Solutions/Documentation
44232566,Add L2 regularization when using high level tf.layers,"Is it possible to add an L2 regularization when using the layers defined in tf.layers? It seems to me that since tf.layers is an high level wrapper, there is no easy way to get access to the filter weights. With tf.nn.conv2d Now what would that look like with tf.layers.conv2d? Thanks!",https://stackoverflow.com/questions/44232566,6255101,Documentation Replication on Other Examples
44478812,What kind of calculation does tf.nn.dynamic_rnn do with its input parameters?,"What kind of calculation does tf.nn.dynamic_rnn perform? How does it use the parameters cell and inputs (to create the result)? I have looked up in the documentation, but I have not found an explanation.",https://stackoverflow.com/questions/44478812,6183280,Documentation Completeness
63359268,TensorFlow 2: Re-saving a SavedModel?,"I am trying to load a model saved in SavedModel format, and then apply some calculations on top of it and re-save the whole pipeline. A minimal code is the following (from this Kaggle kernel): I have tried to follow the TensorFlow documentation of saving custom models to SavedModel format and define a tf.Module: Then I ran it on the test image to build the tf.function and make sure that it works correctly (produces the right output). But when I tried to save it as follows: the resulting model was incorrect. The original one weighted 90 MB while the one in delg_resaved weights only 800 KB. I also tried doing tf.saved_model.load inside the DelgModule.call function, so that the graph creation and variables loading is done entirely inside that tf.function, but the results remained the same.",https://stackoverflow.com/questions/63359268,4789373,Documentation Replication on Other Examples
44707368,Batch variable length sequences in tensorflow,"I am reading variable length input sequences from files as numpy arrays, each of size [timesteps_i, feature_size], and store them in a Python list. Passing this list as argument to tf.train.batch results in a list of the same length, containing Tensors of size [batch_size, timesteps_i, feature_size].",https://stackoverflow.com/questions/44707368,3115923,Documentation Replicability
63578924,Use tf operations when building tf.keras model,"I found it hard to use simple tf operations when building tf.keras model. For a toy example, let's say I want to stack two tensors from previous layers into one, keras doesn't have a stack function but tf does, but in order to use it, I have to do something like: I'm just using tf.stack as a toy example, it could be any tf operations that keras doesn't have (such as tf.image.resize, lots of tf.math operations etc.). I want to know if there is a easy way to use arbitrary tf operations in keras? What about using tf.keras.backend operations? I recon it is probably better to keep every operation as a keras layer. Will using backend operations break that rule?",https://stackoverflow.com/questions/63578924,11939660,Documentation Replicability
63598808,Why should I import keras again even if after importing tensorflow2?,"In TensorFlow version2, when I want to use tf.keras, all the example codes import keras again even if tensorflow has imported like below example. Can't I just skip second line which is from tensorflow import keras?? Why should I import keras separately even if I use keras in forms of tf.keras??",https://stackoverflow.com/questions/63598808,9531939,Documentation Replicability
63600026,What are mixed layers in tf.keras.applications.InceptionV3?,"I am currently trying to understand the architecture of Inseption v3 as implemented in tf.keras.applications.InceptionV3. I am looking at the list of names in the model's layers: I understand how batch normalization, pooling and conv layers transform inputs, but deeper we have layers named mixed1, mixed2, ... and so on. I am trying to understand how they (mixed layers) are transforming their inputs. So far, I couldn't find any information about them. How does a mixed layer work? What does it do?",https://stackoverflow.com/questions/63600026,14142345,Lack of Alternative Solutions/Documentation
61026862,"why ""NumPy operations convert Tensors to numpy arrays automatically""? how does this feature been implemented?","Reading TensorFlow docs: https://www.tensorflow.org/tutorials/customization/basics#numpy_compatibility TF APIs support NumPy objects as inputs and this is easy to understand because TF APIs are just implemented to handle NumPy objects by TensorFlow Team. But On the contrary, why NumPy APIs could handle a tf.Tensor is quite amazing to me. Is this a mechanism that NumPy provided to handle any type of objects? Or just supported by Python in language level ? (I am new to Python)",https://stackoverflow.com/questions/61026862,6770916,Documentation Replication on Other Examples
42049256,TensorFlow: Does queuing examples in parallel speed up a batch creation if tf.train.batch is already dequeuing examples in parallel?,"In the TensorFlow-slim documentation, there is a ParallelReader object that can read TFRecords data in parallel through having multiple readers to take in example strings into queue. However, if I am not mistaken tf.train.batch dequeues examples from a queue and is able to do so in parallel with the argument num_threads. If that is the case, is it necessary that both the batch creation and reading of data must have the same speed otherwise one will cause a slower creation of a batch? I am actually not very sure whether the dequeue operation happens in tf.train.batch or when a queue_runner is manually created to dequeue the examples since I believe tf.train.batch can effectively replace the queue_runner operation. Is this correct?",https://stackoverflow.com/questions/42049256,5107084,Documentation Replication on Other Examples
61175291,Why is optimizer.minimize not working if we pass loss as tf.constant?,"I simply have train = optimizer.minimize(loss = tf.constant(4,dtype=""float32"")) Line of code that i change before everything is working. Why it is giving error ? Because documentation say it can be tensor Here is Docs I really need this thing to work. In this particular example we can have other ways to solve it but i need it to work as my actual code can do this only Error is",https://stackoverflow.com/questions/61175291,6543342,Documentation Replication on Other Examples
61355474,Why does tf.executing_eagerly() return False in TensorFlow 2?,"Let me explain my set up. I am using TensorFlow 2.1, the Keras version shipped with TF, and TensorFlow Probability 0.9. I have a function get_model that creates (with the functional API) and returns a model using Keras and custom layers. In the __init__ method of these custom layers A, I call a method A.m, which executes the statement print(tf.executing_eagerly()), but it returns False. Why? To be more precise, this is roughly my setup The documentation of tf.executing_eagerly says But these cases are not my case, so tf.executing_eagerly() should return True in my case, but no. Why? Here's a simple complete example (in TF 2.1) that illustrates the problem. This example prints tf.executing_eagerly() = False. See the related Github issue.",https://stackoverflow.com/questions/61355474,3924118,Documentation Replicability
61355289,When will tf.print ACTUALLY WORK as expected (i.e. print the values of tensors and variables)?,"First of all, I am using TensorFlow 2.0 and I only care about this version or higher (and I am already caring too much for such a piece of software that only produces headaches). The TensorFlow documentation of tf.print says and then This is all very nice, but I still don't get where tf.print will ACTUALLY WORK (i.e. print the VALUES of variables and tensors) in my code. Of course, needless to say, I couldn't care less about the symbolic representations of tensors, variables or whatever. Whenever I try to use tf.print, I want to see the VALUES (real numbers, vectors or matrices). I've tried to use tf.print in multiple cases and in multiple places, e.g. Ok, so, as you can see, I have no idea where tf.print will do what I want, i.e. I want to see the values of tensors. If something is a tensor, it must have a value. Similarly for variables. So, when will tf.print ACTUALLY PRINT THE VALUES OF TENSORS? I am looking for answers that say e.g., ""tf.print will NEVER work"" or ""it will only work if you are dreaming"". Apart from the jokes and sarcasm, I am really looking for answers that tell me exactly in which places of my code or which stages of developing a model with TF tf.print will actually do what it is supposed to do. Please, don't tell me that tf.print will work when the input is a tensor!!",https://stackoverflow.com/questions/61355289,3924118,Documentation Ambiguity
42399401,Use of grads_ys parameter in tf.gradients - TensorFlow,I want to understand the grad_ys paramter in tf.gradients. I've seen it used like a multiplyer of the true gradient but its not crear in the definition. Mathematically how would the whole expression look like?,https://stackoverflow.com/questions/42399401,2118130,Lack of Alternative Solutions/Documentation
42404564,What does tf.train.get_global_step() do in TensorFlow?,What is the use of the function tf.train.get_global_step() in TensorFlow? In machine learning concepts what is it equivalent to?,https://stackoverflow.com/questions/42404564,775755,Lack of Alternative Solutions/Documentation
42430331,TensorFlow: construct a tensor with recursively defined elements?,I would like to do something like: to construct a tensor 'x'. Is such a construct possible? I looked at 'tf.while_loop' but does not seem to help.,https://stackoverflow.com/questions/42430331,5721911,Documentation Replication on Other Examples
42437115,"Tensorflow: Replacement for tf.nn.rnn_cell._linear(input, size, 0, scope)","I am trying to get the SequenceGAN (https://github.com/LantaoYu/SeqGAN) from https://arxiv.org/pdf/1609.05473.pdf to run. After fixing the obvious errors, like replacing pack with stack, it still doesn't run, since the highway-network part requires the tf.nn.rnn_cell._linear function: the tf.nn.rnn_cell._linear function doesn't appear to be there anymore in Tensorflow 1.0 or 0.12, and I have no clue what to replace it with. I can't find any new implementations of this, or any information on tensorflow's github or (unfortunately very sparse) documentation. Does anybody know the new pendant of the function? Thanks a lot in advance!",https://stackoverflow.com/questions/42437115,5122790,Requesting (Additional) Documentation/Examples
61522019,Is it still necessary to implement `compute_output_shape()` when defining a custom tf.keras Layer?,"I have implemented a custom Layer in tf.keras, using TensorFlow 2.1.0. In the past, when using the stand-alone Keras, it was important to define the compute_output_shape(input_shape) method in any custom layer so that the computational graph could be created. Now, having moved to TF2, I found out that even if I remove that method from my custom implementation the layer still works as expected. Apparently, it works both in eager and graph mode. This is an example of what I mean: Is it safe to say that compute_output_shape() is not necessary anymore? Am I missing something important? In the documentation there's no explicit mention of removing compute_output_shape(), although none of the examples implements it explicitly. Thanks",https://stackoverflow.com/questions/61522019,5499527,Inadequate Examples
69025002,Calculate logarithm in tensorflow not in-place,"I have the following function (see below). It takes an (N,8,3) tensor as argument. I need to choose along axis 0 and 2, so I receive a selection with size (N,1,3) from probabilities along axis 1. I solved that with tf.random.categorical. Sadly this function needs the vector along axis 1 to be calculated with tf.log(vector) beforehand. (I don't see the reason, but that's what the docs say). Now, this calculates log in-place, so it destroys my argument from outside. How is it possible to calculate the logarithm not-in-place (or choose from probability without log(prop) ) ? Thank you",https://stackoverflow.com/questions/69025002,,Documentation Replication on Other Examples
61031226,How to use Keras tf.data with generator ( flow_from_dataframe ) ? to form a perfect input pipeline,Using the input pipelines with tf.data Generating the Dataset using tf.data.Dataset.from_generator() The output I got after fitting the model with the train_dataset,https://stackoverflow.com/questions/61031226,8706702,Documentation Replicability
50329855,How to use the Tensorflow Dataset Pipeline for Variable Length Inputs?,"I am training a Recurrent Neural Network in Tensorflow over a dataset of sequence of numbers of varying lengths and have been trying to use the tf.data API to create an efficient pipeline. However I can't seem to get this thing to work My data set is a NumPy array of shape [10000, ?, 32, 2] which is saved on my disk as a file in the .npy format. Here the ? denotes that elements have variable length in the second dimension. 10000 denotes the number of minibatches in the dataset and 32 denotes the size of a mini-batch. I am using np.load to open this data set and I am trying to create a tf.data.Dataset object using the from_tensor_slices method but it seems that this only works if all input Tensors have the same shape! I tried reading the docs but they have only given a very simple example. So the numpy files have been generated as follows - The code given below is my attempt to create a tf.data.Dataset object The error I get is ""TypeError: Expected binary or unicode string, got array([[[0.0875, 0. ], ..."" So I tried @mrry's answer and I am now able to created a Dataset object. However, I am not able to iterate through this dataset using iterators as said in the tutorial. This is what my code looks like now - The error I get is AttributeError: 'numpy.dtype' object has no attribute 'as_numpy_dtype'. I have no absolutely no clue what this means. This is the complete stack trace -",https://stackoverflow.com/questions/50329855,3760132,Documentation Replication on Other Examples
50340211,Calculate values in a vector based on the elements in another vector in Tensorflow,"I have two vectors: time and event. If one event is 1, the time at the same index should be assigned to func_for_event1. Otherwise, it goes to func_for_event0. How should I implement this logic in Tensorflow? Say tf.cond or tf.where?",https://stackoverflow.com/questions/50340211,1118236,Documentation Replicability
61059725,Why does tf.constant give a dtype error if we pass in a tensor?,"The following code gives the following error: Although from the documentation, setting dtype means that tf.constant is supposed to cast a to the specified data type. So I don't see why this should give a type error. I also know that: does not give an error. So actually, I'm mainly wondering about what's happening under the hood here.",https://stackoverflow.com/questions/61059725,4391249,Documentation Ambiguity
50368759,Reshape one 4D-tensor into 2D whose dimension is None,"I have one question about tf.reshape E.g. one tensor t1 whose shape is [None, h, c, w]. I want to reshape the tensor into 2D, just like: However, the first dimension of t1 is None. How could I process this case, any suggestions?",https://stackoverflow.com/questions/50368759,3387141,Documentation Replicability
42095625,What does the function control_dependencies do?,"I would like to have an example illustrating the use of the function tf.control_dependencies. For example, I want to create two tensors X and Y and if they are equal do or print something. In the code above, X is clearly not equal to Y. What is tf.control_dependencies doing in this case?",https://stackoverflow.com/questions/42095625,5540159,Documentation Replication on Other Examples
61219907,Failed to create control dependencies with tf.control_dependencies(),"I tried to understand tf.control_dependencies(), and wanted to verify it does create control dependencies. Here is the code It returned [], which was not what I expected. If I added the control dependencies in the following way It gave back what I expected [&lt;tf.Operation 'Mul' type=Mul&gt;, &lt;tf.Operation 'Mul_1' type=Mul&gt;]. So my question is, does tf.control_dependencies() really add control dependencies? Or does f.op.control_inputs return all the control inputs?",https://stackoverflow.com/questions/61219907,13160838,Documentation Replication on Other Examples
42287954,tf.argmax() vs tf.arg_max() in TensorFlow,It seems tf.argmax() and tf.arg_max() are doing the same thing for outputting the indies of the biggest element in matrix. https://www.tensorflow.org/api_docs/python/tf/argmax https://www.tensorflow.org/api_docs/python/tf/arg_max Why does TensorFlow have two APIs for this?,https://stackoverflow.com/questions/42287954,5538737,Documentation Ambiguity
42462895,How can I use tf.Session.run() on Android?,"I want to make Tensorflow predictions on the Android. I found example Tensorflow Demo app for Android. But how can I use tf.Session.run, tf.nn.softmax and other functions?",https://stackoverflow.com/questions/42462895,5244666,Documentation Replicability
42675391,tf.nn.sigmoid_cross_entropy_with_logits companies about arguments from documentation,"So I have the following model that I am wanting to test out an idea with. I am particularly interested in tf.nn.sigmoid_cross_entropy_with_logits() because my labels are not mutually exclusive. However, I am getting the following error repeatedly, which seems to be contradicting the tensor flow documentation. Please help!!",https://stackoverflow.com/questions/42675391,4500078,Documentation Replicability
61790621,List of tensors and just tensors,"I am updating codes from tensorflow 1.x to 2.1.0. I changed tensorflow 1.x code to tensorflow 2.1.0 code. But, when I run the updated code, I got the following error. So, I checked the tensorflow 2.1.0 document, and parameters for tf.keras.metrics.Accuracy.update_state() seem to be a list (in form of [ , , , ]). Then, I searched for a way to convert tensor to a list, which is After I run this code, it gives the following error. So, I tried to turn a list of Tensors into Tensors with tf.stack() did not work, as it gave the same initial TypeError saying 'y_pred' is missing at the updated code. torch.stack(), however, gave the following error. So, I am guessing torch.stack() only accepts a tuple, NOT a list. But, tf.stack() seems to accept a list, but it does not turn it into a Tensor? Are my labels and predict even a list of Tensors in the first place? If so, why would tf.stack() not turn them into Tensors? How can I correctly convert labels and predict so that they can be passed into tf.keras.metrics.Accuracy.update_state()? I would very much appreciate if not using compat.v1. unless absolutely necessary.",https://stackoverflow.com/questions/61790621,12997689,Documentation Replicability
61797375,Boolean Column as Tensorflow Feature Columns,I like to add a boolean type column into the feature columns for the input layers. How can I do that? I can't find any class of tf.feature_column that I can treat the boolean type data. Thanks in advance!,https://stackoverflow.com/questions/61797375,13479017,Inadequate Examples
61829273,Error trying to feed a tf.keras model with a tf.data.Dataset instead of tensors,"Why does the following tf2 tf.keras model 'work' when fitted with tensors but generates a ValueError when attempting to fit the same tensors in tf.data.Dataset.from_tensor_slices form? EDIT: Put another way, having developed/fitted/tested etc the model below using numpy arrays. How do those same numpy arrays need to be reshaped(?) so that they can be used to create a dataset with tf.data.Dataset.from_tensor_slices that works with the model? ValueError: If instead of using "".from_tensor_slices"" we use "".from_tensors"" to create X_ds and y_ds then, after zipping, all works well. However, the docs give me the impression "".from_tensors"" is memory heavy and not desirable. Also, I believe that the single element "".from_tensors"" dataset is simply providing the model with two 2D tensors whereas the from_tensor_slices version is a sequence of 1D elements.",https://stackoverflow.com/questions/61829273,9763431,Documentation Replication on Other Examples
61998940,"Which type of parameter the Adam optimizer in GPflow is working on, constrained or unconstrained?","In the document of GPflow like SVGP and natural gradient, the Adam optimizer in TensorFlow is used when it comes to training model parameters (lengthscale, variance, inducing inputs, etc) of the GP model using stochastic variational inference technique, while the natural gradient optimizer for variational parameters. A snippet looks as follows As demonstrated, model.trainable_variables is passed to the Adam optimizer, which is inherited from tf.Module, and is composed of several parameters including lengthscale and variance. What I am concerning is whether the Adam optimizer is working on unconstrained or constrained version of the parameters of the model. A snippet of test code runs as follows and returns As far as I know, parameters of the gaussian process like lenghtscales and the variances of a kernel are nonegative, and they should be constrained when training. I am not an expert of the source code of GPflow or TensorFlow, but it seems that Adam is working on unconstrained parameters. Is this simply a misunderstanding of me, or anything else? Thanks in advance for any help!",https://stackoverflow.com/questions/61998940,13560745,Documentation Ambiguity
43108211,How to add new op in distributed Tensorflow?,"I'm trying to add a new op in distributed tensorflow. Following code is how I create a new operator which can add two array (following this tutorial: https://www.tensorflow.org/api_docs/python/tf/py_func). Then I follow this tutorial(https://www.tensorflow.org/deploy/distributed) to create a cluster of TensorFlow servers and try put an operator in it. I start the nodes, use the following code and command line. Command line: Then execute this file to specify whether ops run. But this code is not working, following is the error According to the N.B. in this page (https://www.tensorflow.org/api_docs/python/tf/py_func), I think I didn't violate any rule of py_func function. Such as serializing the model or forgetting run tf.train.Server and tf.device(). Is there still something I didn't notice?",https://stackoverflow.com/questions/43108211,6478313,Documentation Replicability
43114238,Tensorflow: how do I extract/export variable values at every iteration of training?,"I have been playing around with some neural networks on Tensorflow and I wanted to make a visualization of the neural network's learning process. To do so, I intend to extract the following variables into text/JSON/csv: pre-activation result before 1st layer, activation, bias and weight values for testing and training, each layer and for all time steps. I am looking for a generalizable solution so that I don't have to modify my source code (or at least not more than one or two lines) when applying visualization to future networks. Ideally I could run some function from another python program to read any python/TF code and extract the variables described above. So far I have considered the following solutions: 1) use tf.summary and the filewriter to save as a serialized protocol buffer, then find a way to go from protocol buffer --&gt; JSON format. This unfortunately would not fit the bill as it requires me to modify too much inner code. 2) Perhaps using https://www.tensorflow.org/api_docs/python/tf/train/export_meta_graph Although I am not sure how to implement given my TF foundations are not quite there yet 3) I have also found this solution: But the problem is that it only saves the final values of the weights and biases, whereas I am looking to save their values at all timesteps of training. If anyone has any suggestions on how to tackle this problem or any guidance I would appreciate it. Many thanks",https://stackoverflow.com/questions/43114238,3299870,Documentation Replication on Other Examples
62028248,What are all the random seeds for TensorFlow V2.x and tf.keras?,I only use tf.keras in TensorFlow V2.x. What are all the seeds that I can set? I have only found tf.random.set_seed(). Are there other seeds?,https://stackoverflow.com/questions/62028248,6227592,Lack of Alternative Solutions/Documentation
43131606,What's the difference of add methods in TensorFlow?,"Three add methods: +, tf.add, tf.nn.bias_add. I made test in ipython, here is the test data. All three methods returned array([[11, 22],[13, 24]], dtype=int32). So what's the difference between them? Thanks!",https://stackoverflow.com/questions/43131606,7677894,Documentation Replicability
62039068,Advice on how to create a custom tf.keras optimizer (optimizer_v2),"I want to make an accumulated SGD optimizer for tf.keras (not keras standalone). I have found a couple of implementations of standalone keras accumulated SGD optimizers including this one on pypi. Nevertheless, I am using a project which make use of tf.keras. And as I have seen it's not a good idea to mix them together. The problem is that the documentation for achieving this custom optimizer is not really straight forward. The base class (which I should inherit from) is Optimizer_v2.py which contains some information in the comment section about the task. The required methods that should be overridden are: - resource_apply_dense (update variable given gradient tensor is dense) - resource_apply_sparse (update variable given gradient tensor is sparse) - create_slots (if your optimizer algorithm requires additional variables) - get_config (serialization of the optimizer, include all hyper parameters) Of course of these ones only get_config() actually exists in the base class. resource_apply_dense is actually _resource_apply_dense, resource_apply_sparse is _resource_apply_sparse and create_slots does not even exist in base class. In subclasses as SGD in gradient_decent.py, create_slots also exists as _create_slots. Anyway, apparently the documentation is not updated (there is also an issue regarding this in git but I don't remember the link which pointed this lack of consistency with the documentation) but this makes the whole procedure difficult. For example in SGD I have to override the _resource_apply_dense() method but I cannot understand where the gradients are being calculated and where they are updated. The actual code is given below: which obviously rely on training_ops.resource_apply_keras_momentum and training_ops.resource_apply_gradient_descent to do the actual job. How can I split the 2 parts mentioned in the minimize() method in OptimizerV2 from the above code? The 2 parts are: _compute_gradients() and apply_gradients(). There are a lot of parts that are confusing in this comments like for example in the base class: although if I declare an Adam optimizer and ask for slot names I get an empty list (?). Another confusing issue is the use of private methods which is not clear when they are called and what's their purpose. For example _prepare_local() is contained within SGD and includes a line: Anyway, the problem here is that I do not know which exactly approach to follow to create a custom tf.keras optimizer. Instructions included in comments seem to contradict with the actual implemented subclasses, and the latter also seem to assign the dirty work to the actual C++ function without being clear how this is done or how (in my case) to separate the actions (like the gradient calculation and application). So, is there any advice someone can provide on how to proceed and steps to follow to accomplish this (relatively) simple task? I am using tf 1.15 by the way (so the links are from there).",https://stackoverflow.com/questions/62039068,3584765,Documentation Replication on Other Examples
62223016,Single Prediction Image doesn't need to be rescaled?,"I followed a tutorial to make my first Convolutional Neural Network using Keras and I have a small question regarding the rescaling step. So when we are importing the training set and test set, we create an instance of the tf.keras.preprocessing.image.ImageDataGenerator class and use it as: Along with some other augmentation parameters. My understanding is that we use the rescale parameter to normalize the pixel values of the images imported. But when we load up a single image to run through the CNN, we write something like (code from keras docs): My question is, I cannot see the single input image being rescaled anywhere. Is it being done implicitly, or is there no need to actually perform rescaling? If the latter, then why is it so? Thanks!",https://stackoverflow.com/questions/62223016,11319137,Documentation Replicability
62236460,How to set bounds and constraints on Tensorflow Variables (tf.Variable),"I am using Tensorflow to minimize a function. The function takes about 10 parameters. Every single parameter has bounds, e.g. a minimum and a maximum value the parameter is allowed to take. For example, the parameter x1 needs to be between 1 and 10. I also have a pair of parameters that need to have the following constraint x2 &gt; x3. In other words, x2 must always be bigger than x3. (In addition to this, x2 and x3 also have bounds, similarly to the example of x1 above.) I know that tf.Variable has a ""constraint"" argument, however I can't really find any examples or documentation on how to use this to achieve the bounds and constraints as mentioned above. Thank you!",https://stackoverflow.com/questions/62236460,9762137,Documentation Replicability
62237432,In which cases we use the attribute trainable_variables over trainable_weights and vice-versa of a tf.keras.Model in TF2?,"I was studying how to do transfer learning in TF 2 and I saw that at this tutorial from Tensorflow they use the attribute trainable_variables to reference the trainable variables of a model but in this other tutorial from the keras documentation they use the attribute trainable_weights of a tf.keras.Model. I checked both attributes with a simple model, and they give me the same result. Output: I checked this other issue and tried to follow the definition of both attributes: trainable_variables seems to be here and trainable_weights seems to be here and here, since td.keras.Model also inherits from network.Network. The former seems to be returning the trainable_weights variable. But, I am not sure that this happens in ""all"" cases. So, I am wondering in which cases we use trainable_variables over trainable_weights and vice-versa? and why?",https://stackoverflow.com/questions/62237432,10530728,Documentation Replication on Other Examples
62249084,What is the numpy equivalent of TensorFlow Xavier initializer for CNN?,"I would like to re-create the Xavier initialization in NumPy (using basic functions) in the same way that TensorFlow2 does for CNN. Here is how I learned to do Xavier initialization in NumPy: This is the way I learned Xavier initialization for the logistic regression model. It seems that for Convolution Neural Network it should be different but I don't know how. I'm confused by the TensorFlow documentation when they explain the ""fan_in"" and ""fan_out"". I'm guessing this is where the problem is. Can somebody dumb it down for me, please? Much appreciate it! [UPDATE]: When I follow the tf.keras.initializers.GlorotUniform documentation I still don't come to the same results:",https://stackoverflow.com/questions/62249084,5040482,Documentation Replication on Other Examples
62264567,Does tf.stop_gradient() help memory wise?,Does tf.stop_gradient actually help save GPU memory. I'm asking since some intermediate outputs of layers behind the stop_gradient might not have to be stored (which would've otherwise been necessary for gradient computation).,https://stackoverflow.com/questions/62264567,13706836,Documentation Replicability
62449998,using keras h5 weights in tf.keras model,"I have h5 weights from a Keras model. I want to rewrite the Keras model into a tf.keras model (using TF2.x). I know that only the high level API changed, but do you know if I still can use the h5 weights? Most likely they can be loaded, but is the structure different between Keras and tf.keras weights? Thanks",https://stackoverflow.com/questions/62449998,6510273,Documentation Ambiguity
62476015,TF 2.2 Saved_model from keras with custom signatures and preprocessing,"I am trying to use the tf.saved_model.save after a training a Transformer Model in order to deploy it. My model has multiple inputs and outputs. If I am using the saved_model function for serving, I add some issue about the input shape with the first input, and the second input is not visible when I am using the saved_model_cli show function. I found a way to solve that issue by wrapping the main transformer block by a model module and then save the model. But, I see in the documentation there is another way which consist in using signatures to indicate the inputs/outputs during the tf.saved_model.save. To do that, we need to use the tf.Module class but I haven't understood how to use it in the case we have multiple inputs/outputs exactly (how to make understand the module that inputs tensor should be related to that inputs for the model?) . Does anyone know how to do that with the second method ? Morever can we do preprocessing of the data through the signature ?https://www.tensorflow.org/guide/saved_model",https://stackoverflow.com/questions/62476015,8401969,Documentation Replication on Other Examples
43604608,Why is my tf_gradients returning None?,Basically I'm not sure why it's returning None. I've tried adding print statements and I can see prob_ and log_ come out just fine but I'm not sure what's happening in tf.gradients that is causing the issue above. model.weights are basically the weights of the model that I'm using.,https://stackoverflow.com/questions/43604608,3532564,Documentation Ambiguity
43792961,Understanding the while loop in Tensorflow,"I am using the Python API for Tensorflow. I am trying to implement the Rosenbrock function given below without the use of a Python loop: My current implementation is as follows: I have tried implementing the summation in a tf.while_loop(); however, I found the API somewhat unintuitive when it comes to using an index integer that is meant to remain separate from the data. The example given in the documentation uses the data as the index (or vice-versa):",https://stackoverflow.com/questions/43792961,1309401,Documentation Replication on Other Examples
43827792,How do I use strided_slice to select all the element in tensorflow?,"I read the examples in document: It seems like that I can not simply use input[:,:] to select all the element, instead I have to use the syntax like input[:-1, :-1]. However in this way input[:-1, :-1] , I will miss the last row or last column. What should I do? I take an example: output: I read a lot of material and I found that I can use tf.shape(ph),let see: out: However, if I want to get the result like this: What can I do?",https://stackoverflow.com/questions/43827792,6080827,Documentation Replicability
62956096,Is it possible to extract trained class names from tflite model?,"I have tried to search everywhere, tried everything in tflite_interpreter = tf.lite.Interpreter(model_path='model.tflite'), read tflite documentation but I cannot find the method to extract the class names from the model. Is it possible?",https://stackoverflow.com/questions/62956096,10688345,Lack of Alternative Solutions/Documentation
62962147,TensorFlow - Fashion MNIST Steps Per Epoch,"I'm working with the Kera's Fashion MNIST dataset. When I fit my model, I noticed to complete one epoch it would have to go through 1500 steps. I was looking at the docs for the fit function, but couldn't understand why the default steps were set to 1500. I understand when the steps_per_epoch is None the behavior is dependent on the data type of the dataset, but how can I check if the data type is a tensor or tf.data?",https://stackoverflow.com/questions/62962147,6179818,Documentation Replication on Other Examples
44097181,Why MonitoredTrainingSession can not receive graph as an argument?,Why tf.train.MonitoredTrainingSession does not receive graph as an argument? tf.Session (and tf.train.Supervisor) can do that as below: Only the way of writing as below is recommended?,https://stackoverflow.com/questions/44097181,6472530,Documentation Replicability
63186177,What is the difference between <tf.io.parse_single_example> and <tf.data.experimental.parse_example_dataset>?,"I am beginner in tensorflow and studying how to use tfrecord dataset. is there any difference between &lt;tf.io.parse_single_example&gt; and &lt;tf.data.experimental.parse_example_dataset&gt; ?? version of tensorflow is 2.3.0 I tried to look it up but,could not find this,think both outputs is the same...., which should I use &lt;tf.io.parse_single_example&gt; and &lt;tf.data.experimental.parse_example_dataset&gt; ?",https://stackoverflow.com/questions/63186177,11899917,Documentation Ambiguity
44315874,Is adding a dimension broadcasting?,"Given all of the following are equivalent and produce I understand that bb is ""broadcast"" to the value corresponding to bbb by tf.add (here +). Is the addition of a dimension that transforms b to the value of bbb all broadcasting, or is it something else?",https://stackoverflow.com/questions/44315874,656912,Documentation Replication on Other Examples
63210672,What Does tf.estimator.LinearClassifier() Do?,"In TensorFlow library, what does the tf.estimator.LinearClassifier class do in linear regression models? (In other words, what is it used for?)",https://stackoverflow.com/questions/63210672,13744618,Documentation Replicability
44339463,confusing situations when `tf.constant` not displayed in `tensorboard`?,"Below is the working code where some tf.constant get displayed in tensorboard, some don't. However, I have no idea why those don't get displayed. Could anyone help me out here? Thanks",https://stackoverflow.com/questions/44339463,4333609,Documentation Replicability
63222770,Backpropagating through multiple forward passes,"In usual backprop, we forward-prop once, compute gradients, then apply them to update weights. But suppose we wish to forward-prop twice, and backprop through both, and apply gradients only then (skip on first). Suppose the following: From docs, a tf.Variable is a stateful object, which blocks gradients, and weights are tf.Variables. Examples are differentiable hard attention (as opposed to RL), or simply passing a hidden state between layers in subsequent forward passes, as in diagram below. Neither TF nor Keras have an API-level support for stateful gradients, including RNNs, which only keep a stateful state tensor; gradient does not flow beyond one batch. How can this be accomplished?",https://stackoverflow.com/questions/63222770,10133797,Documentation Ambiguity
63228543,How to use embedding_column with BoostedTreesClassifier,"I am learning the GBDT example on the tensorflow official website:https://www.tensorflow.org/tutorials/estimator/boosted_trees?hl=en The example on the official website is one-hot processing of classification features. But I want to convert discrete features into dense vectors through embedding. In the process, I encountered an unsolvable problem. The following code is an example from the official website. But when I change the'indicator_column' to'embedding_column' and run the'boostedtreeclassifier', an error is reported. Next, I have not made any changes to the data input method of the official website. Then I start to initialize the model and train, and an error will be reported here. The above says: '# NOTE: GBDT requires that all DenseColumns expose a dtype attribute.' 'AttributeError:'EmbeddingColumn' object has no attribute'dtype''. I use 'estimator.LinearClassifier' and there is no problem, but when I use 'tf.estimator.BoostedTreesClassifier', this problem occurs. enter image description here",https://stackoverflow.com/questions/63228543,14041249,Documentation Replication on Other Examples
44357675,Documentation on how to use tf.estimator in TensorFlow,I understand that we can write custom models and encapsulate it using tf.estimator. But I just can't seem to find any documentation with an example. I know that you have to define your model inside a 'model_fn' but what exactly should I return from this function. Also am I supposed to put the the loss and the training step within the 'model_fn' or just the network. How should I modify the code give below to make it work with tf.estimator. Would really appreciate some help.,https://stackoverflow.com/questions/44357675,7656080,Documentation Replicability
63482945,How does tf.nn.dilation2d compute gradient and learn its filters,"I want to understand how the tf.nn.dilation2d is working and how the ""filters"", which refers to a structural element are learned. The official documentation is available here : https://www.tensorflow.org/api_docs/python/tf/nn/dilation2d The documentation didn't reference a scientific paper, and I found a lot of different idea in scientific literature, about morphological filters in deep learnign. Here just some examples : I searched into the code (https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/ops/nn_ops.py#L327-L392), but the tf.nn.dilation2d only call gen_nn_ops.dilation2d I searched into gen_nn_ops.py (which I found inside my python lib folder, probably because it's generated from somewhere else) but I didn't understand what the code is doing. Thank you for your time.",https://stackoverflow.com/questions/63482945,14130094,Documentation Replicability
63495568,Is there a difference in speed between tf.keras.Model subclass implementation and equivalent tf functional API implementation?,"Let's assume we have two equivalent models which have the same inputs, layers and outputs, except one is implemented as a tf.keras.Model subclass, and the other is implemented in the tf functional API. Is there a difference in the speed of inference or training of these models?",https://stackoverflow.com/questions/63495568,11781546,Documentation Replicability
44640357,Does Tensorflow's tf.while_loop automatically capture dependencies when executing in parallel?,"I am interested in implementing a Recursive Neural Network in Tensorflow, like what has been done in How can I implement a recursive neural network in TensorFlow?. However, in his implementation, the parallel_iterations of the tf.while_loop statement was fixed to be 1. I fear that this might be too slow. Since the tree I am going to feed into tensorflow have parts that are not dependent on each other, I would hope that I could set parallel_iterations to a higher value. However, it is inevitable that there are some dependencies required in the tree I feed in as input to tensorflow, and I am afraid that setting it to higher value may break the dependency property. So my question is, had Tensorflow's tf.while_loop automatically captured dependencies already, in order to only use paralleism on placed that are not dependent on each other? The tensorflow documentation says the following: But I am not sure what they mean by ""correct programs"".",https://stackoverflow.com/questions/44640357,3608412,Documentation Ambiguity
63715707,Does image_dataset_from_directory load all the images into memory at once?,"I'm new to machine learning, and I am trying to create an image classifier, I want to load the dataset, but I want to do it in a way such that it does not take up all of my memory. Reading the tensorflow documentation, it says that iteration of a dataset happens in streaming fashion, and I am wondering if tf.keras.preprocessing.image_dataset_from_directory will load the images at once or ""stream"" it a batch at a time. If not I was thinking of making a generator to read file names one at a time and load them when the batches are ready with keras.utils.Sequence.",https://stackoverflow.com/questions/63715707,10171841,Documentation Replication on Other Examples
50514454,End of Sequence Error when using tf.estimator and tf.data,"I am using tf.estimator.train_and_evaluate and tf.data.Dataset to feed data to the estimator: Input Data function: Train Function: The training goes fine, but when it comes to evaluation I get this error: If I don't use Dataset.batch on evaluation dataset (by omitting the line dataset[name] = dataset[name].batch(batch_size) in data_fn) I get the same error but after a much longer time. I can only avoid this error if I don't batch the data and use steps=1 for evaluation, but does that perform the evaluation on the whole dataset? I don't understand what causes this error as the documentation suggests I should be able to evaluate on batches too. Note: I get the same error when using tf.estimator.evaluate on data batches.",https://stackoverflow.com/questions/50514454,8636859,Documentation Replication on Other Examples
43865115,Why tf.nn.separable_conv2d uses with_space_to_batch?,The implementation of tf.nn.separable_conv2d uses tf.nn.with_space_to_batch. But I don't see any difference in the outputs when tf.nn.with_space_to_batch is removed. What is the purpose of space to batch here? Is this a low-level optimization? Related code;,https://stackoverflow.com/questions/43865115,545089,Documentation Replicability
62752605,Loss function in tf.nn.sampled_softmax_loss,"I have a question regarding Tensorflow: Which loss function is used in tf.nn.sampled_softmax_loss? I believe it's cross-entropy, but it is not written on the official website. Can anyone confirm my guess?",https://stackoverflow.com/questions/62752605,13874745,Documentation Replicability
62755661,Multi-threaded processing using custom data generator in tensorflow2,"I have been using a custom data generator in keras, tensorflow 1, with It worked great. Now when I switched to tensorflow2, I found out that multi_gpu_model(model) is no longer supported. As suggested in the docs, I switched to tf.distribute.MirroredStrategy(), since I am running on a headless server with 4 GPUs. I also switched my generator ('training_generator') to tf.data.Dataset format: But how to make it run with multiple threads? Here is what I tried (both from here: https://medium.com/@nimatajbakhsh/building-multi-threaded-custom-data-pipelines-for-tensorflow-f76e9b1a32f5): train_dataset = train_ds.map(lambda x,y: (x,y), num_parallel_calls=workers) generators = tf.data.Dataset.from_tensor_slices(['Gen_0', 'Gen_1', 'Gen_2', 'Gen_3', 'Gen_4', 'Gen_5', 'Gen_6', 'Gen_7','Gen_8','Gen_9', 'Gen_10']) ` This loads the CPU and feeds the GPUs well, but it seems to create copies of my dataset. All I want is to run through the whole dataset once, generating batches in parallel as was possible before in model.fit_generator() Any help or insights are appreciated!",https://stackoverflow.com/questions/62755661,3224242,Documentation Replication on Other Examples
62767445,What's default initial_state in tf.nn.dynamic_rnn,"Usually, we would use cell.zero_state as the initial_state of tf.nn.dynamic_rnn. Now, I'm wondering what's default initial_state in tf.nn.dynamic_rnn if we don't set initial_state. The most similar question I can find is Setting initial state in dynamic RNN But I can't understand what does scratch mean in the answer:",https://stackoverflow.com/questions/62767445,13874745,Documentation Ambiguity
43885770,Clarification of tf.name_scope in TensorFlow documentation,"The TensorFlow documentation mentions the following for tf.name_scope What is the meaning of given values are from the same graph, makes that graph the default graph ? Same graph refers to which graph ? Also, what is the use of values parameter in tf.name_scope ?",https://stackoverflow.com/questions/43885770,6842947,Documentation Ambiguity
62786629,how model info is sent to keras model,"below is the code in tensorflow document. (https://www.tensorflow.org/api_docs/python/tf/keras/Model) in tf.keras.Model, input and output tensor related info is sent. But there is no info about modeling( two dense layer and activation funtion) How modeling info is sent to tf.keras.Model?? thanks.",https://stackoverflow.com/questions/62786629,10748392,Inadequate Examples
44103457,Why tf. Variable's value does not change upon different calls in the same programe?,In the following code Why tf.Variable valur remains same when I print it twice. Since tf.truncate_normal produces random values so I expect that it should be different upon different calls?,https://stackoverflow.com/questions/44103457,7997184,Documentation Replicability
62994289,Saving TFrecords with TPU,"I'm trying to use tf.data.experimental.TFRecordWriter to save dataset on Google cloud bucket using TPU. The code from the example in documentation works: But I have dataset of tuples (string, int64), where first is jpg-encoded image and second is label. When I pass it to writer.write() method, it says: 'tuple' object has no attribute 'is_compatible_with'. I guess I have to pack image and label into tf.train.Example to make it work. I use the following code: But I get the following error: Although I didn't turn off eager mode and this code tf.constant([], dtype = float).numpy() works. Maybe TPU works not in eager mode? Ok, I changed .numpy() to .eval() in the code above. Then I get the foloowing error: What session does TPU use and how do I specify it? When I run the code below: I get an error: But I don't know how to get the current graph and pass it to tf.compat.v1.Session(). When I go another way and type: It says: Is it possible to use .eval() on TPU? Or how do I perform my task another way?",https://stackoverflow.com/questions/62994289,13959198,Documentation Replication on Other Examples
44123088,How tf.nn.softmax_cross_entropy_with_logits can compute softmax cross entropy in tensorflow?,"tf.nn.softmax_cross_entropy_with_logits, Documentation says that it computes softmax cross entropy between logits and labels what does it mean? Is it not applying cross entropy loss function formula on it? Why documentation says that it computes sofmax cross entropy?",https://stackoverflow.com/questions/44123088,7997184,Documentation Ambiguity
63004540,How to pad 1 dimensinal vector in tensorflow? Getting InvalidArgumentError: paddings must be a matrix with 2 columns with tf.pad,"I am trying to use tf.pad. Here is my attempt to pad the tensor to length 20, with values 10. I get this error message I am looking at the documentation https://www.tensorflow.org/api_docs/python/tf/pad But I am unable to figure out how to shape the pad value",https://stackoverflow.com/questions/63004540,3259896,Documentation Replicability
63008991,"why implementing the ""call"" method when subclassing a tf.keras layer(or model) class makes the layer(model) object callable?","When writing customized tf.keras layers, we have to implement the ""call"" method, since a object of a class can be called like a function with ""()"" only(?) if the object has a valid ""__call__"" method. while I didn't find something like in the keras.model source, how could all this work?",https://stackoverflow.com/questions/63008991,12589108,Inadequate Examples
63014913,return the top_k items of each row for sparse tensor,"For the dense tensor, we can use tf.nn.topk to find values and indices of the k largest entries for the last dimension. For the sparse tensor, I would like to efficiently get the top n items of each row, without converting the sparse tensor to dense.",https://stackoverflow.com/questions/63014913,13969647,Documentation Replicability
63020800,"Understanding Tensorflow Object-Detection API, kwargs for Checkpoint class, what is `_base_tower_layers_for_heads`?","Currently, I've been learning how to use Object-Detection API from Tensorflow. I follow a quick start tutorial for training with custom data with this notebook as suggested by them. In the effort to understanding each line of the code, I stumbled upon this snippet code in the ""Create Model and Restore Weight"" part. I don't really understand what are the keyword arguments that are available for the Checkpoint class in that particular snippet code. My question is; is there any documentation out there that shows the list of the keyword arguments? or at least explain what are _base_tower_layers_for_heads and_box_prediction_head? I've read the tf.train.Checkpoint documentation. It says that we can provide models or optimizers for the constructor's keyword argument. I am already familiar with this class to restore the weights to my model, however, I find it is alien to see _base_tower_layers_for_heads or _box_prediction_head for the keyword argument. I do know about 'heads' and different types of 'heads' in the object detection architecture and their relation to transfer learning, what I don't understand is in the context of their data structure. How do I know, these keyword arguments exist? and is there any other else? I would really appreciate it if somebody could give me insights or at least tell me where can I find documentation that I can read to understand it more.",https://stackoverflow.com/questions/63020800,8410038,Requesting (Additional) Documentation/Examples
44162432,Analysis of the output from tf.nn.dynamic_rnn tensorflow function,"I am not able to understand the output from tf.nn.dynamic_rnn tensorflow function. The document just tells about the size of the output, but it doesn't tell what does each row/column means. From the documentation: The outputs tensor is a 3-D matrix but what does each row/column represent?",https://stackoverflow.com/questions/44162432,333125,Documentation Completeness
44395547,tensorflow: what's the difference between tf.nn.dropout and tf.layers.dropout,"I'm quite confused about whether to use tf.nn.dropout or tf.layers.dropout. many MNIST CNN examples seems to use tf.nn.droput, with keep_prop as one of params. but how is it different with tf.layers.dropout? is the ""rate"" params in tf.layers.dropout similar to tf.nn.dropout? Or generally speaking, is the difference between tf.nn.dropout and tf.layers.dropout applies to all other similar situations, like similar functions in tf.nn and tf.layers.",https://stackoverflow.com/questions/44395547,8102163,Documentation Replicability
44681810,Is tf.train.GradientDescentOptimizer a vanilla GradientDescent?,"Is tf.train.GradientDescentOptimizer a vanilla gradient descent? i.e. not SGD, so it is equivalent to a gradient update implemented in numpy.",https://stackoverflow.com/questions/44681810,3851003,Documentation Replication on Other Examples
44690363,How to use tf.train.ExponentialMovingAverage in Android/IOS,"I use freeze_graph to export my model to a file named ""frozen.pb"". But Found that the accuracy of predictions on frozen.pb is very bad. I know the problem maybe MovingAverage not included in frozen.pb. When I use model.ckpt files to restore model for evaluating, if I call tf.train.ExponentialMovingAverage(0.999) , then the accuracy is good as expected, else the accuracy is bad. So How To export a binary model which performance is the same as the one restored from checkpoint files? I want to use "".pb"" files in Android Devices. The official document doesn't mention this. Thanks!! Freeze Command: Evaluate Code:",https://stackoverflow.com/questions/44690363,8058425,Lack of Alternative Solutions/Documentation
63558891,"Tensorflow 2.2, tf.nn.conv1d in Lambda layer","I'd like to perform a convolution in a Lambda layer, but I can't get it to work any way. I realize dimensions are not correct in the above example, but that's my data's actual format. The training samples have a shape of (history_size, num_features) and the kernel has to convolve along history_size, each feature separately. Any help would be appreciated. I cannot find an example on how to perform tf.nn.conv1d manually.",https://stackoverflow.com/questions/63558891,4328352,Documentation Replication on Other Examples
63730066,How to use tf.data.Dataset with kedro?,"I am using tf.data.Dataset to prepare a streaming dataset which is used to train a tf.kears model. With kedro, is there a way to create a node and return the created tf.data.Dataset to use it in the next training node? The MemoryDataset will probably not work because tf.data.Dataset cannot be pickled (deepcopy isn't possible), see also this SO question. According to issue #91 the deep copy in MemoryDataset is done to avoid modifying the data by some other node. Can someone please elaborate a bit more on why/how this concurrent modification could happen? From the docs, there seems to be a copy_mode = ""assign"". Would it be possible to use this option in case the data is not picklable? Another solution (also mentioned in issue 91) is to use just a function to generate the streaming tf.data.Dataset inside the training node, without having the preceding dataset generation node. However, I am not sure what the drawbacks of this approach will be (if any). Would be greate if someone could give some examples. Also, I would like to avoid storing the complete output of the streaming dataset, for example using tfrecords or tf.data.experimental.save as these options would use a lot of disk storage. Is there a way to pass just the created tf.data.Dataset object to use it for the training node?",https://stackoverflow.com/questions/63730066,2137370,Inadequate Examples
63751223,Is there an Adaptive Instance Normalization layer in Keras?,"I was looking through the concept of Adaptive Instance Normalization and was wondering if there is a tf.keras.layers.AdaIN() somewhere. If not, can someone please give any pointers to implement it using keras backend?",https://stackoverflow.com/questions/63751223,10013648,Documentation Replication on Other Examples
64017646,How to create an empty tf.Variable with None dimension,How to create empty tf.Variable. I try to make this but get errors ValueError: initial_value must be specified. Which initial_value should I put? Thx,https://stackoverflow.com/questions/64017646,5546244,Documentation Ambiguity
64039233,difference keras and tf.keras,"I found tf.keras is quite different with keras, in keras, bug occur when you use normal function instead Lambda wrapper function, bug is ""AttributeError: 'NoneType' object has no attribute '_inbound_nodes'"", while in tf.keras, it is allowed use normal function and tf.keras.layers. funtion simultaneously. keras seems use different data structure, create innode between layers.",https://stackoverflow.com/questions/64039233,14331377,Documentation Replicability
64046604,How to make tf.data.Dataset.map function executed only once in first epoch?,I try to implement some transformation on dataset by using tf.data.Dataset. I found the transformation was executed in every epoch. Is it possible that the map function is executed in first epoch?,https://stackoverflow.com/questions/64046604,10088497,Documentation Replication on Other Examples
45030619,Detecting out-of-bounds slicing with tf.slice like in numpy,"In tensorflow, I'm trying to use tf.slice, but as its documentation states, it requires the slice to fit in the input array. For instance, if you try to slice the first 5 positions of the tensor [1,2,3,4] it will crash. I want to have the same functionality we get with python lists or numpy arrays where slicing gets you the intersection of the original array and the slice you asked for. For instance if you ask for positions 2 to 6 of [1,2,3,4] you'll get [2,3,4]. How can I do that in tensorflow? Thanks!",https://stackoverflow.com/questions/45030619,4189580,Documentation Replication on Other Examples
45034619,Tensorflow: vectorize loop,"I have to vectorize a loop in Tensorflow. It's some kind of convolution with position dependent kernel. I was said it might be done using tf.einsum, but I don't know how. The only way I know is to use tf.while_loop, but I want something more elegant. Please, help.",https://stackoverflow.com/questions/45034619,,Documentation Replicability
64326029,Load tensorflow images and create patches,"I am using image_dataset_from_directory to load a very large RGB imagery dataset from disk into a Dataset. For example, The Dataset has, say, 100000 images grouped into batches of size 32 yielding a tf.data.Dataset with spec (batch=32, width=256, height=256, channels=3) I would like to extract patches from the images to create a new tf.data.Dataset with image spatial dimensions of, say, 64x64. Therefore, I would like to create a new Dataset with 400000 patches still in batches of 32 with a tf.data.Dataset with spec (batch=32, width=64, height=64, channels=3) I've looked at the window method and the extract_patches function but it's not clear from the documentation how to use them to create a new Dataset I need to start training on the patches. The window seems to be geared toward 1D tensors and the extract_patches seems to work with arrays and not with Datasets. Any suggestions on how to accomplish this? UPDATE: Just to clarify my needs. I am trying to avoid manually creating the patches on disk. One, that would be untenable disk wise. Two, the patch size is not fixed. The experiments will be conducted over several patch sizes. So, I do not want to manually perform the patch creation either on disk or manually load the images in memory and perform the patching. I would prefer to have tensorflow handle the patch creation as part of the pipeline workflow to minimize disk and memory usage.",https://stackoverflow.com/questions/64326029,14438185,Documentation Replication on Other Examples
45313351,Tensorflow Depthwise Convolution Understanding,"I'm currently trying to understand how Tensorflow's Depthwise Convolution works. As far as I've understood, each channel in the input image is convolved with it's own set of filters, and then the results are concatenated. I'm going to stick with the parameter depth_multiplier=1 for the sake of simplicity in the remainder, so n_inputchannels == n_outputchannels. So in theory, I could split up the depthwise convolution into N individual, regular Conv2Ds, correct? Why does the following code produce different results then I am wondering - is this a precision issue? I'm following the documentation for the ordering [filter_height, filter_width, in_channels, 1] for the depthwise convolution filters, and [filter_height, filter_width, in_channels, out_channels] for the regular convolutions, and NHWC data format. Here I'm computing first the normal tf.nn.depthwise_conv2d and then slice the input and weights accordingly and do tf.nn.conv2ds individually. For these parameters I get about 1e-5 difference, but that tends to get higher when I increase the number of channels. I would be really glad if someone could explain to me what's going on :) Thanks!",https://stackoverflow.com/questions/45313351,3362759,Documentation Replication on Other Examples
64348896,How to feed images to Keras model that have multiple inputs with tf.data.Dataset generator?,"When the model has only one input, I used tf.keras.preprocessing.image_dataset_from_directory method to feed images and labels. If the model has two or more inputs like the following model, how to feed them with tf.data.Dataset generator or better way?",https://stackoverflow.com/questions/64348896,3119984,Inadequate Examples
64356209,How does Model.fit() method's shuffle deals with Batches when using a tf.data.Dataset?,"I am using tensorflow 2. When using the Model.fit() method with a tf.data.Dataset, the argument 'batch_size' is ignored. Thus to train my model on batches, I have to first change my dataset of samples into a dataset of batches of samples by calling tf.data.Dataset.batch(batch_size). Then, after reading the documentation, I don't understand clearly how the .fit() method will shuffle my dataset at each epoch. Since my dataset is a dataset of batches, will it shuffle the batches among each other (the batches remain unchanged) ? Or will it shuffle all the samples and then regroup them into new batches (which is the desired behaviour) ? Thanks a lot for your help.",https://stackoverflow.com/questions/64356209,14449900,Documentation Replicability
45347275,What is the difference between tf.gradients and tf.train.Optimizer.compute_gradient?,"It seems that tf.gradients allows to compute also Jacobians, i.e. the partial derivatives of each entry of one tensor wrt. each entry of another tensor, while tf.train.Optimizer.compute_gradient only computes actual gradients, e.g. the partial derivatives of a scalar value wrt. each entry of a particular tensor or wrt. one particular scalar. Why is there a separate function if tf.gradients also implements that functionality?",https://stackoverflow.com/questions/45347275,852592,Documentation Replication on Other Examples
64380057,TF 2.3.0 training keras model using tf dataset with sample weights does not apply to metrics,"I am passing in sample_weight as the 3rd tuple in tf.data.Dataset (using it in the context of mask, so my sample_weight are either 0, or 1. The problem is that this sample_weight doesn't seem to get applied to metrics calculation. (Ref: https://www.tensorflow.org/guide/keras/train_and_evaluate#sample_weights) Here's code snippet: The loss after training is very close to zero, but sparse_categorical_accuracy is not (about 0.89). So I highly suspect whatever sample_weight (masks) that's passed in to construct the tf.dataset, does NOT get applied when the metrics is reported during training, while loss seems to be correct. I further confirmed by running prediction on the subset that are not masked separately, and confirmed the accuracy is 1.0 Also, according to documentation: https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalAccuracy the metric has 3 args: y_true, y_pred, sample_weight So how does one pass the sample_weight during metric computation? Is this the responsibility of model.fit(...) within the keras framework? I can't find any example googling around so far.",https://stackoverflow.com/questions/64380057,1762295,Inadequate Examples
45373740,Tensorflow ReLU normalizes strangely,"in my opinion the rectified linear unit is supposed to execute the following function: However, this seems not to be the case with tf.nn.relu: The random matrix looks like this: And the output from the relu function like this: So, if I see it correctly, tf.nn.relu does some sort of normalization, right? If yes, why isn't it mentioned in the docs? Okay, I found out that the whole issue was related to my tensorflow installtion which seemed to be corrupt. On another machine, I did get the expected results. Thank you for the help and helpful comments.",https://stackoverflow.com/questions/45373740,5631237,Lack of Alternative Solutions/Documentation
45705070,how to load and use a saved model on tensorflow?,"I have found 2 ways to save a model in Tensorflow: tf.train.Saver() and SavedModelBuilder. However, I can't find documentation on using the model after it being loaded the second way. Note: I want to use SavedModelBuilder way because I train the model in Python and will use it at serving time in another language (Go), and it seems that SavedModelBuilder is the only way in that case. This works great with tf.train.Saver() (first way): tf.saved_model.builder.SavedModelBuilder() is defined in the Readme but after loading the model with tf.saved_model.loader.load(sess, [], export_dir)), I can't find documentation on getting back at the nodes (see ""finalnode"" in the code above)",https://stackoverflow.com/questions/45705070,2210667,Lack of Alternative Solutions/Documentation
64734480,How to implent tf.nn.in_top_k in pytorch,"I want to implent tf.nn.in_top_k in pytorch. Here is the link of tf.nn.in_top_k, It computed precision at k as a bool Tensor and will return a Tensor of type bool. tf.nn.in_top_k I wonder whether there are similar api in pytorch?",https://stackoverflow.com/questions/64734480,9276708,Documentation Replicability
64737363,TensorFlow training gets slower every batch,"I am new to TensorFlow and I get my code running successfully by modifying tutorials from the official website. I checked some other answers on StackOverflow, which says my problem is likely due to something is being added to the graph every time. However, I have no idea where to look for the code that might have caused this. Also, I used tf.py_function to map the dataset because I really need to enable eagerly mode in the mapping. Can anyone please help me? Thanks! Here is the rest of my code.",https://stackoverflow.com/questions/64737363,9712687,Documentation Replication on Other Examples
64759627,TensorFlow custom training step with different loss functions,"According to the TensorFlow documentation, a custom training step can be performed with the following But if I want to use a different loss function like categorical cross-entropy I would need to argmax the logits created in the gradient tape: The problem with this is that the tf.argmax function is not differentiable, so TensorFlow wouldn't be able to compute the gradients and you would get the error: My question: Without changing the loss function how could I make the second example work?",https://stackoverflow.com/questions/64759627,,Documentation Replication on Other Examples
64769187,"Tensorflow - Interpreting the tf.estimator.ProfilerHook ""_Send"" op",I have a deep CNN/RNN that I train on Google AI platform. I distribute the training on 8 GPUs using the tf.distribute.MirroredStrategy. I recently upgraded my runtime version from 1.13 to 1.15 and my training is more than 2x slower than before. I read that tf.estimator.ProfilerHook can be used to identify performance bottlenecks. So I collected the profiling information and rendered it at chrome://tracing. I got this A training step spends an entire 1 second on these _Send ops. What is this? I can't find any documentation on the op or why it's in my graph. What does this mean?,https://stackoverflow.com/questions/64769187,6078821,Lack of Alternative Solutions/Documentation
45761087,Tensorflow 1.3.0 NameError: name 'LinearRegressor' is not defined,"I am following the ""get started"" instruction on the official Website. https://www.tensorflow.org/get_started/ I have tried both virtualenv and pip native installment method. And tensorflow can be imported at this time. But when the tf.estimator.LinearRegressor can not be used. Do I need to link some kind of path to make the APIs started to work? The error is: The code is: Thanks!",https://stackoverflow.com/questions/45761087,8484738,Documentation Replication on Other Examples
64770581,What is the difference between tensorflow.keras.layers and keras.layers?,"I have read a related question's answer about this on stackoverflow which explains the difference between tensorflow.keras.layers and tensorflow.layers, link: What is the difference between tf.keras.layers versus tf.layers? But, it doesn't answer the difference between tensorflow.keras.layers and keras.layers, whereas I noticed I can import them also. So, what is the difference between them?",https://stackoverflow.com/questions/64770581,13371696,Documentation Ambiguity
47981089,How to get TensorFlow source for canned models (Estimators),"We are using the canned dnn (tf.estimator.DNNClassifier and tf.estimator.DNNLinearCombinedClassifier)(and others) estimators in TensorFlow 1.4 but are interested in inspecting and perhaps reimplementing the models with variations in native TensorFlow, as well as for learning purposes. Is there any way to do this or is it available in the docs? Or is there a way to see it in the source?",https://stackoverflow.com/questions/47981089,137783,Documentation Replicability
66858658,"In Keras with Tensorflow, how can I reindex an axis of nd Tensor?","Like this, except without the errors: Closest I found is tf.gather, but I haven't been able to make it work.",https://stackoverflow.com/questions/66858658,2364295,Documentation Replicability
66874943,Why iterations over the same tf.data.Dataset give different data each iteration?,"I'm trying to understand how tf.data.Dataset works. It says on the documentation that take returns a dataset with a certain amount of elements from that dataset. You can then iterate over a single sample (in this case a batch): Outputs: However, iterating over it again, gives different labels: (continuation of last code) Outputs: Shouldn't the labels be the same, given that the dataset is the same?",https://stackoverflow.com/questions/66874943,2076973,Documentation Replicability
66879748,What is the difference between tf.keras.model and tf.keras.sequential?,"In some tf. keras tutorials, I've seen them instantiated their model class like this: model = tf.keras.Sequential() While in some places, they use something like this: model = tf.keras.Model(inputs=input, outputs=output) But seeing here in the docs, they do seem the same, but I am not sure nor is it explicitly mentioned. What are the differences between the two?",https://stackoverflow.com/questions/66879748,8648710,Documentation Ambiguity
66546321,Proper way to input a scalar into a Tensorflow 2 model,"In my Tensorflow 2 model, I want my batch size to be parametric, such that I can build tensors which have appropriate batch size dynamically. I have the following code: The code throws an error in tf.range: I think the problem is with the fact that tf.keras.Input automatically tries to expand the input array at the first dimension, since it expects the partial shape of the input without the batch size and will attach the batch size according to the shape of the input array, which in my case a scalar. I can just feed the scalar value as a constant integer into tf.range but this time, I won't be able to change it after the model graph has been compiled. Interestingly, I failed to find a proper way to input only a scalar into a TF-2 model even though I checked the documentation, too. So, what would be the best way to handle such a case?",https://stackoverflow.com/questions/66546321,1538049,Documentation Replication on Other Examples
66916390,Error when adapting batch size in tf.keras.utils.Sequence,"I study using tf.keras.utils.Sequence on Tensorflow 2.4.1. I used the example code in Sequence in API document (https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence) and finetuned by adding on_epoch_end function to adaptively change the batch_size value on every epoch. However, in practice, the number of steps per epoch, which expected to change depending on the number of batches, remains unchanged. In fact, Tensorflow returns a WARNING, informing that they run out of data, and stop the training immediately. This problem happens when the initialize batch_size is smaller than the current self.batch_size. Here is my guess, Tensorflow did adapt the batch size after every epoch, but somehow the model was still keeping the initial value. This problem never happened in Keras version 1. So far, I have no clue on solving this problem. Edit 1: The number of training data is much larger than the number of batches.",https://stackoverflow.com/questions/66916390,5169676,Documentation Replicability
66942311,"In Tensorflow, what does tf.GradientTape.gradients do when its ""target"" attribute is a multi-dimensional tensor?","In my model, I'm using tf.keras.losses.MSE to calculate the mean squared error of my BATCH_SIZE x 256 x 256 x 3 output and my BATCH_SIZE x 256 x 256 x 3 input. The output of this function appears to be (None,256,256). I then use tf.GradientTape.gradients, with the MSE output as the ""target"" attribute. In the documentation, it says that this attribute can be a tensor. My understanding is that loss is a scalar number which is differentiated against each of the weights during backpropagation. Therefore, my question is: What happens when a multi-dimensional tensor is passed into the gradients function? Is the sum of all elements in the tensor simple calculated? I ask this because my model is not training at the moment, with loss reading at 1.0 at every epoch. My assumption is that I am not calculating the gradients correctly, as all my gradients are reading as 0.0 for each weight.",https://stackoverflow.com/questions/66942311,15170857,Documentation Replicability
66570237,TensorFlow model to Keras functional API?,"I want to have this model as a functional model that uses Keras API, but not sure how. I want my model to be in the form of model = tf.keras.model.Model(....) so I can just evaluate or export the model by calling model. But I don't know how to do this with attention layers in the model. The Keras attention layer documentation stops at that very step and leave it to the user to figure it out. FYI, my model uses IMDB reviews for sentiment analysis.",https://stackoverflow.com/questions/66570237,12655251,Documentation Replicability
66968102,python type hint - can tensorflow data type be used?,Is it possible to use the Tensorflow data types tf.dtypes.DType such as tf.int32 in Python type hint? It causes the error below and wonder if this is possible.,https://stackoverflow.com/questions/66968102,4281353,Documentation Replicability
66997498,Get python string from Tensor without the numpy function,"I'm following the tutorial on https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/word_embeddings.ipynb TextVectorization by defalut splits on whitespace but I want to implement custom split. I want to keep punctuations (which I have implemented in custom_standardization), and split between words and punctuations. For instance, ""fn(1,2)=1+2=3"" needs to split to [""fn"",""("",""1"","","",""2"","")"",""="",""1"",""+"",""2"",""="",""3""]. I'm confident in such spliting given a standard Python string. However the input is tf.Tensor and following the aforementioned tutorial, input_data does not have numpy() function. What's the proper way to do such spliting? Is it possible to retrieve Python string from string Tensor?",https://stackoverflow.com/questions/66997498,746461,Documentation Replicability
66582647,Tensorflow Random segmentation faults,"I am trying to run the demo code from official tensorflow website I am attaching the full code (copied and arranged) here for ease Without any reason, this code enters Segmentation Fault in Tensorflow 2.3.1 right at the beginning Interestingly if I put some random print statements at the very start(those print(""1"") etc statements, the code will execute till the end and suffer segmentation fault at the end(redundant output not shown) Another observation is, if I uncomment the @tf.function on top of my trainStep and testStep functions, the code enters into segfault again but after it prints Start of epoch 0 Can someone explain what is going wrong with my Tensorflow package?",https://stackoverflow.com/questions/66582647,9598527,Documentation Ambiguity
48543654,Why should one use tf.train.Server to execute multiple tf.Session() in parallel?,"The official way to execute multiple tf.Session() in parallel is to use tf.train.Server as described in Distributed TensorFlow . On the other hand, the following works for Keras and can be modified to Tensorflow presumably without using tf.train.Server according to Keras + Tensorflow and Multiprocessing in Python. Is the first method faster than the second method? I have a code written in the second way, and due to the nature of my algorithm (AlphaZero) a single GPU is supposed to run many processes, each of which performs prediction of tiny minibatch.",https://stackoverflow.com/questions/48543654,2830610,Documentation Replicability
67698111,Show version of tensorflow 2.4.1 in python 3.7,"Using python 3.7, how do you print the version of tensorflow if you have tensorflow 2.4.1. None of the documented styles: 'tf. __ version__', tf.version, nor tf.version.VERSION seem to work.",https://stackoverflow.com/questions/67698111,13402234,Documentation Ambiguity
48675932,How to automatically add zero padding in a image to expected shape?,"I have an image with the shape of 200x250x3. I want to add the zero padding on top, left, right, bottom to the image to achieve a target shape of 256x256x3. How could I do it in tensorflow? I found the function tf.pad, but it needs to compute the padding size, while my task have to compute it automatically https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#pad",https://stackoverflow.com/questions/48675932,2938494,Documentation Replicability
67723809,Memory leak when using tf.data Datasets with shuffle,"I have a memory leak somehow when I create my tf.data.dataset pipeline, but I don't know where. My code works fine with ImageDataGenerator but is really slow. Reading a lot of documentation I thought it might be albumentations. However I now switched my transform to be entirely in tensorflow: And I made the shuffle buffer extremely small: Could autotune cause this? On Colab I usually hit the RAM restart at 500 batches I would like to use tf.data.datasets because it's really much faster if possible. Thank you for anyone who can point me to the flaw in my code, I have always used generators and only recently made the switch.",https://stackoverflow.com/questions/67723809,11106507,Documentation Replication on Other Examples
48697799,Tensorflow feature column for variable list of values,"From the TensorFlow docs it's clear how to use tf.feature_column.categorical_column_with_vocabulary_list to create a feature column which takes as input some string and outputs a one-hot vector. For example Let's say ""kitchenware"" maps to [1,0,0] and ""electronics"" maps to [0,1,0]. My question is related to having a list of strings as a feature. For example, if the feature value was [""kitchenware"",""electronics""] then the desired output would be [1,1,0]. The input list length is not fixed but the output dimension is. The use case is a straight bag-of-words type model (obviously with a much larger vocabulary list!). What is the correct way to implement this?",https://stackoverflow.com/questions/48697799,7978198,Documentation Replication on Other Examples
67740346,TimeDistributed layer to apply several convolutional layers error,"I have an issue with the tf.keras.layers.TimeDistributed layer (https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed). I am aware that TimeDistributed can be used to apply a single layer (dense, convolutional...) to a set of inputs, obtaining a set of outputs. Not only that, but until recently I was able to use it to apply an entire ""submodel"" to all the inputs. That is, a series of layers, not just one. An example of this is explained here by Patrice Ferlet (https://medium.com/smileinnovation/training-neural-network-with-image-sequence-an-example-with-video-as-input-c3407f7a0b0f). Using that source as example I can define a sequential ""submodel"" like this: And then include this submodel inside a superior model which calls TimeDistributed with the whole initial submodel (convnet). Now this works well, and I can get the model structure calling But if instead of this I define the convnet model using as backbone a predefined arquitecture from keras, like VGG16, there seems to be an error. (I also needed to change as well keras.Sequential by tf.keras.models.Sequential) When I run this after defining my VGG16-based architecture I get the following error: So now it appears like python is complaining that I am using an input for TimeDistributed that is not a single layer. This doesn't make any sense, since the initial example works well and also involves using several layers with TimeDistributed. Apart from that, the VGG16 model also worked fine some weeks ago. I am running all of this in Google CoLab. Could someone help me figure out what is going on here? Is this caused by the new tensorflow 2.5.0 version? Everywhere I look I see people using TimeDistributed to apply a single layer, but applying a whole sequential model worked just fine until now (despite no apparent mention in the documentation). Thank you!",https://stackoverflow.com/questions/67740346,16059221,Documentation Replication on Other Examples
67747314,Finding precision and recall for the tutorial federated learning model on MNIST,"I'm using this tutorial to try to learn how federated models work through TensorFlow's tutorial here: https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/federated_learning_for_image_classification.ipynb Currently, the model is defined like this which uses accuracy as its metric. I want to either use precision and recall as metrics, or find them after the model is trained, but I can't figure out how to do so. I tried adding precision to metrics like this metrics = [tf.keras.metrics.SparseTopKCategoricalAccuracy(), tf.keras.metrics.Precision()] and run this code but it gives me an error. Error output: Previously, I asked a similar question for a regular centralized model here, but I don't think I can use that same method since you can't get the results of the predictions back in the same way from what I've found. I've also tried looking at other documentation such as this, but it also uses accuracy as the metric, so that wasn't helpful. How can I get the precision and recall of this federated model?",https://stackoverflow.com/questions/67747314,8105367,Lack of Alternative Solutions/Documentation
67759756,"How do I use ""text_dataset_from_directory"" to do binary text classification from tf.dataset object?","Sorry, I am still relatively new to text classification and Tensorflow, so this may look like a very dumb question. I have the song lyrics of two different singers. What I am trying to achieve is to build a binary text classification model to predict whether a song fits to the style of singer A or singer B more. I have the training data (the lyrics text files) of both classes in sub-directories. The directory structure is similar to, And from what I read on the Tensorflow documentation, I could easily construct a dataset by using the text_dataset_from_directory method. So something like, However, I don't know how I could go on from there. I would suppose that the created tf.data.Dataset object would still need Tokenization in the texts component, and the tokenized text would then need padding and embedding before feeding it into a logistic model. But I don't know how to further process it in the tf.data.Dataset object. I saw Tensorflow's Text Embedding Tutorial, but don't really see how it could be changed to become a binary model.",https://stackoverflow.com/questions/67759756,10230473,Documentation Replicability
67760450,How to achieve Tensorflow Python model() (__call__) performance in C(++) API for small inputs?,"The problem I have a (very) small and fast model saved in the SavedModel format which I can load and run with the following code: The predict function runs in 0.05 seconds with a batch of 5 inputs (on a Nvidia GPU). If however I use model.predict_on_batch(inputs) or model.predict(inputs) the performance drops significantly to 0.65 - 0.80 seconds for a batch of 5. This is consistent with the documentation that states that using model() (__call__) is usually faster for smaller inputs. The problem I am having is the fact that I am trying to port my model to a C(++) program. And using TF_SessionRun() for the C api and model_bundle.GetSession()-&gt;Run() I am getting performance similar to ""slow"" Python inference methods. What I have tried Another (very) small model with small batch, same result. I tried disabling optimizations with tf.config.optimizer.set_experimental_options({'disable_meta_optimizer': False}) to make sure this does not negatively impact performance but this made things even slower. I also tried converting the SavedModel to a TensorRT SavedModel. This increases the performance of the model() (__call__) method even further but all the other methods stop working in Python and in the downloaded precompiled Tensorflow C GPU api (2.5.0) and the C++ API compiled with Tensorflow_CC I get an error about the operation not being found (TensorRT does not seem to work). All the performance numbers given were run after a few warmup runs. Performance measured both with Tensorflow profiler and Python's time.time I checked if model() (__call__) is working correctly by checking the output and it is. My question(s) Is there a way to get model() (__call__) performance with the Tensorflow C(++) API? The problem seems to be somewhere in Tensorflows optimization for larger batch sizes which decreases the performance of smaller batch sizes. Is there another API that allows faster inference on small batches out of the box (TensorRT C++ API?)?",https://stackoverflow.com/questions/67760450,16075630,Documentation Replication on Other Examples
48732627,How to do data argumentation with tensorflow dataset api?,"I want to use the tf.data.dataset to do image argumentation while training. The code is similar to official guideline as follow: The train_data and train_label here refer to two lists which contains image file paths and labels seperately. However, this transformation can only return the original image, I also need to argument the images(like flip and rotate). How can I continue to argument the images and return these images?",https://stackoverflow.com/questions/48732627,5321613,Documentation Replication on Other Examples
48736753,"How do you load ""any"" model from disk into a TensorFlow Estimator without having the model_fn source code?","In Keras you can load a model that you had previously trained by using: trained_keras_model = tf.keras.models.load_model(model_name) Is there any equivalent method for doing this using TensorFlow estimator API? According to the documentation, I have to use: trained_estimator = tf.estimator.Estimator (model_fn,model_dir) I want to get the trained estimator using just the files in the model directory. To be more clear my idea was to load ""any"" model from disk without having the model_fn source code. Is it possible to do it this way? This feature is implemented in Keras so I am at a loss to understand why Estimator API cannot do this.",https://stackoverflow.com/questions/48736753,7656080,Documentation Replicability
68422887,Slow tensorflow code; can I batch evaluate and obtain multiple loss scores?,"I recently switched from using the 'keras' package in Python to using 'tensorflow.keras', since this seems to be preferred now. The latest version of Keras was also giving me issues that seemed like I'd have to modify the internal Keras code to fix, whereas tf.keras works fine. However, upon making this switch, some of my code was slowed down by a factor of 30-40. I've identified the following calls to ""model.evaluate"" as a bottleneck, though I'm not sure why it's so much slower than before. The code is structured something like this: I'm thinking the major bottleneck is that I'm making a bunch of SMALL calls to tensorflow, rather than one LARGE call. Using a GPU actually makes it even slower, presumably due to all the loading/unloading. I'd like to just make a call like but model.evaluate() seems to always output a single scalar, when I need the whole list of 10000 loss scores. I have been unable to find a solution in the documentation, is there a builtin way to do sort of a ""batch evaluate"" but get individual loss scores out for each sample?",https://stackoverflow.com/questions/68422887,16470540,Documentation Replication on Other Examples
66711706,"Jax, jit and dynamic shapes: a regression from Tensorflow?","The documentation for JAX says, Now I am somewhat surprised because tensorflow has operations like tf.boolean_mask that does what JAX seems incapable of doing when compiled. EDIT The gradient passes through tf.boolean_mask (obviously not on mask values, which are discrete); case in point here using TF1-style graphs where values are unknown, so TF cannot rely on them:",https://stackoverflow.com/questions/66711706,9973879,Documentation Replication on Other Examples
49688134,TensorFlow session inside Keras custom loss function,"After going through some Stack questions and the Keras documentation, I manage to write some code trying to evaluate the gradient of the output of a neural network w.r.t its inputs, the purpose being a simple exercise of approximating a bivariate function (f(x,y) = x^2+y^2) using as loss the difference between analytical and automatic differentiation. Combining answers from two questions (Keras custom loss function: Accessing current input pattern and Getting gradient of model output w.r.t weights using Keras ), I came up with this: Which yields the error: TypeError: The value of a feed cannot be a tf.Tensor object. because of feed_dict={model.input:input_tensor}. I understand the error, I just don't know how to fix it. From what I gathered, I can't simply pass input data into the loss function, it must be a tensor. I realized Keras would 'understand' it when I call input_tensor. This all just leads me to think I'm doing things the wrong way, trying to evaluate the gradient like that. Would really appreciate some enlightenment.",https://stackoverflow.com/questions/49688134,1058309,Documentation Replicability
68431633,tf.image.stateless_random_crop VS. tf.image.random_crop. Shouldn't these be the same thing?,"In tf 2.5, there are two functions for cropping an image: tf.image.stateless_random_crop, and tf.image.random_crop. The documentation states that stateless_random_crop is deterministic (always returns the same crop given one seed). However, random_crop has a seed parameter and is also deterministic, one would think. What is the actual difference between these two functions? I cannot find information about statelessness in Tensorflow anywhere. The differences between tf.image.stateless_random_crop, and tf.image.random_crop are one line where stateless_random_uniform is used instead of a random_uniform: stateless_random_crop: https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L415-L465 random_crop: https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/random_ops.py#L360-L412 I always thought that random_crop would always return the same crop given a seed, but it looks like maybe that wasn't always true? Any enlightenment about statelessness in Tensorflow is greatly appreciated!",https://stackoverflow.com/questions/68431633,11632499,Documentation Ambiguity
68434740,"calculate the gradient wrt to multiple inputs using tf.gradienttape, but return none","pre-trained DNN model takes two inputs, and I want to compute gradient of output wrt two inputs grad [none,none] How to fix it? Thanks Solved Update use tf.Variable instead of tf.cast",https://stackoverflow.com/questions/68434740,16477354,Documentation Replicability
68439286,Tensorflow tf.gfile.GFile,I have tried changing tf.gfile.GFile to tf.io.gfile.GFile and also tried import tensorflow.compat.v1 as tf but nothing worked. It is not reading the newly saved file. I have saved it after changes.,https://stackoverflow.com/questions/68439286,15998918,Documentation Replicability
49701918,tf.layers.batch_normalization parameters,"I am not sure if it is only me who thinks that tensorflow documentation is a bit weak. I was planing to use the tf.nn.batch_normalization function to implement batch normalization but later recognized the tf.layers.batch_normalization function which seemingly should be the one to use for its simplicity. But the documentation is really poor if I may say it. I am trying to understand how to correctly use it but with the information provided on the Web page is it really not easy. I am hoping that maybe some other people have experience and help me (and possibly many others) to understand it.. Let me share the interface first: Q1) beta values are initialized to zero and gamma values are initialized to 1. But it does not say why. When batch normalization used, I understand that the ordinary bias parameter of the neural network becomes obsolete and beta parameter in the batch normalization step kind of does the same thing. From that angle, setting beta to zero is understandable. But why are gamma values initialized to 1? Is that really the most efficient way? Q2) I see a momentum parameter there as well. The documentation just says "" Momentum for the moving average."". I assume that this parameter is used when calculating the ""mean"" value for a certain mini batch in the corresponding hidden layer. With other words, the mean value used in batch normalization is NOT the mean of current mini batch, it is rather primarily the mean of the last 100 mini batches (since momentum = 0.99). But it is very unclear how this parameter affects the execution in testing, or if I am just validating my model on the dev set by calculating cost and accuracy. My assumption is that anytime I deal with test and dev sets, I set the parameter ""training"" to False so that momentum parameter becomes obsolete for that particular execution and the ""mean"" and ""variance"" values that were calculated during the training are used now instead of calculating new mean and variance values. It is how it should be if I am mistaken but I do not see anything in the documentation if it is the case. Could anyone confirm that my understanding correct? If not, I would really appreciate further explanation on this. Q3) I am having difficulties to give a meaning to the trainable parameter. I assume beta and gamma params are meant here. Why would they not be trainable? Q4) The ""reuse"" parameter. What is it really? Q5) adjustment parameter. Another mistery.. Q5) A kind of summary question.. Here is my overall assumption that needs confirmation and feedback.. Important params here are: - inputs - axis - momentum - center - scale - training And I assume that as long as the training=True when training, we are safe. And as long as training=False when validating dev set or test set or even when using the model in real life, we are safe too. Any feedback will really be appreciated. ADDENDUM: Confusion continues. Help! I am trying to use this function instead of implementing a batch normalizer manually. I have the following forward propagation function that loops through layers of the NN. The tf.layers.batch_normalization(..) function wants to have static dimensions but I do not have it in my case. Since I apply mini batches rather than training the entire train set each time before I run the optimizer, 1 dimension of the X appears to be unknown. If I write: I get: And when this is the case, when I run the whole program I get the following error below. I saw in some other threads that some people say that they could solve the problem by using tf.reshape function. I try it.. Forward prop goes fine but later on it crashes in the Adam Optimizer.. Here is what I get when I run the code above (without using tf.reshape): How do I solve this??? This is so hopeless.. ADDENDUM(2) I am adding more information: The following simply means that there are 5 units in input layer, 6 units in each hidden layer, and 2 units in output layer. Here is the updated version of forward prop function with tf.reshape When I do this, I can run the forward prop function. But it seems to be crashing in later execution. Here is the error that I get. (Note that I print out the shape of input X before and after reshaping in the forward prop function). Regarding the question why the shape of X is not static.. I don't know... HEre is how I setup the dataset. I have 2 csv files that include the train data. And I use the initializable iterator as following: During the training, I keep getting mini batches from the train set. Everything works fine when I disable batch normalization. If I enable batch norm, it requires static shape of the input X (mini batch). I reshape it but this time it crashes later in the execution as seen above. ADDENDUM(3) I guess I figured out where it crashes. It probably crashes when I run the optimizer after calculating the cost. First the sequence of commands: First forward prop, then compute cost, then run optimizer. First 2 seems to be working but not the optimizer. HEre is how I define the optimizer: I have the update_ops there to be able to update the moving averages. If I interpret it right, it is just crashing when it tries to update moving averages. I might be misinterpreting the error msg as well.. ADDENDUM(4) I tried to normalize based on the known dimension and it worked! But that's not the dimension I would like to normalize, which is now confusing. Let me elaborate: nr of units in input layer: 5 nr of units in layer 1 (first hidden layer): 6 so weight1 is (6, 5) matrix Assume that mini batch size is 7. Shape of A[0] (or X_mini_batch) in my case is: (7, 5), where 7 is the # training samples in mini batch, and 5 is the # units in input layer. When calculating Z[1]... Z[1] = weight1 * A[0].transpose ... then shape of Z[1] is (6, 7) matrix, where each column gives 6 features for each train sample. The question is then which column do we want to normalize in Z[1]? What makes sense to me is that you normalize each feature from all given train samples. This means that I need to normalize each row bcz I have different feature values for different train examples in each row. And since Z[1] has the shape (6, 7), if I set axis=0, it should refer to normalization in each row. And 7 is the unknown number in my case so it doesn't hurt. Based on this logic, it works! But I am totally puzzled if axis=0 really refers to each row here... Let me show another example about this axis issue, which has bothered me for a long time now.. The irrelevant from this topic code example: This gives the following output: When I set axis to 0, it is giving the average of each column. And if axis=1, it is giving the average of each row. (Note that cc.shape gives (2,3)) Now the million dollar question: In a 2 dimensional matrix, is axis 0 or 1 when I want to address each row? ADDENDUM(5) I guess I get it now correctly. Let me summarize my axis understanding here. Hopefully I am getting it right now... Here is the Z[1] matrix representation with the shape (6,7): t_ex : train example f: feature In this mini batch above, there are 7 train examples and each train ex has 6 features (since there are 6 units in layer 1). When we say ""tf.layers.batch_normalization(..,axis=0)"", we mean that the normalization has to be done per row for each feature to eliminate the high variance between - say - f1 values in the first row. With other words, we do NOT normalize f1,f2,f3,f4,f5,f6 with each other. We normalize f1:s with each other, and f2:s with each other, and so on..",https://stackoverflow.com/questions/49701918,9328846,Lack of Alternative Solutions/Documentation
49711324,tf.contrib.summary.generic or tf.summary.text in eager mode,It looks like only tf.contrib.summary.scalar is supported when using eager mode. Is there a workaround to use tf.contrib.summary.generic or tf.summary.text?,https://stackoverflow.com/questions/49711324,1375548,Documentation Replication on Other Examples
49745029,How to use Distributed Tensorflow on remote machines?,"I am trying to run a distributed Tensorflow script across three machines: my local machine running the parameter server and two remote machines I have access to running worker jobs. I am following this example from the Tensorflow documentation, passing the IP addresses and unique port numbers to each worker job, and setting the protocol option in tf.train.Server to 'grpc'. However, when I run the script, all three processes are started on my localhost, and none of the jobs are on the remote machines. Is there a step I am missing? My (abridged) code: This code causes two problems: From worker0: From worker1: How do I get the two worker jobs to run on the remote servers?",https://stackoverflow.com/questions/49745029,9621612,Documentation Replication on Other Examples
68869097,How to make Tensorflow checkpoints compatible between minor versions when using tf.keras.mixed_precision.experimental?,"I'm new to Tensorflow so I'm learning about model saving/loading schemes. I am working with a library which provides a model checkpoint (.index and .data files only) which was saved in Tensorflow 2.2. I would like to load this checkpoint using the same library but with Tensorflow 2.6 so that it may be run in Google Colab. When I load the checkpoint in Tensorflow 2.2 (with associated CUDA version), the model works fine. However, if I load the same checkpoint in Tensorflow 2.6 (with latest CUDA versions), the model runs without throwing any errors but the output is completely different. To note, the library does make use of some experimental API's which may be responsible for the version compatibility issues (as noted in the official guide here). In particular, it uses tensorflow.keras.mixed_precision.experimental extensively, as well as tf.config.experimental.set_memory_growth, tf.data.experimental.copy_to_device, and tf.data.experimental.AUTOTUNE. Intuitively, mixed_precision.experimental seems likely to significantly impact the weight saving/loading across minor versions. I do receive the following warnings. They disappear if I switch to using tf.keras.mixed_precision (no experimental), however the model output is still completely wrong. Any recommendations for saving a mixed_precision model checkpoint so that it is compatible across minor versions? Or dealing with experimental Tensorflow API's in general?",https://stackoverflow.com/questions/68869097,16580520,Documentation Replication on Other Examples
68878231,tf.gradients() vs tf.gradientTape.gradient() in graph mode,"I had a question regarding the behavior of tf.gradients() as opposed tf.gradientTape.gradient() in graph mode. Given a differentiable function y = f(x), where x and y are each single tensorflow tensors, is there any difference between the behavior of tf.gradient(y, x) vs tape.gradient(y, x) where tape is an instance of tf.gradientTape (assuming the use of graph mode) ? Not sure why tensorflow has two different gradient methods which can be used with graph mode - maybe there are some subtle differences in the implementations? I’ve looked at the documentation for gradientTape and tf.gradients but it’s not clear whether there is any difference between the behavior of these methods for a single (x, y) pair, or whether it’s just that tf.gradients() can be used in this case for a speedup when using graph mode. Thank you so much for your help!",https://stackoverflow.com/questions/68878231,16569549,Documentation Replication on Other Examples
50153291,Clip to maximum / minimum finite int/float/double?,"I'd like to clip some Tensorflow variables with tf.clip_by_value to the most positive / negative finite value possible. I'd like to reach a similar behavior like numpy.nan_to_num() How do I get these values, given some data type? Does this clipping affect the optimizer?",https://stackoverflow.com/questions/50153291,2219819,Documentation Replicability
50203668,Using tf.custom_gradient in tensorflow r1.8,"Hello, I'm trying to make a custom_gradient op using the function of tf.custom_gradient. I made my test code based on the API explanation online. However, it seems there is a problem in the custom_gradient function. Thanks!",https://stackoverflow.com/questions/50203668,9749663,Documentation Ambiguity
68939916,"TensorFlow text generation RNN example failing on TF 2.6, tf.sparse.to_dense(), Invalid argument: indices[1] = [0] is repeated","I am trying to run through the TensorFlow text generation RNN example, https://github.com/tensorflow/text/blob/master/docs/tutorials/text_generation.ipynb Running on a local Windows computer with TensorFlow 2.6 installed. I was able to run through and train the RNN model successfully. I was getting a ""Tensor' object has no attribute 'numpy"" error but added, and this resolved it. But now trying to test the model with some text I am getting the error, This occurs in tf.sparse.to_dense inside the OneStep function. I added some debug to print the ids_from_chars Also, I had this example running fine on my computer previously. Just had reinstalled TensorFlow and was trying the demo from scratch again. Any idea what is causing this error, or how to fix it?",https://stackoverflow.com/questions/68939916,416206,Documentation Replication on Other Examples
50222149,How to scan through tensor not at dimension 0?,"The tensorflow document states that tf.scan scans on the list of tensors unpacked from elems on dimension 0. The simplest version of scan repeatedly applies the callable fn to a sequence of elements from first to last. The elements are made of the tensors unpacked from elems on dimension 0. My question is: How to scan on the list of tensors on other dimension instead of dimension 0? For example, I have a tensor, ref, defined as below. I want to scan through the ref[1,0], ref[1,1], ref[1,2] and apply a function to each of the, ,say add 1. That is to say, I want ref be after the operation Can I use tf.scan to do that? If yes, how? If not, any how to do in other way? Thanks.",https://stackoverflow.com/questions/50222149,4987560,Documentation Replication on Other Examples
47898147,Tensorflow Module Import error: AttributeError: module 'tensorflow.python.ops.nn' has no attribute 'rnn_cell',"When attempting to pass my RNN call, I call tf.nn.rnn_cell and I receive the following error: Which is odd, because I'm sure I imported everything correctly: But looking at the docs, things have moved around between tensorflow versions. what would you all recommend to fix this?? Line, I'm getting the error against: Specifically: I'm using anaconda 3 to manage all of this so, the dependancies should all be taken care of. I have already tried working around a damn rank/shape error with Tensor shapes which took ages to resolve. Cheers in advance.",https://stackoverflow.com/questions/47898147,5552621,Documentation Replication on Other Examples
50243230,Unable to understand tf.nn.raw_rnn,"In the official documentation of tf.nn.raw_rnn we have emit structure as the third output of loop_fn when the loop_fn is run for the first time. Later on the emit_structure is used to copy tf.zeros_like(emit_structure) to the minibatch entries that are finished by emit = tf.where(finished, tf.zeros_like(emit_structure), emit). my lack of understanding or lousy documentation on google's part is: emit structure is None so tf.where(finished, tf.zeros_like(emit_structure), emit) is going to throw a ValueError as tf.zeros_like(None) does so. Can somebody please fill in what I am missing here?",https://stackoverflow.com/questions/50243230,6546694,Documentation Replication on Other Examples
50246535,Tensorflow estimator input function: defining each feature or not?,"With x is a 120 x 4 feature matrix of Iris data (4 features) and y is a label, I can make an input function for tf.estimator like below then define the feature column like below: But, I found in the internet (I forget the source, still searching) that I also can define the input function like below. The difference with previous method is all four features now defined with only one key, ""x"". then define the feature column like below: I've run both method and both give almost same result. My question: I can't find any documentation that explain the difference between both method, because at a glance dict_x have different shape. Are they still treated equally at input layer on neural networks? I'm new using tf.estimator, Thank You My estimator code if needed:",https://stackoverflow.com/questions/50246535,2147347,Lack of Alternative Solutions/Documentation
68984841,How can I understand the kernel of tf.keras.layers.Dense for rank >2?,"How can I understand the kernel of tf.keras.layers.Dense for rank &gt;2? The official API doc states that: My understanding is that for a rank larger than 2 (for example rank 3) only one kernel is created and thus the same kernel is applied on all slices of the second dimension, like above. That would consequently mean that the outputs for different indices of the second dimension are not independent of each other (especially during training). Is my understanding correct? And if yes, is there a simple way to use a stack of kernels instead or do I have to implement the tensor multiplication?",https://stackoverflow.com/questions/68984841,16787662,Documentation Replicability
50252720,How to combine prefetch_to_device with make_initializer,"Tensorflow 1.8 has introduced tf.contrib.data.prefetch_to_device, which can be used with tf.data.Dataset.apply, according to the official documentation. I have written some custom data loading, for which I would like to utilize this to prefetch to GPU. The problem is that this operation needs to be the last one in all transformations of the dataset. I am using tf.data.Iterator.make_initializer and tf.data.Iterator.from_structure as follows. This is a minimal example that raises the error. Of course, this does not happen when I turn the order around, but then the prefetching transformation is not taken into account for the initializer. How can I get this to work to create the initializer after telling it to prefetch? Or is that simply not possible (currently)? I would like to avoid having to create a new Iterator, unlike this test case from tensorflow.",https://stackoverflow.com/questions/50252720,3344139,Documentation Replication on Other Examples
67049850,Tensorflow tf.switch_case not working with keras Input,I want to use tf.switch_case to be able to redirect learning flow for different branchs of the network using inputs however tf.switch_case does not work with Keras.Tensor ...,https://stackoverflow.com/questions/67049850,7182711,Documentation Ambiguity
50263822,Which one to use tf.layers or tf.keras.layers for high level OO code in tensorflow?,Tensorflow have seemingly similar packages: tf.layers and tf.keras.layers. Which one is preferrable? It seems that tf.keras.layers extends tf.layers.,https://stackoverflow.com/questions/50263822,250560,Documentation Replicability
48033687,How to use tf.train.shuffle_batch to train NN?,"I've trained my neural built with tensorflow network and got some overfit I'd like to reduce. I hoped learning the model on batches could help ad I tried to test this idea. I found tf.train.shuffle_batch() and fought this may do the thing. So I tried and it didn't work. Tensorflow's documentation doesn't help. I've found one topic, but the example there only prints arrays out. It was promising to use it to learn NN but in my case istead of getting data divided to n-element batches I got them multiplied n-times in additional dimension. Here is the code sample: and here is the output: And the error list concludes with: The conlusion is not surprising when I fed the NN with 3D array but why do I get such a batch when I expect x:(15, 60) and y:(15, 1)? Why do I get x:(15, 165, 60) y:(15, 165, 1) and how to get useful batches? I'm using tensorflow-gpu but hope this should work as well, right?",https://stackoverflow.com/questions/48033687,8214796,Documentation Replication on Other Examples
67066760,Configuring labels in TensorFlow BinaryCrossentropy loss function,"I want to compute cross-entropy loss using tf.keras.losses.BinaryCrossentropy. The documentation has the following example, and specifies that true labels and predicted labels should have the shape [batch_size]: From the example, it is inferred that each sample's label should be formatted as [probability of belonging to Class 0, probability of belonging to Class 1]. Is it correct? If it is, why y_true[1] probabilities do not add up to 1?",https://stackoverflow.com/questions/67066760,5425172,Documentation Ambiguity
48048297,Explicit CPU placement in TensorFlow,"I found there are a piece of code in official model sample which confused me. Why using tf.device(""/cpu:0"") here? Except the case GPU memory leak, is there any other situation which we need to designate CPU operations explicitly?",https://stackoverflow.com/questions/48048297,3134227,Documentation Replication on Other Examples
50500579,what is the difference between tf.nn.max_pool() and tf.layers.max_pooling2d(),"I am a beginner of tensorflow,i have met two methods about create max_pool layer these days. one is ""tf.nn.max_pool()"" and the other is ""tf.layers.max_pooling2d()"".i want to learn about its difference,and when to use them suitably.based on this ,i have read its official document and searched in google,it is not any help at all.i have searched it in stackoverflow , there is a similar answer(tf.nn.conv2d vs tf.layers.conv2d）but it didn't solve my problem.does any one can help me? thanks in advance.",https://stackoverflow.com/questions/50500579,7955072,Inadequate Examples
48065164,TensorFlow weighted_cross_entropy_with_logits produces wrong result,"I am trying to use tf.nn.weighted_cross_entropy_with_logits API, but I found I just can not get the right result when the weight is not 1.0 (1.0 means no weight). running result:",https://stackoverflow.com/questions/48065164,5532550,Documentation Replicability
48072635,Why and when does the tensor's shape information unspecific?,"I found piece of code like this: as Clarification on tf.Tensor.set_shape() said, set_shape can make the shape information more specific. But why data's shape information here is not specific? When does the tensor's information unspecific?",https://stackoverflow.com/questions/48072635,3134227,Documentation Replicability
67101417,Using saved models from TF hub to extract feature vectors,"I've been playing with different models from TF hub to extract feture vectors: What i don't quite understand is how input image should be preprocessed. Every model from the hub has this generic instruction: and this is indeed what i see quite often online: BUT, color values are expected to be in range [0,1], in this case colors are in range [0,255] and should be scaled down: Is it just a common mistake or is the TF documentation is not up to date? I was playing with models from tf.keras.applications and reading source code in github. I noticed in some of the models (EfficientNet) first layer is: but in some models there is no such layer, instead and an utility function scales colors to [0,1] range, for example tf.keras.applications.mobilenet.preprocess_input. So, how important for TF hub saved models image colors to be in [0,1] range?",https://stackoverflow.com/questions/67101417,15641209,Documentation Replication on Other Examples
48101576,TensorFlow - Read video frames from TFRecords file,"TLDR; my question is on how to load compressed video frames from TFRecords. I am setting up a data pipeline for training deep learning models on a large video dataset (Kinetics). For this I am using TensorFlow, more specifically the tf.data.Dataset and TFRecordDataset structures. As the dataset contains ~300k videos of 10 seconds, there is a large amount of data to deal with. During training, I want to randomly sample 64 consecutive frames from a video, therefore fast random sampling is important. For achieving this there are a number of data loading scenarios possible during training: I have decided to go for option (3) and use TFRecord files to store a preprocessed version of the dataset. However, this is also not as straightforward as it seems, for example: I have wrote the following code to preprocess the video dataset and write the video frames as TFRecord files (each of ~5GB in size): This works fine; the dataset is nicely written as TFRecord files with the frames as compressed JPG bytes. My question regards, how to read the TFRecord files during training, randomly sample 64 frames from a video and decode the JPG images. According to TensorFlow's documentation on tf.Data we need to do something like: There are many example on how to do this with images, and that is quite straightforward. However, for video and random sampling of frames I am stuck. The tf.train.Features object stores the frames as frame/00001, frame/000002 etc. My first question is how to randomly sample a set of consecutive frames from this inside the dataset.map() function? Considerations are that each frame has a variable number of bytes due to JPG compression and need to be decoded using tf.image.decode_jpeg. Any help how to best setup reading video sampels from TFRecord files would be appreciated!",https://stackoverflow.com/questions/48101576,3419427,Documentation Replicability
67811443,Argmax on tf.sparse.SparseTensor,"Given a Sparse Tensor x of shape [B, d, d', v], how can I calculate tf.argmax(x, axis=2) while keeping sparsity? Thanks in advance!",https://stackoverflow.com/questions/67811443,3162981,Documentation Replicability
49765437,Tensorflow Type Error,"Getting strange type error with TensorFlow code: Getting the error for line if not tf.equal(curMaxAbs,tf.maximum(curMaxAbs, maxG)) How in the world could curMaxAbs have type int32, when it is only reassigned with the curMaxAbs = tf.maximum(curMaxAbs, maxG) statement, and the tf.maximum function returns the same type as the first argument? (per https://www.tensorflow.org/api_docs/python/tf/maximum).",https://stackoverflow.com/questions/49765437,3661120,Documentation Replication on Other Examples
49768997,Writing TFRecords in batches,"All documentations I found regarding TFRecords are generating tf.train.Example()s one by one, and writing them using Since I'm dealing with very big data, I know that I'll pay a high overhead price for writing examples separately Is there any way to write multiple tf.train.Example() to a TFRecord at once?",https://stackoverflow.com/questions/49768997,5368083,Documentation Replication on Other Examples
49808208,"What's the difference between tf.constant([1,2,3]) and tf.constant([[1,2,3]])","Is tf.constant([1,2,3]) creating a scalar and tf.constant([[1,2,3]]) creating an array?",https://stackoverflow.com/questions/49808208,9639109,Documentation Replicability
68550779,Slicing a 2D tensor similar to numpy np.ix_,"I have learned how to slice a tensor on one dimension here. I have learned how to slice a 2D tensor giving a 1D tensor of specific values here. Both use tf.gather() but I'm pretty sure I need tf.gather_nd() though I'm obviously using it wrong. In numpy, I have a 5x5 2D array, and I can slice a 2x2 array by using np.ix_() with row and column indices (I always need the same indices for rows and columns, resulting in a squared matrix): Reading over the tf.gather_nd() docs I assumed this is the way to do it in TF, but I'm using it wrong: I would have to do something like: Which leads me down another rabbit hole I'm not keen on. My indices vector is a lot longer of course. My indices, BTW, are 1D integer tensors themselves. So bottom-line I want to slice a with the same indices for rows and columns as I do with np._ix(), and my indices are something like:",https://stackoverflow.com/questions/68550779,4095235,Documentation Replication on Other Examples
49845685,Tensorflow: NaN for custom softmax,"Simply exchanging the nn.softmax function for a combination which uses tf.exp, keeping everything else like it was, causes not only the gradients to contain NaN but also the intermediate variable s. I have no idea why this is.",https://stackoverflow.com/questions/49845685,4308982,Documentation Replicability
68596492,Cannot avoid tensorflow function retracing,"I implemented my custom layer to perform resizing of images with padding. I just want to resize my images (proportionally) that's why I implemented custom layer instead of using tf.image.resize(). And I want to do this using special preprocessing layer instead of doing it manually in Python. Then I can save and load the model that takes images of different sizes. So it will be very portable and so on. But when I use the predict() method to test the model, I get the following red warnings: WARNING:tensorflow:6 out of the last 12 calls to &lt;function _make_execution_function..distributed_function at 0x0000026D2E2C53A8&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details. Here is my code I tried to disable eager execution, use tf.function with different arguments, but nothing helps. Maybe it's just unavoidable when you have inputs of variable length? And maybe I should not be worried about this, because it's normal. You should rebuild the graph if you have new shape. Of course, it takes time, but preprocessing images in Python function before feeding to the model also takes time. So maybe it doesn't matter. What do you think?",https://stackoverflow.com/questions/68596492,,Documentation Replication on Other Examples
50276275,KeyError: u'NearestNeighbors' on loading saved model from tf.contrib.factorization.KMeansClustering,"I am trying to do the following: I am using the exact script at: https://www.tensorflow.org/api_docs/python/tf/contrib/factorization/KMeansClustering I am using following code for saving the model: Input Reciever: Export: To import I use: On last step I run into this issue: Traceback (most recent call last): File ""restore_model.py"", line 6, in &lt;module&gt; tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], ""/Users/z001t3k/work/codebase/ContentPipeline/cep-scripts/cep/datacollection/algorithms/cluster_model/1525963476"") File ""/Users/z001t3k/python_virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 219, in load saver = tf_saver.import_meta_graph(meta_graph_def_to_load, **saver_kwargs) File ""/Users/z001t3k/python_virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1955, in import_meta_graph **kwargs) File ""/Users/z001t3k/python_virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py"", line 743, in import_scoped_meta_graph producer_op_list=producer_op_list) File ""/Users/z001t3k/python_virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py"", line 432, in new_func return func(*args, **kwargs) File ""/Users/z001t3k/python_virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 460, in import_graph_def _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def) File ""/Users/z001t3k/python_virtualenvs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/importer.py"", line 227, in _RemoveDefaultAttrs op_def = op_dict[node.op] KeyError: u'NearestNeighbors'",https://stackoverflow.com/questions/50276275,781469,Documentation Replicability
45774938,tensorflow: tf.split is given weird parameters,"Here is code(from here): I have issue understanding x = tf.split(0, n_chunks, x) , more specificaly third parameter(x-input). By documenation this should be axis...but that can't be, right? Isn't x one dimensional? I apologise if it's trivial, I'm beginner and can't sem to get it. Maybe it's just formality, but if it is I don't understand how it works...",https://stackoverflow.com/questions/45774938,8081595,Documentation Ambiguity
45784815,How best to implement a matrix mask operation in tensorflow?,"I had a case where I needed to fill some holes (missing data) in an image processing application in tensorflow. The 'holes' are easy to locate as they are zeros and the good data is not zeros. I wanted to fill the holes with random data. This is quite easy to do using python numpy but doing it in tensorflow requires some work. I came up with a solution and wanted to see if there is a better or more efficient way to do the same thing. I understand that tensorflow does not yet support the more advanced numpy type indexing yet but there is a function tf.gather_nd() that seems promising for this. However, I could not tell from the documentation how to us it for what I wanted to do. I would appreciate answers that improve on what I did or especially if someone can show me how to do it using tf.gather_nd(). Also, tf.boolean_mask() does not work for what I am trying to do because it does not allow you to use the output as an index. In python what I am trying to do: What I ended up doing in Tensorflow to achieve same thing (skipping filling the array steps)",https://stackoverflow.com/questions/45784815,7447161,Inadequate Examples
64780641,Whats the equivalent of tf.keras.Input() in pytorch?,"can someone tell me what the equivalent of tf.keras.Input() in pytorch is? At the documentation it says, ""Initiates a Keras Tensor"", so does it just creates a new empty tensor? Thanks",https://stackoverflow.com/questions/64780641,8677804,Lack of Alternative Solutions/Documentation
65099915,how do i convert a tf.Variable to numpy?,How could I convert a tf.Variable to a numpy array? I want to get [4.0],https://stackoverflow.com/questions/65099915,11069811,Documentation Replicability
46298583,Tensorflow embeddings,"I know what embeddings are and how they are trained. Precisely, while referring to the tensorflow's documentation, I came across two different articles. I wish to know what exactly is the difference between them. link 1: Tensorflow | Vector Representations of words In the first tutorial, they have explicitly trained embeddings on a specific dataset. There is a distinct session run to train those embeddings. I can then later on save the learnt embeddings as a numpy object and use the tf.nn.embedding_lookup() function while training an LSTM network. link 2: Tensorflow | Embeddings In this second article however, I couldn't understand what is happening. This is given under the training embeddings sections. My doubt is: does the gather function train the embeddings automatically? I am not sure since this op ran very fast on my pc. Generally: What is the right way to convert words into vectors (link1 or link2) in tensorflow for training a seq2seq model? Also, how to train the embeddings for a seq2seq dataset, since the data is in the form of separate sequences for my task unlike (a continuous sequence of words refer: link 1 dataset)",https://stackoverflow.com/questions/46298583,4341842,Documentation Replication on Other Examples
65157852,How to mix tensorflow keras model and transformers,"I am trying to import a pretrained model from Huggingface's transformers library and extend it with a few layers for classification using tensorflow keras. When I directly use transformers model (Method 1), the model trains well and reaches a validation accuracy of 0.93 after 1 epoch. However, when trying to use the model as a layer within a tf.keras model (Method 2), the model can't get above 0.32 accuracy. As far as I can tell based on the documentation, the two approaches should be equivalent. My goal is to get Method 2 working so that I can add more layers to it instead of directly using the logits produced by Huggingface's classifier head but I'm stuck at this stage. Method 1: Method 2: Rest of the code for either method is identical, I am using Tensorflow 2.3.0 and have tried with transformers versions 3.5.0 and 4.0.0.",https://stackoverflow.com/questions/65157852,8840524,Documentation Replication on Other Examples
65436819,Keras: How to use `image_dataset_from_directory` to load test set?,"I am using tf.keras.preprocessing.image_dataset_from_directory to load dataset as follows, However, when I check the document looks like this argument labels seem to be a must-have one, but my test data has no labels, so how can I load test data? Is there a convenient and unified way to do this?",https://stackoverflow.com/questions/65436819,9444831,Documentation Replicability
65437493,convert string to float array in csv using tf.data,I have a csv like this : I want to convert column text_weight to tf.data . But I find nothing about it in tensorflow document website .,https://stackoverflow.com/questions/65437493,6862189,Lack of Alternative Solutions/Documentation
65464181,An alternative to tf.distribute.cluster_resolver.TPUClusterResolver( tpu_name) to be used in Sagemaker?,"According to the tensorflow documentation in tf.distribute.cluster_resolver.TPUClusterResolver, it says that this resolver works only on Google Cloud platform. But from this issue in github, I found out that a similar code also works in Azure. Is there a way I can bypass this resolver and initialize my tpu in sagemaker ? Even better, if I can find a way to insert the name or url of sagemaker gpu to the resolver and initiate it from there ?",https://stackoverflow.com/questions/65464181,9279666,Documentation Replicability
65481591,Keras Generator to tf.data.Dataset,"I am working with the Mask RCNN keras implementation but the data generator hard locks on my systems when using use_multiprocessing=True. The data generator runs fine in single thread. I am trying to convert the data generator to a tf.data.Dataset as recommended by tensorflow. I have no idea how to do this and have been unable to find any documentation on this. Mask RCNN data generator: I have tried to use the tf.data.Dataset.from_generator() however it requires the output_types= argument and the Mask RCNN outputs a number of lists, I can not figure out how to define output_types=. I am using python3.7, keras==2.2.5, tensorflow==2.2.0",https://stackoverflow.com/questions/65481591,2600161,Documentation Replication on Other Examples
46647805,building a tf.estimator input_fn: feature is not in features dictionary,"I have a corpus of records that represent matchups in a video game. I want to feed this to a tf.estimator.DNNClassifier. The records contain text representations of the 5 heroes on team 0 and the 5 heroes on team 1, the map the game was played on, and the winner of the game. I want to represent these three features as three sparse vectors. I'm not using pandas or numpy right now. I would prefer to keep it as simple as possible for the time being, until I can elaborate my tf knowledge. (But no simpler!). Perhaps the best way to ask the question is to show what I have and ask for help filling in the blank, at make_input_fn I've been all over the docs today, but all of the code examples I can find show how to feed pandas and numpy data structures into input_fn, and obsure the underlying mechanics of the process by calling out to helper functions that don't work for me. (e.g., https://www.tensorflow.org/get_started/input_fn and https://www.tensorflow.org/tutorials/wide) tf version 1.4.0-dev20171008 When I run I get this stack trace. I think it doesn't like the return value from _fn. But that dictionary does have the names of the features that I gave to the model AFAICT.",https://stackoverflow.com/questions/46647805,86432,Documentation Replicability
46650905,Got stuck when using hdfs for logdirs of MonitoredTrainingSession of distributed tensorflow,"I am using between-graph replica with hdfs, but my program got stuck when the parameter of MonitoredTrainingSession-logdirs is set to hdfs://default/mypath4train_logs. The code is like: I have read the documentation on tensorflow guide how to run TensorFlow on hadoop, and done whatever it says, in which, most importantly, the paths to libjvm.so and libhdfs.so have been appended to LD_LIBRARY_PATH. However, the chief worker's log stopped at: I debugged it, it seems that the program got stuck on tensorflow::FileSystem::RecursivelyCreateDir, yet without any error. Here's exactly how I debugged it: Notice that the last command of gdb has never returned. I have also found these tow similar questions yet are either unsolved or unhelpful for my problem: Distributed Tensorflow 1.0 Supervisor stuck if logdir is in HDFS How to use hdfs directory path in tf.train.MonitoredTrainingSession API for writing logs and checkpoints",https://stackoverflow.com/questions/46650905,8746240,Documentation Replicability
65835387,ValueError: too many values to unpack (expected 2) when using tf.keras.preprocessing.image_dataset_from_directory,"I want to create a dataset-variable as well as a labels-variable using the function tf.keras.preprocessing.image_dataset_from_directory (https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory). The documentation states: My code is the following: I expect to get a tuple as return values, but instead I get the error message: When I save the output in one variable (just train_ds) and I inspect the variable, I get the following output: How can I access the two tuples inside seperatly?",https://stackoverflow.com/questions/65835387,12336925,Documentation Ambiguity
65863738,Changing order of Input Image in 3D convolutions,"According to the official documentation of tf.keras.layers.Conv3D . Now the whole idea around channels and batch shape makes sense, but will changing the general order of (conv_dim1, conv_dim2,conv_dim2) as (x,y,z) to say (z,x,y) affect the performance. Does Conv3D worry about order of x-y-z dimension ? I was training a U-net segmentation model and upon changing the order of axis I saw difference in performance. (x,y,z) order converges faster as compared to (y,x,z). I just wanted to make sure what's the correct way..",https://stackoverflow.com/questions/65863738,5066344,Documentation Ambiguity
66231467,How to set a minimum number of epoch in Optuna SuccessiveHalvingPruner()?,"I'm using Optuna 2.5 to optimize a couple of hyperparameters on a tf.keras CNN model. I want to use pruning so that the optimization skips the less promising corners of the hyperparameters space. I'm using something like this: Sometimes the model stops after 2 epochs, some other times it stops after 12 epochs, 48 and so forth. What I want is to ensure that the model always trains at least 30 epochs before being pruned. I guess that the parameter min_early_stopping_rate might have some control on this but I've tried to change it from 0 to 30 and then the models never get pruned. Can someone explain me a bit better than the Optuna documentation, what these parameters in the SuccessiveHalvingPruner() really do (specially min_early_stopping_rate)? Thanks",https://stackoverflow.com/questions/66231467,11433296,Requesting (Additional) Documentation/Examples
47380573,How to properly update variables in a while loop in TensorFlow?,"Can someone please explain (or point me to the relevant place in the documentation that I've missed) how to properly update a tf.Variable() in a tf.while_loop? I am trying to update variables in the loop that will store some information until the next iteration of the loop using the assign() method. However, this isn't doing anything. As the values of mu_tf and sigma_tf are being updated by the minimizer, while step_mu isn't, I am obviously doing something wrong, but I don't understand what it is. Specifically, I guess I should say that I know assign() does not do anything until it is executed when the graph is run, so I know that I can do sess.run(step_mu.assign(mu_tf)) and that will update step_mu, but I want to do this in the loop correctly. I don't understand how to add an assign operation to the body of the loop. A simplified working example of what I'm doing follows here:",https://stackoverflow.com/questions/47380573,8931942,Documentation Replicability
47389988,How to control GPU memory size with tf.estimator,I'm trying to control the size of GPU memory allocated for one tensorflow estimator tf.estimator.Estimator. The purpose is to only allocate half to run other tensorflow net on the same GPU. I found for the contrib version but not for the official. Someone knows if it's possible?,https://stackoverflow.com/questions/47389988,7007134,Documentation Replication on Other Examples
66287320,Tensorflow 2.0: flat_map() to flatten Dataset of Dataset returns cardinality -2,"I am trying to run the following code (as given in Tensorflow documentation) to create windows of my data and then flatten the dataset of datasets. The problem is that flat_windows.cardinality().numpy() returns cardinality to be -2 which is creating problem for me during training. I tried looking for ways to set_cardinality of a dataset but couldn't find anything. I also tried other ways of flattening a dataset of datasets, but again no success. Edit-1: The problem with the training is that the shape is unknown (at Linear and Dense layers) when I am training a subclass model (given below). The model trains well when I train the model eagerly (through tf.config.run_functions_eagerly(True)) but that is slow. Therefore I want the input data to be known for the model training.",https://stackoverflow.com/questions/66287320,1434693,Documentation Replication on Other Examples
66623244,Output probability of prediction in tensorflow.js,"I have a model.json generated from tensorflow via tensorflow.js coverter In the original implementation of model in tensorflow in python, it is built like this: In tensorflow, the probability can be generated by score = tf.nn.softmax(predictions[0]), according to the tutorial on official website. How do I get this probability in tensorflow.js? I have copied the codes template as below: How should I modify the above code? The output is just the same as the value of variable 'predictions': Please help!!! Thanks!",https://stackoverflow.com/questions/66623244,15365699,Documentation Replicability
47768298,How to understand using tf.cond with tf.Print?,"Look at the code: I'm very puzzled, the output of tf.Print is 3.0 As we know, tf.Print(z, [z]) will output the value of z only when z is evaluated, but I don't think I have evaluated z. Another question is about tf.cond, how does it add node to graph, for example how does add tf.Print to graph, I think it should relate some tensor with the return of tf.Print, otherwise tf.Print won't be executed. I'm so puzzled.",https://stackoverflow.com/questions/47768298,8165066,Documentation Replicability
66772069,Eager execution disabled in Keras model on `predict_step`,Why does tensorflow disable eager execution inside the predict_step function of a tf.keras.Model? Maybe I am getting wrong something but here's an example: Is this the intended behavior? Is there a way to force the predict_step to run in eager mode?,https://stackoverflow.com/questions/66772069,13637002,Documentation Ambiguity
52572275,tensorflow: how to interleave columns of two tensors (e.g. using tf.scatter_nd)?,"I've read the tf.scatter_nd documentation and run the example code for 1D and 3D tensors... and now I'm trying to do it for a 2D tensor. I want to 'interleave' the columns of two tensors. For 1D tensors, one can do this via (aside: is there a better way to do this? I'm all ears.) This gives the output [ 1 2 10 3 4 11 5 6 12] Yay! that worked! Now lets' try to extend this to 2D. This gives the error ValueError: The outer 1 dimensions of indices.shape=[6,1] must match the outer 1 dimensions of updates.shape=[3,6]: Dimension 0 in both shapes must be equal, but are 6 and 3. Shapes are [6] and [3]. for 'ScatterNd_2' (op: 'ScatterNd') with input shapes: [6,1], [3,6], [2]. Seems like my indices is specifying row indices instead of column indices, and given the way that arrays are ""connected"" in numpy and tensorflow (i.e. row-major order), does that mean I need to explicitly specify every single pair of indices for every element in updates1? Or is there some kind of 'wildcard' specification I can use for the rows? (Note indices1 = tf.constant([[:,0], [:,1], [:,3], [:,4], [:,6], [:,7]]) gives syntax errors, as it probably should.) Would it be easier to just do a transpose, interleave the rows, then transpose back? Because I tried that... ...and got a much longer error message, that I don't feel like posting unless someone requests it. PS- I searched to make sure this isn't a duplicate -- I find it hard to imagine that someone else hasn't asked this before -- but turned up nothing.",https://stackoverflow.com/questions/52572275,4259243,Documentation Replication on Other Examples
70686521,"Slice of 20 elements of rank1 tensor then reshaping throws ""Input to reshape is tensor with 10272 values, but requested shape requires multiple of 20""","My input tensor Data = Input(shape=(856,)) is a vector of float32 values concatenated from many different devices. I am trying to apply different TensorFlow functions to different subslices of each input chunk. Some of these functions include a 1D Convolution which requires a reshape. Doing this crashes after trying to fit my model. It throws the following errors: I am not sure how slicing 20 elements from a tensor of 856 could result in a tensor of 10272 values. I have also tried using the tf.slice function a couple of different ways; both fail. Referencing the docs: https://www.tensorflow.org/guide/tensor_slicing And fails, stating: For reference, here is what some of the values look like in the input data",https://stackoverflow.com/questions/70686521,11590962,Documentation Replicability
70686756,Tensorflow tf.data.Dataset.cache seems do not take the expected effect,"I am trying to improve my model training performance following the Better performance with the tf.data API guideline. However, I have observed that the performance using .cache() is almost the same or even worse if compared to same settings without .cache(). The data in datafile_list hold 9.92GB which fairly fits the system total physical RAM available (100GB). System swap is disabled. By training the model using the dataset: results in: This result is frustrating. By the docs: And from this guide: However, the elapsed time took by all epochs are almost the same. In addition, during the training both CPU and GPU usage are very low (see images below). By commenting out the line raw_dataset = raw_dataset.cache() the results do not show any notable difference: As pointed out in the docs, my expectations were using cache would result in a much fast training time. I would like to know what I am doing wrong. Attachments GPU usage during training using cache: GPU usage during training WITHOUT cache: System Stats (Memory, CPU etc) during training using cache: System Stats (Memory, CPU etc) during training WITHOUT cache:",https://stackoverflow.com/questions/70686756,3055724,Documentation Replication on Other Examples
64847875,"tf.keras.Model save: ""AssertionError: Tried to export a function which references untracked object Tensor""","I'm running this Tensorflow NMT tutorial: https://github.com/tensorflow/addons/blob/master/docs/tutorials/networks_seq2seq_nmt.ipynb When I try to save the decoder: decoder.save('decoder'), I get: AssertionError: Tried to export a function which references untracked object Tensor(""LuongAttention/memory_layer/Tensordot:0"", shape=(1024, 23, 256), dtype=float32).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly. I also tried registering the LuongAttention object like:",https://stackoverflow.com/questions/64847875,14643783,Documentation Replicability
52582188,How can I choose one of three data input pipelines in the graph runtime in the Tensorflow?,"I build three data input pipelines when I construct the graph. I want to choose one of them in the graph runtime according to the global_step like this: But in the tensorflow, there variables like global_step are tensors, they should be operated by tf functions, not by python. I have tried to use tf.cond, but it only can solve the problem of two options. In this situation, there are three options. I do not know how I can solve it. Thanks for your help in advance.",https://stackoverflow.com/questions/52582188,10410925,Documentation Replicability
52589043,How to read TFRecord files in TensorFlow from a remote HDFS file system in parallel and efficiently,"I am reading TFRecords data files from a remote HDFS file system through tf.data.TFRecordDataset API in TensorFlow. However, when I increase the num_parallel_reads, say from 1 to 32, not only I don't see improvement in reading speed but I see it takes longer to read the data. Are there any tips to set num_parallel_reads properly to enhance the performance? I know tf.data.TFRecordDataset uses parallel interleaves which is supposed to be good.",https://stackoverflow.com/questions/52589043,4844279,Documentation Replication on Other Examples
52591291,Tensorflow Estimator: loss not decreasing when using tf.feature_column.embedding_column for a list of categorical variables,"I'm very new to Tensorflow Estimator. I wonder if it's possible to pass an array of categorical variables as feature to the estimator and it automatically converts it to an array of embeddings. For example, the following is a record in a CSV file. It contains 2 lists of categorical variables(enclosed in brackets), ""country"" and ""watch"", 2 categorical variables, ""day_of_week"" and ""day_period"" and one target, ""movie_id"" in this case. According to the doc https://www.tensorflow.org/api_docs/python/tf/feature_column, ""day_of_week"" and ""day_period"" can be represented as ""categorical_column_with_vocabulary_list"". This is straightforward. However, ""country"", and ""watched"" are a list of categorical variables. I'd like to merge each categorical variable in a list to an embedding. From the same doc, a ""tf.feature_column.embedding_column"" would do the trick. The following function builds columns representing the above input. The following is a function generating training dataset The following is a code snippet to create and train an estimator. I got no error and it seems like everything should work as expected but the training loss is so huge and fluctuating around 3,xxx and never decreasing. See below I wonder if I've done something wrong when preparing the training data? thanks Peeranat F.",https://stackoverflow.com/questions/52591291,3079777,Documentation Replicability
52597523,How to load_weights to a Keras model from a Tensorflow checkpoint,"I have some python code to train a network using Tensorflow's TFRecords and Dataset APIs. I have built the network using tf.Keras.layers, this being arguably the easiest and fastest way. The handy function model_to_estimator() converts a Keras model to an estimator, which allows us to take advantage of the Dataset API nicely, and automatically save checkpoints to checkPointDirectory during training, and upon training completion. The estimator API presents some invaluable features, such as automatically distributing the workload over multiple GPUs, with, e.g. Now for big models and lots of data, it is often useful to execute predictions after training using some form of saved model. It seems that as of Tensorflow 1.10 (see https://github.com/tensorflow/tensorflow/issues/19295), a tf.keras.model object supports load_weights() from a Tensorflow checkpoint. This is mentioned briefly in the Tensorflow docs, but not the Keras docs, and I can't find anyone showing an example of this. After defining the model layers again in some new .py, I have tried but this gives a NotImplementedError: I would like to do as suggested by the Warning and use the 'object-based saver' instead, but I haven't found a way to do this via a RunConfig passed to estimator.train(). So is there a better way to get the saved weights back into an estimator for use in prediction? The github thread seems to suggest that this is already implemented (though based on the error, probably in a different way than I am attempting above). Has anyone successfully used load_weights() on a TF checkpoint? I haven't been able to find any tutorials/examples on how this can be done, so any help is appreciated.",https://stackoverflow.com/questions/52597523,9731282,Inadequate Examples
46046145,TensorFlow: How to get sub array for each row in tensor,"I have following code: Expected output as following [[1,2,3],[2,3],[1]] I have tried several functions, etc tf.gather, tf.slice. All of them do not work. What is the magic_slice_function?",https://stackoverflow.com/questions/46046145,1114397,Documentation Replicability
70747499,Using tf.map_fn when the function has multiple outputs,"I can easily use tf.map_fn when the function has one output: output: But, when the function has two outputs: I get an error. Not sure what is going on. I read the information about tf.map_fn in here https://www.tensorflow.org/api_docs/python/tf/map_fn, but not sure how to fix this: map_fn also supports functions with multi-arity inputs and outputs: If elems is a tuple (or nested structure) of tensors, then those tensors must all have the same outer-dimension size (num_elems); and fn is used to transform each tuple (or structure) of corresponding slices from elems. E.g., if elems is a tuple (t1, t2, t3), then fn is used to transform each tuple of slices (t1[i], t2[i], t3[i]) (where 0 &lt;= i &lt; num_elems). If fn returns a tuple (or nested structure) of tensors, then the result is formed by stacking corresponding elements from those structures. Output:",https://stackoverflow.com/questions/70747499,11861082,Documentation Replication on Other Examples
65201252,How to shuffle TFrecords files before feeding them to the model,"I am fitting a Neural network model using TFrecords and keras. I have a relatively big dataset which is pretty heterogeneous. I already used the shuffle my dataset during the training of the model like in the documentation exemple :https://keras.io/examples/keras_recipes/tfrecord/ (but can't shuffle all because it would cost too much memory) and I also separated my dataset into small shards each of equal size. However I have reasons to think that this ""approximate"" shuffling is not enough and I also think that feeding already shuffled data would increase training speed. So now my question is: After I have separated my dataset into Tfrecords shards, Is it possible to efficiently make code that takes randomly 2 shards, load them, shuffle them and then rewrite 2 shards (which are now shuffled between two shards). So that I can repeat this process a lot of time, which would result in correctly shuffled TFrecords files. More precisely, I take 2 shards: shard1.tfrec and shard2.tfrec, load them into one tf.data.dataset, shuffle it, and then output 2 shards of equal size again.",https://stackoverflow.com/questions/65201252,14485071,Documentation Ambiguity
46370159,Outputting batch/epoch training loss during `tf.train.MonitoredTrainingSession`,"I would like to output my loss with MonitoredTrainingSession every epoch or batch. Ideally I would love to get a flag that the epoch is ended or be able to provide a callback like in keras. I see that I can also do it by manually counting steps, but I want to use the tf functionality, which seems still poorly documented. From what I could find in their documentation, one can use tf.train.LoggingTensorHook to print the tensors every n steps. The problem however is that it prints with frequency different from what I request. When I run following with every_n_iter=4 I get output every 2nd iteration: I am getting output like: etc. That is it outputs every 2nd step, not every 4th. The documentation says: I am running it on one local machine. Why one ""local step"" equals two loop python iterations? Why two and not five? Looking at the Python source does not seem helping. Any Google folks aware of what it is doing?",https://stackoverflow.com/questions/46370159,1716733,Lack of Alternative Solutions/Documentation
46372554,When feeding a dictionary to a tensorflow function I get Why do I get TypeError: unhashable type: 'numpy.ndarray',I am working on a Tensor Flow Coursera Course and I dont understand why I am getting a type mismatch. This is the function I am defining: And when running this: I get this type error: I checked the Tensorflow documentation for tf.one_hot and there shouldn't be a problem with np.arrays. https://www.tensorflow.org/api_docs/python/tf/one_hot,https://stackoverflow.com/questions/46372554,5462568,Documentation Ambiguity
53367734,How to implement average pooling in case of Conv1d in tensorflow?,I want to implement the average pooling in conv1d. But tf.nn.avg_pool function can only be implemented on 4 dimensional tensor. So what should I do to overcome this problem?,https://stackoverflow.com/questions/53367734,10631580,Documentation Replicability
46386211,When would I want to set a stride in the batch or channel dimension for TensorFlow convolution?,"Tensor flow implements a basic convolution operation with tf.nn.conv2d. I am specifically interested in the ""strides"" parameter, which lets you set the stride of the convolution filter -- how far across the image you shift the filter each time. The example given in one of the early tutorials, with an image stride of 1 in each direction, is The strides array is explained more in the linked docs: Note the order of ""strides"" matches the order of inputs: [batch, height, width, channels] in the NHWC format. Obviously having a stride of not 1 for batch and channels wouldn't make sense, right? (your filter should always go across every batch and every channel) But why is it even an option to put something other than 1 in strides[0] and strides[3], then? (where it being an ""option"" is in regards to the fact that you could put something other than 1 in the python array you pass in, disregarding the documentation quote above) Is there a situation where I would have a non-one stride for the batch or channels dimension, e.g. If so, what would that example even mean in terms of the convolution operation?",https://stackoverflow.com/questions/46386211,2343795,Requesting (Additional) Documentation/Examples
46394659,Does tf.histogram_fixed_width() support back propagation?,"I want to use the histogram of the output from a CNN to compute the loss. I am wondering whether tf.histogram_fixed_width() supports the gradient to flow back to its former layer. Only it works, I can add a loss layer after calculating the histogram.",https://stackoverflow.com/questions/46394659,8666472,Documentation Replicability
71446995,Problem with tensorflow function when function is called within itself,"It seems tensorflow cannot build @tf.function when the function is called within itself. Consider below for a simple calculation of factorial of a number: When @tf.function is commented, it works fine but with @tf.function it runs forever. Any ideas how to fix this condition?",https://stackoverflow.com/questions/71446995,18444524,Documentation Replication on Other Examples
53424304,Failed to make Dataset.filter() work in the model/official/resnet/resnet_run_loop.py file,"In the official resnet model, I want to filters the dataset from test.bin by the value of 'label' when eval_only set to be True. I tried the tf.data.Dataset.filter() function to get only one class of test data but it didn't work. I put this code in the resnet_run_loop.process_record_dataset function, but it raised an error I found that the shape of tensor 'label' is (?,) :'Tensor(""arg1:0"", shape=(?,), dtype=int32, device=/device:CPU:0)'",https://stackoverflow.com/questions/53424304,5801328,Documentation Replication on Other Examples
65547615,How to create variables inside tf.function decorated method?,"Cannot create a tf.Variable (while I should) in a tf.function decorated method: I get: I think there is some workaround using: but I also think that it affects performance, which the whole point of tf.function in the first place.",https://stackoverflow.com/questions/65547615,20280771,Documentation Replicability
53466500,Can't save and load a trained CNN model for binary image classification,"I have built a Binary Image classifier using Convolutional Neural Networks using TensorFlow.It is running fine, however, each time it takes too long to train from scratch. So, I want to save the trained model and load it next time. I can't seem to understand how to implement these guides in my program as shown in the TensorFlow documentation. Here's the full code: I have tried to use saver = tf.train.import_meta_graph('D:\\Project\\Final_Project\\chest_xray\\check_point-78.meta') to import the graph but I get this error Process finished with exit code 1",https://stackoverflow.com/questions/53466500,8023082,Documentation Replicability
65585095,Why tf.keras.optimizers.SGD has no global_step,"In TF 1.14, below code will raise exception. The exception is It seems that tf.keras.optimizers in unavailable in TF 1.14. However, why the global_step is gone in tf.keras.optimizers.SGD? It is supposed to be there in tf.train.Optimizer of TF 1.14",https://stackoverflow.com/questions/65585095,5405823,Documentation Replicability
46752071,Feed a Tensor of SparseTensors to estimators,"To get started with TF, I wanted to learn a predictor of match outcomes for a game. There are three features: the 5 heros on team 0, the 5 heroes on team 1, and the map. The winner is the label, 0 or 1. I want to represent the teams and the maps as SparseTensors. Out of a possible 71 heroes, five will be selected. Likewise for maps, out of a possible 13, one will be selected. This fails on team_0s = tf.constant(list(map(lambda r: sparse_team(r.team_0), records))) It's very difficult to understand what tf wants me to return in my input_fn, because all of the examples I can find in the docs ultimately call out to a pandas or numpy helper function, and I'm not familiar with those frameworks. I thought that each dictionary value should be a Tensor containing all examples of a single feature. Each of my examples is a SparseTensor, and I want to simply embed them as their dense versions for the sake of the DNNClassifier. I'm sure my mental model is horribly broken right now, and I appreciate any help setting it straight. Error output:",https://stackoverflow.com/questions/46752071,86432,Inadequate Examples
65596022,How to convert ragged tensor to list of tensors inside a graph,"I have a ragged tensor with variable shape in the 2nd dimension N x ? x 4, I'd like to convert it to a list of tensors. Down below is a function that works, but only when it's not decorated with tf.function. I need this function to run inside a tf graph.",https://stackoverflow.com/questions/65596022,8161411,Documentation Replication on Other Examples
46759271,Image pixel value normalized for tf.image.decode_jpeg and tf.train.shuffle_batch?,"I am trying to use the tf.train.shuffle_batch function from tensorflow, then I need to first load the images using tf.image.decode_jpeg(or other similar functions to load png and jpg). But I just found out that the images are loaded as probability map, which means the max of the value of pixel is 1, and the min of the value of the pixel is 0. Below is my code updated from a github repo. I don't know why the values of pixels are normalized to [0,1], and I don't find related documentation on tensorflow. Could anyone help me? Thanks.",https://stackoverflow.com/questions/46759271,3173450,Documentation Replication on Other Examples
53523729,How to select a proper API/implementation of an LSTM cell in TensorFlow?,"In Tensorflow there are many different implementations of the same task under different APIs. For LSTM cell one can find many implementations, e.g., Which of the above implementation shall I select? Is there a general guideline? For example always use tf.nn &gt; tf.keras &gt; tf.layers &gt; tf. contrib. In a slightly related question here (for batch normalization), the approved answer says tf.contrib is not a good choice as it is for early implementation. Therein, the KERAS API also uses tf.nn. So it looks like tf.nn &gt; tf.keras &gt; tf.contrib.",https://stackoverflow.com/questions/53523729,3484477,Documentation Replicability
71588962,Solving a set of linear systems in tensorflow,"I'm having a problem understanding the working mechanism of tensorflow's function: tf.linalg.solve. I want to solve a set of linear systems (AX = Y), where the linear coefficients (A) were shared but there are multiple batches of Y, which are different. Using numpy, I can simply do it via: which gives me a quite consistent solution. But when I switch to tensorflow, it gives me the results: According to the document, I assume the solution should be solved according to the corresponding vec. But it does not seem to give me the expected results in tensorflow. Since I'm a new user, I could have messed up something. It would be appreciated if any information could be offered.",https://stackoverflow.com/questions/71588962,12416654,Documentation Ambiguity
53539040,tf.nn.bidirectional_dynamic_rnn returns single value in Tensorflow,"tf.nn.bidirectional_dynamic_rnn is usually used as follows. However, when I see the following code, I'm not sure what value is returned in _. Can you tell me what is returned in _?",https://stackoverflow.com/questions/53539040,8940996,Documentation Replicability
71596616,Predicting Data using an Untrained Keras Model,"Essentially, I want to propagate data through a Keras model, without first training the Keras model. I tried using both predict() and feeding in raw tensors into the model. The data is a 2D Numpy float64 array with shape (3, 3), filled entirely with zeros. The model itself is outlined below: The model needed two different activation functions in it's output layer, hence why I used Functional API. I first attempted to use hyperparameter.predict(data[0]), but kept getting the following error: I fiddled around with array dimensions a bit, but the model continued to give the same error. I then tried feeding raw tensors into the model, with the following code: This code continued to give the following error: I've looked online for a while, and I've searched through the tf.data documentation, but I'm still not sure how to fix this. Again, I've tried multiple variations of this code, and I continue to get mostly the same errors.",https://stackoverflow.com/questions/71596616,11442631,Documentation Ambiguity
53541803,Rows or elements selection on sparse tensor,"In tensorflow, how can we do tf.gather or tf.gather_nd in sparse tensor? How can we extract select specific rows or specific elements from sparse tensor without converting it into dense tensor?",https://stackoverflow.com/questions/53541803,10596177,Documentation Replication on Other Examples
53549352,minimize a function in Tensorflow,"How can I get the gradients of a function using tf.gradients? the below is working when I use GradientDescentOptimizer.minimize(), tf.gradients seems to be evaluating 1 at the deriv of x^2+2 which is 2x What am I missing ?",https://stackoverflow.com/questions/53549352,9650073,Documentation Replicability
53568337,Print accuracy when training tf.estimator.DNNClassifier,"I am new to tensorflow, using official tutorial tf.estimator.DNNClassifier and custom estimator to build simple NN to solve classification problem. While training : It will report loss at specific time as following: I want to print classification accuracy every batch or epoch, likes the log Info in keras: How can I find the tutorial on this problem ? All I find were talking about more lower API (tensor, session, etc.).",https://stackoverflow.com/questions/53568337,10083164,Documentation Replicability
53569622,Difference between tf.train.Checkpoint and tf.train.Saver,"I found there are different ways to save/restore models and variables in Tensorflow. These ways including: In tensorflow's documentations, I found some differences between them: How tf.train.Checkpoint can load graph without .meta file? or more generally What is the difference between tf.train.Saver and tf.train.Checkpoint?",https://stackoverflow.com/questions/53569622,1462770,Documentation Ambiguity
65953591,Which format should have time series input for LSTM-Model in Tensorflow?,"I have a problem with the input for the fit-function of an LSTM-Model in TensorFlow. I have an input with the following shape: (5, 128, 78, 80) The fields are: (number of samples, timesteps, feature1, feature2) The output has the shape: (5, 128, 78, 2) This is my model: I get the following error: ValueError: Input 0 of layer sequential_38 is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (5, 128, 78, 80) So I think, I have to change the shape of my data, but I don't know how. I tried already different values for input and input_shape-attribute. I read in https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM that the input has to be a tensor with shape [batch, timesteps, feature]. So I put the two features in a nested array, and gave [batch, timesteps, array of the features] to the fit-function. But it told me that the data could not be converted to a tensor. Also explicit converting with tf.convert_to_tensor did not work. I would be really glad, if someone could explain me, how I can pass input data with two features to an LSTM-model.",https://stackoverflow.com/questions/65953591,11535546,Documentation Ambiguity
53578484,tf.gather with indices of higher dimention than input data?,"Reading Dynamic Graph CNN for Learning on Point Clouds code, I came across this snippet: debugging the line I made sure that the dims are Reading the tf.gather doc I couldn't understand what the function does with dimensions higher that the input dimensions",https://stackoverflow.com/questions/53578484,7034613,Documentation Replicability
65982015,tf.keras.preprocessing.sequence.pad_sequences in JavaScript,How can we implement tf.keras.preprocessing.sequence.pad_sequences in TensorFlow.js?,https://stackoverflow.com/questions/65982015,14226505,Documentation Replication on Other Examples
53612973,TensorFlow Sigmoid Cross Entropy with Logits for 1D data,"Suppose we have some 1D data (e.g. time series), where all series have fixed length l: and we want to perform semantic segmentation, with n classes: then the output for a single example has shape [n, l] (i.e. the data_format is not ""channels_last"") and the batched output has shape [b, n, l], where b is the number of examples in the batch. These classes are independent, so it is my understanding that the use sigmoid cross entropy is applicable here as the loss rather than softmax cross entropy. I have a few small related questions in regards to the expected format for and use of tf.nn.sigmoid_cross_entropy_with_logits: This Colab, highlights my confusion and demonstrates that the data_format does in fact matter..., but the documentation does not explicitly state which is expected.",https://stackoverflow.com/questions/53612973,5623899,Lack of Alternative Solutions/Documentation
53634736,TensorFlow Estimator API: use numpy functions?,"I have written several custom metrics which I would like to add to tf.summary and view in tensorboard via metrics_op in my EstimatorSpec. Is it strictly required that I covert these functions to their tf graph equivalents to be compatible, or is there a way to just dump them in, e.g.",https://stackoverflow.com/questions/53634736,5623899,Documentation Replicability
53649402,Disable TensorBoard logging on tf.Estimator methods,Is there a way disable the automatic TensorBoard logging when using the tf.estimator.Estimator class? I read through the tf.estimator.RunConfig and couldn't find a solution.,https://stackoverflow.com/questions/53649402,5368083,Lack of Alternative Solutions/Documentation
37979238,TensorFlow: product equivalent of tf.add_n,"Is there a product equivalent of tf.add_n, returning the elementwise product of a list of tensors?",https://stackoverflow.com/questions/37979238,5640923,Documentation Replicability
71711292,Tensorflow - mapping images with labels,"I need the solution, that works with TPU, neither ImageDataGenerator or tf.py_function doesn't work. My code for CPU: image_name_to_label is a python dict I don't want to convert data to tfrecords",https://stackoverflow.com/questions/71711292,5558021,Documentation Replicability
53677345,Passing >2GB data to tf.estimator,"I have x_train and y_train numpy arrays, each of &gt;2GB. I want to train model using the tf.estimator API, but I am getting the errors: I am passing the data using: The tf.data documentation mentions this error and provides solution using traditional TenforFlow API with placeholders. Unfortunately, I don't know how this could be translated into tf.estimator API?",https://stackoverflow.com/questions/53677345,3986320,Documentation Replication on Other Examples
47485498,Tensorflow tf.nn.embedding_lookup,"is there a small neural network in tf.nn.embedding_lookup?? When I train some data, a value of the same index is changing. So is it trained also? while I'm training my model I checked the official embedding_lookup code but I can not see any tf.Variables for train embedding parameter. But when I print all tf.Variables then I can found a Variable which is within embedding scope Thank you.",https://stackoverflow.com/questions/47485498,8956473,Documentation Replicability
54557468,"In tf.keras.layers.Embedding, why it is important to know the size of dictionary?","Same as the title, in tf.keras.layers.Embedding, why it is important to know the size of dictionary as input dimension?",https://stackoverflow.com/questions/54557468,9890698,Documentation Replicability
54606302,tf.data.Dataset from tf.keras.preprocessing.image.ImageDataGenerator.flow_from_directory?,"How do I create a tf.data.Dataset from tf.keras.preprocessing.image.ImageDataGenerator.flow_from_directory? I'm considering tf.data.Dataset.from_generator, but it's unclear how to acquire the output_types keyword argument for it, given the return type:",https://stackoverflow.com/questions/54606302,587021,Documentation Replication on Other Examples
66385626,How does TensorFlow SavedModel handle additional dependencies,"I am trying to export my TF model using the tf.saved_model.save() function. However, I'm using additional external libraries during preprocessing. A working code using the example of nltk looks like this: As far as I've understood the docs, a savedModel includes the trained parameters and computations, but NOT my code. But when I load the same model in another script like so (i.e. without importing my external library at all) I get my output without any errors This is especially baffling to me as I am even using nltk to download a dataset of stopwords. How exactly do SavedModels deal with these external libraries during exportation?",https://stackoverflow.com/questions/66385626,8921867,Documentation Replicability
54686895,Tensorflow dilation behave differently than morphological dilation,"As the following piece of code shows, the tensorflow tf.nn.dilation2D function doesn't behave as a conventional dilation operator. Returns the following tensor: I don't understand neither why it behaves like that, neither how I should use tf.nn.dilation2d to retrieve the expected output: Can someone enlighten the succinct documentation of tensorflow and give an explanation of what the the tf.nn.dilation2D function does ?",https://stackoverflow.com/questions/54686895,1782553,Documentation Ambiguity
54701429,Tensorflow: why tf.nn.conv2d runs faster than tf.layers.conv2d?,"I am writing a simple implementation of AlexNet. I tried with using tf.nn.conv2d and tf.layers.conv2d, and the results turn out that the loss dropped faster when using tf.nn.conv2d, even the structure is exactly the same. Does anyone know any explanation for that?",https://stackoverflow.com/questions/54701429,8859758,Documentation Replication on Other Examples
54721543,"tensorrt not support: tf.unpack, tf.slice, tf.tile, tf.expand_dims, tf.fill, tf.cast, tf.floor_div, tf.range","There are lots of unsupported operation for tensorrt when convert from tensorflow model to uff model ( such as: tf.unpack, tf.slice, tf.tile, tf.expand_dims, tf.fill, tf.cast, tf.floor_div, tf.range). Is there a simple method to solve the problem? This is the warnings when i convert convert from pb to uff model:",https://stackoverflow.com/questions/54721543,988709,Documentation Replicability
54761088,tf.nn.relu vs tf.keras.activations.relu,"I see both tf.nn.relu and tf.keras.activations.relu computes only the ReLU function (no additional fully connected layer or something, as described here), so what's the difference between them? Does one just wraps the other?",https://stackoverflow.com/questions/54761088,5476824,Documentation Replicability
72928149,Difference between Experimental Preprocessing layers and normal preprocessing layers in Tensorflow,"I am trying to figure out which I should use for Data Augmentation. In the documentation, there is: tf.keras.layers.RandomFlip and RandomRotation Then we have in tf.keras.layers.experimental.preprocessing the same things, randomFlip and RandomRotation. Which should I use? I've seen guides that use both. This is my current code: and this is a part of my model: I am a bit confused here..",https://stackoverflow.com/questions/72928149,14072615,Documentation Replication on Other Examples
66659610,How does tf.keras.metrics.TopKCategoricalAccuracy differ from Precision@k?,"Coming from recommender systems, precision@k is a popular metric. On the tensorflow docs for tf.keras.metrics.TopKCategoricalAccuracy it states https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TopKCategoricalAccuracy Which seems to be exactly the same as precision@k. Am I missing something or are they equivalent and it just comes down to TF/recommender terminology?",https://stackoverflow.com/questions/66659610,15409191,Documentation Replicability
54897832,Feeding large numpy arrays into TensorFlow estimators via tf.data.Dataset,"TensorFlow's tf.data.Dataset documentation on consuming numpy arrays states that in order to use numpy arrays in combination with the Dataset API, the arrays have to be small enough (&lt;2 GB in total) to be used as tensors, or they can be fed into the dataset via placeholders. However, if you use Dataset in conjunction with estimators (where placeholders are not available), the documentation does not provide a solution on working with large arrays without placeholders. Are there other options for passing placeholder values into estimators that can be used or is the solution to provide the data in tfrecord or csv format?",https://stackoverflow.com/questions/54897832,4443082,Inadequate Examples
48169791,How to change the value of a tensor which is not a tf.Variable in TensorFlow?,"I know that there is a tf.assign function in TensorFlow, but this function is mainly aimed at mutable tensor (tf.Variable). How to modify the value of the tensor? For example, the following code, How to modify the value of feature_map and do not affect its derivation? More specifically, when I change the value of feature_map, it does not affect its derivation process. For example, y = a^2, y'= 2a, I just need to change a = 1 to a = 2. Other_op = tf.gradients(feature_map, X) Different feature_map would achieve the different values, but it does not destroy the graph structures of operation.",https://stackoverflow.com/questions/48169791,3571955,Documentation Replicability
67197448,How to extract multiple rows from tensor at the same time?,"TL;DR: TensorFlow tensor is of shape (50, 50, 6), want these indices (:, :, (0, 2, 3)). How to extract them? Here is an example array I am working with: I want to extract the the first, third, and fourth row; in other words I need all these entries (:, :, (1, 3)), which works for numpy arrays: Working with a tensor t = tf.convert_to_tensor(a) and then calling the index like throws an error: For numpy I have found the following relevant questions, but they naturally focus on numpy arrays: How to slice a 2D array non-consecutively in Python Slicing a numpy array along a dynamically specified axis Looking at the TF documentation I found gather_nd and boolean_mask, which I feel are helpful, but I must freely admit that I have not understood the docs at this part. On SO I found this question How to select elements of a tensor along a specific axis in TensorFlow, which focuses on single elements; I am looking for complete dimensions (if that's the right wording here). How can I do the numpy thing in TensorFlow?",https://stackoverflow.com/questions/67197448,12859833,Documentation Replication on Other Examples
67211152,Tensorlow - please decipher what the tf.where document says,Please decipher what the tf.where documentation says about what it does when both x and y are provided. I suppose it tries to say it will produce a result by: Is this correct?,https://stackoverflow.com/questions/67211152,4281353,Documentation Replicability
67226579,What causes stated console output in Tensorflow?,"Tensorflow puts out a lot of output in console, could you tell me what code in my program causes this output? Also, how do I suppress it? .... Variable/Initializer/initial_value/7859, Variable/Initializer/initial_value/7860, Variable/Initializer/initial_value/7861, Variable/Initializer/initial_value/7862, Variable/Initializer/initial_value/7863, Variable/Initializer/initial_value/7864, Variable/Initializer/initial_value/7865, Variable/Initializer/initial_value/7866, Variable/Initializer/initial_value/7867, Variable/Initializer/initial_value/7868, Variable/Initializer/initial_value/7869, Variable/Initializer/initial_value/7870, Variable/Initializer/initial_value/7871, Variable/Initializer/initial_value/7872, Variable/Initializer/initial_value/7873, Variable/Initializer/initial_value/7874, Variable/Initializer/initial_value/7875, Variable/Initializer/initial_value/7876, Variable/Initializer/initial_value/7877, Variable/Initializer/initial_value/7878, Variable/Initializer/initial_value/7879, Variable/Initializer/initial_value/7880, Variable/Initializer/initial_value/7881, Variable/Initializer/initial_value/7882, Variable/Initializer/initial_value/7883, Variable/Initializer/initial_value/7884, Variable/Initializer/initial_value/7885, Variable/Initializer/initial_value/7886, Variable/Initializer/initial_value/7887, Variable/Initializer/initial_value/7888, Variable/Initializer/initial_value/7889, Variable/Initializer/initial_value/7890, Variable/Initializer/initial_value/7891, Variable/Initializer/initial_value/7892, Variable/Initializer/initial_value/7893, Variable/Initializer/initial_value/7894, Variable/Initializer/initial_value/7895, Variable/Initializer/initial_value/7896, Variable/Initializer/initial_value/7897, Variable/Initializer/initial_value/7898, Variable/Initializer/initial_value/7899, Variable/Initializer/initial_value/7900, Variable/Initializer/initial_value/7901, Variable/Initializer/initial_value/7902, Variable/Initializer/initial_value/7903, Variable/Initializer/initial_value/7904)' with input shapes: [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [6], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7],..... Here is my code EDIT: The 'Variable/Ini..../&lt;a_number&gt;' happens right before the print('loc 4') EDIT: I suspect that this problem is being caused in the line self.__input = tf.Variable(arg_data_train). arg_data_train is actually a python list of python lists. I still don't understand the console output though.",https://stackoverflow.com/questions/67226579,7971339,Documentation Replicability
67241187,Uninitialized weights after checkpoint restore in Tensorflow,I made use of checkpoint to save the weights of my module using this code: Where deep_cfr_solver._policy_network is a subclass of tf.Module Afterwards I try to load this module again in another function: Where state is a GameState of OpenSpiel. But the last line of this code results in the following error: The code I use is based on this documentation,https://stackoverflow.com/questions/67241187,7657749,Documentation Replication on Other Examples
48222274,tensorflow ctc_loss how to understand labels param,"tensorflow document: labels.values[i] must take on values in [0, num_labels], allow values[i] have zero value, labels is SparseTensor, indices: specifies the indices of the elements in the sparse tensor that contain nonzero values, values:which supplies the values for each element in indices according to sparsetensor define, labels.values[i] is nonzero value, but tf.nn.ctc_loss labels.values allow labels.values[i] have zero value, how to understand labels.values[i] allow have zero value",https://stackoverflow.com/questions/48222274,9207839,Documentation Replicability
48224021,Create tf.estimator.Estimator from tf.saved_model.SavedModel,I have a TensorFlow model on disk saved using tf.saved_model.builder.SavedModelBuilder. How to create a TF Estimator from it?,https://stackoverflow.com/questions/48224021,1370397,Documentation Replication on Other Examples
48235239,The fit function from tf.contrib.learn.LinearRegressor asks to switch to tf.train.get_global_step,"I am trying to get a LinearRegressor to work and I get an error for which there doesn't seem to be much documentation about. When I do: I get the error: I am not sure how to proceed. I read from the docs: But I'm not sure to what I should change to, or how to switch to the global step. I tried using tf.estimator.LinearRegressor mainly because I'm out of ideas, and did something like this: But got no output at all.",https://stackoverflow.com/questions/48235239,463065,Lack of Alternative Solutions/Documentation
48243764,Concat two padded senteces and insert to conv1d i tensorflow?,What dimensions are required in tf.nn.conv1d ? and how to perform max pooling afterwards?,https://stackoverflow.com/questions/48243764,7905379,Documentation Replicability
48281583,Tensorflow manual evaluation with tf.estimators,"I'm using Tensorflow api tf.estimator to train and evaluate models, building a custom estimator as explained in this blog, and running the process with this convenient way: This process is very efficient to train and monitor tf.metrics metrics into Tensorboard (e.g. with tf.metrics.accuracy). For one of my projects, I would like to add some more (complicated) metrics coming from a standalone python function, and visualize them in the Tensorboard. This function would take as an argument the training logdir (to load the model and run it on data) and would return some scalar (or tensor) to be monitored into the Tensorboard. Last but not least, the function will need the GPU resources, so I expect this function to be called after evaluation: I was thinking about using tf.summary.FileWriter and/or Training Hooks (see Tensorflow documentation) but I don't know how to use them in a straightforward way with tf.estimator api. Thank you very much for your help! M.",https://stackoverflow.com/questions/48281583,9224055,Documentation Replication on Other Examples
67308892,ANN : I cannot train my custom loss function using gradient descent,"I am trying to train my multi-input (100 features) multi-output (3 outputs) model using gradient descent but the loss function does not tends towards 0. The loss function is on the following picture : loss(y_true, y_pred). This loss function rewards the classification (if you get the right sign) of data with large value. Here is the code of my loss function : I am using tf.raw_ops because it seems that sign() and abs() are differentiable (weird..) : https://www.tensorflow.org/api_docs/python/tf/raw_ops . I implemented this loss function to train the following model (ANN with 3 layers with 50 nodes each) : The loss does not tend towards zero and varies between 0.99 and 1.05. I am using tensorflow 1.14.0. Can you help me with this problem ? Thank you all in advance !",https://stackoverflow.com/questions/67308892,15788458,Documentation Replicability
48299597,How to efficiently shuffle a large tf.data.Dataset when using tf.estimator.train_and_evaluate?,"The tf.estimator.train_and_evaluate documentation makes it clear that the input dataset must be properly shuffled for the training to see all examples: In my application, I would like to uniformly sample examples from the full tf.data.Dataset with arbitrary evaluation frequency and shuffle()'s buffer size. Otherwise, the training can at most see the first: elements, effectively discarding the rest. Is there an efficient way to work around that without loading the complete dataset in the system memory? I considered sharding the dataset based on the buffer size, but if the evaluation does not occur frequently, it will iterate on the same shard multiple times (a repeat() closes the pipeline). Ideally, I would like to move to another shard after a complete iteration over the dataset, is that possible? Thanks for any pointers!",https://stackoverflow.com/questions/48299597,2529808,Documentation Replicability
48311823,regarding tf.nn.conv1d and its corresponding transpose operation,"In the latest tensorflow version, there is tf.nn.conv2d_transpose for the 2D deconvolution operation. However, there is no corresponding 1D deconvolution operation for tf.nn.conv1d. How to perform the deconvolution for 1D data?",https://stackoverflow.com/questions/48311823,288609,Inadequate Examples
67344068,TensorFlow 2 - does NumPy numerical values and arrays cause new graphs for TF Function?,"Does ""numerical Python values"" stated in the Hands on ML 2 include NumPy int, float, and array? Do we need to explicitly create a TF Tensor or a TF DataSet from a NumPy construct as the argument of a TF Function? Hands on ML 2 Chapter 12 Auto Graph and Tracing: The 3rd rule stated in the TensorFlow document Rules of tracing corresponds with Python int, float, boolean, str , etc that will cause a new graph part. But not sure if the 5th rule (all other Python types) applies to NumPy constructs. I suppose the fact tf.numpy_function exists suggests that the TF Function tracing will generate a new graph, but need a definite confirmation.",https://stackoverflow.com/questions/67344068,4281353,Documentation Replication on Other Examples
67847893,Understanding true_classes in log_uniform_candidate_sampler,"https://www.tensorflow.org/tutorials/text/word2vec uses tf.random.log_uniform_candidate_sampler for negative sampling. The tutorial sets true_classes to context_class. My experiment shows no matter what I set for true_classes, the function always yields good results. What does true_classes mean in this function?",https://stackoverflow.com/questions/67847893,746461,Documentation Replication on Other Examples
48825785,How can I filter tf.data.Dataset by specific values?,"I create a dataset by reading the TFRecords, I map the values and I want to filter the dataset for specific values, but since the result is a dict with tensors, I am not able to get the actual value of a tensor or to check it with tf.cond() / tf.equal. How can I do that?",https://stackoverflow.com/questions/48825785,4137497,Documentation Replication on Other Examples
67872566,cProfile with tensorflow tf.functions,"When using cProfiler with tf.function-decorated functions, it seems to show the time it takes for first building the graph only. Is there a way to also see, how much time is spent in total in these graphs? This yields:",https://stackoverflow.com/questions/67872566,15746088,Documentation Replicability
67882420,How tf.keras.preprocessing.image_dataset_from_directory display output to console,I am using the below function to read images from a directory and it display below text in the output After exploring the above function here I am not able to find out how it is displaying the text in the console. One thing that I noticed is that the output of the function is a dataset but how does it generate the text in console? Please help me understand How tf.keras is showing this output in the console. What is the exact code behind this?,https://stackoverflow.com/questions/67882420,6244166,Documentation Replication on Other Examples
67947583,"Defining a callable ""loss"" function","I am trying to optimize a loss function (defined using evidence lower bound) with tf.train.AdamOptimizer.minimize() on Tensorflow version 1.15.2 with eager execution enabled. I tried the following: and got the following : RuntimeError: ""loss"" passed to Optimizer.compute_gradients should be a function when eager execution is enabled. This works fine if I disable eager execution but since I need to save a tensorflow variable as a numpy array so I need eager execution enabled. The documentation mentions that when eager execution is enabled, the loss must be a callable. So the loss function should be defined in a way that it takes no inputs but gives out loss. I am not exactly sure how do I achieve such a thing. I tried train_op = optim.minimize(lambda: loss) but got ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [] and loss &lt;function &lt;lambda&gt; at 0x7f3c67a93b00&gt;",https://stackoverflow.com/questions/67947583,6639856,Documentation Replication on Other Examples
48914952,num_buckets as a parameter in a tensorflow feature column,Currently Tensorflow documentation define a categorical vocabulary column this way: However this suppose that we input manually the vocabulary list. In case of large dataset with many columns and many unique values I would like to automate the process this way: To do so I need to retrieve the parameter list_of_unique_values_in_the_column. Is there anyway to do that with Tensorflow? I know there is tf.unique that could return unique values in a tensor but I don't get how I could feed the column to it so it returns the right vocabulary list.,https://stackoverflow.com/questions/48914952,2175173,Documentation Replication on Other Examples
68002490,Make tf.nn.conv2d_transpose in Pytorch,My question is how to make operation of tf.nn.conv2d_transpose in pytorch. See the example below: I want to get the same result than y_up_tf using y_tor and feats_tor in Pytorch.,https://stackoverflow.com/questions/68002490,16243152,Documentation Replicability
48981022,what is the difference between tf.nn.convolution and tf.nn.conv2d?,"I want to make dilated convolution on a feature. In tensorflow I found tf.nn.convolution and tf.nn.conv2d. But tf.nn.conv2d doesn't seem to support dilated convolution. So I tried using tf.nn.convolution. Do the 2 formulations below give the same result? tf.nn.conv2d(x, w, strides=[1, 1, 2, 2], padding='SAME',data_format='NCHW') tf.nn.convolution(x, w, strides=[1, 1, 2, 2], padding='SAME',data_format='NCHW')",https://stackoverflow.com/questions/48981022,9411147,Documentation Replication on Other Examples
49003393,Tensorflow seq2seq Sequence loss function : trying to get per-iteration cross-entropy loss (to avoid OOM),"I am trying to add a mechanism so that the cross entropy loss calculated in seq2seq.sequence_loss (https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/sequence_loss) is done so not in a batch but per iteration I tried to use tf.while_loop and tf.scan , but have been facing issues for 2 days. My (vain)attempt with tf.scan is below. This function is supposed to replace and emulate the result of this one : https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/contrib/seq2seq/python/ops/loss.py Firstly , can it even be done? If yes , where am I messing up ? Any help would be appreciated :D",https://stackoverflow.com/questions/49003393,9250129,Documentation Ambiguity
68095664,problem in converting tensorflow 1.x code to tensorflow 2.x,"I have a code in tf 1.x which I have converted in tf 2.x using 'tf upgrade v2.....' command but there is 1 segment which I am having trouble in converting,i.e. I want to change the above two lines such that eager execution is not disabled and one more line which is: In the above line I don't want to use keras.layers.conv2D but I want to use tf.nn.conv2d().",https://stackoverflow.com/questions/68095664,16296144,Documentation Replicability
49066695,How to use tf.nn.raw_rnn function in Tensorflow?,"I am trying to implement LSTM based network where after hidden state computation we also apply linear + sigmoid transformation at each time step. I have found the official documentation and a nice article that describe tf.nn.raw_rnn function suitable for this task however I struggle to understand why it does not work in my particular case. So, let our input to LSTM be a minibatch of size [num_steps x batch_size x size], concretely [5, 32, 100]. Let LSTM have 200 hidden units. Then the output of the LSTM is [5, 32, 200] tensor which we can later use for loss computation. I assume the input [5, 32, 100] tensor is first unstacked into an array of [32, 100] tensors and then stacked back if we use tf.nn.dynamic_rnn with time_major=True in Tensorflow: In addition after each LSTM cell I need to perform linear + sigmoid transformation to squash each [32, 200] tensor into [32, 1] for example. Our tf.nn.dynamic_rnn won't work for that since it only accepts cells. We need to use tf.nn.raw_rnn API. So, here is my try: This unfortunately does not work. The loop_fn iterates only two times instead of num_steps times as I expected and its output is Tensor(""Train/Model/TensorArrayStack/TensorArrayGatherV3:0"", shape=(?, 32, 200), dtype=float32) not [5, 32, 1] as we intended. What am I missing here?",https://stackoverflow.com/questions/49066695,1984680,Documentation Replication on Other Examples
68136894,how to construct object of the same type that `tf.one_hot` returns?,I have a function input_preprocess that I am using in the data pipeline: The problem is that whatever tf.one_hot returns is a sequence but what tf.zeros returns is not. I get the following error: How can I manually construct something that could stand in for what tf.one_hot returns?,https://stackoverflow.com/questions/68136894,1105837,Documentation Replicability
49899526,Tensorflow input pipeline where multiple rows correspond to a single observation?,"So I've just started using Tensorflow, and I'm struggling to properly understand input pipelines. The problem I'm working on is sequence classification. I'm trying to read in a CSV file with shape (100000, 4). First 3 columns are features, 4th column is the label. BUT - the data represents sequences of length 10 i.e. rows 1-10 are sequence 1, rows 11-20 are sequence 2 etc. This also means each label is repeated 10 times. So at some point in this input pipeline, I'll need to reshape my feature tensor like tf.reshape(features, [batch_size_, rows_per_ob, input_dim]). And only take every 10th row of my label tensor like label[::rows_per_ob] Another thing I should point out is that my actual dataset is in the billions of rows so I have to think about performance. I've put together the below code from documentation and other posts on here, but I don't think I fully understand this because I'm seeing the following error: There seems to be an out of range error. I also can't figure out what to do with these batches once I get them working. Initially, I thought I would reshape them then just feed them into ""feed_dict"", but then I read that this is really bad, and I should be using a tf.data.Dataset object. But I'm not sure how to feed these batches into a Dataset. I'm also not entirely sure when would be the optimal time in this process to reshape my data? And a final point of confusion - when you use an Iterator with a Dataset object, I see that we use the get_next() method. Does this mean that each element in the Dataset represent a full batch of data? And does this then mean that if we want to change the batch size, we need rebuild the entire Dataset object? I'm really struggling to fit all the pieces together. If anyone has any pointers for me, it would be very much appreciated! Thanks!",https://stackoverflow.com/questions/49899526,5323535,Documentation Replication on Other Examples
49949463,Does tf.transpose also change the memory (like np.ascontiguousarray)?,"If I use tf.transpose does it also change the memory layout? In numpy it is used the function np.ascontiguousarray. I mean this would be important if I use cuda. Because it makes a difference if the memory layout is [N C H W] or [N H W C]. (N ... Nr of samples, H ... array height, W ... array width, C ... array depth, e.g. RGB) How to check this?",https://stackoverflow.com/questions/49949463,4355878,Documentation Replication on Other Examples
49959130,how to insert two or more label lists in the tf.estimator.inputs.numpy_input_fn?,"I am using the tf.estimator.inputs.numpy_input_fn to feed data in my model and train it in a similar way with the MNIST example. The only difference is that I need to insert two numpy lists of labels instead of one. I tried passing them in a dictionary like this: Then when I try to retrieve them in the model like so: I get the following error: I also tried to change the name of the variable ""labels"" to be ""targets"" according to tensorflow inputs.numpy_input_fn documentation: and I get this error: If you have any solution or suggestion to my problem, please let me know. Thanks a lot in advance. Antonios",https://stackoverflow.com/questions/49959130,7184238,Documentation Ambiguity
68707494,Tensorflow assign sparse input over axis,"I have sparse data over one axis, e.g. For efficiency, I would like to input batches in the format and in tensorflow, de-sparse that data. I know there is the function tf.sparse but that doesn't work over axes, which is inefficient in this case. Is there a function in tensorflow to do something like this:",https://stackoverflow.com/questions/68707494,200663,Documentation Replication on Other Examples
49969957,Tensorflow Adam Optimizer,"Okey I have been reading some of the posts regarding AdamOptimizer in tensorflow. I think there is some confusion around, at least among beginners in NNs like me. If I understood correctly, tf.train.AdamOptimizer keeps a so-called ""adaptative learning rate"". I thought that this learning rate would grow smaller as time increases. However, when I plot the function by which the learning rate is scaled, taken from the docs, this is what I get: So, for t = 1, the value for the user-selected learning rate is multiplied by 0.3 Then it decreases quite fast until 0.15 of its value, and then increases with time, slowly approaching the limit = user-selected learning rate. Isn't it a bit weird? I guess somewhere I am wrong, but I would've expected the learning rate to start at a higher value and then progressively decreasing towards smaller values.",https://stackoverflow.com/questions/49969957,9527947,Documentation Ambiguity
49987839,How to handle None in tf.clip_by_global_norm?,I have read in answers to this question here that tf.clip_by_global_norm() handles None values by simply ignoring them (comment by danijar in comments to the answer by @danijar) but when i try to apply it i seem to be doing something wrong as it throws ValueError: None values not supported. Can somebody please tell me what am i doing wrong or if tf.clip_by_global_norm() does not handle None gradients and i have to take care of them manually The official documentation seems to agree with @danijar's comments. see here,https://stackoverflow.com/questions/49987839,6546694,Documentation Replicability
50454095,tf.gradients - dimensions of output,Here is my code: it returns: I am expecting to get 3 by 3 matrix or as per tf.gradients docs list of dim 3 with 3 elements for each list entry. What I am missing? UPDATE: I see in docs tf.gradients but why sum and how do I get all entries of Jacobian?,https://stackoverflow.com/questions/50454095,1700890,Lack of Alternative Solutions/Documentation
69195950,Problem with inputs when building a model with TFBertModel and AutoTokenizer from HuggingFace's transformers,"I'm trying to build the model illustrated in this picture: I obtained a pre-trained BERT and respective tokenizer from HuggingFace's transformers in the following way: The model will be fed a sequence of italian tweets and will need to determine if they are ironic or not. I'm having problems building the initial part of the model, which takes the inputs and feeds them to the tokenizer in order to get a representation I can feed to BERT. I can do it outside of the model-building context: but I'm having troubles building a model that does this. Here is the code for building such a model (the problem is in the first 4 lines ): And when I call the function for creating this model I get this error: One thing I thought was that maybe I had to use the tokenizer.batch_encode_plus function which works with lists of strings: but I get this error: and beside the fact I haven't found a way to convert that tensor to a list with a quick google search, it seems weird that I have to go in and out of tensorflow in this way. I've also looked up on the huggingface's documentation but there is only a single usage example, with a single phrase, and what they do is analogous at my ""out of model-building context"" example. EDIT: I also tried with Lambdas in this way: but I get the following error: EDIT 2: I also tried the approach suggested by @mdaoust of wrapping everything in a tf.py_function and got this error. Then I defined Tout as the type of the value returned by the tokenizer: transformers.tokenization_utils_base.BatchEncoding and got the following error: Finally I unpacked the value in the BatchEncoding in the following way: And get an error in the line below:",https://stackoverflow.com/questions/69195950,11579184,Inadequate Examples
69370662,How to get Value and Index number of tensor?,"It will print tf.Tensor(0.35625213, shape=(), dtype=float32). Here I just need the number 0.35625213 and its index.",https://stackoverflow.com/questions/69370662,9591542,Documentation Replicability
50635729,ValueError: Cannot use 'Enter' as input to 'Merge' because 'Enter' is in a while loop,"Problem occured when I use tf.nn.dynamic_rnn,I don't quite understand what does this 'Enter' and 'Merge' mean in this context. Hope you can help me, thanks! Here is the error log picture",https://stackoverflow.com/questions/50635729,4265099,Documentation Replication on Other Examples
50988466,Using L-BFGS optimizer with Tensorflow estimator API,"I am using Tensorflow Estimator API but haven't figured out how to use the L-BFGS optimizer available at tf.contrib.opt.ScipyOptimizerInterface. It seems the estimator API expects some optimizer from the tf.train module but no BFGS implementation is available there. The only one defined in contrib does not follow the same interface. To be more specific, in the official tutorial to define custom estimators, it's shown how to use the AdagradOptimizer: However, the API of the ScipyOptimizerInterface is as follows: Taking a full example: If I un-comment the line to use the ScipyOptimizer instead, I obviously get an error as follows Is there an easy way to use the Scipy optimizer? Thanks in advance.",https://stackoverflow.com/questions/50988466,2689882,Documentation Replication on Other Examples
51776390,how to use tensorboard debugger with datalab which uses tf.estimator on google cloud platform,"When I start tensorboard via datalab it uses the google syntax which is described here. This document only mentions start, stop and list. However, there is a debugger pane which I can not use. This document describes how to use tensorboard debugger with a tf.estimator but it uses a different syntax. Is there someway to blend the two so the debugger is usable with datalab?",https://stackoverflow.com/questions/51776390,1008596,Documentation Replication on Other Examples
51806852,Can't save custom subclassed model,"Inspired by tf.keras.Model subclassing I created custom model. I can train it and get successfull results, but I can't save it. I use python3.6 with tensorflow v1.10 (or v1.9) Minimal complete code example here: Error message: I looked into the error line and found out that get_config method checks self._is_graph_network Do anybody deal with this problem? Thanks! Update 1: On the keras 2.2.2 (not tf.keras) Found comment (for model saving) file: keras/engine/network.py Function: get_config So, obviously it won't work... I wonder, why don't they point it out in the documentation (Like: ""Use subclassing without ability to save!"") Update 2: Found in keras documentation: So, there is no way to save model by using subclassing. It's possible to only use Model.save_weights()",https://stackoverflow.com/questions/51806852,3049753,Requesting (Additional) Documentation/Examples
51824310,Difference between Keras and tensorflow implementation of LSTM with dropout,"I was reviewing the documentation for the LSTM cell in tensorflow and Keras. In particular, I want to apply dropout as well. Here is what I have in Keras and would like to apply the same LSTM cell in tensorflow: Therefore, I know that I need to use tf.nn.rnn_cell.LSTMCell in tensorflow with num_units = num_units_2. Second, I need a DropoutWrapper as: Now, I want to apply dropout and recurrent_dropout similar to the Keras code. Therefore, I found that tensorflow's implementation of dropout will apply a different dropout mask at every time step unless variational_recurrent is set to True (Yet I'm not sure how variational_recurrent works in details). Additionally, I'm not sure if the LSTM in Keras apply different Mask at each time step as well. Second, I was confused about the difference between the output_keep_prob and the state_keep_prob as both mention: output_keep_prob: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added... Any help is much appreciated!!",https://stackoverflow.com/questions/51824310,7886651,Documentation Replication on Other Examples
51856041,Input dimension for tf.nn.in_top_k,"I am following the TF documentation with respect to in_top_k: https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k where it states that targets should be a vector of batch size Nevertheless, I'm continuously prompted with the following error: In my case, my predictions and targets inputs have the following shapes: From my understanding, there is still something not ok with my targets label, and despite using different combinations of tf.reshape or tf.squeeze I cannot seem to find where is the error. Is there any way to work around this issue?",https://stackoverflow.com/questions/51856041,10228489,Documentation Ambiguity
51858891,Loading a NumPy array into a Tensor,"According to the TensorFlow webpage at (https://www.tensorflow.org/versions/r1.3/programmers_guide/datasets), the tf.read_file can be used to load an image file from a given filename, and convert it to a Tensor: In my case however, I want to load a NumPy array rather than an image. So the filename above points to a NumPy array on my machine. If I were to use tf.read_file(filename) on this filename, then according to the documentation, this function returns a string Tensor (a byte array). How can I convert this into a Tensor representing the data in the NumPy array? Is there an equivalent function to tf.image.decode_image() for decoding a NumPy array?",https://stackoverflow.com/questions/51858891,3320135,Documentation Replication on Other Examples
51858970,"tf.gradients() sums over ys, does it?","https://www.tensorflow.org/versions/r1.6/api_docs/python/tf/gradients In the documentation for tf.gradients(ys, xs) it states that I am confused about the summing part, I have read elsewhere that this sums the derivatives dy/dx across the batch for every x in the batch. However, whenever I use this I fail to see this happening. Take the following simple example: This gives the following output: This is the output I would expect, simply the derivative dy/dx for every element in the batch. I don't see any summing happening. I have seen in other examples that this operation is followed by dividing by the batch size to account for tf.gradients() summing the gradients over the batch (see here: https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html). Why is this necessary? I am using Tensorflow 1.6 and Python 3.",https://stackoverflow.com/questions/51858970,8820311,Documentation Ambiguity
51859776,lambda layer function definition without tf.keras.backend (Python Keras Package),"tf.keras.layers.Lambda documentation explains how a function can be defined in a lambda layer. That document provides the following function as an example, But according to that, tf.keras.backend must be used to conduct operations on the input Tensor object. Is there any way we can use default python packages and user-defined function to define the steps of a lambda function. If it's possible, please be kind enough to provide some examples.",https://stackoverflow.com/questions/51859776,261433,Inadequate Examples
51883196,Tensorflow: tf.reverse_sequence - seq_dim and batch_dim,"I am trying to learn Tensorflow and was looking at tf.reverse_sequence. It has two parameters seq_dim and batch_dim. From the documentation given here I understand that setting batch_dim = 0 means we go through the the rows from top and setting seq_dim = 1 means we go through columns from left to right but what do these numbers mean? I can't understand from the documentation when I should set batch_dim = 1 or 2. I tried reverse_sequence on x = [[1,2,3], [4,5,6], [7,8,9]] and got [[3,2,1], [6,5,4], [9,8,7]] with batch_dim = 0 and seq_dim = 1. But I always get errors when I change seq_dim and batch_dim values from 1 and 0 respectively. Could someone explain the meaning of these values?",https://stackoverflow.com/questions/51883196,7460955,Lack of Alternative Solutions/Documentation
67361081,Tensorflow 2 - what is 'index depth' in tensor_scatter_nd_update?,Please explain what is index depth of tf.tensor_scatter_nd_update. Why indices is 2D for 1D tensor?,https://stackoverflow.com/questions/67361081,4281353,Documentation Replicability
67362672,tensorflow 2 - how to directly update elements in tf.Variable X at indices?,Is there a way to directly update the elements in tf.Variable X at indices without creating a new tensor having the same shape as X? tf.tensor_scatter_nd_update create a new tensor hence it appears not updateing the original tf.Variable. tf.Variable assign apparently needs a new tensor value which has the same shape of X to update the tf.Variable X.,https://stackoverflow.com/questions/67362672,4281353,Documentation Replication on Other Examples
48348775,How do I profile a tf.data.Dataset?,I'm trying to understand what bottlenecks I have in my input_fn with tf.data.Dataset so I figured I'd use tf.profiler but it only shows the iterator op. How can I get the profiler to output the relevant ops in my Dataset pipeline instead? Output:,https://stackoverflow.com/questions/48348775,7287271,Documentation Replicability
48369961,Tensorflow Estimator API with large number of features,"I am using Tensorflow Estimator API to train some models, but what found that I have to use tf.feature_column for every feature I have dataset consists of this columns : why do I have to make them like this .. etc, seems not the best way to handle the preprocessing for the features. is there a better way or we will be in this pain if we ever go to tf.estimator",https://stackoverflow.com/questions/48369961,9195600,Documentation Replication on Other Examples
48396599,"Canonical Tensorflow ""for loop""","What is the canonical way of running a Tensorflow ""for loop""? Specifically, suppose we have some body function which does NOT depend on the loop iteration, but must be run n times. One might think that a good method might be to run this inside of a tf.while_loop like this: In fact, that is precisely what the highest rated answer in this question suggests: How can I run a loop with a tensor as its range? (in tensorflow) However, the tf.while_loop docs specifically say If you put a counter in the body, then it seems that that condition is violated. So it seems that there must be a different way of setting up a ""for loop"". Furthermore, even if there is no explicit error, doing so seems like it will create a dependency between iterations meaning that I do not think they will run in parallel.",https://stackoverflow.com/questions/48396599,3309610,Lack of Alternative Solutions/Documentation
67462119,How to convert TF object detection models to tf.keras?,"Is there a way to convert TF object detection models from here to tf.keras? If not, how can I get the total number of variables in the model?",https://stackoverflow.com/questions/67462119,15571511,Documentation Replication on Other Examples
48445751,Keras: Constrained dictionary search with CTC decode,"I'm trying to constrain the CTC decoding to a specific (external) dictionary in Keras with a the tensorflow backend. In the tensorflow documentation for Keras' ctc_decode, it is written that when greedy=False a dictionary will be used. Here is the documentation for tf.nn.ctc_beam_search_decoder, which will be called by this option as far as I understand. Since there is no way to pass an external dictionary or language model (to constrain the search), I assume that with greedy=False it creates its own dictionary from the training data. Is this correct? Is there a way to constrain the search to a specific (external) dictionary?",https://stackoverflow.com/questions/48445751,1578793,Documentation Replication on Other Examples
48471926,In Tensorflow's Dataset API how do you map one element into multiple elements?,"In the tensorflow Dataset pipeline I'd like to define a custom map function which takes a single input element (data sample) and returns multiple elements (data samples). The code below is my attempt, along with the desired results. I could not follow the documentation on tf.data.Dataset().flat_map() well enough to understand if it was applicable here or not. Results: Desired results:",https://stackoverflow.com/questions/48471926,4790871,Documentation Replicability
67523944,"Tensorflow2 - Use ""tf.data.experimental.make_csv_dataset"" with ""tf.keras.preprocessing.timeseries_dataset_from_array""","I am trying to get TensorFlow to read +100 CSV files that don't fit in memory (+1GB size each). The files contain time series data (EEG signals), with the labels in the first column. From the TensorFlow documentation it seems like I should be able to use the tf.data API to load my data off-disk. For the sake of simplicity and reproducibility, let's consider the following ""sample_data.csv"" dataset: I've tried using tf.data.experimental.make_csv_dataset to load the CSV files into tf.data.Dataset objects, and then tf.keras.preprocessing.timeseries_dataset_from_array to process the data into sliding windows with overlap. For the dataset above, I would do: Which we can check works correctly by looking at the output from list(input_data.as_numpy_iterator()). We can then feed input_data to the next function: Which unfortunately throws this error: I also tried using my_dataset = input_data.window(3, shift=2) (see the tf.data.Dataset.window documentation) and it didn't throw an error, but it seems to be returning an empty dataset? See ""_VariantDataset shapes: (None,)"" in the output: If I load the ""sample_data.csv"" in memory using pandas and then feed the timeseries_dataset_from_array function a numpy array instead, it works correctly. Any ideas on how to solve this? What's the best method to input overlapping windows from off-memory time-series data into TensorFlow? Thank you!",https://stackoverflow.com/questions/67523944,6395699,Documentation Replication on Other Examples
67542939,tensorflow function in graph mode (tf.function) is slow when return object,"I have a function wrapped by @tf.function, the function was quick when it doen't return any object. But it becomes significant slower when it return an object. Any way to boost the performance even when it has to return an object? Below is a toy example",https://stackoverflow.com/questions/67542939,10545499,Documentation Replication on Other Examples
68162847,Wrapping image_data_generator.flow_from_dataframe in tf.data pipeline.. what step should I take?,"I am trying to wrap the image_data_generator.flow_from_dataframe using tf.data but I am finding difficulties, I would really appreciate some help?",https://stackoverflow.com/questions/68162847,13430221,Documentation Replicability
49155119,Using TensorFlow ``grad_loss / grad_ys`` parameter to add gradients,"I'm trying to use the grad_loss parameter in optimizer.minimize(loss, grad_loss=) to modify the network gradients with existing gradients. I followed the comments here: Use of grads_ys parameter in tf.gradients - TensorFlow and I would like to run a toy example, in which I recreate the default 1 values for grad_ys, as specified in the documentation. Here's the relevant code segment: The first part extracts gradients using compute_gradients. The last line computes gradients of the loss function loss_op but attempts to use 1-filled vectors for the grads. As far as I understand, this should behave similarly to funning minimize without the grad_loss parameter. Unfortunately, this fails since it expects grad_loss to be a Tensor (and have a dtype) and not a list. Looking into gradients_impl.py I see that the function expected grad_loss to be of the same dimension as loss (which in this case is a scalar). I would appreciate any assistance in this simple example - how do I add elements to the gradients this way? EDIT: I guess the question boils down to the definition of grad_loss: ""A Tensor holding the gradient computed for loss."" How do I generate such a tensor from a set of gradients obtained by compute_gradients? Thanks.",https://stackoverflow.com/questions/49155119,635622,Documentation Replication on Other Examples
68194101,How to determine class frequencies from (batched) tf.data.Dataset object,I would like to understand the frequencies of cat vs dog in my dataset to understand the balance of the dataset (which will then determine what classification metric to use and/or what class weights to assign). How can I do this please with a tf.data.Dataset object as generated below?,https://stackoverflow.com/questions/68194101,,Documentation Replication on Other Examples
49346599,tf.feature_column.input_layer returning wrong shape of tensor,"When I use tf.feature_column.input_layer, it seems to be returning a tensor with shape [number of features, batch size] when it should be returning the opposite - [batch size, number of features]. The code is: The documentation clearly states I am using my own custom estimator function (deep_q_model_test in the code above), and within that I have the first line as: And the shape shown in my print (and also after inspecting the tensor board) is: I am using the prebuilt pandas input function as well to be fed into it: tf.estimator.inputs.pandas_input_fn. The feature columns were built with it being: Also the neural net is actually trains and inspecting the tensorboard it does show the shape is flipped. The problem with this is when I run a batch size which is different, lets say for prediction where i want to only predict 1 observation, it wont work. EDITED Adding in the actual code for the model",https://stackoverflow.com/questions/49346599,4148062,Documentation Replication on Other Examples
49370940,One hot encoding characters,"Is there a possibilty to one-hot encode characters of a text in Tensorflow or Keras? Beside that, tf.keras.preprocessing.text.one_hot works really strange, since the response does not really seem one-hot encoded, since the following code: Lead to this result: Every time I run this program, the output is a different 3d vector, sometimes it is [1,1,1] or [2,1,1]. The documentation says, that unicity is not guaranteed, but this seems really senseless to me.",https://stackoverflow.com/questions/49370940,3921232,Documentation Ambiguity
49405794,Why tensor_summary doesn't work?,"I use tf.summary.tensor_summary in my code, following this: https://www.tensorflow.org/api_docs/python/tf/summary/tensor_summary But I didn't see anything new in tensorboard, and tensorboard also printed some warnings: How to make this work? Do I need install some plugin? I didn't find any docs on this. UPDATE: So I here is how I create my summary: Then I use a MonitoredTrainingSession, by default it will save the summary, and I can see my loss and wealth scalar summry, but not this wealth_tensor summary.",https://stackoverflow.com/questions/49405794,2218586,Lack of Alternative Solutions/Documentation
49418325,"Use ""tf.contrib.factorization.KMeansClustering""","Referring to this Link, (the Link) I try to practice using tf.contrib.factorization.KMeansClustering for clustering. The simple codes as follow works okay: My question is why would this ""input_fn"" code does the trick? If I change the code to this, it will run into an infinite loop. Why?? From the document (here), it seems that train() is expecting argument of input_fn, which is simply a A 'tf.data.Dataset' object , like Tensor(X). So, why do I have to do all these tricky things regarding lambda: tf.train.limit_epochs()? Can anyone who is familiar with the fundamental of tensorflow estimators help to explain? Many Thanks!",https://stackoverflow.com/questions/49418325,7270211,Inadequate Examples
49435335,Verify that keras GaussianNoise is enabled at train time when using inference with edward,"I would like to check if noise is truly added and used during training of my neural network. I therefore build my NN with keras like this: Then I use edward to execute inference: According to the documentation, the closest I get to this is through ed.MAP's run() and update() functions. Preferably, I would do something like this: Apparently the way I use GaussianNoise doesn't seem to add noise to my input since the following unittest fails: I also made sure that during the inference.update(...) the assertion assert tf.keras.backend.learning_phase() == 1 passes. Where could have something gone wrong here?",https://stackoverflow.com/questions/49435335,7353970,Documentation Replication on Other Examples
49494632,How can we do tf.tile with extra dimensions in the output?,"Suppose I have a tensor = tf.constant([1, 2]), what's the best way of creating a tensor of [[tensor, tensor], [tensor, tensor]], which is [[[1, 2], [1, 2]], [[1, 2], [1, 2]]]? The use case looks similar to tf.tile. However, tf.tile does not create the extra dimensions.",https://stackoverflow.com/questions/49494632,2100910,Documentation Replicability
49497353,Use tf.image.random_flip_left_right method to batch of images,Does any way to use tf.image.random_flip_left_right method for batch of images? Thanks.,https://stackoverflow.com/questions/49497353,3153317,Documentation Replicability
49500873,Using tf.boolean_mask with greater_equal to slicing a tensor,I am trying to use tf.boolean_mask to select certain values from a tensor. To which I get the following. I had expected to get How can I do this?,https://stackoverflow.com/questions/49500873,868166,Documentation Replicability
49542954,What are tf.TensorArray objects?,"I am not able to get an understanding of tf.TensorArray objects. What are they and where are they needed? I have some (highly likely faulty) understanding - they are used in while_loops especially to write the information to the loop_state. If somehow we end up increasing the number of tensors or their dimensions across iterations it throws back an error. hence normal way to pass the loop_state across iterations collecting information from each iteration, which would be passing a list, would throw back an error. So we create tf.TensorArray objects and write the information to them at each iteration and pass these tf.TensorArray objects across loops and for some reason that way it is able to pass through I couldnt find any blog or documentation explaining the working of a tf.TensorArray objects and documentation doesn't help much either So, if this is not the best place to be asking this question, kindly direct me to nearest help.",https://stackoverflow.com/questions/49542954,6546694,Lack of Alternative Solutions/Documentation
49555016,Compute gradients for each time step of tf.while_loop,"Given a TensorFlow tf.while_loop, how can I calculate the gradient of x_out with respect to all weights of the network for each time step? Some notes",https://stackoverflow.com/questions/49555016,2393597,Documentation Replicability
49605958,what's the purpose of tf.nn.dropout?,"I notice there are two APIs in TensorFlow concerning with dropout, one is tf.nn.dropout, the other is tf.layers.dropout. I just wonder what's the purpose of tf.nn.dropout? According to https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf, there should be a parameter to distinguish between training and testing stage. I see tf.layers.dropout provides the proper behavior, so why another function tf.nn.dropout? Anyone has any idea? Thanks.",https://stackoverflow.com/questions/49605958,6902674,Documentation Ambiguity
68354367,Getting an error when using tf.keras.metrics.Mean in functional Keras API,"I'm trying to add a Mean metric to a Keras functional model (Tensorflow 2.5), and am getting the following error: Here is the code: If I remove the following line (from which the exception is thrown): the code works as expected. I Tried disabling eager execution, but I get the following error instead: The above usage was pretty much copied from the tf.keras.metrics.Mean documentation (see Usage with compile() API)",https://stackoverflow.com/questions/68354367,6133787,Lack of Alternative Solutions/Documentation
49619995,How to control when to compute evaluation vs training using the Estimator API of tensorflow?,"As stated in this question: The accepted answer suggested the use of Experiment (which is deprecated according to this README). All I found on online points towards using the train_and_evaluate method. However, I still do not see how to switch between the two processes (train and evaluate). I have tried the following: Here is what I think my code should be doing: However, I get the following logs: From the logs, it seems that the training stops after the first evaluation step. What am I missing from the documentation? Could you explain me how I should have implemented what I think my code is doing? Additional info I am running everything using the MNIST dataset, which has 50,000 images in the training set, so (I think) the model should run for *num_epochs*50,000/batch_size ≃ 7,000 steps* I sincerely appreciate your help! EDIT: after running experiments I realize that max_steps controls the number of steps of the whole training procedure, not just the amount of steps before computing the metrics on the test set. Reading tf.estimator.Estimator.train, I see it has a steps argument, which works incrementally and is bounded by max_steps; however, tf.estimator.TrainSpec does not have the steps argument, which means I cannot control the number of steps to take before computing metrics on the validation set.",https://stackoverflow.com/questions/49619995,4833773,Lack of Alternative Solutions/Documentation
49633383,tensorflow tf.nn.rnn_cell.BasicLSTMCell,"I'm trying to play with ""tf.nn.rnn_cell.BasicLSTMCell"" with TF on python. Reading here ""https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell"", in the ""init"" method, I see the param ""num_units"" with description ""int, The number of units in the LSTM cell."" But ... wait a moment ""number of UNITS""? Which type of units? The class is called ""...LSTMCell"" but from ""num_units"" it seem that we are speaking about a layer, not a single neuron. I'm confused. Any help is appreciated. TIA",https://stackoverflow.com/questions/49633383,7835905,Documentation Replicability
49662470,Tensorflow global_step TypeError,"I'm adding Tensorboard to an existing small Tensorflow project that I know works to practice working with Tensorboard but I get a type error that global_step must be a string or tensor, however I have assigned global_step to a tf.Variable(0, name='global_step', trainable=False) just like the documentation and every example I see online. Any idea of what I'm missing would be super appreciated. ---&gt; 70 [summary_merge, tf.train.global_step(sess, global_step_tensor) ,update_model, weights], feed_dict) TypeError: Fetch argument 0 has invalid type , must be a string or Tensor. (Can not convert a int into a Tensor or Operation.)",https://stackoverflow.com/questions/49662470,5780994,Lack of Alternative Solutions/Documentation
50036568,compute the gradient (dy/dx) in tensorflow with the same size of y,"I want to calculate the gradient of dy/dx and get the gradient in size of y plus the dimensions of x to indicate diffrentiation wrt variable in x. for example if Y[100x1]=A[100x50]X[50x1] so return Y[100x1x50x1]. (for each parameter in x give me the diffrentiation of Y) I've tried the tf.gradients operator: https://www.tensorflow.org/api_docs/python/tf/gradients However, it returns the sum(dy/dx) instead of dy/dx",https://stackoverflow.com/questions/50036568,9651398,Documentation Replication on Other Examples
50047909,Benefit of applying tf.image.per_image_standardization() over batch_norm layer in Tensorflow?,"What is the benefit in applying tf.image.per_image_standardization() before the first layer of the deep neural network over adding the Batch_Norm layer as the first layer? In order to normalize the [0.0, 255.0] float value image pixels before feeding into the network, which method would be suitable?",https://stackoverflow.com/questions/50047909,8865844,Documentation Replication on Other Examples
68782423,TensorFlow reshape after text_dataset_from_directory,"After the data is loaded with tf.keras.preprocessing.text_dataset_from_directory, the shape is (10, 1). However, the shape needs to be (10,). How could the shape be changed?",https://stackoverflow.com/questions/68782423,14637258,Documentation Replicability
50054453,Tensorflow shape inference static RNN compiler error,"I am working on OCR software optimized for phone camera images. Currently, each 300 x 1000 x 3 (RGB) image is reformatted as a 900 x 1000 numpy array. I have plans for a more complex model architecture, but for now I just want to get a baseline working. I want to get started by training a static RNN on the data that I've generated. Formally, I am feeding in n_t at each timestep t for T timesteps, where n_t is a 900-vector and T = 1000 (similar to reading the whole image left to right). Here is the Tensorflow code in which I create batches for training: The tf.nn.static_bidirectional_rnn documentation claims that the input must be a ""length T list of inputs, each a tensor of shape [batch_size, input_size], or a nested tuple of such elements."" So, I go through the following steps in order to get the data into the correct format. Without altering the batch any further, I make the following call: Note that I do not explicitly tell Tensorflow the dimensions of the matrices (this could be the problem). They all have the same dimensionality, yet I am getting the following bug: At which point in my stack should I be declaring the dimensions of my input? Because I am using a Dataset and hoping to get its batches directly to the RNN, I am not sure that the ""placeholder -&gt; feed_dict"" route makes sense. If that in fact is the method that makes the most sense, let me know what that looks like (I definitely do not know). Otherwise, let me know if you have any other insights to the problem. Thanks!",https://stackoverflow.com/questions/50054453,9707928,Documentation Replication on Other Examples
68788593,Apply Tensorflow tf.keras.initializers.GlorotNormal(seed=1) to tf.Variable,How to apply the initializer to the tf.Variable function? Am I on the right track? I want the shapes to be as follow -,https://stackoverflow.com/questions/68788593,13494561,Documentation Replicability
50078749,Tensorflow-hub Text-Module Preprocessing,"I'm playing around with the new Modules which are available on the tensorflow-hub (which I really like - thanks for that). Whats unclear to me, is the preprocessing which should take place when feeding a sentence. The module documentation says, that in the preprocessing step the inputj sentences gets splitted at the spaces. However, when I run the following program, I only get a single vector: How do I get the embeddings for each word, and what does the single vector represents? A fixed-dimensional representation of the sentence, the Unknown-Word embedding or something else? Thanks in advance! Edit: It seems the result is a combined embedding created with tf.nn.embedding_lookup_sparse. (Thanks for the confirmation @svsgoogle)",https://stackoverflow.com/questions/50078749,863543,Documentation Replication on Other Examples
50083475,"what we do wtih tf.stack([tf.range(tf.shape(self.a)[0], dtype=tf.int32), self.a], axis=1)",What is done with this command? What does tf.stack stands for what?,https://stackoverflow.com/questions/50083475,9708375,Documentation Replicability
68839011,Python/Keras: LeakyRelu using tensorflow,"I am having problems installing keras. The following are giving me too much trouble to get around (even when doing updates on the terminal): So instead of initialising a ANN with ann = Sequential(), I do ann = tf.keras.models.Sequential(). This by importing: I would like to use LeakyReLU as an activation function. However, this one seems to be different to implement and the keras documentation is not helping me that much compared to how others tend to do. I've seen that ann.add(LeakyReLU(alpha=0.05)) is needed. However, what about the other parameters like unit or input_dim? How can I implement this using my code?",https://stackoverflow.com/questions/68839011,12463547,Lack of Alternative Solutions/Documentation
50486241,Tensorflow: tf.while_loop() with vector condition,"The function tf.while_loop() (https://www.tensorflow.org/api_docs/python/tf/while_loop) repeats the body ""b"" while the condition ""c"" is true. For example: How can I adjust this, when the condition is a vector, e.g. ? The problem is that the above returns a vector (instead of True or False), same as e.g. I need something that returns true when all the vector elements are true, and false otherwise. I would suggest But this does not work, with the following error: Any ideas?",https://stackoverflow.com/questions/50486241,2594778,Documentation Replication on Other Examples
50820781,quesion about the axis of tf.stack(),"I read the doc of tf.stack() on tensorflow stack . There is an example on the page: what I don't understand is the second example where axis=1. From the result it seems it converts the three inputs rows to columns first and then put them toghter along the axis=1, but I think the result should be can anyone help explain this? Thanks!",https://stackoverflow.com/questions/50820781,1347796,Inadequate Examples
50825446,Restoring a trained generator network in DCGAN,"I have a question regarding the saving and storing models in tensorflow. I know how to save a model with tf.train.Saver() and load it later through meta file. My problem is this: I have trained a variant of DCGAN (Deep Convolution GAN), now I want to use only generator network for other tasks. Unfortunately, I do not know how to get entire generator network such that if I feed it with a new vector z, it generates an output based on the trained parameters. All the example I found in the stackoverflow, or tensorflow documentation, just mention very simple operations with two numbers. This is not I want. I want to understand if you have trained a giant network, say with 50 layers, how to load it and feed it with new input and get the output without going into the different parameters and layers in the trained network. I want to load it as a blackbox.",https://stackoverflow.com/questions/50825446,9932068,Documentation Replication on Other Examples
50828432,tensorflow: how to make distributed training with tf.estimator.train_and_evaluate,"refer to tf.estimator.train_and_evaluate example on https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate I want to make the example to run distributed, but it seems doesn't work that the training process did not start in distributed mode. my question is : what can I do For Making the source code run distributed? platform: system: Ubuntu 18 tensorflow: v1.8 the following log is my source code and operate step: 1 source code: 2 run it on single mode in which the train run well. looking the following log clxman@clxman-VirtualBox:~/test$ python test_c.py 3 config TF_CONFIG and start script by command line Respectively 4 the following log is from the chief session, we can see the training process did not start. the string ""data"" means that the training process has called the data_input function which feeds train data",https://stackoverflow.com/questions/50828432,9933306,Documentation Replication on Other Examples
50840759,"Incorrect name returned in Tensorflow causes ""Tensor which does not exist"" error while invoking get_tensor_by_name","As per the documentation TensorFlow would append ""_1"", ""_2"", and so on to the name in tf.Graph namespace, in order to make it unique. Here I define two convolutional operations. It is expected that the first one will be named as ""conv2d"" and second one ""conv2d_1"". But when I try to obtain the name of the second convolution it returns ""conv2d_2"". I causes error when I try to invoke get_tensor_by_name. Here is the code: I could not understand why conv_out2.name returns ""conv2d_2"" instead of ""conv2d_1""",https://stackoverflow.com/questions/50840759,8284911,Documentation Ambiguity
69587392,How to apply map() when working with a batched Dataset?,"I am creating a timeseries Dataset using tf.keras.utils.timeseries_dataset_from_array. According to the docs, it returns a tf.data.Dataset instance. I also pass the batch size argument when calling the timeseries_dataset_from_array function, so my dataset is a BatchDataset. I am using map on this batched Dataset (ds), passing my_fun. Data in the code below is a pandas dataframe containing continuous timesteps. What does the my_fun function expect as input parameters - aka what does it apply on each iteration? Whole batches of shape (samples, sequence_length, features) or a single element of shape (None, sequence_length, features)? I am confused because when I print the single argument that I define in my my_fun function, it yields a shape of (None, None, features), but I cannot inspect it further... My code is inspired from the TF tutorial (see split_window function) https://www.tensorflow.org/tutorials/structured_data/time_series",https://stackoverflow.com/questions/69587392,17161135,Documentation Replication on Other Examples
50997477,Keras and Tensorflow: Saving non-sequential model weights,"(I'm using Tensorflow 1.8.0 ...) The documentation from Keras on how to save a model mentions no difference between saving a sequential model vs. one created from the functional API. But, all of the following blocks of code fail: or or or They raise a NotImplementedError. In the Keras module, the relevant lines are which shows up in .save and get_config (the latter is also called by to_json and to_yaml. The only thing that DOES work is the following in which case the weights are saved successfully and can be successfully loaded with net.load_weights. However, replacing the second line of the above blocks of code, net = tf.keras.models.Model(), with net = tf.keras.models.Sequential(), making net a sequential model, allows everything above to work. Is it really not possible to save the structure of a Keras model made with the functional API (using Model rather than Sequential)? Right now, can we only save weights?",https://stackoverflow.com/questions/50997477,9878957,Lack of Alternative Solutions/Documentation
50998956,How to use tf.nn.avg_pool to get same results as by tf.reduce_mean,"In the following code, I want to use tf.nn.avg_pool to get the same results as when I use tf.reduce_mean. But how to fix the parameters of tf.nn.avg_pool to get similar results?",https://stackoverflow.com/questions/50998956,9854153,Documentation Replication on Other Examples
69777802,How to create the same structure of tf.data.experimental.make_csv_dataset from pandas,"tf.data.experimental.make_csv_dataset creates a TF Dataset ready for Kears supervised training. How to create the same from Pandas? Tried below but the label is dictionary instead of int32. By the way, the data structure ready for Keras supervised training is (features, labels) but which document defines it?",https://stackoverflow.com/questions/69777802,4281353,Lack of Alternative Solutions/Documentation
51895395,How to pass tf.glorot_uniform_initializer() into tf.Variable(0 not get_variable(),"For tensorflow I would like to create a tf variable using tf.Variable and usingtf.glorot_uniform_initializer()with the shape = [ 2 , 3 ] I know I can use but it may potential gets scope error, tf.Variable() won't. If anyone can help me, I would much appreciate it.",https://stackoverflow.com/questions/51895395,8461300,Documentation Replication on Other Examples
51923689,tf strided_slice equivalent in numpy?,"I'm trying to ""backport"", in a sense, the tensorflow strided slice operation tf.strided_slice (https://www.tensorflow.org/api_docs/python/tf/strided_slice) into numpy, but I can't figure out how to do it. More specifically, given the arguments for a call to strided slice, how would I manipulate these into array indices to produce the same result that a call to tf.strided_slice would produce? (I know that tf.strided_slice is for use on tensors and numpy slice indexing is for use on numpy arrays)",https://stackoverflow.com/questions/51923689,5294613,Documentation Replication on Other Examples
72329108,Is there a simple way to know which Tensorflow ops have a registered GPU kernel?,"I have been trying to optimize some Tensorflow code that was pretty memory inefficient (use of large dense tensors containing very sparse information), and would thus limit batch size and scalability, by trying to make use of SparseTensors. After some struggle I finally come up with a decent solution with satisfactory speedup on CPU and very low memory usage, and when the time comes to use a GPU I realize that the previous memory inefficient is orders of magnitude faster... Using tensorboard profiling I've discovered that two of the operations I have used in my """"optimized"""" version only run on CPU (namely UniqueV2 and sparse_dense_matmul), but I could not see any hint of that in the documentation. The only related piece of documentation states: In turn there is nothing in the tf.cast documentation hinting that the op has no GPU kernel. Thus, is there a simple way to know whether a TF ops has a registered GPU kernel, without having to use a GPU to find it out? The custom ops guide suggest that this could be seen by looking at the ops C files, but this seems a rather cumbersome way to do it... I'm using TF v2.8 Thanks!",https://stackoverflow.com/questions/72329108,19167343,Inadequate Examples
53828895,Does distributed TensorFlow requires a distributed file system to save checkpoints?,"I am trying to run some sample codes based on this tf official tutorial. And I watched this video which is really nice. As mentioned in the video above, the chief worker is responsible for saving checkpoints, and it's implemented by tf.train.MonitoredTrainingSession. Then I thought that only the chief worker needs a directory to save checkpoints. When I run the codes with ps0 on machine1, worker0 on machine2, everything seems ok. But when I run with ps0, worker0 on machine1, ps1 and worker1 on machine2, errors occurs, and the error in worker0's log is like: But the directory ./train_dir/dist_worker_0/model.ckpt-0_temp_cf2b45f059b74507a65cae9b7a9ea5b4 really exists (on machine1). part of the code (actually from the official tutorial): I searched some questions on stackoverflow and issues on github, answers to similar questions suggest to use HDFS. Doesn't ""Chief worker is responsible for saving chechpoints"" mean that I only need a local directory on the machine where the chief worker is located? Am I misunderstanding something? Do I really need to use HDFS or such?",https://stackoverflow.com/questions/53828895,4922937,Lack of Alternative Solutions/Documentation
54929570,Not able to use tf.stop_gradient,I am currently trying to understand how tf.stop_gradient works and to that end i used this small code snippet Error: TypeError: Fetch argument None has invalid type If i comment out the line using tf.stop_gradient the code runs fine and as expected. Please guide me on how to use tf.stop_gradient,https://stackoverflow.com/questions/54929570,6546694,Documentation Replication on Other Examples
54934603,"tensorflow documentation says ""WARNING: Avoid writing code which relies on the value of a Variable..."" what does it mean?",The tf.Variable documentation contains the following warning: I don't quite understand what time means and why the example above is broken. What does it mean that one cannot rely on the value of a Variable? Is it possible to have an example where the code above works not as expected?,https://stackoverflow.com/questions/54934603,1754568,Requesting (Additional) Documentation/Examples
54945641,"keras, model still setting expected input shape from training despite input_shape(None, ...)","I have a simple CNN model written in the tf.keras framework, which I wish to use with variable input size. According to this ""documentation"" I can use variable input size by setting input_shape=(None, None, n_channels), and I have used a GlobalMaxPooling2D layer before my dense layer to standardize the input to the dense layer. Yet when I train the model with one size of image and try to predict on a different size I get the error: This is the code used to define my model: So in essence my question is why is keras still defining an expected input shape, and is there any way to disable this implicit standardize_input_data that's going on?",https://stackoverflow.com/questions/54945641,5445177,Documentation Replicability
54966581,tf.boolean_mask not accepting the axis argument,"Here is my code: Error, I'm getting: The tf.boolean_mask() is not accepting the axis argument but is a valid argument as can be seen in the documentation: https://www.tensorflow.org/api_docs/python/tf/boolean_mask",https://stackoverflow.com/questions/54966581,9902786,Documentation Ambiguity
54985037,How to convert TF Tensor holding value into Tensor holding categorical values,I'm paring TFRecords which provide me a label as numerical value. But I need to convert this value into categorical vector while I'm reading proto records. How can I do that. Here is code snippet for reading the proto records: I know that there is tf.keras.utils.to_categorical function but it does not take a Tensor as an input.,https://stackoverflow.com/questions/54985037,779856,Documentation Replication on Other Examples
54989442,"RNN in Tensorflow vs Keras, depreciation of tf.nn.dynamic_rnn()","My question is: Are the tf.nn.dynamic_rnn and keras.layers.RNN(cell) truly identical as stated in docs? I am planning on building an RNN, however, it seems that tf.nn.dynamic_rnn is depricated in favour of Keras. In particular, it states that: But I don't see how the APIs are equivalent, in the case of variable sequence lengths! In raw TF, we can specify a tensor of shape (batch_size, seq_lengths). This way, if our sequence is [0, 1, 2, 3, 4] and the longest sequence in the batch is of size 10, we can pad it with 0s and [0, 1, 2, 3, 4, 0, 0, 0, 0, 0], we can say seq_length=5 to process [0, 1, 2, 3, 4]. However, in Keras, this is not how it works! What we can do, is specify the mask_zero=True in previous Layers, e.g. the Embedding Layer. This will also mask the 1st zero! I can go around it by adding ones to the whole vector, but then thats extra preprocessing that I need to do after processing using tft.compute_vocabulary(), which maps vocabulary words to 0 indexed vector.",https://stackoverflow.com/questions/54989442,4729764,Documentation Replication on Other Examples
55005915,How to resize image to put into tf.train.Example,"I have an image (JPEG or PNG) as a byte buffer (read from the internet), and this is the way I was putting it in a tf.train.Example before: However, for my usecase, the images are too big, so I'd like to resize them either before I put them in the tf.train.Example or just after (whichever is easiest). Here's what I'm trying: I suspect this is valid right up until I actually try to put it in the tf.train.Example, at which point it tells me TypeError: &lt;tf.Tensor 'EncodeJpeg:0' shape=() dtype=string&gt; has type Tensor, but expected one of: bytes. I've tried figuring out how to get the Tensor into a BytesList or something like it, but I haven't been able to find any documentation for this. I suspect there may be a better way to approach the entire process however. How can I do this the right way?",https://stackoverflow.com/questions/55005915,1129436,Lack of Alternative Solutions/Documentation
55044905,"Tensorflow low level api, batch normalization problem","The tf.layers.batch_normalization documentation says it will be removed in a future version, and should be replaced by tf.keras.layers.BatchNormalization, but i cannot find a way to replace the functionality using tensorflow low level api. which outputs: If we instead use keras as suggested in the documentation we get an empty output: Since UPDATE_OPS is empty, the model is unable to update the batch normalization moving_avg_mean and moving_avg_variance during training using keras (resulting in a much higer test error). Any suggestion how to solve this is greatly appreciated! The example above is taken from an older post of how to use tf.layers.batch_normalization",https://stackoverflow.com/questions/55044905,10455493,Documentation Replication on Other Examples
72983226,Evalulate Tensorflow Keras VS KerasRegressor Neural Network,"I'm attempting to find variable importance on a Neural Network I've built. Using tensorflow, it seems you can use either the tensorflow.keras way, or the kerasRegressor way. Admittedly, I have been reading documentation / stack overflow for hours and am confused on the differences. They seem to perform similarly but have slightly different pros/cons. One issue I'm running into is when I use tf.keras to build the model, I am able to clearly compare my training data to my validation/testing data, and get an 'accuracy score'. But, when using kerasRegressor, I am not. The difference here is the .evaluate() function, which kerasRegressor doesn't seem to have. Questions: kerasRegressor Code: Now if I want to test the accuracy, I have to use .predict() tf.Keras neural Network: This is the evaluation I need to see, and cannot with kerasRegressor: Possible Workaround, still error:",https://stackoverflow.com/questions/72983226,10817864,Documentation Ambiguity
55094952,Understanding Tensorflow control dependencies,"I am trying to gain a stronger grasp of TensorFlow. I came across the concept of control dependencies. I understand that the order of ops as specified by us is not really relevant to Tensorflow during execution. In order to optimise the speed of execution TensorFlow decides its own order of calculating nodes. But we can customise order of execution by using tf.control_dependencies. I am not able to understand the use cases of the function. Can anyone direct me to some resource(other than the documentation) or explain the working of this function? An example: The output of the code is 8. So I infer that since z=x+y,the assign node has not been evaluated(right?). But doesn't this mean that the result of tensorflow may be erroneous? This means we need to create new nodes during every operation to force TensorFlow to calculate all the nodes leading up to the result. But in say training a neural network with 10000 steps if each step creates a new set of 1000 weights/parameters won't the space complexity explode?",https://stackoverflow.com/questions/55094952,10726895,Inadequate Examples
55109696,TensorFlow - Difference between tf.keras.layers.Layer vs tf.keras.Model,"Reading through the documentation of implementing custom layers with tf.keras, they specify two options to inherit from, tf.keras.Layer and tf.keras.Model. Under the context of creating custom layers, I'm asking myself what is the difference between these two? Technically what is different? If I were to implement the transformer encoder for example, which one would be more suitable? (assuming the transformer is a only a ""layer"" in my full model)",https://stackoverflow.com/questions/55109696,5368083,Documentation Replication on Other Examples
55122902,from_tensor_slices() with big numpy array while using tf.keras,"I have some training data in a numpy array - it fits in the memory but it is bigger than 2GB. I'm using tf.keras and the dataset API. To give you a simplified, self-contained example: So, executing this results in an error ""Cannot create a tensor proto whose content is larger than 2GB"". The documentation lists a solution to this problem: https://www.tensorflow.org/guide/datasets#consuming_numpy_arrays - just use tf.placeholders and then feed_dict in session run. Now the main question is: how to do this with tf.keras? I cannot feed anything for the placeholders when I call model.fit() and in fact when I introduced the placeholders I got errors saying ""You must feed a value for placeholder tensor"".",https://stackoverflow.com/questions/55122902,1185138,Documentation Replicability
55125115,Tensorflow difference between tf.losses.softmax and tf.nn.softmax,What is the difference between tf.nn.softmax_cross_entropy_with_logits and tf.losses.softmax_cross_entropy and when to use which function??,https://stackoverflow.com/questions/55125115,6553104,Documentation Replicability
55141486,Unable to see keras model graph in Tensorboard when using TensorFlow 2.0 Alpha,"I am trying custom training on TensorFlow 2.0 alpha and at the same time I am trying to add some metrics and my training graph to TensorBoard. Consider the following contrived example This, does not show the model graph properly, on doing In the graphs tab I am seeing I am getting the graph when I am training through model.fit or estimator. For example, here is the graphs section when I use model_to_estimator to convert a model The guide article does not track metrics through tensorboard, and I did not find any sections on the new workflow for custom adding and tracking of metrics in TensorBoard on alpha (https://www.tensorflow.org/alpha). My contrived implementation is based on the API documentation of tf.summary (https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/summary)",https://stackoverflow.com/questions/55141486,3673043,Documentation Replication on Other Examples
55160136,Tensorflow 2.0 and image processing,"I'm just converting some code to tensorflow v2, but i'm facing an issue. I used to use some tf.contrib.image functions in tensorflow v1 (dense_image_warp and rotate). I don't know how to replace them in v2. Those functions are not part of the tf.image module. The tf_upgrade_v2 script can't convert those methods. Does someone know how to replace them to be v2 compatible ? Thank you",https://stackoverflow.com/questions/55160136,9266202,Documentation Replicability
55168906,Tensorflow - tf.nn.weighted_cross_entropy_with_logits - logits and targets must have the same shape,"I've just started using tensorflow for a project I'm working on. The program aims to be a binary classifier with input being 12 features. The output is either normal patient or patient with a disease. The prevalence of the disease is quite low and so my dataset is very imbalanced, with 502 examples of normal controls and only 38 diseased patients. For this reason, I'm trying to use tf.nn.weighted_cross_entropy_with_logits as my cost function. The code is based on the iris custom estimator from the official tensorflow documentation, and works with tf.losses.sparse_softmax_cross_entropy as the cost function. However, when I change to weighted_cross_entropy_with_logits, I get a shape error and I'm not sure how to fix this. I have searched and similar problems have been solved by just reshaping the labels - I have tried to do this unsuccessfully (and don't understand why tf.losses.sparse_softmax_cross_entropy works fine and the weighted version does not). My full code is here https://gist.github.com/revacious/83142573700c17b8d26a4a1b84b0dff7 Thanks!",https://stackoverflow.com/questions/55168906,11204499,Documentation Replication on Other Examples
55176818,How to support masking in custom tf.keras.layers.Layer,"I'm implementing a custom tf.keras.layers.Layer that needs to support masking. Consider the following scenario Now per the documentation I wonder, what does that mean? Looking through TensorFlow's custom layers guide and the tf.keras.layer.Layer documentation it is not clear what should be done to support masking",https://stackoverflow.com/questions/55176818,5368083,Lack of Alternative Solutions/Documentation
55190692,Transposed convolution in tf.keras.layers,How can I convert this function to an equivalent tf.keras (Tensorflow 2.0) implementation?,https://stackoverflow.com/questions/55190692,1050648,Documentation Replicability
55199181,keras v. tf.keras compile command for sequential model,"I've been getting up to speed in keras, not realizing that tf.keras is also a thing (and for newbies, it's easy to get cross ways with imports in python). In trying to convert a script from keras to tf.keras, it appears that the commands are not consistent? In general, is tf.keras supposed to follow the keras documentation, or are they diverging? My specific issue is that this works with keras, but not tf.keras: This gives error: This seems inconsistent with the tf.keras docs (https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#compile). Any idea what is going on?",https://stackoverflow.com/questions/55199181,2364295,Documentation Ambiguity
55213452,Tensorflow: stacking subarrays in a tensor,"I have a tensor that looks like: I would like to concatenate/stack the values at each index in the inner array so it looks like: I've tried various forms of tf.concat and tf.stack/tf.unstack, but can't seem to get it right. Does anyone know how to do this?",https://stackoverflow.com/questions/55213452,1733587,Documentation Replicability
55253299,How to log a tensorflow layer output in tf.estimator.Estimator(),"In this tutorial, they successfully log the softmax function by giving a name to the tf.nn.softmax node. Now, instead of the softmax, I would like to also log the output of the last Dense layer. But it gives me the following error My question is: how to log the layer output in tensorflow?",https://stackoverflow.com/questions/55253299,9793316,Documentation Replication on Other Examples
55264696,Tensorflow dynamic_rnn deprecation,"It seems that the tf.nn.dynamic_rnn has been deprecated: I have checked out keras.layers.RNN(cell) and it says that it can use masking which I assume can act as a replacement for dynamic_rnn's sequence_length parameter? But there is no further information even in the Embedding docs for how I can use mask_zero=True to accommodate variable sequence lengths. Also, if I am using an embedding layer just to add a mask, how do I prevent the Embedding from changing my input and being trained? Similar to this question RNN in Tensorflow vs Keras, depreciation of tf.nn.dynamic_rnn() but I want to know how to use the mask to replace sequence_length",https://stackoverflow.com/questions/55264696,10083000,Documentation Replication on Other Examples
55300544,TF 2.0 @tf.function example,"In the tensorflow documentation at the autograph section we have the following code snippet I have a small question concerning the step variable, it's an integer and not a tensor, autograph supports built-in python type such as integer. Therefore the tf.equal(step%10,0) could be changed to simply step%10 == 0 right ?",https://stackoverflow.com/questions/55300544,8593338,Documentation Replication on Other Examples
55308630,update instruction of the deprecated tensorflow dataset sliding window with tf.data.experimental.CsvDataset,I am trying without success to update my code with the instruction given in the tensorflow documentation (api r1.13). I am using the tf.data.experimental.CsvDataset and the deprecated tf.contrib.data.sliding_window_batch for a RNN and all works fine (except the deprecated sliding_window warning message). For the update I have simply replaced with and I got the following error for a csv file with 50 columns: How can I solve this problem for any csv file (with any number of columns)?,https://stackoverflow.com/questions/55308630,4328242,Documentation Replication on Other Examples
55310900,TensorFlow: How to find minimum/maximum coordinates of segmentations in a tensor excluding zeros?,"For the computation of Intersection over Union (IoU) I want to find coordinates of minimum and maximum values (the border pixels) in a segmentation image image_pred that is represented by a float32 3D tensor. In particular, I aim at finding top left and bottom right corner coordinates of objects in an image. The image is entirely comprised of black pixels (value 0.0) except where the object is located, I have color pixels (0.0 &lt; values &lt; 1.0). Here's an example for such a bounding box (in my case, the object is the traffic sign and the environment is blacked out): My approach so far is to tf.boolean_mask for setting every pixel to False except for the color pixels: and then use tf.where to find the coordinates of the masked image. To determine the horizontal and vertical coordinate values of the top left and bottom right corners of the rectangle, I thought about using tf.recude_max and tf.reduce_min, but since these do not return a single value if I provide an axis, I am unsure if this is the correct function to use. According to the docs, if I do not specify axis, the function will reduce all dimensions which is not what I want either. Which is the correct function to do this? The IoU in the end is a single 1D float value.",https://stackoverflow.com/questions/55310900,7353970,Documentation Replication on Other Examples
55347304,Error when applying sample/class weights to fit generator,"I am using a tf.keras.Model fit_generator (https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit_generator) to feed batches of data to a model. According to TensorFlow Documentation, the fit generator should be able to accept size 2 (inputs, targets) or 3 (inputs, targets, sample_weights) tuple. We have the size 2 working, but we have unbalanced classes, so I have determined sample weights. When the fit generator returns a size 3 tuple, I get the error: ”tensorflow.python.framework.errors_impl.InvalidArgumentError: Can not squeeze dim[0], expected a dimension of 1, got [batch_size]"" I am using tensorflow 1.12 Loss Function is tf.losses.softmax_cross_entropy",https://stackoverflow.com/questions/55347304,11257466,Documentation Replication on Other Examples
55363728,How to feed .h5 files in tf.data pipeline in tensorflow model,"I'm trying to optimize the input pipeline for .h5 data with tf.data. But I encountered a TypeError: expected str, bytes or os.PathLike object, not Tensor. I did a research but can't find anything about converting a tensor of string to string. This simplified code is executable and return the same error: Apparently the fname is a tensor of string but the positional argument waits for only a string. I can't find any documentation on this. And the answer of another post doesn't solve this problem. In my case, I work only with h5 where one h5 store one batch. Update Solution: Thanks to the comment of @kvish, the part of loading .h5 file is solved. The code is upgraded with a simple conv layer, the placeholders have been taken. Each .h5 is one batch. I want to prefetch in parallele multiple batches(h5py doesn't support multithread reading so I write batches into multiple files). One can copy-paste-and-launch: Somehow there will be another cudnn issue which isn't related to this post. tensorflow-cpu v1.12: work fine tensorflow-gpu v1.12: runtime issue happens",https://stackoverflow.com/questions/55363728,9217178,Lack of Alternative Solutions/Documentation
55379830,How do I resize image with unknown size in Tensorflow(tf.shape(input) method doesn't work),"According to this post, one can use tf.shape() to resize image with unknown size like placeholder. But the method doesn't seem to work for me. I have some simple code that looks like: Basically, my code does the following: Given an input 1D data x, the program tries to stretch or compress the sequence by some random factor and return the tuned sequence. Since I didn't find any Tensorflow function that directly performs this operation, I use tf.resize by treating the data as 1xD image where D is the length of the signal. But I got an error: So it seems like tf.shape(x) returns a Tensor rather than integer values that specify the shape of the tensor(verified by Tensorflow document). How can I solve this?",https://stackoverflow.com/questions/55379830,8577452,Documentation Replication on Other Examples
55414091,Can we not give the name for a tensor in tf.get_variable in tensorflow?,"In tf.get_variable(), there is a need to give a name for a variable. In contrast, it is not a must to specify names for variables in tf.Variable() function and tensorflow will give tensors default names. So, is there any trick for allowing us to omit manual name specification.",https://stackoverflow.com/questions/55414091,7309971,Documentation Replicability
55420520,How do you apply layer normalization in an RNN using tf.keras?,"I would like to apply layer normalization to a recurrent neural network using tf.keras. In TensorFlow 2.0, there is a LayerNormalization class in tf.layers.experimental, but it's unclear how to use it within a recurrent layer like LSTM, at each time step (as it was designed to be used). Should I create a custom cell, or is there a simpler way? For example, applying dropout at each time step is as easy as setting the recurrent_dropout argument when creating an LSTM layer, but there is no recurrent_layer_normalization argument.",https://stackoverflow.com/questions/55420520,38626,Documentation Replication on Other Examples
55422537,Testing TF serving model fails with bytes as strings and strings as bytes confusion,"I'm having a problem serving my text classification model on Tensorflow 1.12. I'm using tf.estimator.inputs.pandas_input_fn to read in my data, and tf.estimator.DNNClassifier to train/evaluate. I'd then like to serve my model. (Apologies in advance, it's tough to provide a full working example here, but it's very much like the example TF provides at https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier ) I'm currently saving my model with ... This actually fails to run with the error: I tried to save a second way doing: This actually works, until I try testing it with the saved_model_cli. Some output for saved_model_cli show --all --dir TEST_SERVING/1553879255/: But now I can't seem to test it. Ok, lets turn it into a bytes object by changing to b[""What is going on""] and b[""Help me""]... Any ideas/thoughts?? Thanks!",https://stackoverflow.com/questions/55422537,5088987,Documentation Replication on Other Examples
55451403,How to use tf.data.Options()?,"Is one dataset created accompanied by default options? If not, how to use the tf.data.Options()? Just dataset = dataset.with_options(options)? Is there anything else to be noticed?",https://stackoverflow.com/questions/55451403,6128145,Documentation Replication on Other Examples
73074491,Replace tf.gather with other OPs which are supported by TFLite GPU delegate,"Does anyone know how to replace tf.gather with other OPs which are supported by TFLite GPU delegate? I want to convert a TensorFlow model with a warp operation to a TFLite model to be used on Android devices, but the warp needs to use tf.gather to get corner pixel values and tf.gather seems not supported by GPU on Android.",https://stackoverflow.com/questions/73074491,15080050,Documentation Replication on Other Examples
55514435,Keras CuDNNLSTM implicit activation function?,"While in tf.keras.layers.LSTM there is an activation parameter (default tanh) the CuDNNLSTM doesn't have any, while having a activity_regularizer parameter. Am I missing something? CuDNNLSTM is not a tf.keras.layers.LSTM wrapper, but a standalone entity, so how do I set the activation function for it?",https://stackoverflow.com/questions/55514435,7034613,Documentation Replicability
51971050,Graph optimizations on a tensorflow serveable created using tf.Estimator,"Context: I have a simple classifier based on tf.estimator.DNNClassifier that takes text and output probabilities over an intent tags. I am able to train an export the model to a serveable as well as serve the serveable using tensorflow serving. The problem is this servable is too big (around 1GB) and so I wanted to try some tensorflow graph transforms to try to reduce the size of the files being served. Problem: I understand how to take the saved_model.pb and use freeze_model.py to create a new .pb file that can be used to call transforms on. The result of these transforms (a .pb file as well) is not a servable and cannot be used with tensorflow serving. How can a developer go from: There's documentation that suggests that this is certainly possible, but its not at all intuitive from the docs as to how to do this. What I've Tried: My idea was to load the servable, extract the graph_def from the meta_graph_def, transform the graph_def and then try to recreate the servable. This seems to be the incorrect approach. Is there a way to successfully perform transforms (to reduce file size at inference) on a graph from an exported servable, and then recreate a servable with the transformed graph? Thanks. Update (2018-08-28): Found contrib.meta_graph_transform() which looks promising. Update (2018-12-03): A related github issue I opened that seems to be resolved in a detailed blog post which is listed at the end of the ticket.",https://stackoverflow.com/questions/51971050,3222797,Lack of Alternative Solutions/Documentation
51999636,Keras model failed to learn anything after changing to use tf.data api,"I was trying to convert a simple Keras model to use tf.data api for data loading, but somehow the accuracy remains about 10% during the whole 10 epochs. In comparison, the original code without using tf.data api can easily achieve about 98% accuracy. Did I do anything wrong? The version using tf.data api The version without using tf.data api",https://stackoverflow.com/questions/51999636,10263076,Documentation Replication on Other Examples
36223157,Set weight and bias tensors of tensorflow conv2d operation,"I have been given a trained neural network in torch and I need to rebuild it exactly in tensorflow. I believe I have correctly defined the network's architecture in tensorflow but I am having trouble transferring the weight and bias tensors. Using a third party package, I converted all the weight and bias tensors from the torch network to numpy arrays then wrote them to disk. I can load them back into my python program but I cannot figure out a way to assign them to the corresponding layers in my tensorflow network. For instance, I have a convolution layer defined in tensorflow as According to the tensorflow documentation, the tf.nn.conv2d operation uses the shape defined in the kernel_1 variable to construct the weight tensor. However, I cannot figure out how to access that weight tensor to set it to the weight array I have loaded from file. Is it possible to explicitly set the weight tensor? And if so, how? (The same question applies to bias tensor.)",https://stackoverflow.com/questions/36223157,6114511,Lack of Alternative Solutions/Documentation
52035692,Tensorflow v1.10: store images as byte strings or per channel?,"It is known that, at the moment, TF's Record documentation leaves something to be desired. My question is in regards to what is optimal for storing: as a TF Record. Namely, this questions considers storing the sequence and class probabilities as channels vs as a byte string and whether or not the meta information should go in as features of a tf.train.Example or as the context of a tf.train.SequenceExample. (see questions at the bottom). For example, lets assume my looks sequence like this i.e. it is a 2 channel sequence of fixed length (in this example, 2) where the values can only be integer value. and that we have three classes for which we are trying to segment the sequence into where in effect both seq and cls_probs are numpy.arrays. The network only requires this information. However, I also have some meta data which I would like to keep with the sequence. e.g. Then I have several ways I could construct my tf.train.Example: where f'{variable}'.encode('utf-8') is the currently not suggested fb'&lt;string&gt;' (note: f-strings only work with python3.6+). This format is somewhat nice as each sequence channel is explicit. However it is also verbose and requires preprocessing when loaded to be feed into the network. or, I could dump my array to an string TF Records also accept another form: tf.train.SequenceExample. SequenceExample expects context features and an ordered list of unnamed features. So restructuring above's as channels example: likewise we can create the as string example: Here I gave a M.W.E. for how one could construct an example (ready to be exported to a TF Record) as both tf.train.Example and tf.train.SequenceExample. Further, I demonstrated both how to do this per channel or by dumping as a byte string. Both of these methods (as channels / as strings) include the meta information within the example. Thus my questions are: Does anyone know if there are any notable advantages / disadvantages for any of four these strategies? For those who would like to test this on larger less dummy like data, some functions for producing this code can be found below Lastly, I would like to point out this medium post which greatly elaborates on TF's docs.",https://stackoverflow.com/questions/52035692,5623899,Documentation Replication on Other Examples
52046902,cudnnLSTM won't restore into a cudnnCompatibleLSTM,"I'm trying to train an elementary network on a GPU machine (AWS p3x2, Volta) with TF 1.9 / 1.10. Not Keras -- TF only. Based on the [rather limited] documentation my aim is to train with cudnnLSTM cell, save a checkpoint, and then restore for inference on a CPU. Per that aim, I thought that cudnnCompatibleLSTM is the way to go as it is supposed to suck in the weights from the GPU-specific LSTM implementation. I get the following error, no matter what I try: Another related issue is that cudnnCompatibleLSTM and cudnnLSTM are not the same mathematically. I get different results for initialized cells. [initialized by some tf.constant() as initializer, no save/restore]. Seems that cudnnLSTM does depend on the random seed [dropout is zero], which means that there are some unique tensor/tensor initialization going on, separating it from cudnnCompatibleLSTM. Does anybody have a clue?",https://stackoverflow.com/questions/52046902,6281444,Documentation Replicability
70196272,Overcoming incompatibilities between tensorflow 1.x and 2.x when trying to view layer activity with backend,I would like to run newer tensorflow routines like: for which I get error in 1.x: ImportError: cannot import name image_dataset_from_directory while preserving older functionality of 1.x like running the routine to see activations in various layers like: for which I get the following error in tf 2.x: ValueError: Input tensors to a Functional must come from tf.keras.Input. Received: 0 (missing previous layer metadata). The code: The documentation I looked at suggests the problem may have something to do with eager computation mode eg https://github.com/tensorflow/tensorflow/issues/34201 But I cannot figure out how to resolve this. Thank you for suggestions!,https://stackoverflow.com/questions/70196272,11726927,Lack of Alternative Solutions/Documentation
52073782,Computations (such as tf.greater and tf.cond) on random value tensors not working as expected,"I am a tensorflow beginner. According to the documentation, tf.greater returns the truth value of (x&gt;y) element-wise My code is as below: The output I got is: x is bigger than y so it should return True and x+y should be 1.10271299 why is my expected output different than the actual output?",https://stackoverflow.com/questions/52073782,8496110,Documentation Replication on Other Examples
70219114,Define a tensor of constants for tf.while_loop,I want to somehow maintain a list of constants in tf.while_loop that can support the following functions TensorArray would not work here since it does not support rewrites. What other options do I have?,https://stackoverflow.com/questions/70219114,14370198,Documentation Replicability
52090848,how to make embedding column through features directly?,"I'm learning wide&amp;deep model for ctr. My data has a feature user_id which has more than 2**26 values. How I can get embedding column through this feature? I used user_id = tf.feature_column.categorical_column_with_hash_bucket('user_id', hash_bucket_size=2**26), user_id_emb = tf.feature_column.embedding_column(user_id, dimension=95), but it shows out of memeory.",https://stackoverflow.com/questions/52090848,8686837,Documentation Replication on Other Examples
36295624,Tensorflow loss not decreasing and acuraccy stuck at 0.00%?,"I""m trying to train CNN with UCF101 single frame data. As far as I understand the problem is either with the weight Initialization or the loss with either the tf.nn.softmax and tf.nn.softmax_cross_entropy_with_logitsor the cost and optimizer function. Also is there any way to use xavier initialization ??",https://stackoverflow.com/questions/36295624,2527680,Documentation Replicability
52134130,How to restrict the absolut value of each dimention of a sparse gradient from being too large?,"Consider the code below: The output is: I know there are two ways to restrict gradients from being too large. tf.clip_by_value to restrict each dimentions, and tf.clip_by_global_norm to restrict according global gradients norms. However, tf.clip_by_value will cast a sparse gradient into a dense one, which significantly increase the memory usage and decreases the calculation efficiency, just as the warning indicates, while tf.clip_by_global_norm will not. Although I can understand why this is designed, how can I restrict the absolut value of each dimention of a sparse gradient from being too large without efficiency decrease? Please don't tell me just use tf.clip_by_global_norm, I know this is ok for most cases, but is not what I want.",https://stackoverflow.com/questions/52134130,1337994,Lack of Alternative Solutions/Documentation
70303232,why am i getting a tensorflow warning when running this script?,"the code does the job by detecting and extracting the faces from the input folder and putting the extracted faces in the output folder without any issues but i wanna know why is this warning is showing up in the first place and is there any way i can fix it ""properly"" instead of suppressing it details the warning message is WARNING:tensorflow:5 out of the last 9 calls to &lt;function Model.make_predict_function..predict_function at 0x0000000013E161F0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for more details.",https://stackoverflow.com/questions/70303232,10662486,Documentation Replication on Other Examples
52190877,Tensorflow: tf.case giving errors,"When I print output of tf.case, it gives ValueError: Operation 'case_11/cond/Merge' has been marked as not fetchable. The code is following : PS: But this is not the issue with tf.cond(). Please help me here . Thanks!",https://stackoverflow.com/questions/52190877,8548844,Documentation Replicability
70328363,Extra dimension to MaxPool1D layer from Conv1D layer,"I'm very new to Tensorflow (this is my first project using it), and I don't really understand how input shapes work. I am trying to train a CNN-LSTM on a set of financial time series data. For my use case, I have a tf.keras.data.DataLoader object which is meant to serve batches of training data to the model. One training instance corresponds to the price history over the last 30 days, and hence should have shape (30,). running the following code: I get that x.shape is (4, 30), where the Dataset object I have defined serves training instances in batches of 4. Here is my code: with accompanying error message: I don't understand what's going on--why is there an extra dimension coming out of the Conv1D layer? I mean, should the output of 1-D convolution not simply be (BATCH_SIZE, WIDTH, 32) (padding was set to 'same')? I apologize if this is addressed in the documentation, but I have been looking everywhere for an answer and I can't seem to fix this problem. I would really appreciate some help here. Thanks!",https://stackoverflow.com/questions/70328363,8163401,Lack of Alternative Solutions/Documentation
52219099,Is there an equivalent to tf.convert_to_tensor in tensorflow c++?,I have mean.npy file which I have to convert into tensor. I found that tf.convert_to_tensor does that but could not find the equivalent for it in C++. Does anybody know the equivalent function in C++ ?,https://stackoverflow.com/questions/52219099,3719483,Documentation Replicability
70381758,Equivalent of tf.linalg.diag_part in PyTorch,"As I'm reimplementing some code, I'm wondering if there is any equivalent of tf.linalg.diag_part (docs) in PyTorch ..?",https://stackoverflow.com/questions/70381758,14824108,Documentation Replicability
70424291,How to use tf.gather similar to the numpy slicing,"I have a list of indices representing the rows and columns that I'd like to access. When I use them as [r,c], I am able to obtain the corresponding elements in a numpy array. I would like to perform the same operation in tf and am using tf.gather. https://www.tensorflow.org/api_docs/python/tf/gather I have tried different commands, but could not manage to get the same results as I do in numpy.",https://stackoverflow.com/questions/70424291,7447976,Documentation Replication on Other Examples
52300519,Calculating custom metrics with tf.estimator.DNNRegressor in TensorFlow 1.10,How to configure a tf.estimator.DNNRegressor to report different metrics like RMSE and MAE while evaluating? (One can ask the same question for tf.estimator.DNNClassifier and AUC metric),https://stackoverflow.com/questions/52300519,2737801,Documentation Replicability
36508860,How do I actually execute a saved TensorFlow model?,"Tensorflow newbie here. I'm trying to build an RNN. My input data is a set of vector instances of size instance_size representing the (x,y) positions of a set of particles at each time step. (Since the instances already have semantic content, they do not require an embedding.) The goal is to learn to predict the positions of the particles at the next step. Following the RNN tutorial and slightly adapting the included RNN code, I create a model more or less like this (omitting some details): Then I create a saver = tf.train.Saver(), iterate over the data to train it using the given run_epoch() method, and write out the parameters with saver.save(). So far, so good. But how do I actually use the trained model? The tutorial stops at this point. From the docs on tf.train.Saver.restore(), in order to read back in the variables, I need to either set up exactly the same graph I was running when I saved the variables out, or selectively restore particular variables. Either way, that means my new model will require inputs of size batch_size x num_steps x instance_size. However, all I want now is to do a single forward pass through the model on an input of size num_steps x instance_size and read out a single instance_size-sized result (the prediction for the next time step); in other words, I want to create a model that accepts a different-size tensor than the one I trained on. I can kludge it by passing the existing model my intended data batch_size times, but that doesn't seem like a best practice. What's the best way to do this?",https://stackoverflow.com/questions/36508860,4185992,Documentation Replication on Other Examples
71068368,"Tensorflow custom op, gpu kernel returns a tensor of zeros","I am trying to implement a custom op and I am using the example in the official documentation as a benchmark to test the correct compilation of the op, I've just modified the gpu kernel in order to see if it was actually executed but when I test the op it returns all zeros. kernel_example.h kernel_example.cc kernel_example.cu.cc To compile the op I use the following Makefile I'm currently working directly on google Colab, loading the previous files in /content and running the compilation with the following lines The cpu op compilation works fine. The gpu op compilation does not give any error but when I test the op it does not work properly. With an input of input = tf.ones((4,)), considered the update I made in the gpu kernel, I expect to get [3., 3., 3., 3.] but I actually get [0., 0., 0., 0.], when eager execution is active. Whereas when I run the same in graph mode I get [2., 2., 2., 2.], which is the result I obtain from the cpu implementation, this makes me think the gpu kernel is kind not executed. I've tried to link the cuda path in the Makefile (LCUDA), as suggested in another similar question, but this did not solve my problem. I feel something is wrong with compiling options but I really cannot find what the problem could be. Any idea how to make the gpu kernel work correctly ?",https://stackoverflow.com/questions/71068368,18090135,Documentation Replication on Other Examples
53003208,How to save a Tensorflow Checkpoint file from Google Colaboratory in when using TPU mode?,"When I use saver = tf.train.Saver() and save_path = saver.save(session, ""checkpointsFolder/checkpoint.ckpt"") I get a UnimplementedError (see above for traceback): File system scheme '[local]' not implemented error Here is the full error Looking up this error, I found the following: From Google official TPU debugging guide https://cloud.google.com/tpu/docs/troubleshooting Someone else with a similar issue TPU local Filesystem doesn't exist? However, I do not have a Google Cloud service, I am just using Google Colab. Is there a way to save a Tensorflow checkpoint when in TPU mode?",https://stackoverflow.com/questions/53003208,3259896,Documentation Replication on Other Examples
71090570,How do I create a tf.Tensor from a pandas DataFrame containing arrays?,"I have a pandas DataFrame like below. Output: I would to create a Tensor from the values of the col column, in order to use them in a specific use case. I have tried converting the values like Which looks like: When I try to convert this to a Tensor, by I get an exception: I can see from the documentation that the tf.constant method should work on a very similar array The values variable I create have .shape like (2,) while the image below have (2, 3), which might be the problem. I can't seem to get the dtype and/or shape to match exactly, and I'm unsure how to get it to work. Any ideas?",https://stackoverflow.com/questions/71090570,11764097,Documentation Replicability
53027716,differentiable histogram in tensorflow,"How can one get a differentiable histogram function in tensorflow. I want to use the histogram of images to calculate a particular loss function. But the histogram function provided by tensorflow, tf.histogram_fixed_width() are not differentiable. How can I go about this ?",https://stackoverflow.com/questions/53027716,4207206,Documentation Replicability
53032922,TensorFlow while loop with condition dependent on body,"I want to have a while loop with the condition dependent on a tensor computed in the loop body, but I don't know how to accomplish this with tf.while_loop(). My input processing includes random cropping, but some crops can lead to low-quality examples and I want to discard those and try a new random crop until an example of sufficient quality is obtained. The inputs are cropped by and the condition is Going over the documentation and examples of tf.while_loop(cond, body, loop_vars, ...), what I understand is that both cond and body should take the same arguments given in loop_vars. I don't see how I can have cond depend on img_crop which would be calculated inside body, and isn't provided in loop_vars. I could equivalently compute cond using crop_begin_index without actually cropping, but it depends on the random values computed inside the loop, so I have the same problem. Is this indeed a limitation of TF looping? If not, how can I rewrite my code to use tf.while_loop()?",https://stackoverflow.com/questions/53032922,2260153,Documentation Replication on Other Examples
53033780,Indexing Ops in TF,"Is there a way to index an operation in tensorflow? In particular, I'm interested in indexing by the iterator variable of tf.while_loop. More concretely, let's say I have my_ops = [op1, op2]. I would like to have: which unfortunately will not work, since a python array supports only integer indexing.",https://stackoverflow.com/questions/53033780,868829,Documentation Replicability
53053649,Tensorflow: Truncating a Tensor Seems to Have No Effect,"I'm using the tf.data.Dataset API and am trying to truncate a bunch of tensors to length 100. Here's what my dataset looks like: My reviews are just movie reviews (strings), so I perform some preprocessing and map that function on my dataset: However, my code fails with: On an input of length 240. So, it seems like my padding step calculates 100 - 240 = -140 and I get this error. Here's my question: how is this possible, given that I truncate to length 100 with: It seems clear that this line isn't having any effect, so I'm trying to understand why. The docs are very clear that this is acceptable syntactic sugar for tf.slice: Any ideas? Thanks!",https://stackoverflow.com/questions/53053649,1316501,Documentation Ambiguity
71129505,"Is it possible to split a tensorflow dataset into train, validation AND test datasets when using image_dataset_from_directory?","I am using tf.keras.utils.image_dataset_from_directory to load a dataset of 4575 images. While this function allows to split the data into two subsets (with the validation_split parameter), I want to split it into training, testing, and validation subsets. I have tried using dataset.skip() and dataset.take() to further split one of the resulting subsets, but these functions return a SkipDataset and a TakeDataset respectively (by the way, contrary to the documentation, where it is claimed that these functions return a Dataset). This leads to problems when fitting the model - the metrics calculated on validation sets (val_loss, val_accuracy) disappear from model history. So, my question is: is there a way to split a Dataset into three subsets for training, validation and testing, so that all three subsets are also Dataset objects? Code used to load the data Model compilation and fitting When using a normal Dataset, val_accuracy and val_loss are present in the history of the model: But when using a SkipDataset, they are not:",https://stackoverflow.com/questions/71129505,11212528,Documentation Replication on Other Examples
53079436,tensorflow Tf.cond giving unexpected output,"I seem to be having a misunderstanding on how tf.cond works. In the tensorflow documentation, it gives the following example: The result of the example, if x&lt;y is True is tf.add(x,z) else tf.square(y) Following this example, I am trying to build a small example with tf.cond and the result doesnt go along the lines mentioned in the documentation. in my example, deterministic_action = 4, random_action = 11, chose_random=False. The stochastic_action should be 4, instead it is 1. Where did the value 1 come from? here is the output:",https://stackoverflow.com/questions/53079436,1059860,Documentation Replication on Other Examples
71149271,"How to remove single feature from tensorflow dataset, how to use apply on single feture?","I created dataset from csv file with dataset = tf.data.experimental.make_csv_dataset() function but My dataset has categorical and numeric features. Question 1: The question is how can I modify only single feature, for example weight +2, to: I try to do something like: but the error is: ""TypeError: 'FilterDataset' object is not subscriptable"" Example from the documentation https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply doesn't show it. Question 2: How can I remove single feature ? Is there any equivalent to pandas drop column?",https://stackoverflow.com/questions/71149271,13824257,Documentation Replication on Other Examples
37376861,what does the tf.nn.lrn() method do?,Here is the code-snipped from the cifar10-tutorial. It's from the cifar10.py. What does the tf.nn.lrn-Method do? I can't find a definition in the API Documentation on https://www.tensorflow.org/versions/r0.8/api_docs/python/index.html,https://stackoverflow.com/questions/37376861,3019308,Lack of Alternative Solutions/Documentation
53107594,How do I import and use quantized_matmul and quantized_biasadd operations in tensorflow,"I am trying to use the quantized matmul operation in tensorflow. However I am not sure if I am doing it right. I could not find an example as to how to call the function and use it. I know that quantized conv2D can be called as tf.nn.conv2D(), just wanted to know a similar way of calling quantized_matmul operation as well. The class and the functions are present in tensorflow library and are documented below: Quantized Bias Add: https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/quantized-bias-add Quantized Matmul: https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/quantized-mat-mul Thanks and Regards, Abhinav George",https://stackoverflow.com/questions/53107594,9104473,Documentation Replication on Other Examples
53164055,training tensorflow data API with procedural on-the-fly data generation,"My question is similar to this one, in that I would like to generate batches of training data on the fly. I have a function get_random_batch(batch_size, input_path, target_path, **other_kwargs) which returns inputs and targets, but it is a pure vanilla python / numpy function, not tensorflow. So it returns numpy arrays, not tensors. (The function is quite a large complex one with some 3rd party libs. To port it to tensorflow is not very feasible. To preprocess everything is also not feasible as the data is huge, and I don't have the space to store it all preprocessed! In fact I don't even train a single epoch, I just take random selections for hundreds of thousands of iterations). For a few years I've been using tensorflow's low level training API: generate a batch of input-target pairs, run forward pass with feed (into placeholders) and fetch, calculate loss, apply gradients, repeat etc. Now I finally would like to try out the newer data API (so that I can use the Keras API for building the model and 'fitting' etc), but I can't figure out how to migrate. All of the documentation I've seen assumes that the data loading and preprocessing is part of the graph, and the output of the dataset are already tensors. -- Update: Ok this seems to work with tf.data.Dataset.from_generator",https://stackoverflow.com/questions/53164055,214488,Documentation Ambiguity
53167302,Does tf.keras.layers.Conv2D as first layer in model truly need input_shape?,"According to the official document on tf.keras.layers.Conv2D, but actually without input_shape it does work in both graph execution and eager execution environment. In graph execution, works without any error and in eager execution, also does. Q1: Does tf.keras.layers.Conv2D as the first layer in a model truly need to specifying input_shape? Q2: If not, when is it needed and why is it mentioned so in the official document? UPDATE1: Tutorial on tf.keras says UPDATE2: git blame of docstring in TensorFlow source revealed that this document is copied from Keras API (which is not TensorFlow keras API).",https://stackoverflow.com/questions/53167302,8384504,Documentation Replication on Other Examples
53206900,Sound way of managing multiple sessions and graphs,"I'd like to manage multiple Keras models in multiple sessions. My application is constructed such that models can be live at the same time, in addition to creating, saving and loading them. What is the proper way of managing this situation? Currently one model is represented by an instance of a wrapper class. This is used in the training, saving, loading and prediction. One tf.Graph and tf.Session is created per instance, and they are used in every function requiring the actual model. Similar functions using the with statements are created for compiling the network, fitting, saving (weights to .h5 and model to JSON) and loading. So whenever the model is needed, the graph and session are brought to context. This resulted in a strange error (Q for further context), and I was left wondering, what is the standard way of dealing with this. I tried to release all possible resources before creating or loading a model, but it hasn't helped. This function is just a compilation of all possible routines scraped off the internet, and is purely guesswork. I've not found good documentation of a similar situation. So I'd very much appreciate any real insight into this. I might need to delete the old question, as it's quite all over the place. At the time of asking I had no idea what was going on. But it's there for now. Some specific questions have arisen. The underlying issue with the error has been finally (and somewhat embarassingly) resolved by updating all packages.",https://stackoverflow.com/questions/53206900,7089239,Lack of Alternative Solutions/Documentation
71292087,How to stack tensors without using tf.stack?,"Is there any way to merge Tensors in Tensorflow? For example: I have 128 Tensor shape all are (40, 10), Now, I want merge them to shape(128, 40, 10). I can't use *tf.stack([Tensor1, Tensor2, Tensor3, ...])* directly. So, Is there any function that can help achieve this?",https://stackoverflow.com/questions/71292087,16647718,Documentation Replication on Other Examples
53233123,tf.data: Combining multiple from_generator() datasets to create batches padded across time windows,"I am working on a timeseries problem where each timeseries is fairly long (10^3-10^4 timesteps, and each timeseries is of different length). For each sequence, I can define a Python generator that yields values one timestep at a time. I am using the tf.data.Dataset.from_generator() constructor to wrap these generators into the tf.data API. The documentation suggests using from_generator() along with the tf.contrib.data.parallel_interleave() transformation to parallelize the extraction from my Python generators. My downstream use for these data is a stateful RNN (e.g. LSTM or GRU). I want to chunk up the timeseries into smaller (~10^2) windows and use each chunk as a training example (i.e., truncated BPTT). Since my data are streaming, I think that means saving up window_size timesteps of each generator before passing it on through the pipeline, to be batched with the other generators' data. I also want to save the RNN state across these chunks so I can still learn long-term dependencies. My issue comes with wanting to create padded batches of these generators' batched outputs. Ideally, I would want to present to my neural network windows of the generator outputs, with padding as necessary when some subset of the generators exhaust themselves before others. I know that if I consume the entire generator output for each generator, then use Dataset.padded_batch() I can do this (and can then slice the padded batch across the time dimension into windowed chunks as necessary). However, I want to pass each window to the neural network it becomes available. If one of the generators exhausts itself before the others, I want to pad it with the padding value until all others have, so I can reset the RNN state and begin the next batch of generators with an empty initial RNN state. I am stuck here because the dataset resulting from tf.contrib.data.parallel_interleave() transformation discards each generator when it becomes exhausted, and the timeseries do not maintain a consistent ordering across samples from it. Here is a small example: Output: My desired output would be something like Here, the internal states of the RNNs would be reset after batch 2 and 5. Again, the desired output can be simple to create if I consume the entirety of each generator's output, then pad, batch, and slice, but I want to produce batches as the generators, which may be each receiving data in real-time from e.g. a separate simulation, make them available.",https://stackoverflow.com/questions/53233123,10630478,Documentation Replication on Other Examples
71294464,@tf_gradient peculiar implementation in StyleGan,"I've been reading the source code for the StyleGAN implementation, and I cannot understand the peculiar use of the @tf_gradient decorator. Let us take the concrete example of their implementation of Leaky_Relu. The way I would do it is as follows : Which follows the tf documentation for the use of tf.custom_gradient. But in the styleGan paper, they implement it as follows (I removed the ""variable_scope"" in my implementation as I'm not sure what it does): There are two @tf.custom_gradient decorators used, and I don't understand why since there clearly aren't any second order derivatives being computed (as they are identically 0 anyway for LRelu). Is this a trick to somehow speed up computations ? If so, how does it work ? EDIT : To clarify why I think this is somehow a ""trick"" to make computations of gradients faster, the authors make the following comment in the code : And for completeness, here is the repo from which I took the code from",https://stackoverflow.com/questions/71294464,3842374,Documentation Replication on Other Examples
53247534,"TFRecord IO slower than python hdf5 reader, how do I improve its speed?","I was following the official TF giude to use the tf.data.Dataset API for building data pipeline, but I found it ~2 times slower than my python data pipeline using hdf5. My experiment setting is: - batch size=1000, - print log every 10 million samples to show the time, - run a fake model that only takes data as input and do not compute anything. what's worse is that when I set batch size=10000, it becomes: I've tune the num_parallel_calls to 2, 10. Not working. I've also tuned the prefetch(n) from 1 to 1000, which has little improvement. My question is: Is there any way to improve my tfrecord data pipeline? Am I missing something in my code? Appreciate it for any help.",https://stackoverflow.com/questions/53247534,6088463,Documentation Replicability
53272508,inception v3 using tf.data?,"I'm using a bit of code that is derived from inception v3 as distributed by the Google folks, but it's now complaining that the queue runners used to read the data are deprecated (tf.train.string_input_producer in image_processing.py, and similar). Apparently I'm supposed to switch to tf.data for this kind of stuff. Unfortunately, the documentation on tf.data isn't doing much to relieve my concern that I've got too much data to fit in memory, especially given that I want to batch it in a reusable way, etc. I'm confident that the tf.data stuff can do this; I just don't know how to do it. Can anyone point me to a full example of code that uses tf.data to deal with batches of data that won't all fit in memory? Ideally, it would simply be an updated version of the inception-v3 code, but I'd be happy to try and work with anything. Thanks!",https://stackoverflow.com/questions/53272508,7274120,Lack of Alternative Solutions/Documentation
71335830,What is the difference between tf.keras.layers.Input() and tf.keras.layers.Flatten(),"I have seen multiple uses of both tf.keras.layers.Flatten() (ex. here) and tf.keras.layers.Input() (ex. here). After reading the documentation, it is not clear to me",https://stackoverflow.com/questions/71335830,9758352,Documentation Ambiguity
53307954,TensorFlow Custom Estimator predict throwing value error,"Note: this question has an accompanying, documented Colab notebook. TensorFlow's documentation can, at times, leave a lot to be desired. Some of the older docs for lower level apis seem to have been expunged, and most newer documents point towards using higher level apis such as TensorFlow's subset of keras or estimators. This would not be so problematic if the higher level apis did not so often rely closely on their lower levels. Case in point, estimators (especially the input_fn when using TensorFlow Records). Over the following Stack Overflow posts: and with the gracious assistance of the TensorFlow / StackOverflow community, we have moved closer to doing what the TensorFlow ""Creating Custom Estimators"" guide has not, demonstrating how to make an estimator one might actually use in practice (rather than toy example) e.g. one which: While I still have many questions regarding this (from the best way to encode data into a TF Record, to what exactly the serving_input_fn expects), there is one question that stands out more prominently than the rest: How to predict with the custom estimator we just made? Under the documentation for predict, it states: (perhaps) Most likely, if one is using estimator.predict, they are using data in memory such as a dense tensor (because a held out test set would likely go through evaluate). So I, in the accompanying Colab, create a single dense example, wrap it up in a tf.data.Dataset, and call predict to get a ValueError. I would greatly appreciate it if someone could explain to me how I can:",https://stackoverflow.com/questions/53307954,5623899,Requesting (Additional) Documentation/Examples
72360420,How to evaluate only a random subset of all possible operations per pass inside a graph?,"I need the same result as in op_results_eager but with statements that allow wrapping call() as tf.function while still evaluating only the sampled operations. As you can see, I tried to build the indexing as a switch case, but that doesn't even give the right result in eager execution.",https://stackoverflow.com/questions/72360420,8690766,Documentation Replication on Other Examples
72379091,Tf.data.Dataset store a array column Error,"Is there any way to store inside a tf.data.Dataset formed from a pandas dataframe, a column where each cell corresponds to an array of floats? I have a pandas dataframe where a column has a list of floats for each row, and I transform it to a tf.data.Dataset in the following way to send it to a Tensorflow model: But I can't store the array column, is there any way to do it?",https://stackoverflow.com/questions/72379091,15692031,Documentation Replication on Other Examples
53895757,How can I know which TFRecords file is reading now?,"I have a list named t_list = [a.tfrecords, b.tfrecords, c.tfrecords, d.tfrecords], which contains 4 TFRecords paths named a.tfrecords, b.tfrecords, c.tfrecords and d.tfrecords. I'm reading these TFRecords with dataset = tf.data.TFRecordDataset(t_list). And I set a epoch_num by dataset = dataset.repeat(epoch_num) I have two questions about this function: Thanks!",https://stackoverflow.com/questions/53895757,10153344,Documentation Replication on Other Examples
53915078,"What are b, y, x and c which get flattened and returned along with the max-pooled features in tf.nn.max_pool_with_argmax?","I went through the documentation of tf.nn.max_pool_with_argmax where it is written The variables b, y, x and c haven't been explicitly defined hence I was having issues implementing this method. Can someone please provide the same.",https://stackoverflow.com/questions/53915078,7184172,Documentation Replicability
53919290,tensorflow sparse categorical cross entropy with logits,"I am a novice programmer trying to follow this guide. However, I ran across an issue. The guide says to define the loss function as: This gives me the following error: which I take to mean that from_logits is an argument not specified in the function, which is supported by the documentation, which that tf.keras.losses.sparse_categorical_crossentropy() has only two possible inputs. Is there a way to specify that logits are being used or is that even necesarry?",https://stackoverflow.com/questions/53919290,9185745,Documentation Replication on Other Examples
53924692,Why can tf.random.truncated_normal get a shape that is not a vector even though it says it only receives shape of a vector?,"I am working with TensorFlow in Python. I read through the documentation of tf.random.truncated_normal that the input 'shape' gets 1-D tensor or python array, i.e. a vector (according to https://www.tensorflow.org/guide/tensors). However, with the example I'm using, 'shape' is a 4-D tensor. Or is it considered a vector? Perhaps I have problem with the definition of vectors and tensors?",https://stackoverflow.com/questions/53924692,10832672,Documentation Replicability
72465331,not able to embed python tensorflow code in c++,"I am building NIST FRVT11 library. I have core code in python, that I want to call from c++, I have followed offical Python C API docs to use python functions in c++. I am able to load and predict with keras model once during initialization stage. But cannot predict from the same model after that model.predict function takes inifite time. I'm not even able to load the model, this time tf.keras.models.load_model takes inifite time. I am not getting any error that I can paste here :( NIST wants everything to be packaged within library, so I have built python locally and attaching that in the submission. python: 3.9.13 tensorflow: 2.8.0 keras: 2.8.0 python code c++ code",https://stackoverflow.com/questions/72465331,10412923,Documentation Replication on Other Examples
53951941,How can I load a saved model from object detection for inference?,"I'm pretty new to Tensorflow and have been running experiments with SSDs with the Tensorflow Object Detection API. I can successfully train a model, but by default, it only save the last n checkpoints. I'd like to instead save the last n checkpoints with the lowest loss (I'm assuming that's the best metric to use). I found tf.estimator.BestExporter and it exports a saved_model.pb along with variables. However, I have yet to figure out how to load that saved model and run inference on it. After running models/research/object_detection/export_inference_graph.py on the checkpoiont, I can easily load a checkpoint and run inference on it using the object detection jupyter notebook: https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb I've found documentation on loading saved models, and can load a graph like this: However, when I use that graph with the above jupyter notebook, I get errors: Is there a better way to load the saved model or convert it to an inference graph? Thanks!",https://stackoverflow.com/questions/53951941,1192963,Documentation Replication on Other Examples
53966148,Tensorflow: How to use boolean_mask in a way that perserves 2D tensor shape,"When I use tf.boolean_mask, the result is flattened. Is there a way to use it that preserves the original 3 array shape? The shapes of the individual arrays should change since they're shorter now. I'm looking for something like this",https://stackoverflow.com/questions/53966148,3259896,Documentation Replicability
54047604,How to assign custom gradient to TensorFlow op with multiple inputs,"I'm trying to use TensorFlow's @tf.custom_gradient functionality to assign a custom gradient to a function with multiple inputs. I can put together a working setup for only one input, but not for two or more. I've based my code on TensorFlow's custom_gradient documentation, which works just fine for one input, as in this example: This example runs silently, then closes. No issues, no errors. The variable optimizes as expected. However, in my application, I need to do such a calculation with multiple inputs, so something of this form: Running this in place of the example (and adding another variable input to the call of my_identify) results in the following error output. Best as I can tell, the last parts of the error are from the dynamic generation of the op -- the information format matches the C++ formatting required in the op establishment (though that's about all I know about it). Based on other custom gradient options, I surmised that the issue was a lack of supplied gradient for the second input argument. So, I changed my function to this: This results in the following more familiar error: The @custom_gradient decorator is only identifying the last returned element as a gradient. So, I tried putting the two gradients into a tuple as (grad, grad) such that there would only be ""two"" outputs for the function. TensorFlow rejected this too, this time because it can't call a tuple like it would a Tensor -- entirely reasonable, in hindsight. I've fussed around with the example some more, but to no avail. No matter what I try, I can't get the custom-defined gradient to deal with multiple inputs. I'm hoping that somebody with more knowledge than I regarding custom ops and gradients will have a better idea on this -- thanks in advance for the help!",https://stackoverflow.com/questions/54047604,10554082,Documentation Replication on Other Examples
54055707,Keras model.fit() with tf.dataset fails while using tf.train works fine,"Summary: according to the documentation, Keras model.fit() should accept tf.dataset as input (I am using TF version 1.12.0). I can train my model if I manually do the training steps but using model.fit() on the same model, I get an error I cannot resolve. Here is a sketch of what I did: my dataset, which is too big to fit in the memory, consists of many files each with different number of rows of (100 features, label). I'd like to use tf.data to build my data pipeline: I'd like to try a very basic logistic regression model: If I train it manually everything works fine, e.g.: However, if I instead try to use model.fit like this: I get an error message ValueError: Cannot take the length of Shape with unknown rank. inside the keras'es _standardize_user_data function. I have tried quite a few things but could not resolve the issue. Any ideas? Edit: based on @kvish's answer, the solution was to change the map from a lambda to a function that would specify the correct tensor dimensions, e.g.: and now, all needed to do is to call this function from map:",https://stackoverflow.com/questions/54055707,628654,Documentation Ambiguity
54059805,How to convert tensor dtype=tf.float32_ref to dtype=tf.float32?,I want to use the modify word_embeddings dtype from float32_ref to float32 through the function tf.cast(): But it did not work as expected and word_embeddings_modify dtype still tf.float32_ref.,https://stackoverflow.com/questions/54059805,9799714,Documentation Replicability
72627862,Can tf.dataset use mutliprocessing?,"The docs for tf.data.Dataset.map() state In contrast, the older(?) tf.keras.utils.GeneratorEnqueuer had the use_multiprocessing argument.",https://stackoverflow.com/questions/72627862,2135504,Documentation Replicability
72668593,Switching from keras.layers.LSTM to keras.layers.LSTMCell,"I currently have the following working LSTM implementation in Keras: I ran into my question because I want to be able to change the LSTM to a peephole LSTM. On this link I found a peephole LSTM implementation for Keras: https://docs.w3cub.com/tensorflow~1.15/keras/experimental/peepholelstmcell However when I change my code to the following: I get the following error: TypeError: call() missing 1 required positional argument: 'states' In the documentation it says that the Peephole LSTM implementation is equivalent to the tf.keras.layers.LSTMCell (https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell). Although it seems similar, this is different from to tf.keras.layers.LSTM that I am using (https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM). I cannot find a clear way to go from my first piece of code, to a piece of code that implements layers.LSTMCell. When I just change layers.LSTM to layers.LSTMCell I get the same type error as above, but then in the documentation I cannot find any parameter for the 'states' that is missing. How can I go from my current LSTM implementation, to one that uses layers.LSTMCell / experimental.PeepholeLSTMCell ?",https://stackoverflow.com/questions/72668593,18588854,Documentation Completeness
54155481,TensorFlow per_image_standardization vs mean standardization across full dataset,"I am curious about the difference between standardizing each image individually vs standardizing across the full data set. I am using tensorflow/models/official/resnet which is built using tf.estimator. The tf estimator supports an input pipeline function that produces a tf Dataset. The Dataset object applies the tf.image.per_image_standardization op that standardizes by subtracting the mean of the image itself from each pixel and enforces unit variance. This is different from other ML preprocessing that standardizes the image based on the mean across the whole dataset, such as with sklearn.preprocessing.StandardScaler. I'm confused as to whether any aspect of this input pipeline is persisted in the tf SavedModel exported from the tf.estimator.Estimator. So I'm wondering if I need to still apply feature standardization when serving the model, either via tf.contrib.predictor or when deploying the model in any other dnn format. Should I be applying standardization across the dataset even though I'm using the per_image_standardization? If so, should I just export the mean value from the whole image set somehow so that when serving the model the server can just pick up the mean value from the whole dataset and apply standardization that way?",https://stackoverflow.com/questions/54155481,779837,Documentation Replication on Other Examples
72707453,How to save a tensorflow dataset to multiple shards without using enumerate,"I have a tensorflow dataset with some elements in it, and I want to save it with tf.data.Dataset.save such that each element gets its own shard. Thus if the dataset contains 2,000 elements, it would be saved to 2,000 shards. The documentation here specifies how to create 1 shard only, but not how to make a shard for each element. Below, I am able to do it with enumerate, but is there another way to do it without also saving the index from enumerate?",https://stackoverflow.com/questions/72707453,9909857,Documentation Replicability
54175038,Creating a serving graph separately from training in tensorflow for Google CloudML deployment?,"I am trying to deploy a tf.keras image classification model to Google CloudML Engine. Do I have to include code to create serving graph separately from training to get it to serve my models in a web app? I already have my model in SavedModel format (saved_model.pb &amp; variable files), so I'm not sure if I need to do this extra step to get it to work. e.g. this is code directly from GCP Tensorflow Deploying models documentation",https://stackoverflow.com/questions/54175038,10647086,Documentation Ambiguity
72720129,Understanding tf.keras.metrics.Precision and Recall for multiclass classification,"I am building a model for a multiclass classification problem. So I want to evaluate the model performance using the Recall and Precision. I have 4 classes in the dataset and it is provided in one hot representation. I was reading the Precision and Recall tf.keras documentation, and have some questions: Any clarification of this function will be appreciated. Thanks in advance",https://stackoverflow.com/questions/72720129,17534198,Documentation Replicability
54211834,how to use tf.print (not tf.Print) in high-level Estimator api,"Currently, I'm using tf.Print to print(debug) tensor in estimator, but this api is marked deprecated, and recommend me to use tf.print instead. According to the RFC, by using tf.print, I need to have control of the running session, but Estimator is designed to hide session and graph from users. So, how to use tf.print in Estimator?",https://stackoverflow.com/questions/54211834,1285444,Documentation Replicability
72772487,Memory Leak With Custom Object Detection Model Tensorflow,"I am biggner in tensorflow. I used transfer learning machanism and create custom object detection model using ""ssd_resnet101_v1_fpn_keras"" pre-trained model. I follow the below documentation for custom traning: I observed one issue while I used it for detection it takes lot of RAM and not releasing it. I am sharing you the code snippet where it took lot of RAM and not releasing it. Memory profiler info: As you can see, it's take 191.8 Mb RAM. It's not releasing it after competion the process. I used gc.collect() and tf.keras.backend.clear_session() for releasing the memory. Both is not working for me. Please anyone can help me how can I solve this problem.",https://stackoverflow.com/questions/72772487,8938829,Documentation Replication on Other Examples
74143214,Loss reduction in tf.distributed.MirroredStrategy(),"I'm confused regarding using distributed strategy and the correct way of reduction in loss functions. I implemented a U-Net using tf.distribute.MirroredStrategy(). Everything works fine using default loss BinaryCrossentropy as follows: However, I want to create custom loss functions. To start with, I wrote a wrapper containing BinaryCrossentropy, to get familiar with the correct way of using the reduction methods. I followed the instructions in https://www.tensorflow.org/tutorials/distribute/custom_training#define_the_loss_function and used tf.nn.compute_average_loss in order to divide by the global batch_size. which is called in the following way: It also works, but I realised a difference of factor of number or replica compared to using tf.keras.losses.BinaryCrossentropy(). I.e., when using two kernels, using BinaryCrossentropy() directly yields a loss twice as large as my custom loss. Thus, to geht the same, I would need to divide by the batch size per replica instead of global batch size, i.e., the way it should NOT be done according to the documentation. However, the documentation refers to building an own training routine, whereas I am using model.compile() and model.fit() methods. Can anybody explain this behaviour to me? UPDATE: The use of tf.nn.compute_average_loss or the use of any reduction on the batch axis is not needed when using model.compile() and model.fit() at all - the reduction and scaling is done automatically. However, I still do not know how model.fit() does work internally. Thanks and cheers, everybody",https://stackoverflow.com/questions/74143214,20293308,Documentation Ambiguity
56767246,How to wrap tf.cond function with keras.layers.Lambda?,"I'm trying to define a custom layer in keras,but I can't find a way to warp tf.cond with layers.Lambda function",https://stackoverflow.com/questions/56767246,11701537,Documentation Replicability
74182037,"How to ""update"" from module tf.keras.preprocessing.image to tf.keras.utils.image_dataset_from_directory for features extraction","This code part is common to both ""problematic"" codes below: The following code I successfully use to extract features of a set of images using module tf.keras.preprocessing.image. Thereafter I train a simple nearest-neighbor model using brute-force algorithm and I'm able to find three other images that are really similar to the query image as you can see below: But as pointed in the documentation this preprocessing module is deprecated. So, I would like to ""update"" the code as suggested in the documentation: ""Prefer loading data with tf.keras.utils.image_dataset_from_directory, and then transforming the output tf.data.Dataset with preprocessing layers"". For that I'm trying the following: And now I begin with the features extraction After that I do the same and train the nearest neighbor model with this features, but the results are catastrophic as you can see below: What I'm doing so wrong that I have such different results? == EDIT 1 == Answering @DWKOT using the same image we have following results: And the code that give us the distance to the 5 nearest neighbors: With the following results: With the second code we have following result for query / similar image: / And following results for the first five ""similar"" images: What is also strange as I would expect similar images having values next to zero and different ones far from zero...",https://stackoverflow.com/questions/74182037,3499881,Documentation Replication on Other Examples
52671481,Why are variables defined with 'self' automatically given a ListWrapper() while inheriting from tf.keras.Model?,"I am not familiar with ListWrapper(), but it is being applied to all list variables created with self when my class inherits from tf.keras.Model. https://www.tensorflow.org/api_docs/python/tf/keras/models/Model This is bad because it is causing an IndexError when I use it in certain functions, or even by just passing it through my Tensorflow model. (I am using eager execution) A small reproduction of the problem can be seen with this code: Output: Switching the inheritance to be from object solves the issue, which is how I know its tf.keras.Model that is causing this. I tried looking it up but can't find anything on this. Any tips? Thanks!",https://stackoverflow.com/questions/52671481,10458815,Documentation Replication on Other Examples
52711895,How to run define Tensorflow graph were all variables are in float16 instead instead of float32,"By default, the variables Tensorflow is in float32. To save memory, I'm trying to run in float16. In my graph, every place where I could define the datatype as float16, I did. However, I get an error when I run the code Here's my code below. And this is this is the error message The error comes from line tf.nn.sampled_softmax_loss. At first I thought perhaps tf.segment_mean may cast the output as a float32, so I tried casting averaged_embeds to float16 but I still get the same error. From the documentation, there doesn't seem to be a way to define any data types in sampled_softmax_loss https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss",https://stackoverflow.com/questions/52711895,3259896,Lack of Alternative Solutions/Documentation
52731151,Tesnorflow: How to provide your own `sampled_values` for tf.nn.sampled_softmax_loss?,"In tf.nn.sampled_softmax_loss, one of the optional inputs is to put your own samples values. I would like to provide my own samples values so that I can use float16 (half precision) variables. If sampled_values is left blank, Tensorflow will use log_uniform_candidate_sampler to get values, which can only return float32. Here are all the inputs. https://www.tensorflow.org/api_docs/python/tf/nn/sampled_softmax_loss This is the information they give for the sampled_values arg : I'm trying to figure out how to provide this tuple. What exactly are the sampled_candidates, true_expected_count, sampled_expected_count ? I know that it's sampling the weights and corresponding biases, so do I put them together in it's own tuple for sampled_candidates ? Also, am I putting the int for the place of the weight in the matrix, or am I putting the whole embedding itself? I've also looked at Tensorflow's math supplimental on negative sampling but I couldn't find any information for my issue https://www.tensorflow.org/extras/candidate_sampling.pdf In my search, I found this very similar question on a google forum https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/6IDJ-XAIb9M The Answer given is However, I still don't know how to input a value. I'm not sure what the datatypes should be, and if N is the int place in the embedding matrix, or the embedding itself. Also, I'm guessing N should be a list of values itself, the size of the number of negative labels we have to sample. I was wondering if I could get a example with some values. For example, for a negative sampling of 3, do I do something like this? sampled_values = ([4,29, 12], [1, 1, 1], [0, 0, 0]) Also, the documentation says that the tuple should be "" returned by a *_candidate_sampler function"" Does that mean I need to provide a function that returns the tuple, instead of the tuple itself?",https://stackoverflow.com/questions/52731151,3259896,Documentation Replication on Other Examples
52802359,Problem with tf.SparseTensor and tf.while_loop,"I face a problem when I try to change the shape of tf.SparseTensor inside a tf.while_loop. Let's say I have this sparse tensor: So, I want to take a slice from the first 3 rows. I know for that purpose I can use tf.sparse_slice but this is an example. In my real code, I gather multiple rows from the sparse Tensor which they are not serial. The code I wrote is this: which does't work for some reason when I run it. I get this: According to https://www.tensorflow.org/api_docs/python/tf/while_loop it says that: The shape_invariants argument allows the caller to specify a less specific shape invariant for each loop variable, which is needed if the shape varies between iterations. The tf.Tensor.set_shape function may also be used in the body function to indicate that the output loop variable has a particular shape. The shape invariant for SparseTensor and IndexedSlices are treated specially as follows: a) If a loop variable is a SparseTensor, the shape invariant must be TensorShape([r]) where r is the rank of the dense tensor represented by the sparse tensor. It means the shapes of the three tensors of the SparseTensor are ([None], [None, r], [r]). NOTE: The shape invariant here is the shape of the SparseTensor.dense_shape property. It must be the shape of a vector. What am I missing here?",https://stackoverflow.com/questions/52802359,9854132,Documentation Replication on Other Examples
37044006,Tensorflow conditional throwing value error,I am trying to use conditionals with tensorflow and I am getting the error: Below is the code I use that is throwing the error. It is saying the error is in the conditional Stack trace: I have tried changing the WORKING to be an array instead of a scalar. I believe that the problem is that tf.equal is returning an int32 instead of the bool that it is supposed to return according to the documentation,https://stackoverflow.com/questions/37044006,2187510,Documentation Ambiguity
70880589,what does cardinality mean in relation to an image dataset?,"After successfully creating a tensorflow image Dataset with: dataset = tf.keras.utils.image_dataset_from_directory(...) which returns Found 21397 files belonging to 5 classes. Using 17118 files for training. There is the cardinality method: dataset.cardinality() which returns a tensor containing the single value tf.Tensor(535, shape=(), dtype=int64) I've read the docs here but I don't understand what 535 represents or why its different to the number of files? I ask, because I would like to understand how cardinality plays into this equation: steps_per_epoch = dataset.cardinality().numpy() // batch_size",https://stackoverflow.com/questions/70880589,14777655,Lack of Alternative Solutions/Documentation
52814880,Neuron freezing in Tensorflow,"I need to implement neurons freezing in CNN for a deep learning research, I tried to find any function in the Tensorflow docs, but I didn't find anything. How can I freeze specific neuron when I implemented the layers with tf.nn.conv2d?",https://stackoverflow.com/questions/52814880,6194504,Documentation Replicability
52827483,Tensorflow: What if my cond is not a scalar in tf.cond in while loop?,"in tf.cond of tensorflow, cond has to be a scalar, but in my case cond need to be rank 1 with shape [batch_size]. Is there any method to solve this problem? Have tensorflow provided a solution to it?",https://stackoverflow.com/questions/52827483,7547975,Documentation Replicability
37086098,Does tensorflow map_fn support taking more than one tensor?,Does tf.map_fn support taking more than one tensors as is supported by python's native map function (example provided below)?,https://stackoverflow.com/questions/37086098,6227444,Documentation Replicability
70932051,How to improve the execution time of gradient estimation using tf.GradientTape in tensorflow 2.x,"The objective is to implement https://web.casadi.org/blog/tensorflow/, which was written using Tensorflow 1.x and gpflow 1.x, using Tensorflow 2.x and gpflow 2.x . I have implemented it in Tensorflow 2.x (I attached them at the end of the question). However, still, I could not get as faster as the initial implementation, i.e., using Tensorflow 1.x and gpflow 1.x. Here are execution time Using Tensorflow 1.x and gpflow 1.x. Using Tensorflow 2.x and gpflow 2.x. According to these execution times, it mainly depends on the gradient estimation nlp_grad_f. Tensorflow 1.x uses graph-based gradient estimation which seems to be rather faster than Tensorflow 2.x tf.GradientTape My question as follows: What's the proper way to implement the gradient estimation in Tensorflow 2.x or how to improve this code further? I've checked https://www.tensorflow.org/api_docs/python/tf/function and what I could to make it faster. I want it to reduce as faster as I get in Tensorflow 1.x. For reference, I include all the codebase, in case someone wants to execute and see it. ocp.py tensorflow_casadi_mod_ocp.py",https://stackoverflow.com/questions/70932051,1574779,Documentation Replication on Other Examples
70950860,How to update tf.contrib.layers.real_valued_column code to tf 2.0,"I came across a Gaussian Kernel classifier tutorial in Python that utilizes tensorflow. In part of the tutorial, tf.contrib.layers.real_valued_column is referenced. However, as far as I know, tf.contrib is deprecated. How can I change the statement to work in tensorflow 2.0 ? I tried using tf_slim and tf.keras, but I was unable to find the equivalent statements. Thanks in advance.",https://stackoverflow.com/questions/70950860,18096774,Lack of Alternative Solutions/Documentation
52888624,What dose zero_debias mean in tf.train.ExponentialMovingAverage?,"tf.train.ExponentialMovingAverage has a parameter 'zero_debias', but I don't know what will happen if I set it to True. So what's this parameter for? Great thanks in advance.",https://stackoverflow.com/questions/52888624,9589731,Documentation Replicability
37159372,API Reference for RNN and Seq2Seq models in tensorflow,Where can I find the API references that specifies the available functions in the RNN and Seq2Seq models. In the github page it was mentioned that rnn and seq2seq were moved to tf.nn,https://stackoverflow.com/questions/37159372,1363836,Documentation Replicability
52938457,Tensorflow debugging,Is there a way to print or obtain the above tensor x1 or x in tensorflow. The tf.print and sess.eval(x1)? It seems to not print the stacked frame. Tried the tfdg too,https://stackoverflow.com/questions/52938457,3129625,Documentation Replicability
71019644,Equivalent tensorflow expression to numpy mask,"I have a numpy array named PixelData of unknown shape, and I am using the following condition to filter values in the array greater than some value x using a mask: When I convert this numpy array to a tensor, I cannot perform the same masking operation. I have tried using tf.where as follows: In the official documentation, they always seem to define the mask dimensions in advance to equal the dimensions of the tensor being masked, but then they talk about the dimensions being broadcasted automatically, so I am a bit confused. Are these two functions equivalent? Are there any situations where they may produce different outputs?",https://stackoverflow.com/questions/71019644,17815854,Documentation Ambiguity
52956268,tf.data.Dataset: Map fails to split string,"I have a tf.data.Dataset that I've created like this: I want to split just the reviews (strings) on whitespace. When I do this: Python complains, telling me: I've looked at the docs and it's not obvious why Python thinks I've given two arguments...any ideas? Thanks!",https://stackoverflow.com/questions/52956268,1316501,Documentation Replicability
52959039,Using tf.layer.conv proper usage - activation and activity_regularizer,"In tensorflow's tf.layer.conv1d webpage https://www.tensorflow.org/api_docs/python/tf/layers/conv1d, they provide the option to set the ""activation"" and ""activity_regularizer"" for your model. I already have a model built, so it would have been nice if I could have just set activity_regularizer = tf.layer.batch_normalization(training=True) or activation=tf.nn.leaky_relu(alpha = 0.001).However, if i try to put any inputs into tf.layer.batch_normalization or tf.nn.leaky_relu they give errors saying there are arguments missing (""inputs"" for former and ""features"" for latter). TLDR: seems to ""work"" (it runs at least), but this: says there are missing arguments for activation and activity_regularizer. I may be using activation and activity_regularizer completely wrong so feel free to correct me. I am hoping there is an easy fix to this, otherwise the only option for me is to write extra lines of code to separate the activation and batch normalizations from conv1d. Although, I do not see the purpose of having activation and activity_regularizer built-in tf.layers.conv1d if I can't change their parameters. Last point: I am particularly worried about tf.layer.batch_normalization because I am assuming it is set at the default training=False and trainable = True which should not always be the case in the block of code that ""worked"".",https://stackoverflow.com/questions/52959039,9912116,Documentation Replication on Other Examples
52966243,Tensorflow: NotImplementedError: The reduce() transformation does not currently support nested datasets as inputs,"In Tensorflow 1.12 the tf.data.Dataset.reduce() and tf.data.Dataset.window() methods are introduced. From the release notes: However how to use these functions? This gives a NotImplementedError: I use tensorflow version '1.12.0-rc1' EDIT: From https://www.tensorflow.org/versions/r1.12/api_docs/python/tf/contrib/data/sliding_window_batch THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.data.Dataset.window(size=window_size, shift=window_shift, stride=window_stride).flat_map(lambda x: x.batch(window.size)) But how to use this if the dataset is generated with So each item in the dataset contains two elements. Then there is a TypeError: EDIT: Solved by using zip",https://stackoverflow.com/questions/52966243,987397,Documentation Replication on Other Examples
52969867,make tensorflow dataset from huge number of images(*.jpg) and labels(*.mat),"I have a huge number of images with their labels (.mat) file (cannot use tf.data.Dataset.from_tensor_slices()) and I want to use tf.data API to make a tensorflow dataset out of it. As I read in the documentation, I can use tf.data.TextLineDataset for large number of data(I have to have a txt file with the address of all the images and send the path of the txt file as tf.data.TextLineDataset argument). Then, I can use map method to read txt file (tf.read_file) decode jpg image (tf.image.decode_jpeg) and do some basic transformation on the image. However, I cannot use scipy.io.loadmat in any part of map method because I have no string indicating the path to the mat file. All I have is tf.Tensor. I don't think that reading all images and making a TFRecord out of it is that much efficient in this case because then I am basically doing every thing two times. Once, reading the whole images and making TFRecord, and once again, reading TFRecord to make tensorflow dataset. Any idea how I can resolve this issue? This is my code: and then:",https://stackoverflow.com/questions/52969867,10354279,Documentation Replication on Other Examples
52976606,Global step not incrementing with batch norm and custom estimator,"I have a customer estimator that has several layers that look like the following in the model function: Because I'm using batch norm, the training op is set up like this: Where the optimizer is usually tf.train.AdamOptimizer. However, when I go to train the estimator the global step never increments (so training will run forever), and I get this: WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize. I am passing tf.train.get_global_step() to minimize, so I'm not sure why it never gets updated. My hunch is that it has something to do with the batch normalization because when I remove that or replace it with dropout, everything works fine (even when keeping the update ops lines that are required for batch normalization per the documentation). Anyone know what is going on? Happy to post more code if helpful.",https://stackoverflow.com/questions/52976606,9226290,Documentation Replication on Other Examples
37234114,How to do gradient descent for not all variables in tensorflow,"In tensorflow, tf.train.GradientDescentOptimizer does gradient descent for all variables in default. Can i just do gradient descent for only a few of my variables and 'lock' the others?",https://stackoverflow.com/questions/37234114,5881181,Documentation Replicability
71059361,How to I train a keras functional API model with batched tf Dataset objects? (BatchDataset),"I am constructing a tf keras model using the functional API. This model will train fine on large memory mapped arrays. However, for numerous reasons it can be advantageous to work with tensorflow Dataset objects. Therefore, I use from_tensor_slices() to convert my arrays to Dataset objects. The problem is that the model will no longer train. The keras docs: Model training APIs indicate that dataset objects are acceptable. The guide I'm following on how to train is found here: Using tf.data with tf keras Guides on how to use the keras functional API are here. However, training a functional API model with a tf Dataset object is not outlined. A MWE is provided here: The error I receive is: ValueError: Input 0 of layer dense_1 is incompatible with the layer: expected axis -1 of input shape to have value 75 but received input with shape (75, 1) In addition, the error from my actual model I'm trying to train is slightly different but seems to be malfunctioning under the same principle. It is the following: What is the proper way to train a keras functional API model on a BatchDataset object?",https://stackoverflow.com/questions/71059361,9008259,Documentation Replication on Other Examples
38045785,Does the example for decaying the learning rate in TensorFlow website actually decay the learning rate?,"I was reading the decaying learning rate and thought there might be a mistake in the docs and wanted to confirm. It says that the decay equation is: however, if global_step = 0 I'd guess there is never a decay, right? However, look at the example: It has a global_step = tf.Variable(0, trainable=False) that is set equal to zero. Thus, no decay. Is this correct deduction? I thought there might be a caveat due to integer division when staircase function is set to true, but even in integer division that still seems that there is no decay. Or is there a misunderstanding of what staircase does?",https://stackoverflow.com/questions/38045785,1601580,Documentation Replicability
71773122,cannot import name 'normalization' from 'tensorflow.python.keras.layers',"I am trying to use tensorflow_rankings, but cannot import it due to the above error. I am using tensorflow 2.8.0 and tensorflow_rankings 0.5.0, which seem to be the latest stable builds. They are what get automatically installed from I am on Python 3.8.10, Windows 11. The TF 2.8.0 docs show there is a Normalization layer in tf.keras.layers. The error seems to come from: in Any advice?",https://stackoverflow.com/questions/71773122,15848470,Documentation Replicability
71791115,Nan Loss when training Deep neural Recommender model using tensorflow,"I am trying to follow tensorflow documentation and applying same technique to one of toy dataset. During training I am getting all loss as Nan. I have tried to debug the same using Debugger V2 and I could see that tf.keras.layers.GlobalAveragePooling1D is giving Nan due to division by 0, which is causing all values to be Nan during backpropagation. But what is not clear from the debugger V2 GUI why the sum is becoming 0. I did try to reduce the number of features and the size of the dataset, but each of this activity is giving me new error (probably I shall start a separate question thread for each issues at a later point ). Below is the code for reference. I am providing the dataset as well here. I had tried below code on Google Colab. Preparing Data Reading data to tensorflow dataset Preparing train and test dataset User model class Item model Query model . Creating Deep model Creating deep model for the Item model Combining both query and candidate model training the model Below id the outout that I got",https://stackoverflow.com/questions/71791115,12271381,Documentation Replication on Other Examples
71813366,How to add loss function with activity regularizer in distributed training,"I have an deep autoencoder with an activity_regularizer in the bottleneck layer: I then added some tf2 preprocessing feature layers: So now I can't just add a loss function like model.compile(loss='mse') since the input are raw features and the outputs are scalars. I need to compute the loss based on the output of preprocessed and the final outputs. This answer suggested I use the model.add_loss() function like: This worked but when I tried training this using tf.distribute.MultiWorkerMirroredStrategy() I got the error: Reading https://www.tensorflow.org/tutorials/distribute/custom_training the documentation states: So I'm now a bit confused. Does the above statement apply to the activity_regularizer I added? How does this work with the global_batch_size? How can I solve this in distributed training? TLDR: Do I need to ""scale the loss value by number of replicas"" when using hidden layer activity_regularizer in distributed training?",https://stackoverflow.com/questions/71813366,7242490,Documentation Ambiguity
38111170,How is the input tensor for TensorFlow's tf.nn.dynamic_rnn operator structured?,"I am trying to write a language model using word embeddings and recursive neural networks in TensorFlow 0.9.0 using the tf.nn.dynamic_rnn graph operation, but I don't understand how the input tensor is structured. Let's say I have a corpus of n words. I embed each word in a vector of length e, and I want my RNN to unroll to t time steps. Assuming I use the default time_major = False parameter, what shape would my input tensor [batch_size, max_time, input_size] have? Maybe a specific tiny example will make this question clearer. Say I have a corpus consisting of n=8 words that looks like this. Say I embed it in a vector of size e=3 with the embeddings 1 -&gt; [10, 10, 10], 2 -&gt; [20, 20, 20], and 3 -&gt; [30, 30, 30], what would my input tensor look like? I've read the TensorFlow Recurrent Neural Network tutorial, but that doesn't use tf.nn.dynamic_rnn. I've also read the documentation for tf.nn.dynamic_rnn, but find it confusing. In particular I'm not sure what ""max_time"" and ""input_size"" mean here. Can anyone give the shape of the input tensor in terms of n, t, and e, and/or an example of what that tensor would look like initialized with data from the small corpus I describe? TensorFlow 0.9.0, Python 3.5.1, OS X 10.11.5",https://stackoverflow.com/questions/38111170,1120370,Documentation Replicability
38114534,Basic 1d convolution in tensorflow,"OK, I'd like to do a 1-dimensional convolution of time series data in Tensorflow. This is apparently supported using tf.nn.conv2d, according to these tickets, and the manual. the only requirement is to set strides=[1,1,1,1]. Sounds simple! However, I cannot work out how to do this in even a very minimal test case. What am I doing wrong? Let's set this up. OK, now generate a basic convolution test on two small arrays. I will make it easy by using a batch size of 1, and since time series are 1-dimensional, I will have an ""image height"" of 1. And since it's a univariate time series, clearly the number of ""channels"" is also 1, so this will be simple, right? BOOM. Error. OK, For a start, I don't understand how this should happen with any dimension, since I've specified that I'm padding the arguments in the convolution OP. but fine, maybe there are limits to that. I must have got the documentation confused and set up this convolution on the wrong axes of the tensor. I'll try all possible permutations: Result: Hmm. OK, it looks like there are two problems now. Firstly, the ValueError is about applying the filter along the wrong axis, I guess, although there are two forms. But then the axes along which I can apply the filter are confusing too - notice that it actually constructs the graph with input shape (5, 1, 1, 1) and filter shape (1, 1, 1, 3). AFAICT from the documentation, this should be a filter that looks at on example from the batch, one ""pixel"" and one ""channel"" and outputs 3 ""channels"". Why does that one work, then, when others do not? Anyway, sometimes it does not fail while constructing the graph. Sometime it constructs the graph; then we get the tensorflow.python.framework.errors.InvalidArgumentError. From some confusing github tickets I gather this is probably due to the fact that I'm running on CPU instead of GPU, or vice versa the fact that the convolution Op is only defined for 32 bit floats, not 64 bit floats. If anyone could throw some light on which axes I should be aligning what on, in order to convolve a time series with a kernel, I'd be very grateful.",https://stackoverflow.com/questions/38114534,11730,Documentation Ambiguity
38136081,Setting num_epochs on tf.train.string_input_producer produces an error,Setting num_epochs on tf.train.string_input_producerto anything other than None produces the error What causes this and how can it be fixed?,https://stackoverflow.com/questions/38136081,4392784,Documentation Replicability
71893462,How to retrieve file paths from a tf.data.Dataset created with from_tensor_slices() and shuffled after every epoch,"First of all, I would like to say that this is my first question in stackOverflow, so I hope that the question as a whole respects the rules. I realize that the question is a bit long, but I would like to provide as much background and detail as possible . I am currently developing a real-time image binary classification system based on Tensorflow 2.8.0 and I am quite new at it. Here are some of the peculiarities of the data that I have for the mentioned project: After reading the official documentation and tutorials, I thought that, in order to load, preprocess and feed data to my CNN, the best option was to build an input pipeline using the tf.data.Dataset class due to the ease of opening FITS files. My general procedure follows this idea: Here are some code fragments in case they help to understand my goal: Following the previous idea, I have successfully created and tested different types of pipelines for different types of partitions of my dataset: unlabeled (remember that only a portion of the data is labeled), labeled and weighted labeled (I wanted to see if my models improve by specifying class weights when training). However, in order to monitor results and make proper adjustments to my model, I would like to retrieve the usual predictions, real labels and images next to the file paths preserving the ability to shuffle the data after every epoch. I have managed to solve my question if I do not shuffle data with .shuffle(reshuffle_each_iteration=True), but models' performance is supposed to increase if data is shuffled after each epoch, according to several sources. I have read different posts in stackOverflow related to my question. I will list those posts next to the problems that I have found for my particular use case: I have also tried to keep a separate tf.data.Dataset with only the file paths but if I call the shuffle method with the reshuffle_each_iteration=True option in both tf.data.Dataset instances, the order of their elements does not match even if I set the same seed. In short, is it possible to achieve what I want? If so, how should I proceed? Thank you very much in advance.",https://stackoverflow.com/questions/71893462,18816743,Documentation Replication on Other Examples
53760992,Difference between tf.layers.dense and tf.nn.xw_plus_b,"What is the difference between tf.layers.dense and tf.nn.xw_plus_b in TF? What is the default activation used in tf.layers.dense when ""activation"" argument is passed as None?",https://stackoverflow.com/questions/53760992,8238383,Documentation Replicability
71933464,How to make true_fn of tf.cond skip a for loop in tensorflow v1.0/python?,"I want to use tf.cond to mimic the python if-else logic in the _preprocessing_fn of transform.py. Specifically, if the condition of tf.cond is true, I want to skip the current iteration of the for loop. This seems problematic because true_fn and false_fn parameters of tf.cond are expected to return Tensors according to the documentation. However, in my case, I want true_fn (aka skip_feature_fn)to simply ""continue"" to the next for loop iteration. Also, I want false_fn to take in two inputs (feature and sp) and simply feed them to some other API (e.g. tft.vocabulary). I don't expect either of true_fn or false_fn to return anything. Could someone help me accomplish my goal? Here is the code snippet I'm working with: Thank you.",https://stackoverflow.com/questions/71933464,4982651,Documentation Replication on Other Examples
71947836,Merge two tensorflow datasets into one dataset with inputs and labels,"I have two tensorflow datasets that are generated using timeseries_dataset_from_array (docs). One corresponds to the input of my network and the other one to the output. I guess we can call them the inputs dataset and the targets dataset, which are both the same shape (a timeseries window of a fixed size). The code I'm using to generate these datasets goes like this: The problem is that when calling model.fit, tf.keras expects that if a tf.data.Dataset is given in the x argument, it has to provide both the inputs and targets. That is why I need to combine these two datasets into one, setting one as inputs and the other one as targets.",https://stackoverflow.com/questions/71947836,10649437,Documentation Replication on Other Examples
71992472,"Deep dream ""guide"" image in tensorflow","I'm trying to modify the deep dream code from the Tensorflow docs here: https://www.tensorflow.org/tutorials/generative/deepdream Specifically, I want to use a ""guide image"" to produce the dream features. This was originally shown in Caffe in this notebook (at the bottom): https://github.com/google/deepdream/blob/master/dream.ipynb In their example, they used an image of flowers and produced flower-like features on top of an image of clouds. To do this, they provide an alternate loss function. From the Caffe notebook: In Caffe, it looks like this: I translated this to Tensorflow like so: However, tape.gradient(loss, img) returns None. I thought that it was because argmax is not differentiable. However, if I gather from the layer_activations instead -- tf.gather(layer_activation, max_act_idx, axis=1) -- then it produces a gradient (but not the desired image). So it's clearly able to step back through the tape, from the returned loss value to the input image, but only in this second case. What's going on here?",https://stackoverflow.com/questions/71992472,1431917,Documentation Ambiguity
72041726,Is there a difference between creating tf.Variable and keras.layers.Layer.add_weight(),I've seen both approaches in Tensorflow documentation: #1 #2 Is there a fundamental difference between creating a variable with self.add_weight() or tf.Variable() inside your custom layers? Examples are taken from https://www.tensorflow.org/tutorials/customization/custom_layers and https://www.tensorflow.org/guide/keras/custom_layers_and_models respectively.,https://stackoverflow.com/questions/72041726,16241829,Documentation Replicability
72042341,"Convert pandas data frame column, which has values of vectors, into tensors","My question is how to convert a vector on pandas data frame into tensors. The data frame has a resume column which has a vector representations of each resume document. I need to convert this column of the dataset into Tensors. The code is here below. The resume column has a list of numbers or we can say vectors and the category column of the data frame has scalar values. I tried to convert into tensors in this way: tf.convert_to_tensor(output[[""Resume""]]) Other approachs I have tried are numeric_dict_ds = tf.data.Dataset.from_tensor_slices((dict(output[[""Resume""]]), output[[""Category""]])) And the last approach was numeric_dataset = tf.data.Dataset.from_tensor_slices((numeric_features, target)) But None of them is working",https://stackoverflow.com/questions/72042341,,Documentation Replicability
38356622,Distributing graphs across several machines in Distributed Tensorflow,"I am currently working on a project using Distributed Tensorflow. My goal is to run several independent graphs across several different machines. As an example, I want to do something like this (assume that the server is open on each machine) My current attempt at this is shown in the following code. However, this code runs the sessions in serial, but I want them to be executed in a parallel distributed manner. While reading the documentation about Distributed Tensorflow, I found that tf.device allows me to set which CPU or GPU to run Tensorflow Ops on. Is there something similar that allows me to set the session target to specify which machine will run which op? Or is there another way of distributing Tensorflow Ops?",https://stackoverflow.com/questions/38356622,6585219,Documentation Replication on Other Examples
72165812,Connecting BatchDataset with Keras VGG16 preprocess_input,"I am using tf.keras.preprocessing.image_dataset_from_directory to get a BatchDataset, where the dataset has 10 classes. I am trying to integrate this BatchDataset with a Keras VGG16 (docs) network. From the docs: However, I am struggling to get this preprocess_input working with a BatchDataset. Can you please help me figure out how to connect these two dots? Please see the below code: This will throw TypeError: 'BatchDataset' object is not subscriptable: From TypeError: 'DatasetV1Adapter' object is not subscriptable (from BatchDataset not subscriptable when trying to format Python dictionary as table) the suggestion was to use: However, this also fails: This is all using Python==3.10.3 with tensorflow==2.8.0. How can I get this working? Thank you in advance.",https://stackoverflow.com/questions/72165812,11163122,Documentation Replication on Other Examples
72184958,Pydantic: Type hinting tensorflow tensor,"any idea of how to type-hint tf tensors using pydantic??. Tried default tf.Tensor and tf.flaot32 Looking at documentation in pydantic, i believe something like this arbitrary class need to be defined... with following in main..",https://stackoverflow.com/questions/72184958,10027860,Documentation Replicability
75403101,How do I distribute datasets between multiple GPUs in Tensorflow 2?,"I'm trying to understand how to use multiple gpus to train a model on data too large for the GPU memory. Using tf.distribute.MirroredStrategy seems to copy the full data set to each GPU. What I'm hoping to do is to send a subset of the full dataset to each GPU (2 or 4 gpus) and use MirroredStrategy to reconcile parameter updates on each epoch. MirroredStrategy.distribute_datasets_from_function() looks promising. https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#distribute_datasets_from_function Problem details: A fairly complicated multimodal NN with ~200k parameters synthesizing many text, transactional, and structured inputs and with multiple regression and probabilistic outputs. I'm looking at moving development from a single GPU with 24gb memory to cloud compute with multiple 16gb cards on a single node. The input and targets are currently dictionaries of numpy arrays. I'm hoping for a toy example converting those dictionaries into a distributed data set through to training with different subsets of the full data set assigned to each GPU. I attempted this: This runs on a 50% sample of the data, but returns OOM on the full sample on 2 GPUs. The full data set appears to be copied to each of the 2 16gb GPUs available. The same model runs with a 100% sample on a single 24gb GPU.",https://stackoverflow.com/questions/75403101,1998707,Documentation Replication on Other Examples
75401761,Change Verbosity of Keras train_on_batch()?,"I am training a GAN using Keras's train_on_batch() command. This is very similar to Keras's fit(). However, in the documentation for fit(), there is a parameter for verbose, which changes how often a progress bar is printed to the console. My model has many batches, and so it is printing tons of progress bars to the command line. Unfortunately, train_on_batch() does not have a verbose parameter. Is there a workaround for this? Is there a Keras global variable/environment variable that I can set? I don't want to disable my program from printing to the console, I just want to change the verbosity of specifically train_on_batch(). For clarify, I am using Keras directly from the Keras package, I am not using tf.keras.",https://stackoverflow.com/questions/75401761,12276162,Documentation Replicability
75474546,Different behaviour between tf.reduce_dims + broadcasting and tf.reduce_dims + tf.tile,What could be causing that this code behaves differently in TensorFlow by exchanging both lines as marked in the code? The broadcasting would happen after creating the mask so I don't think this is caused by the tf.math.equal function. In the end the sum is close but different in both cases. Thanks!,https://stackoverflow.com/questions/75474546,11268438,Documentation Replicability
75478235,tf.image.ssim() not accepting 'return_index_map' argument,"The documentation for Tensorflow's tf.image.ssim() says that if you want to return an element-wise structural similarity map, you can set ""return_index_map"" to True. It is returning an error when I define the argument as either True or False. I am running Tensorflow 2.10 on an Apple M1 Max. When I run: I get the error:",https://stackoverflow.com/questions/75478235,20324823,Documentation Replicability
75482826,What is negative_slope argument of tf.keras.layers.ReLU?,"tf.keras.layers.ReLU has negative_slope argument which is explained as Float &gt;= 0. Negative slope coefficient. Default to 0. Is this to make it as Leaky ReLU? If so, is it the same with the alpha argument of the tf.keras.layers.LeakyReLU?",https://stackoverflow.com/questions/75482826,4281353,Documentation Replicability
75555845,How to check whether masking works OK in Tensorlow / Keras,I want to train a transformer model that maps input vectors to an output vector. The input vector is padded with zero to make all the input vectors have the same length. To skip the padded values I use the tf.keras.layers.Masking layer. Here is the full model: The TransformerEncoder's documentation can be found here: https://keras.io/api/keras_nlp/layers/transformer_encoder/ This model works and generates outputs but I have no idea if the padded values are being skipped or not. Can anyone tell me if the model is OK or not? Or how can I check whether the padded values are skipped?,https://stackoverflow.com/questions/75555845,12014637,Documentation Replication on Other Examples
75572543,What to look out for when passing a generator into model.fit in tensorflow?,"I want to replace the x and y training data parameters in tf.keras.Model.fit with a generator. However, some subtlety seems to escape me, as the model accuracy doesn't improve with the generator when training. As far as I understand the documentation, the generator is supposed to yield tuples (x_vals,y_vals), such that x_vals is a concatenation of batch_size-many training samples along a new 0th dimension, and 'v_vals' is the concatenation of their corresponding labels. As long as the generator fulfills this, as I understand it, we can just replace the x parameter in tf.keras.Model.fit with the generator and omit the y parameter, though to define an epoch, we also need to specify 'steps_per_epoch' in fit. There however seems to be something here I misunderstood or forgot, because starting with a model and input data that trains (i.e. its accuracy improves) and replacing the training data array with a generator as discussed, results in a model that doesn't train (i.e. its accuracy instead goes up a little, then however goes back down till its equal to chance). The corresponding code: The model also trains if one first let's the generator generate a long list of samples and then passes those into fit as xand y:",https://stackoverflow.com/questions/75572543,8536211,Documentation Replication on Other Examples
55560676,How to use tf.while_loop with eager execution?,"In the documentation, the body of a tf.while_loop needs to be a python callable. works but throws a ValueError: Attempt to convert a value (None) with an unsupported type() to a Tensor In 2.0, eager execution is default, I wonder what's the problem?!",https://stackoverflow.com/questions/55560676,7779411,Documentation Replicability
55573670,Unexpected output for tf.nn.sparse_softmax_cross_entropy_with_logits,"The TensorFlow documentation for tf.nn.sparse_softmax_cross_entropy_with_logits explicitly declares that I should not apply softmax to the inputs of this op: However if I use cross entropy without softmax it gives me unexpected results. According to CS231n course the expected loss value is around 2.3 for CIFAR-10: However without softmax I get much bigger values, for example 108.91984. What exactly am I doing wrong with sparse_softmax_cross_entropy_with_logits? The TF code is shown below.",https://stackoverflow.com/questions/55573670,9565342,Documentation Replication on Other Examples
55590729,How to implement big int or big decimal as a datatype in tensorflow,Tensorflow has no big integer or big decimal as a dtype. And it says that tf.strings can represent any kind of bytes array. Can I implement big data type using tf.strings? Or is there any other way?,https://stackoverflow.com/questions/55590729,7111053,Documentation Replicability
75639356,"Explanation on ""Tensorflow AutoGraph support print and assert""","In AutoGraph converts Python into TensorFlow graphs it says: However, Introduction to graphs and tf.function says: The first documents gives the impression that I can use print and TensorFlow AutoGraph can convert into Tensorflow operations. However apparently it is not the case as in the second document. Please help understand if the sentence in the first document stating ""We/AutoGraph support even print and assert"" is still correct or if I misunderstand something. In my understanding, AutoGraph is the one being used under @tf.function and tf.py_function.",https://stackoverflow.com/questions/75639356,4281353,Documentation Replication on Other Examples
75639435,tf.executing_eagerly() returns true even when in graph mode,Why tf.executing_eagerly() returns True after tf.config.run_functions_eagerly(False)?,https://stackoverflow.com/questions/75639435,4281353,Documentation Replicability
75640862,tf.py_function is only for Eager Mode?,"Is tf.py_function only for Eager mode, and not for Graph mode? According to tf.py_function says, it gives the impression that tf.py_function is to be used at Eager mode. I believe pdb works with Python interpreter and GIL is inside it, but there is no Python interpreter executing Python code while executing TensorFlow Graph in Graph mode. Hence I believe it is only for Eager Mode. Please confirm if this is correct.",https://stackoverflow.com/questions/75640862,4281353,Documentation Replicability
55602703,"Can I add tf.keras.backend function in TensorFlow ""custom"" layer?","TensorFlow can implement Keras with tf.keras. Keras has three backend implementations available: the TensorFlow backend, the Theano backend, and the CNTK backend. If I wanted to build a custom layer, can I add tf.keras.backend.theano.tensor.dot and tf.keras.backend.theano.gradient.disconnected_grad in call function? For example, there is one program in TensorFlow（https://www.tensorflow.org/tutorials/eager/custom_layers）： Can I change call function into:",https://stackoverflow.com/questions/55602703,7241796,Documentation Replication on Other Examples
55627995,Keras2 ImageDataGenerator or TensorFlow tf.data?,"With Keras2 being implemented into TensorFlow and TensorFlow 2.0 on the horizon, should you use Keras ImageDataGenerator with e.g, flow_from_directory or tf.data from TensorFlow which also can be used with fit_genearator of Keras now? Will both methods will have their place by serving a different purpose or will tf.data be the new way to go and Keras generators deprecated in the future? Thanks, I would like to take the path which keeps me up to date a bit longer in this fast moving field.",https://stackoverflow.com/questions/55627995,6510273,Documentation Replicability
55638989,Eager tf.GradientTape() returns only Nones,"I try to calculate the gradients with Tensorflow in the eager mode, but tf.GradientTape () returns only None values. I can not understand why. The gradients are calculated in the update_policy () function. The output of the line: is Here is the code.",https://stackoverflow.com/questions/55638989,8807447,Documentation Replicability
55640642,Error when attempting to change tensor shape in keras model,"I want to change the shape and the content of the tensor in a keras model. Tensor is the output of a layer and has shape1=(batch_size, max_sentences_in_doc, max_tokens_in_doc, embedding_size) and I want to convert to shape2=(batch_size, max_documents_length, embedding_size) suitable as input of the next layer. Here sentences are made of tokens, and are zero-padded so every sentence has length=max_tokens_in_sentence. In detail: So passing from shape1 to shape2 is not only a reshape as mathematical operations are involved. I created the function embedding_to_docs(x) that iterates on the tensor of shape1 to transform it into shape2. I call the function using a Lambda layer in the model, it works in debug with fictious data, but when I try to call it during the build of the model an error is raised: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn. How to fix this?",https://stackoverflow.com/questions/55640642,11348106,Documentation Replication on Other Examples
73165980,Tensorflow: how to feed a variable-time-step input to a RNN,"I have a simple X_train and Y_train data: Arrays are numpy arrays. I am now trying to use the tf.data.Dataset class to load these as tensors. Before I have done a similar thing successfully using the following code: As this input is fed into a RNN, I have used the expand_dims method in the first RNN layer (the expand_dimension is passed as a function to overcome an apparent bug in tensorflow: see https://github.com/keras-team/keras/issues/5298#issuecomment-281914537): This worked although because I had arrays of equal length. In the example I posted instead the 1st array has 13 numbers and the 2nd one 18. In this case the method above doesn't work, and the recommended method seems to be using tf.data.Dataset.from_generator. Reading this How to use the Tensorflow Dataset Pipeline for Variable Length Inputs?, the accepted solution shows something like the following would work (where I am not caring here about y_train for simplicity): However, the syntax in tensorflow has changed since this answer, and now it requires to use the output_signature argument (see https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator). I've tried different ways but I'm finding hard to understand from tensorflow documentation what the output_signature should exactly be in my case. Any help would be much appreciated.",https://stackoverflow.com/questions/73165980,13454852,Documentation Ambiguity
55681290,Feeding Dataset Iterator to Tensorflow,Can i get a full example somewhere where they feed tf.data.Dataset iterator to a model? I'm trying to feed this data into a model without the help of tf.Estimators.,https://stackoverflow.com/questions/55681290,11360841,Lack of Alternative Solutions/Documentation
55703097,Training while loop in Tensorflow,"I've attempted converting a Python-side training loop to Tensorflow to (hypothetically) make the code run faster - not having to pass control over to cpu constantly. However, I can't manage using tf.while_loop. Here's the code that works: Now, if I create an update function to pass tf.while_loop, an error is thrown. I don't quite understand what is happening even after reading the documentation. weights is a Variable after all. What could be done to correctly make the training loop?",https://stackoverflow.com/questions/55703097,7089239,Documentation Replication on Other Examples
73198391,import import tensorflow_docs error on Colab,I've been using colab for deep learning for a over a month and all of a sudden import tensorflow_docs as tfdocs stopped working. Is anyone encountering the same issues??? I'm running tf.version 2.8.2 Error: UPDATE 01-Sep-2022 I updated python in colab to 3.9.1 using @Kor suggestion and confirmed it. Then ran: The tensorflow doc completed installation but still threw an error on import:,https://stackoverflow.com/questions/73198391,11963507,Documentation Replicability
75838967,AttributeError: module 'tensorflow' has no attribute 'contrib'. tried other forums already,"I tried using tf_updatev2, that didn't work. also tried manually replacing files with tf.compat.v1.estimator since that's supposed to work in tfv2.xxx",https://stackoverflow.com/questions/75838967,18569492,Documentation Replicability
73206182,"Why is the gradient of `where(x > 1, log(x), 0)` nan?","Why is the gradient of tf.where(x &gt; 1, tf.math.log(x), 0) nan when x is 0.0, but not when it's -1 or 1? Minimal example: Output:",https://stackoverflow.com/questions/73206182,2491528,Documentation Replicability
75851842,tensorflow map function to mulitple tensors,"I am using the following function in a custom layer in TensorFlow to rearrange query, key values: and it throws this warning: WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23. Instructions for updating: Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089 Is there a more TensorFlowic way of doing this? I tried using map_fn as follows and it throws the the same warning and an error: From documentation, it seems tf.map_fn but it seems to work on a stack of tensors. Will it be better to stack the tensors?",https://stackoverflow.com/questions/75851842,13262692,Documentation Replication on Other Examples
73213159,How to apply tf.data transformations to a DataFrame,"I want to apply tf.data transformations to a panda dataframe. According to the tensorflow docs HERE I can apply tf.data to a dataframe directly but the dtype of the dataframe should be uniform. When I apply tf.data to my dataframe like below it generates this error When I print df['reports'].dtype it is dtype('O') which seems to be not uniformed, if this is the case then how can I convert this dataframe to uniform dtype.",https://stackoverflow.com/questions/73213159,19443650,Documentation Replicability
55718702,How to correctly train with tf.keras.layers.BatchNormalization: Is there still a tf.GraphKeys.UPDATE_OPS dependency?,"My goal is how to correctly train with batch normalizations layers in TensorFlow (TensorFlow version 1.13.1 for Python in Graph Mode) using the recommended tf.keras.layers.BatchNormalization class (https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization). An older recommended approach was to use tf.layers.batch_normalization. The documentation (https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization) indicates that it is currently deprecating instead in favor of tf.keras.layers.BatchNormalization. While using the older class, the documentation indicates we must explicitly add dependency on the mean and variance update operations, which would otherwise be dangling nodes outside from any dependencies in training operations: My question: Is this explicit dependence on UPDATE_OPS still needed when training batch norms in TF 1.13 with tf.keras.layers.BatchNormalization? I don't see this mentioned in the documentation, however, I would be much more comfortable if someone knew for sure (and even better if can point to official documentation or code) that these operation dependencies are implicitly taken care of.",https://stackoverflow.com/questions/55718702,6367958,Lack of Alternative Solutions/Documentation
55731549,How to use tf.keras Sequential with tf.distribute.ParameterServerStrategy and tf.train.MonitoredSession?,"I'm trying to set up a really easy Minimal Working Example for the following: Use a model built with tf.keras in a tf.train.MonitoredSession using a tf.Server with a tf.distribute.ParameterServerStrategy. My goal in the end is to use a tf.keras model in a distributed environment using two workers each having one GPU and a parameter server. The model is built according to the example and documentation found here: https://www.tensorflow.org/versions/r1.12/api_docs/python/tf/keras/models/Sequential The parameter server strategy is used according to the documentation found here: https://www.tensorflow.org/versions/r1.12/api_docs/python/tf/contrib/distribute/ParameterServerStrategy The overall setup including the device placement and the use of a MonitoredSession is taken from: https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md I'm already using the ""allow_soft_placement"" option and I'm ""emulating"" a distributed setup on my local machine having only a single CPU, since there are different problems in the real distributed setup which I'm trying to solve by using a MonitoredSession where the variable initialization is handled automatically. This Code works with a ""normal"" (not monitored) tf.Session and variable initialization - global, local, model variables and tables etc. The line which unfreezes the graph is necessary to be able to use a tf.data.Dataset in the tf.keras.Model's fit function, since an iterator has to be created - which causes an error in a frozen graph. This the code I'm trying to run. I use tensorflow 1.12.0 and python 3.6.7. I've also tried python 2.7, same result. The code requires no setup besides installing tensorflow. The code requires no extensive setup since the dataset is loaded from the web and converted to a tf.data.Dataset since this is how I want to organize my pipeline with the real data. The MNIST data setup example is taken from https://www.kaggle.com/margaretmz/mnist-with-tf-keras. I expect the code to not fail due to wrong variable or operation placement since I'm basically leaving all these decisions to the implementation of tensorflow by using strategy.scope() and tf.device(tf.train.replica_device_setter(worker_device=""/job:worker/task:%d"" % task,cluster=cluster))",https://stackoverflow.com/questions/55731549,6489953,Documentation Replication on Other Examples
55740550,"Tensorflow, iterate tensors","I defined my loss function and I want to iterative each item of the batch to calculate the loss function. I used tf.map_fn however, I found it is very slow. Are there any suggestions? The batch size is 1024.",https://stackoverflow.com/questions/55740550,5046896,Documentation Replicability
73248420,"Input to reshape is a tensor with 497664 values, but the requested shape has 3072 [Op:Reshape]","I am creating patches of xception model, when I apply patches class and then reshape it then it generates an error of incompatibility between patches and reshape layers. I did not find any suitable documentation to study about calculation of parameters in patch layer to reshape them in proper dimensions for the next layer. That's why when I run this code it generates the error mentioned in title. How would I know the math to go from one layer to another and define proper size of patches = tf.reshape(patches, [1, 32, 32, 3]) to make patches from it's above layer patches = Patches(patch_size)(xception_input) without errors. My code is something like below Model code Output shape When I change the patch code part like below Output and it generates this error",https://stackoverflow.com/questions/73248420,,Lack of Alternative Solutions/Documentation
55764694,How to use gradient_override_map in Tensorflow 2.0?,"I'm trying to use gradient_override_map with Tensorflow 2.0. There is an example in the documentation, which I will use as the example here as well. In 2.0, GradientTape can be used to compute gradients as follows: There is also the tf.custom_gradient decorator, which can be used to define the gradient for a new function (again, using the example from the docs): However, I would like to replace the gradient for standard functions such as tf.square. I tried to use the following code: However, there are two issues: The gradient replacement does not seem to work (it is evaluated to 10.0 instead of 0.0) and I need to resort to session.run() to execute the graph. Is there a way to achieve this in ""native"" TensorFlow 2.0? In TensorFlow 1.12.0, the following produces the desired output:",https://stackoverflow.com/questions/55764694,7469434,Documentation Replication on Other Examples
55770009,How to use a pre-trained embedding matrix in tensorflow 2.0 RNN as initial weights in an embedding layer?,"I'd like to use a pretrained GloVe embedding as the initial weights for an embedding layer in an RNN encoder/decoder. The code is in Tensorflow 2.0. Simply adding the embedding matrix as a weights = [embedding_matrix] parameter to the tf.keras.layers.Embedding layer won't do it because the encoder is an object and I'm not sure now to effectively pass the embedding_matrix to this object at training time. My code closely follows the neural machine translation example in the Tensorflow 2.0 documentation. How would I add a pre-trained embedding matrix to the encoder in this example? The encoder is an object. When I get to training, the GloVe embedding matrix is unavailable to the Tensorflow graph. I get the error message: RuntimeError: Cannot get value inside Tensorflow graph function. The code uses the GradientTape method and teacher forcing in the training process. I've tried modifying the encoder object to include the embedding_matrix at various points, including in the encoder's init, call and initialize_hidden_state. All of these fail. The other questions on stackoverflow and elsewhere are for Keras or older versions of Tensorflow, not Tensorflow 2.0.",https://stackoverflow.com/questions/55770009,5875503,Documentation Replication on Other Examples
73264100,Use tf.nn.local_response_normalization in keras layers,"I am using keras to add layers, for example: Now I am implementing LRN. However, the keras library does not have LRN to my limited knowledge. The old tf.nn library does have a LRN function called tf.nn.local_response_normalization. Is it possible to mix tf.nn with keras?",https://stackoverflow.com/questions/73264100,9274598,Documentation Replicability
55778682,fix/freeze individual kernel weights in a convolutional operation,"I'm trying out a workaround for fixing individual kernel weights in a convolutional operation in TensorFlow using Python 3.7. I do it by creating A 1 in the ""mask"" tensor indicates that I want to fix/freeze that specific weight during training, i.e. not update it in the backward pass. Now, this workaround works perfectly fine when applied to a fully connected layer but fails when applied to a convolutional layer and I can't figure out why or how to make it work. Something seems to be happening in the tf.nn.conv2d() function call (see code example below) and according to the documentation this is what they do: But since I use weights_frozen which is a tensor and depends on the trainable variable, non-trainable variable and mask_weights it should get zero-valued gradients on the positions where I have a 1 in the mask_weights tensor. As mentioned, I expect to get zero-valued gradients on the positions where I have a 1 in the mask_weights tensor, but instead they are non-zero and therefore those weights are being trained, which is not the behavior I'm trying to achieve.",https://stackoverflow.com/questions/55778682,9496160,Documentation Ambiguity
55788007,Unexpected results when using tfrecords loaded using tf.data.Dataset.list_files() with shuffle argument,"I'm hoping to get clarification on how the shuffle argument in tf.data.Dataset.list_files() works. The documentation states that when shuffle=True, the filenames will be shuffled randomly. I've made model predictions using a tfrecords dataset that has been loaded using tf.data.Dataset.list_files(), and I would've expected the accuracy metric to be the same no matter the order of the files (i.e. whether shuffle is True or False), but am seeing otherwise. Is this expected behavior or is there something wrong with my code or intepretation? I have reproducible example code below. Oddly, as long as tf.random.set_random_seed() is set initially (and it seems it doesn't even matter what seed value is set), then the predictions results are the same no matter whether shuffle is True or False in list_files(). tensorflow==1.13.1, keras==2.2.4 Thanks for any clarifications! Edit: re-thinking it through and wondering if Y = [y[0] for _ in range(steps) for y in sess.run(Y)] is a separate and independent call? Split dataset into multiple tfrecords files so we can reload it with list_files() later: At this point, I exit (ipython) and restart again:",https://stackoverflow.com/questions/55788007,6921786,Documentation Replication on Other Examples
75993467,tensorflow unique function with 2d?,tf.unique currently only works on 1D tensors. How can I find index of unique values in a 2D tensor.,https://stackoverflow.com/questions/75993467,4358622,Documentation Replicability
75996642,Is there a good equivalent of pandas' `apply` for TensorFlow datasets?,"BACKGROUND The use of tf.data.Dataset is promoted by TensorFlow as the best practice for implementing input pipelines due to their efficient implementation of common operations such as batching, shuffling, as well as their seamless integration with the Keras API. I may just be lousy at looking up the documentation on the matter, but it seems to me that the major drawback of TensorFlow datasets is that they are quite unwieldy, if not impossible to work with, when trying to implement feature engineering tasks whereby a new column is created via the application of some generic Python function. This is in contrast to pandas' very nifty apply() function which can produce new columns from preexisting ones both efficiently (i.e., via vectorization) and in a pythonic manner. To the best of my understanding, the closest thing to pandas' apply() is TensorFlow dataset's map(). However, one can't simply use it with arbitrary Python functions since they'd first need to be converted to tensors. This becomes very difficult as one has to have arcane knowledge of the miscellaneous tensor analogues of arbitrary Python functions (e.g., tf.strings.length() instead of Python's len()). Even when one finds such functions, the idiosyncracies of tensor operations makes them very un-pythonic and prone to obscure dimensionality or type errors. I've read about TensorFlow's py_function as some sort of wrapper that magically converts Pythonic code into a tensor representation, but judging from the documentation, it became clear to me that this is far from the case. QUESTION Is TensorFlow's tf.data just not mature yet to be able to handle feature engineering in the same way that pandas apply() does? If not, what am I missing in my understanding? MINIMUM WORKING EXAMPLE In the code below, I compare a pandas DataFrame df with the equivalent TensorFlow dataset ds. My goal is to engineer two extra features, namely As you can see for yourself, the first operation works intuitively for both pandas and TensorFlow, but the second only works easily for pandas. Getting it to work with TensorFlow is either extremely complex, or just plain impossible. The output is of the above is as follows.",https://stackoverflow.com/questions/75996642,5640161,Documentation Ambiguity
73279782,Tensorboard profiling a predict call using Cloud TPU Node,"I've been trying to profile a predict call of a custom NN model using a Cloud TPU v2-8 Node. It is important to say that my prediction call takes about 2 minutes to finish and I do it using data divided in TFRecord batches. I followed the official documentation ""Profile your model with Cloud TPU Tools"" and I tryied to capture a profile: I could generate some data in both cases (Tensorboard UI and profiler call), but when I try to open it in Tensorboard pointing the logdir path, I received a ""No dashboard are active for the current data set"" message. Is there any way to profile a Tensorflow/Keras prediction call with a model running in a Cloud TPU Node? Curious fact - There seems to be an inconsistency in the Tensorflow docs and Cloud TPU docs: in Tensorflow Optimization Docs we can see that tf.profiler.experimental.start/stop calls are not supported by TPU hardware, but in Google Cloud docs this is the recommended method to capture a profile in TPU. Config:",https://stackoverflow.com/questions/73279782,11648055,Documentation Replication on Other Examples
76040030,Problem using Huggingface imagenet-1k dataset in Keras / Tensorflow,"I'm having a problem using the imagenet-1k dataset from Huggingface with a Keras model. I'm just experimenting with simple models, but am stuck trying to get the dataset to work with the model fit function. Here is how I load the dataset: Here is the fit invocation: I get the following error: When I inspect one of the elements of the datasets it looks like a tf.Tensor, so I don't understand why it can't be passed directly. None of the examples or docs I can find make it clear how to do this. Huggingface examples for images produce the same format that I'm getting, but apparently there is a step I'm missing before it can be used with model.fit()",https://stackoverflow.com/questions/76040030,21668078,Documentation Replication on Other Examples
73328337,Tensorflow 2 SSD MobileNet model breaks during conversion to tflite,"I've been trying to follow this process to run an object detector (SSD MobileNet) on the Google Coral Edge TPU: Edge TPU model workflow I've successfully trained and evaluated my model with the Object Detection API. I have the model both in checkpoint format as well as tf SavedModel format. As per the documentation, the next step is to convert to .tflite format using post-training quantization. I am to attempting to follow this example. The export_tflite_graph_tf2.py script and the conversion code that comes after run without errors, but I see some weird behavior when I try to actually use the model to run inference. As a result, I have no way to tell if the script broke my model or not before I even convert it to tflite. Why would model not be callable in this way? I have even verified that the type returned by tf.saved_model.load() is the same when I pass in a saved_model before it went through the export_tflite_graph_tf2.py script and after. The only possible explanation I can think of is that the script alters the object in some way that causes it to break. Everything runs with no errors, but when I try to actually run an image through the model, it returns garbage. I tried removing the quantization to see if that was the issue, but even without quantization it returns seemingly random bounding boxes that are completely off from the model's performance prior to conversion. The shape of the output tensors look fine, it's just the content is all wrong. What's the right way to get this model converted to a quantized tflite form? I should note that I can't use the tflite_convert utility because I need to quantize the model, and it appears according to the source code that the quantize_weights flag is deprecated? There are a bunch of conflicting resources I see from TF1 and TF2 about this conversion process so I'm pretty confused. Note: I'm using a retrained SSD MobileNet from the model zoo. I have not made any changes to the architecture in my training workflow. I've confirmed that the errors persist even on the base model pulled directly from the object detection model zoo.",https://stackoverflow.com/questions/73328337,19746925,Documentation Ambiguity
55868686,how to use eager execution to save and restore in TensorFlow？,"We always use tf.train.Saver() to save and restore weights, like in this example. But how to use eager execution to save? how to change the following example? Another question, is it a good idea to use eager? I found tf.contrib.eager.Saver here, but it says, What does it mean?",https://stackoverflow.com/questions/55868686,7241796,Documentation Replication on Other Examples
73336326,computing gradients in parallel in Tensorflow,"I need to compute gradient of a scalar valued function with vector inputs for various inputs. I am currently doing something like below. But no matter what value I set for parallel_iterations of the tf.while_loop, it is only computing gradients for one input at a time. What am I missing?",https://stackoverflow.com/questions/73336326,1794777,Documentation Replicability
55909188,How can I apply a TensorFlow 2D Convolution (tf.nn.conv2d) to a single (non-batch) 2D image?,"I would like to use the function tf.nn.conv2d() on a single image example, but the TensorFlow documentation seems to only mention applying this transformation to a batch of images. The docs mention that the input image must be of shape [batch, in_height, in_width, in_channels] and the kernel must be of shape [filter_height, filter_width, in_channels, out_channels]. However, what is the most straightforward way to achieve 2D convolution with input shape [in_height, in_width, in_channels]? Here is an example of the current approach, where img has shape (height, width, channels): I am reshaping the input as follows: [in_height, in_width, in_channels]-&gt;[1, in_height, in_width, in_channels]-&gt;[in_height, in_width, in_channels] This feels like an unnecessary and costly operation when I am only interested in transforming one example. Is there a simple/standard way to do this that doesn't involve reshaping?",https://stackoverflow.com/questions/55909188,9672143,Documentation Replication on Other Examples
73365851,Tensorflow Functional API LSTM tf.keras.experimental.SequenceFeatures,"Does anyone know how to preserve the sparse zeros in a tensor when using tf.keras.experimental.SequenceFeatures? It seems to require a sparse tensor, but that eliminates the zeros in a sequence. Does anyone know of another way to input sequence data into tensorflow using features? Thanks this is a toy example of the issue",https://stackoverflow.com/questions/73365851,9234083,Documentation Replicability
55916743,How to get gradients with respect to input and change input (rather than trainable vars) to minimize loss in TF 2?,"I want to use a trained model to change the input so it minimizes the loss (rather than changing the trainable variables) a la Deep Dreaming in Tensorflow 2.0 but I am not having success. Say I have a basic NN as the one in the docs Which I train using a simple tf.GradientTape function What's the idiomatic way to create a function that will instead calculate and apply the gradients to the input - images. I assumed it will be as simple as However, that doesn't work.",https://stackoverflow.com/questions/55916743,1540616,Documentation Replication on Other Examples
55936016,TensorFlow 2.0 clip_by_value with dynamic bounds,"It is not clear from the TensorFlow documentation whether I can have dynamic range constraints imposed on a tf.Variable via tf.clip_by_value. From my testing it doesn't seem to work, but I would like to be sure, and if it isn't then I also would like to know how to achieve this (there are certain parts of my parameter space that cause NaNs in my loss function, and these constraints can only be described in terms of combinations of parameters). Here is my test scenario: Output: The constraint on eta should enforce that x + eta is positive, but already we see negative values appearing in the first two loops. Or is this perhaps an issue of the order of updating of variables? For example in the minimise loop I guess it is important that the x variables get updated first so that the constraint on the eta variables gets calculated correctly? I guess there is no guarantee of that, and I need to tell TensorFlow to do this? Edit: I attempted to manually clip the variables in the optimisation loop, like so: and it seems to kind of work in this case, but in other test cases it seems to make the minimiser go haywire. I guess because it fights with the minimiser about what values the variables should have. So I'm not sure that this is a good solution. Edit 2: Well for my specific case I realised that I could solve my problem with math, i.e. by a change of variables I can make the problem not require any explicit constraints. Which is probably the best thing to do if it is possible. But it won't always be, so I am still curious how to do it with constraints.",https://stackoverflow.com/questions/55936016,1447953,Documentation Replication on Other Examples
55951969,how to properly use Keras imports since it is embedded in TF?,"how to i properly import anything from keras within tensorflow? if I e.g., from tensorflow import keras from keras.layers.core import Dense does that import from Keras or from tf.keras?",https://stackoverflow.com/questions/55951969,6510273,Documentation Replicability
55964427,tf.keras HDF5 Model and Keras HDF5 Model,"I want to convert a Keras model to Tensorflow Lite model. When I examined the documentation, it is stated that we can use tf.keras HDF5 models as input. Does it mean I can use my saved HDF5 Keras model as input to it or tf.keras HDF5 model and Keras HDF5 models are different things? Documentation: https://www.tensorflow.org/lite/convert Edit: I could convert my Keras model to Tensorflow Lite model with using this API, but I didn't test it yet. My code:",https://stackoverflow.com/questions/55964427,10170533,Documentation Replication on Other Examples
73414383,Idiomatic tensorflow expression of for-loop which considers value of preceding iteration,"It might not even be possible, but I would like to express the following code without the for-loop. tf.scan is prohibitively slow and therefore not a good solution. I am perfectly happy to accept any answer which gives a solution or an argument why this is not possible.",https://stackoverflow.com/questions/73414383,9067359,Documentation Replicability
55986982,What is the way to use Tensor flow 2.0 object in open cv2 python and why is it so circuitous?,"I load an image using tensor flow api (2.0) like so : Now that I have this object, I want to show this image, I can simply use matplotlib.pyplot, and this works. However attempting this with OpenCV2 is problematic from the start, most of the examples are from 1.0 with .eval() session based suggestion for numpy conversion. One way would be to first convert tensor flow object to numpy, here is the function to do that from API documentation : I dont understand why this does not works and I get a number of errors while all I want is to do something simple and then use some open cv2 functions like remap, resize etc.: Update 5/5/2018 : After searching more I found out that this has something to do with Tensorflow graph execution. I have a function This works nicely when called eagerly with .numpy() attribute, however when called like following code and when you try to inspect what real_image is and its type returns Please advice. Update 5/5/2018 : I decided to do a preprocessing of the data so I don't have to worry about the using any opencv functionality during the load time of the data. However during training time I still want to do some openCV operations. Now as per the suggestion of @giser_yugang I tried using py_function, I wrap opencv operations in py_function and call that function in a wrapper tf.function. This wrapper tf.function I call in train step. However the output I get from this wrapper function is like so : Then if I try to consume this tensor in the next train step operation I get a If I don't use this py_function wrapper in my train step and directly try the numpy operations using opencv I get another error I guess both ways you cant win !",https://stackoverflow.com/questions/55986982,4930939,Documentation Ambiguity
73437417,"tf.keras.utils.image_dataset_from_directory, but labels are from csv?","Please tell me where I'm going wrong. I am working on the kaggle dog breed classification challenge and I want to try one-hot encoding vs label encoding. The images are not split in the images directory so I can not use 'inferred' with tf.keras.utils.image_dataset_from_directory, the labels are in a separate csv file that I put into a df (label_int). label_int is a list of all my labels in integer form, in the same order as my images in the directory. oh_input is a list of one-hot vectors. The error: I have tried other datatypes in the tf.one_hot function and none of them working. I'm guessing I am just missing some fundamental syntax error. https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory I have also tried labels= *(a numpy array of integer values) with label_mode =int and that gave me another error. I also tried labels = None and it says 'Found 10222 files belonging to 1 classes.' so I'm not sure if I can add in the labels later to the fit function.",https://stackoverflow.com/questions/73437417,7525231,Documentation Replication on Other Examples
55996869,Tensorflow Matrix square Root code source,Where can we get the source code of the tf.linalg.sqrtm and can we modify it ?,https://stackoverflow.com/questions/55996869,10112379,Documentation Replicability
56002345,Why get_tensor_by_name can't get the weights of layers defined by tf.keras.layers properly,"I try to get the weights of layers defined by tf.keras.layers by using get_tensor_by_name in tensorflow. The code is presented as follows The name of the weight is dense/kernel:0. However, the output of sess.run(w) is weird which is not an array of floats. In fact, if I use tf.layers.dense to define the network, everything goes fine. So my question is that how I can get the weights of layers defined by tf.keras.layers by using tensor name properly.",https://stackoverflow.com/questions/56002345,8601361,Documentation Replication on Other Examples
73451503,Using Tensorflow dataset and rejection_resample() for undersampling image data,"I am reading images out of a directoy. The structure is, I have 10 times the number of 'No Findings' images as 'Fibrosis' images. I want to undersample 'No Findings' (or even better oversample 'Fibrosis') This is how I set up my tensorflow dataset. As specified in the docs, I try rejection_resample() for undersampling, Notice I have to use tf.squeeze( image_label) in the first lambda to get the shapes to match. (I am not entirely confident in this) When I try to take() with my new dataset with ... More often than not it complains Then fails",https://stackoverflow.com/questions/73451503,1732297,Documentation Replicability
73469081,Impact of batch_size in predict method of tf.keras.Model,The predict method of a tf.keras.Model takes the following arguments: What is the point of specifying the batch_size? What are the ways in which it impacts the predictions?,https://stackoverflow.com/questions/73469081,11330010,Documentation Replication on Other Examples
56038372,Does wrapping tf.data.Dataset into tf.function improve performance?,"Given the two examples below, is there a performance improvement when autographing the tf.data.Dataset? Dataset not in tf.function Dataset in tf.function",https://stackoverflow.com/questions/56038372,7128838,Documentation Replicability
56047272,Explicit vs implicit type definition in TensorFlow,"I'm just beginning to learn TensorFlow. Quoting from the documentation: The second constant is implicitly typed as a float32. Is that based on the explicit typing of the first constant? And does that imply that the first dtype is required? tf.constant documentation would imply that it does not: But then it would be unnecessary to explicitly type the 3.0 constant above. I'm just looking for some clarification on this, since, like I said, I'm just starting out.",https://stackoverflow.com/questions/56047272,7287543,Documentation Replication on Other Examples
74201166,tensorflow.keras seed does not seem to affect random initializer,"I am using Tensorflow 2.2. The script below seems to indicate that setting the seed in tensorflow.keras through set_seed() does not randomize the initializers value, unless the seed is explicitly set into the initializer itself. The output is: My guess is that I should use the function tf.keras.utils.set_random_seed() instead, which I understood it could be deprecated. However, this in theory should be equivalent to setting the three seeds above: https://www.tensorflow.org/api_docs/python/tf/keras/utils/set_random_seed. This returns: AttributeError: module 'tensorflow.keras.utils' has no attribute 'set_random_seed' Any ideas?",https://stackoverflow.com/questions/74201166,6713310,Documentation Replication on Other Examples
56802840,What exactly tensorflow.gather() does?,I saw code for triplet loss that contains the function tf.gather(). What this function does? I have gone through the tensorflow's official website for definition but still unable to get it.,https://stackoverflow.com/questions/56802840,9678047,Lack of Alternative Solutions/Documentation
56804123,Tensorflow-Lite : Exporting a GraphDef from tf.Session convert failed,"I want to convert my tensorflow Spectrogram session to .tflite file for using in Android. I try to follow the TFLite official example but it is failed during the convert procedure. Please give me some advice for fixing this error. Thanks a lots! I got the error message from Jupyter Notebook I want to make customize Spectrogram library by tf.signal.stft function, and save to .tflite file for using in Android device. I can't find anymore tutorial about TFLite converter, only the official document.😭 Please help me to fix this error, thanks a lots !! 🙏",https://stackoverflow.com/questions/56804123,7651473,Documentation Replication on Other Examples
74213661,Compression for TFR records,"How does one enable compression for TensorFlow Records (TFRs) for use with the tf.data API. The documentation, oddly, completely ignores the topic (here). Whereas, the tf.data API clearly describes how to open a TFR file with compression (here) I have found some code from 2018 but this seems to rely on the older ""compat.v1"" libraries: When I use this I get badly formed files with this. For example, without compression my files are 500kB and with compression they are 20B! I would very impressed if this were actually true, however this suggests a problem.",https://stackoverflow.com/questions/74213661,985088,Documentation Replication on Other Examples
56900599,Tensorflow 2: Getting Tensor Value,"I am switching to TF2 and I just followed this tutorial, where the train and step functions are defined now as ""@tf.function"". How can I print the values of the Tensors y_pred and loss?",https://stackoverflow.com/questions/56900599,2335224,Documentation Replicability
56905939,Effective way to read images from a csv file and return a tf.data.Dataset object,I have a csv file that contains two columns: Each row in the csv corresponds to one item (sample). I want to create a tf.data pipeline that reads the file path and loads the numpy array and the label associated with it. How would I go about doing so so that I can return a tf.data.Dataset object? The documentation on the website is not very informative and I cannot figure out where to start from.,https://stackoverflow.com/questions/56905939,5052482,Lack of Alternative Solutions/Documentation
56930821,Why does embedding vector multiplied by a constant in Transformer model?,"I am learning to apply Transform model proposed by Attention Is All You Need from tensorflow official document Transformer model for language understanding. As section Positional encoding says: My understanding is to add positional encoding vector directly to embedding vector. But I found embedding vector multiplied by a constant when I looked at the code. The code in section Encoder as follows: We can see x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) before x += self.pos_encoding[:, :seq_len, :]. So why does embedding vector multiplied by a constant before adding positional encoding in Transformer model?",https://stackoverflow.com/questions/56930821,7389608,Documentation Replication on Other Examples
74359221,Tensorflow v2.10 mutate output of signature function to be a map of label to results,"I'm trying to save my model so that when called from tf-serving the output is: where label1 and label2 are my labels and x.xxxxx are the probability of that label. This is what I'm trying: Side Note: Apparently in TensorFlow v2.0 if signatures is omitted it should scan the object for the first @tf.function (according to this: https://www.tensorflow.org/api_docs/python/tf/saved_model/save) but in reality that doesn't seem to work. Instead, the model saves successfully with no errors and the @tf.function is not called, but default output is returned instead. The error I get from the above is: I wrapped the result in tf.constant above because of this error, thinking it might be a quick fix, but I think it's me just being naive and not understanding Tensors properly. I tried a bunch of other things before learning that [all outputs must be return values].1 How can I change the output to be as I want it to be?",https://stackoverflow.com/questions/74359221,498391,Documentation Replication on Other Examples
56970612,Fitted values and weights in tensorflow (tesorflow DNNRegressor),I am using tensorflow version 2.0.0-beta1. I have created the input function to feed into tf.estimator.DNNRegressor. Below is the model that I am creating using DNNRegressor. Now I want to identify 1) Fitted values of my model on training data. 2) Weights associated with each variable in model(i.e. tf.estimator.DNNRegressor) I have search through the documentation of tensorflow and other sources but didn't get this information.,https://stackoverflow.com/questions/56970612,6244166,Lack of Alternative Solutions/Documentation
56969703,How to use `tf.scatter_nd` with multi-dimensional tensors,"I'm trying to create a new tensor (output) with the values of another tensor (updates) placed according to idx tensor. The shape of output should be [batch_size, 1, 4, 4] (like an image of 2x2 pixels and one channel) and update has shape [batch_size, 3]. I've read Tensorflow documentation (I'm working with gpu version 1.13.1) and found tf.scatter_nd should work for my problem. The issue is that I cannot make it work, I think I'm having problems understanding how I have to arange idx. Let's consider batch_size = 2, so what I'm doing is: I expect it to work, but it doesn't, it gives me this error: ValueError: The outer 3 dimensions of indices.shape=[2,1,3,2] must match the outer 3 dimensions of updates.shape=[2,3]: Shapes must be equal rank, but are 3 and 2 for 'ScatterNd_7' (op: 'ScatterNd') with input shapes: [2,1,3,2], [2,3], [4] I don't understand why it's expecting updates to have dimension 3. I thought idx has to make sense with output_shape (that's why I used expand_dims) and also with updates (specify the two indices for the three points), but it's obvious I'm missing something here. Any help would be appreciated.",https://stackoverflow.com/questions/56969703,7327257,Documentation Replication on Other Examples
56972492,Profile summary by operation type,"I'm profiling my model in Tensorflow 2.0. I'm able to get a profile &amp; view with Tensorboard by following the instructions for a function. I use tf.summary.trace_on &amp; tf.summary.trace_export: https://www.tensorflow.org/tensorboard/r2/graphs Now I'd like to create a summary by operation type, similar to the node type summary in the tflite profiler (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark). TensorFlow 1.14 tf.profile.Profiler looks to be able to do it with the 'cmd' option: https://www.tensorflow.org/api_docs/python/tf/profiler/profile How do I do this with TensorFlow 2.0?",https://stackoverflow.com/questions/56972492,10052081,Documentation Replication on Other Examples
57014236,How to use the Embedding Projector in Tensorflow 2.0,"With the tf.contrib module gone from Tensorflow, and with tf.train.Saver() also gone, I cannot find a way to store a set of embeddings and their corresponding thumbnails, so that the Tensorboard Projector can read them. The Tensorboard documentation for Tensorflow 2.0 explains how to create plots and summaries, and how to use the summary tool in general, but nothing about the projector tool. Has anyone found how to store datasets for visualization? If possible, I would appreciate a (minimal) code example.",https://stackoverflow.com/questions/57014236,1357690,Documentation Replication on Other Examples
74434308,Setting only global level seed gives same output in consecutive iterations of loop in Tensorflow,"I am testing out the tf.random.set_seed according to the rules given at - https://www.tensorflow.org/api_docs/python/tf/random/set_seed In particular I am testing the second rule - where we set only global level seed and no operation level seed. According to the documentation (the link is mentioned above), the second rule is: To explain the second rule, the documentation uses the following snippet: and states that Now, I tested this rule on a 1D tensor of shape (3,) to check if the output of shuffling the tensor does not give the same sequence within consecutive iterations of the loop as follows: I got the following output: From the output you can see that the sequence on line number 7 and 8 match. Also the sequence on line number 13 and 14 match. According to the documentation, tensorflow should not output the same sequence in a consecutive iteration. Then why am I getting this kind of output? Have I misunderstood the concept? To test this further, I also tested to following snippet which I used to generate 14 1-D tensors and check if any tensor is repeated within consecutive runs as follows: And I got the following output: You can see that no two consecutive tensors are repeated. Why didn't I see this behaviour for my first snippet?",https://stackoverflow.com/questions/74434308,7422352,Documentation Ambiguity
74442047,How to calculate the matrix's inverse using tflite,"as far as i known, the matrix's inverse is a common operator. while tf.raw_ops.MatrixInverse is not supported in tflite and BatchMatrixInverse is not available in GraphDef version 1205. How can i calculate the inverse of the matrix in tflite? Best wishes",https://stackoverflow.com/questions/74442047,7693201,Documentation Replicability
74441089,Can we stack a tf.keras.layers.LSTM model and tfp.layers.DenseVariational together?,Can we stack a tf.keras.layers.LSTM model and tfp.layers.DenseVariational together? I used it in one of my code but I am not sure if it is logical.,https://stackoverflow.com/questions/74441089,14290952,Documentation Replication on Other Examples
57056756,Prepare a tensorflow dataset with *.jpeg in a directory with corresponding labels in a .csv file,"Indeed this question is similar to prior but for me attempting prior answers tossed a errors I cannot scale probably due to deprecated functions and thus update is requested (pleading on my knees really)-- maybe windows 10 is the problem In the Google tensorflow documentation they trumpet the ease of tensorflow and they of course make it seem easy by having a dataset prepped and ready to go such as mnist = tf.keras.datasets.mnist But for people doing real work with new data they are not going to have data so neatly prepared. Instead they (i.e. me) are going to take many pictures of different similar things (say 64 nuclei of cancer cells from 64 patient cases-- 250 by 250 pixels each with RGB) and they will store the 64 jpegs in a single directory lets say C:\\Users\\dalton\\Desktop\\breast\\nuclei\\*.jpg There are no other jpgs in the directory other than those of interest Then in a csv text file (essentially a spreadsheet with one column saved as csv file or could be txt) they record the label corresponding to each picture -- this text file has 64 entries of a 1 or 0 whereby 1= very bad cancer cell and 0= not so bad. Lets say this txt file is located at: C:\\Users\\dalton\\Desktop\\breast\\nuclei\\n_labels.txt They (me) will use 32 of the pictures and corresponding labels for train and 32 for test. So from this basic data of 64 jpegs and 64 labels (as 64 entries in a single text file)how does one get to the same point as given in the loading of an easy example. Or does one need a single spreadsheet with the jpeg filenames in one column and the labels in another so as to prepare a list or tuple? In essence how does Real world work join final common pathway same as mnist = tf.keras.datasets.mnist I think an answer to this will help many many people besides me. I have tried innumerable examples in GitHub, here, and tried R versions and total frustration. Sincere thanks to an answer. LD attempted solutions used examples as per: https://www.tensorflow.org/datasets/datasets https://gist.github.com/eerwitt/518b0c9564e500b4b50f",https://stackoverflow.com/questions/57056756,11791681,Documentation Replication on Other Examples
74456127,AttributeError: type object 'DatasetV2' has no attribute 'save',"I want to save the tf.data.Dataset,but get the message ""AttributeError: type object 'DatasetV2' has no attribute 'save'""",https://stackoverflow.com/questions/74456127,20517027,Documentation Replicability
57081006,Create a Keras Layer Subclass using Conv Layers,I would like to create a custom tf.keras.layers.Layer resembling the below function: I went through the documentation available here and here and subsequently I created the below class: I want to use this Layer several times throughout my model. I have several questions around this: How can I use this function to compute the shape of the output layer?,https://stackoverflow.com/questions/57081006,5052482,Documentation Replicability
57083881,Tensorflow 2.0 Saving trained parameters to be restored in a new file,"I need to save trained variables of a TensorFlow 2.0 model using one of TF's built in functions like tf.train.Checkpoint or any other, and want to call them in a new file. I am not using tf.Keras.Sequantial and don't want to use something like model.save_weights() I have tried tf.train.Checkpoint to save variables, but not sure how to restore them. I used to work with tf.train.Saver() in TF 1.0 to save variables using sessions and restore them using tf.train.import_meta_graph and tf.train.latest_checkpoint. However, I haven't been able to find equivalent functionalities in TF 2.0 documentation so far. saver = tf.train.Checkpoint() saver.listed = [W, b_v, b_h] saver.mapped = {'W':saver.listed[0],'b_v':saver.listed[1], 'b_h':saver.listed[2]} save_path = saver.save('trained_parameters') restorer = tf.train.Checkpoint() restorer.restore('trained_parameters')",https://stackoverflow.com/questions/57083881,11799681,Documentation Replication on Other Examples
57098652,Check if a tf.Tensor is mutable,How to check if a tf.Tensor is mutable? I want to assert the arguments of a function have the correct types. A tf.Tensor can be mutable:,https://stackoverflow.com/questions/57098652,8704463,Documentation Replicability
57116074,TensorFlow - return distinct sub-tensors of multidimensional tensor,"In TensorFlow, the tf.unique function can be used to return the distinct elements of a 1-dimensional Tensor. How can I get the distinct sub-Tensors along the axis 0 of a higher-dimensional Tensor? For example, given the following Tensor, the desired distinct function would return the specified result: How can the distinct multidimensional elements be generated for Tensors of any number of dimensions?",https://stackoverflow.com/questions/57116074,3582363,Documentation Replication on Other Examples
57120680,Deep copy of tensor in tensorflow python,"In some of my code, I have created a neural network using tensorflow and have access to a tensor representing that network's output. I want to make a copy of this tensor so that even if I train the neural network more, I can access the original value of the tensor. Following other answers and tensorflow documentation, I have tried the tf.identity() function, but it does not seem to be doing what I need. Some other links suggested the use of tf.tile(), but this did not help either. I do not wish to use sess.run(), evaluate the tensor, and store it elsewhere. Here is a toy example that describes what I need to do: The result of the above code is that t2 and t3 have the same value. What I want is for t3 to keep its value from being copied. Thanks in advance for your help.",https://stackoverflow.com/questions/57120680,8083568,Documentation Ambiguity
57134808,tf.keras.optimizers.Adam with tf.estimator model in Tensorflow 2.0.beta is crashing,"I am using Tensorflow 2.0.beta with Python 3.6.6 on Mac OS (nightly: tf-nightly-2.0-preview 2.0.0.dev20190721 but I never managed to have it working with compat module in Tensorflow 2.0). I am traying to migrate a tf.estimator model from Tensorflow 1.12 (fully working) to Tensorflow 2.0. Here is the code: predictions=predictions, loss=loss, train_op=train_op, export_outputs=predictions_output) If I keep the compat.v1 module it is working: If I try to use something without compat.v1 it is crashing: with the following error (I am running the code locally for the moment, not on GCP): Any idea how to fix that ? The error messages was changing over time since Tensorflow 2.0 alpha. I am also looking for a full working example of tf.estimator working with Tensorflow 2.0. I have issue to export the model as well. In the official documentation of Tensorflow 2.0 they only use in their example compat.v1 and don't export the model. All the online course on tf.estimator from GCP are using older version of Tensorflow (1.12 - 1.14).",https://stackoverflow.com/questions/57134808,6430839,Inadequate Examples
57138511,How to addin new Tensorflow layers to TensorRT engine?,"I have tensorflow freezed model from which TensorRT engine is produced. I can't retrain model since I don't have all those required images. But Tensorflow process has some post processing layers and I like to add into TensorRT engine. What would be the best approach? Can I create plugin layer using TensorRT layers? Those Tensorflow layers are mostly available in TensorRT as follows. TensorRT has scale for resize_area, conv for Smoother. Not sure tf.equal in TensorRT. How to addin those layers to TensorRT? Possible to use graphsurgeon or UFF model?",https://stackoverflow.com/questions/57138511,2467772,Documentation Replication on Other Examples
57140835,How to convert a tf.estimator to a keras model?,"In package tf.estimator, there's a lot of defined estimators. I want to use them in Keras. I checked TF docs, there's only one converting method that could convert keras. Model to tf. estimator, but no way to convert from estimator to Model. For example, if we want to convert the following estimator: How could it be converted into Keras Model?",https://stackoverflow.com/questions/57140835,5777564,Documentation Replicability
74545053,Is there an equivalent of tf.gradients function in tensorflow C or C++ API?,I want to implement a tensorflows function tf.gradients in C or C++ API? Tensorflow C++ has the worst documentation in the world and C API aren't documented at all. Can you suggest if there is an implementation or which API parts should I use to develop this myself.,https://stackoverflow.com/questions/74545053,2426998,Documentation Completeness
57155780,tf.function uses all CPU RAM,"I cannot understand why the function I have posted below uses up all of my RAM. I could understand if I were running it eagerly, but I thought the point of a tf.function was to create a graph that is reused, much like creating an operation and running it in tf 1.x. I am new to tensorflow 2.0 so I might have the wrong idea about what tf.function is doing. Could anyone help me understand this? Thanks EDIT: Here is the code where I use this function",https://stackoverflow.com/questions/57155780,8286551,Documentation Replication on Other Examples
74568105,I have a error when i'm working with seq2seq model: Module 'tensorflow' has no attribute 'contrib',When i'm having create a tf.data dataset I tried replace tf.contrib with tf.compat.v1.estimator but it did't work. enter image description here,https://stackoverflow.com/questions/74568105,18940844,Documentation Replicability
57170737,Cannot run tflite model on GPU (Jetson Nano) using Python,I have a quantized tflite model that I'd like to benchmark for inference on a Nvidia Jetson Nano. I use tf.lite.Interpreter() method for inference. The process doesn't seem to run on the GPU as the inference times on both CPU and GPU are the same. Is there any way to run a tflite model on GPU using Python? I tried to force GPU usage by setting tf.device() method but still doesn't work. The official documentation has something called delegates for GPU acceleration but I can't seem to find anything for Python.,https://stackoverflow.com/questions/57170737,8380398,Documentation Replication on Other Examples
74622205,Tensorflow 2.11.0 training loss doesn't decrease,"WARNING:tensorflow:5 out of the last 5 calls to &lt;function _BaseOptimizer._update_step_xla at 0x7fb02c6a7ee0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for more details. Does this warning related to the optimizer? The optimizer will not work? enter image description here I check the grads seems have the value.",https://stackoverflow.com/questions/74622205,20640035,Documentation Replication on Other Examples
57246091,How to cast int32 tensor to float32,How can I cast an int32 tensor to float32 in tensorflow. I don't understand what tf.cast does. It does not seem to do anything. outputs;,https://stackoverflow.com/questions/57246091,3926152,Documentation Replicability
57277926,Can Keras' model.predict return a dictionary?,"The documentation https://keras.io/models/model/#predict says that model.predict returns Numpy array(s) of predictions. In the Keras API, is there is a way to distinguishing which of these arrays are which? How about in the TF implementation? At the top of the same page of documentation, they say that ""models can specify multiple inputs and outputs using lists"". It seems that nothing breaks if instead, one passes dictionaries: When calling model.fit the same documentation says ""If input layers in the model are named, you can also pass a dictionary mapping input names to Numpy arrays."" It would be nice if either the keys from my_output_dict or the names of the dictionary values (layers) in my_output_dict were attached to the outputs of my_model.predict(...) If I save the model to TensorFlow's saved_model format protobuf using tf.keras.model.save the tf.serving API works this way-- with named inputs and outputs...",https://stackoverflow.com/questions/57277926,8456120,Lack of Alternative Solutions/Documentation
57296471,How can one use TensorFlow.js tf.data.generator for remote data sources since generators can't use callbacks,"When introducing the tf.data.Dataset API, the Deep Learning with JavaScript book says: But the documentation I've read about generators says a generator can't produce values via callbacks. But how else can one access remote sources? I don't see how one can use tf.data.generator in such cases. MDN documentation on yield states:",https://stackoverflow.com/questions/57296471,1230710,Documentation Ambiguity
74702527,Tensorflow - constructing a tensor from particular values extracted from two different tensors,"I'm trying to construct a single tensor using values from two different tensors and an array of two dimensional indices, in a manner compatible with TensorFlow autodiff. In a first step I want to extract the elements of a tensor D of shape (n,n) whose values are the same as those in another tensor a. In particular, I'm looking for a better way to implement the following loop: In the append step, I'm just using the first instance where the values are equal because the function I'm interested in is independent of this choice. I need to use isclose because the two arrays are float32 arrays and are not exactly equal to one another. Then in a second step I want to combine P_x with P_y = tf.gather(g, indices) to construct a tensor P. Assume that P_x and P_y are both of shape (n, ). Then, I'm pretty new to TensorFlow, so despite looking through the docs I don't see a way to do all of these operations using gather, scatter etc., which seems to be necessary to make autodiff work. When I use loops and other methods, I get gradients = none.",https://stackoverflow.com/questions/74702527,3131493,Documentation Replication on Other Examples
57301699,Why we always use seed =1234 in tf.compat.v1.random.set_random_seed(1234).Is there any specific reason?,"Why we always use , seed = 1234 in tf.compat.v1.random.set_random_seed(seed).Is there any specific reason?""",https://stackoverflow.com/questions/57301699,11867096,Documentation Replicability
57316557,"tf.keras.layers.pop() doesn't work, but tf.keras._layers.pop() does","I want to pop the last layer of the model. So I use the tf.keras.layers.pop(), but it doesn't work. When I use tf.keras._layers.pop(), it works. I don't find docs about this usage. Could someone help explain this?",https://stackoverflow.com/questions/57316557,11255365,Documentation Replication on Other Examples
57349824,"Recurrent neural network, time series prediction with newer Tensorflow 1.14","How to use new tf.keras API with recurrent neural network? I have checked the documentation but there is no example of such a situation. There is this great book Hands on machine learning from 2017. Since that year the API of tensorflow has evolved and I am trying to rewrite recurrent neural network for time series prediction with using version 1.14 code. The code from the book is using older tf.nn.dynamic_rnn and tf.nn.rnn_cell.BasicRNNCell: And this code works just fine (except that it throws warnings about deprecation left and right). I wanted to use tf.keras API as suggested in warning. My code is the same except: But this yields following exception: so I understand that the problematic line is After checking and comparing documentation for both cells https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn and https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN I can't find the culprit. What is the difference with these two cells? How to use tf.keras API with time series? Full old code: https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb Full ""my"" code:",https://stackoverflow.com/questions/57349824,2710943,Documentation Replication on Other Examples
57392510,TensorFlow simple example help - custom gradient,"How do you pass a custom gradient into a gradient optimization function in TensorFlow. I have illustrated what I am trying to do, with a simple example (trying to minimize z = 2x^2 + y^2 + 2). I have been looking at: https://www.tensorflow.org/api_docs/python/tf/train/Optimizer The problem seems to work if you pass in optimizer = tf.train.GradientDescentOptimizer(0.55) and train = optimizer.minimize(z) This code works: But I want to specify the gradient in the problem. aka I am trying to do this: How do you do this properly?",https://stackoverflow.com/questions/57392510,11895031,Documentation Replication on Other Examples
74809872,How can I use a function like tf.TensorArray in Pytorch?,I need to change code from tensorflow to pytorch. Is there a similar function like tf.TensorArray? For example:,https://stackoverflow.com/questions/74809872,20781411,Documentation Replicability
57403472,How do I add a new feature column to a tf.data.Dataset object?,"I am building an input pipeline for proprietary data using Tensorflow 2.0's data module and using the tf.data.Dataset object to store my features. Here is my issue - the data source is a CSV file that has only 3 columns, a label column and then two columns which just hold strings referring to JSON files where that data is stored. I have developed functions that access all the data I need, and am able to use Dataset's map function on the columns to get the data, but I don't see how I can add a new column to my tf.data.Dataset object to hold the new data. So if anyone could help with the following questions, it would really help: I have all the methods for taking the input as the elements from the columns and performing everything required to get the features for each element, I just don't understand how to get this data into the dataset. I could do ""hacky"" workarounds, using a Pandas Dataframe as a ""mediator"" or something along those lines, but I want to keep everything within the Tensorflow Dataset and pipeline process, for both performance gains and higher quality code. I have looked through the Tensorflow 2.0 documentation for the Dataset class (https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset), but haven't been able to find a method that can manipulate the structure of the object. Here is the function I use to load the original dataset: Then, I have methods which allow me to take a string input (column element) and return the actual feature data. And I am able to access the elements from the Dataset using a function like "".map"". But how do I add that as a column?",https://stackoverflow.com/questions/57403472,7508594,Inadequate Examples
57414387,Meaning of tf.keras.layers.LSTM parameters,"I am having trouble understanding some of the parameters of LSTM layers in the tf.keras.layers API. I am investigating using CuDNNLSTM layers instead of LSTM layers (to speed up training), but before I commit to CuDNN layers, I would like to have a full understanding of the parameters that I lose by using a CuDNNLSTM instead of a LSTM layer. I have read the docs, but they seem to assume some prior knowledge of LSTMs that I do not have. I have listed the pararameters that CuDNNLSTM does not have (that LSTM has) and interspersed with my questions about them, respectively. I've read a lot about LSTMs, and am at a point where I've decided to start training things, otherwise I won't absorb much more hypothetical knowledge. I've tried a lot of things in modeling, too, but the network I'm training is really simple so nothing seems to impact the results.",https://stackoverflow.com/questions/57414387,4982425,Documentation Replicability
57449484,What is trainable parameter in tensorflow?,tf.compat.v1.layers.batch_normalization takes trainable as an input. The documentation says: I think only scaling factor (gamma) and offset (beta) should be added to trainable variables and I am skeptical if even moving averages will get added to GraphKeys.TRAINABLE_VARIABLES. Can somebody tell me how trainable input is influencing the behavior of batch_normalization,https://stackoverflow.com/questions/57449484,6546694,Documentation Replicability
57453826,Write tf.dataset back to TFRecord,"After creating a tf.data.Dataset, I would like to write it to TFRecords. One way to do that is to iterate through the complete dataset and write after serializeToString into TFRecords. But it is not the most efficient way to do it. Are there easier ways to do this? Are there any APIs available in TF2.0?",https://stackoverflow.com/questions/57453826,11698102,Documentation Replication on Other Examples
57460127,Tensorflow 2 creating custom dataset,"I am trying to build a custom dataset-loader, which laods ICDAR-Dataset. My frist step was to embed a dataset inside my loader as suggested also here in this post, but the problem is that you have to implement all the nice features that the tenfsoflow-2 class ""Dataset"" offers manually. My second try was to subclass the Dataset-Class, something like: But the problem is i did not find any documentation what dataset-class internally really does, the only implementation i found was this one. So question is does anybody know how to build a custom ""dataset"" in tf2 by subclassing tf.data.Dataset. By the way i also tried tensorflow_datasets, bit it does not really worked, shince it will downlaod the dataset, and split them manually which is in this is alreay seperated by train and test and also ICDAr can not be downlaoded without registration. The content of the ICDAR-Dataset is as following: Image: @https://rrc.cvc.uab.es/?ch=4 owns the copyrights of this image. Words and bounding boxes for the above image: Thanks does anyone know how to",https://stackoverflow.com/questions/57460127,2335224,Lack of Alternative Solutions/Documentation
74861849,How to fix the batch size in keras subclassing model?,"In tf.keras functional API, I can fix the batch size like below: My question is, how do I can fix the batch size when I use the keras subclassing approach?",https://stackoverflow.com/questions/74861849,8551737,Documentation Replicability
74906407,How Do I Convert input arg tf.string to string in tf.function,"I want to implement universal input, something like this so I want to convert tf.string to string. use tf.print function can normally print the input args，Do I have any way to simulate the function of tf.print and assign the printed value to a variable to implement tf.string to string? or Is there any other way I can do this?",https://stackoverflow.com/questions/74906407,20852172,Documentation Replication on Other Examples
57526164,What is the relationship between `tf.function` and `autograph.to_graph` in Tensorflow?,"Similar results can be obtained via tf.function and autograph.to_graph. However this seems to be version dependant. For example, the function (taken from the official guide): Can be evaluated in graph mode using: So:",https://stackoverflow.com/questions/57526164,3860928,Documentation Replicability
57529534,How is tf.data.Dataset use optimised by tf.function in Tensorflow 2.0?,"The official documentation of Tensorflow 2.0 advices to use tf.data.Dataset along with tf.function. There are two examples of such uses: Finally, the autograph doc states that the iterating on a Dataset is optimized by tf.function. This SO answer shows indeed that using a Dataset as a parameter of tf.function increases performances. So, how does tf.data.Dataset benefit from tf.function, and how can it explain the speedup of this SO answer:",https://stackoverflow.com/questions/57529534,3860928,Documentation Ambiguity
57527609,How to show in tensorboard the tf.data.Dataset.map subgraph in Tensorflow 2.0?,"According to the documentation, the tf.data.Datasets work in graph mode (in both eager and graph mode): In Tensorflow 1.X, we can easily plot this graph in Tensorboard: The processing functions are plotted in a subgraph. For example, In Tensorboard, a subgraph appears, corresponding to the _parse_function: However, in Tensorflow 2.0, this does not produce any visible element in the Tensorboard graph. The following code does not produce any graph according to Tensorboard: So, since a graph is created when calling the map, is there a way to access/visualize this graph?",https://stackoverflow.com/questions/57527609,3860928,Documentation Replicability
75136950,How to visualize tf.compat.v1 static graph in tensorboard?,"For a given graph, how can we visualize the graph using tensorboard for tf.compat.v1 ? Sharing this here after searching everywhere. Most of the documentations explains tf.keras and not for tf.compat.v1 static graphs",https://stackoverflow.com/questions/75136950,10545426,Documentation Replication on Other Examples
75148775,Problem understanding output of tf.sets.intersection,"I am confused by the output of the following code, which is a simplified version of the example given in the documentation of tf.sets.intersection. It creates two sparse tensors which describe the location of two sets of numbers (all ones) placed on a 2x2 grid, and computes the intersection of the sets position-wise: This gives: Now, shouldn't the intersection (I) be an empty set because none of the input elements in the sparse tensors have the same coordinates? Instead, the computation returns a sparse tensor with one element (the number one) at (1, 0).",https://stackoverflow.com/questions/75148775,143931,Documentation Replicability
75225610,How does tf.GradientTape record operations inside the with statement?,"I don't understand how tf.GradientTape record operations like y=x**2 inside the ""with"" statement (following operations). What Python syntax can be used to achieve this behavior?",https://stackoverflow.com/questions/75225610,7973784,Documentation Replicability
75305478,Can tf.gradienttape() calculate gradient of other library's function,"If I include inside the tf.GradientTape() some functions from other Python libraries, like `sklearn.decomposition.PCA.inverse_transform()', can TensorFlow calculate gradients from that function? Specifically, can tf automatically differetiate pca_inverse_tranform = pca.inverse_transform(h2)?",https://stackoverflow.com/questions/75305478,9919423,Documentation Replicability
75354761,Looking for source of information relating the most efficient ways to use tf.js methods,"Is there any source of information relating efficiency of different tfjs methods, for example after benchmarking it seems like using casting on a tf.add operation is much slower than summing the pre existing tensors. Any ideas? During bench marking I see operations suche as argMax or max are very slow.",https://stackoverflow.com/questions/75354761,9194598,Documentation Replicability
75371111,"when trying to load external tfrecord with TFDS, given tf.train.Example, how to get tfds.features?","What I need help with / What I was wondering Hi, I am trying to load external tfrecord files with TFDS. I have read the official doc here, and find I need to define the feature structure using tfds.features. However, since the tfrecords files are alreay generated, I do not have control the generation pipeline. I do, however, know the tf.train.Example structre used in TFRecordWriter during generation, shown as follows. The doc only describes how to translate tfds.features into the human readable structure of the tf.train.Example. But nowhere does it mention how to translate a tf.train.Example into tfds.features, which is needed to automatically add the proper metadata fileswith tfds.folder_dataset.write_metadata. I wonder how to translate the above tf.train.Example into tfds.features? Thanks a lot! BTW, while I understand that it is possible to directly read the data as it is in TFRecord with tf.data.TFRecordDataset and then use map(decode_fn) for decoding as suggested here, it seems to me this approach lacks necessary metadata like num_shards or shard_lengths. In this case, I am not sure if it is still ok to use common operations like cache/repeat/shuffle/map/batch on that tf.data.TFRecordDataset. So I think it is better to stick to the tfds approach. What I've tried so far I have searched the official doc for quite some time but cannot find the answer. There is a Scalar class in tfds.features, which I assume could be used to decode Int64List. But How can I decode the BytesList? Environment information",https://stackoverflow.com/questions/75371111,7811775,Documentation Completeness
41449338,TensorFlow for MultiGPU,"If someone can help me understand the situation it would be great. Thanks in advance. My setup: OS: Ubuntu 16.04, 2 Titan X GPUs. TensorFlow (version 0.12.1) installed in a conda environment using pip as on TF docs. Python 3.5. Code: I ran the following code to test my 2 GPU setup. Once each with random_matrix = tf.zeros(...) and random_matrix = tf.random_uniform(...). The outputs are shown below. Questions: 1) When I run with tf.zeros. The timings on CPU and GPU are identical. But with tf.random_uniform I see that the GPU is faster (as I had expected). Why is tf.zeros slower on GPU? What am I missing? 2) I have fixed the global seed and the local seed. Why are the outputs within the GPUs different for the tf.random_uniform case? Thanks a lot for any insights in advance. Timings with tf.random_uniform(): Timings with tf.zeros():",https://stackoverflow.com/questions/41449338,1882931,Documentation Replicability
66144466,Tensorflow Serve SignatureDefs for classifier,"I trained a BERT text classifier following these steps, with own texts and some modifications: https://www.tensorflow.org/tutorials/text/classify_text_with_bert To export the model and run it with Tensorflow Serve works well: https://www.tensorflow.org/tfx/serving/docker Unfortunately, I can not really figure out how to define the SignatureDefs for a classifier, so that the classifier endpoint for Tensorflow Serve is defined. The :predict endpoint works well and seems to be the default signature. Obviously, I have to define the signatures, when I save the model. Since the documentation is not very exhaustive on this topic, I am not sure how to define the classifier signature. https://www.tensorflow.org/tfx/serving/signature_defs In the above example for BERT, the serving_results just define the reloaded_model with tf.constant(examples) and instantiates it with serving_results = tf.sigmoid(serving_results['classifier')]. So, I assume I have to give the activation function and the classifier signature as arguments, when calling the model.save method. predictendpoint works, classify endpoint gives the error: {""error"": ""No classification inputs found in SignatureDef: inputs {\n key: \""text\""\n value {\n name: \""serving_default_text:0\""\n dtype: DT_STRING\n tensor_shape {\n dim {\n size: -1\n }\n }\n }\n}\noutputs {\n key: \""classifier\""\n value {\n name: \""StatefulPartitionedCall_2:0\""\n dtype: DT_FLOAT\n tensor_shape {\n dim {\n size: -1\n }\n dim {\n size: 1\n }\n }\n }\n}\nmethod_name: \""tensorflow/serving/predict\""\n""} I would be grateful for any hints.",https://stackoverflow.com/questions/66144466,10263591,Lack of Alternative Solutions/Documentation
56289685,TensorflowJS predict asynchronously,"I have been trying to figure out how to perform predictions using tensorflowJS in an asynchronous fashion. All my attempts resulted in the predict function blocking my code. Looking at the docs, I see that most of the functions are defined as async functions and return a promise like for example tf.loadLayersModel, which also works for me asynchronously without any blocking. However, predict doesn't return a promise but directly a tf.Tensor. I tried wrapping the predictions in a customly defined async function like: But still predict is blocking the execution of my code. What is the right approach to use TensorflowJS for asynchronous inference?",https://stackoverflow.com/questions/56289685,9357259,Documentation Replicability
56491633,What is the difference between tf.scatter_add and tf.scatter_nd when indices is a matrix?,"Both tf.scatter_add and tf.scatter_nd allow indices to be a matrix. It is clear from the documentation of tf.scatter_nd that the last dimension of indices contains values that are used to index a tensor of shape shape. The other dimensions of indices define the number of elements/slices to be scattered. Suppose updates has a rank N. First k dimensions of indices (except the last dimension) should match with first k dimensions of updates. The last (N-k) dimensions of updates should match with the last (N-k) dimensions of shape. This implies that tf.scatter_nd can be used to perform an N-dimensional scatter. However, tf.scatter_add also takes matrices as indices. But, its not clear which dimensions of indices correspond to the number of scatters to be performed and how do these dimensions align with updates. Can someone provide a clear explanation possibly with examples?",https://stackoverflow.com/questions/56491633,1313405,Inadequate Examples
51691199,How does tf.create_partitioned_variables work?,"I am trying to figure out how to use tf.create_partitioned_variables I am reading the documentation but I am having a hard time understanding. Could anyone explain how it works and give some examples of its usage? From what I understand I can use it to get a list of slices from a variable. I just dont understand how I get the slices ex: how would i get a list of [[1.],[3.]] from tf.Variable(np.array([[1.0],[3.0]]), dtype=tf.float32) or list of from",https://stackoverflow.com/questions/51691199,774972,Inadequate Examples
45517940,What's the difference between tf.cond and if-else?,"What difference between tf.cond and if-else? The outputs are both 1. Does it mean only tf.placeholder can work, and not all the tensor, such as tf.variable? When should I choose if-else condition and when to use tf.cond? What are the diffences between them?",https://stackoverflow.com/questions/45517940,8165066,Documentation Replicability
64799299,Use TensorBoard to visualize graph from tf_agents,"I'm quite new to RL and currently teaching myself how to implement different algorithms and hyper-parameters using tf_agents library. I've been playing around with the code provided from this tutorial https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb. After learning how to use TensorBoard I've come to wonder how I can visualize a graph from tf_agents library. Every TensorBoard tutorials/posts seems to implement its own model or define tf.function to log graph. However, I just can't apply such methods to the tutorial above. If someone can help me visualize a model graph using tf_agents in TensorBoard, it will be very much appreciated. Thanks!",https://stackoverflow.com/questions/64799299,14622788,Documentation Replication on Other Examples
52533156,Weight Initialization Tensorflow tf.estimator,Is there a way to adjust the weight initialization in the pre-built tf.estimator? I would like to use the method after Xavier (tf.contrib.layers.xavier_initializer) or from He. Which method is used by default? I couldn't figure it out from the documentation. I use the DNNRegressor.,https://stackoverflow.com/questions/52533156,10203191,Documentation Ambiguity
45208516,K-means example(tf.expand_dims),"In Example code of Kmeans of Tensorflow, When use the function 'tf.expand_dims'(Inserts a dimension of 1 into a tensor's shape.) in point_expanded, centroids_expanded before calculate tf.reduce_sum. why is these have different indexes(0, 1) in second parameter?",https://stackoverflow.com/questions/45208516,8282898,Documentation Replicability
58842107,How do I update a model using a pre-release version of Tensorflow to run in a Google Colab instance?,"I'm trying to use the WikiReading dataset and model in a project and train it using a Google Colaboratory instance. For that purpose, I'm adapting the code to a Jupyter Notebook, which also uses a more recent version of Tensorflow than the provided baseline Bag of Words model did. The original paper and accompanying baseline model was published in August 2016, which predates Tensorflow 1.0.0. I've gone some way towards the process of updating the model to Tensorflow v1.x, but I appear to have hit a roadblock. From my (fairly limited) understanding, the last step in the model is to apply a softmax function on the results. In the original model, this was done using the following function call: This is then used in this statement for the optimization: I can't seem to find any documentation relating to the tf.contrib.learn.ops.softmax_classifier() function online. I'm assuming it takes in 4 tensors in some order, most likely something like the first holds the batch to classify, the second one holds a list of the answers to predict with the 3rd and 4th holding the embedding and biases for each answer. My problem is that I cannot find a function that maps neatly to that format with the same output and I'm not sure what transformations to apply to my tensors to get a similar result using something like tf.nn.softmax() without access to the documentation of tf.contrib.learn.ops.softmax_classifier(). How should I go about tackling this problem? Should I just rewrite the model_fcn()? The full Jupyter notebook can be accessed here I have found the source for the mentioned function here. The deprecation warning says this about updating this function: Now, I'm replicating the operations of this function as follows: but this gives me: This makes perfect sense when I look at the constant definitions: I guess my questions now are: As I try to update and train this model, I have grown more and more confused by the way it is defined. This is largely due to my inexperience with machine learning in general and TensorFlow in particular, but there are some things that don't make sense, at least to me. I have adjusted the ANSWER_NUM and ANSWER_DIM as I mention above and updated other functions and parameters (seen in the updated model_fn above) which gives me a valid data graph but the following error when trying to fit: This probably just requires a formatting step (that I'm not entirely sure of yet) before feeding running softmax_cross_entropy(). However, I also noticed that there is no definition of the hidden layers anywhere in the original model and HIDDEN_SIZE is unused. Is there an implicit definition of those layers somewhere in the model that I'm missing? At this point, I feel like it will be easier to just use a Keras functional model in TensorFlow 2.x to get as close as possible to the perceived original model.",https://stackoverflow.com/questions/58842107,12367101,Lack of Alternative Solutions/Documentation
58398790,Does Tensorflow have an inverse of tf.unravel_index?,"tf.unravel_index takes a flat index into an array and a shape, and returns the tuple that represents that index in the array. Is there an inverse? tf.ravel_index doesn't exist, but maybe it's under a different name?",https://stackoverflow.com/questions/58398790,635523,Documentation Replicability
58577878,Seq2Seq Training helper module in tensorflow,"In the latest tf 2.0 update tensorflow removed the contrib module from the framework. Thought they have compensated for most of the function from it in tf.compat.v1 but I couldn't find the substitute for the functions such as tf.contrib.seq2seq.BasicDecoder , tf.contrib.seq2seq.dynamic_decode , tf.contrib.seq2seq.GreedyEmbeddingHelper and tf.contrib.seq2seq.TrainingHelper. How to use these functions in my model?",https://stackoverflow.com/questions/58577878,8103827,Documentation Replication on Other Examples
59142986,serving_input_receiver_fn() function without the deprecated tf.placeholder method in TF 2.0,"I have a functioning tf.estimator pipeline build in TF 1, but now I made the decision to move to TF 2.0, and I have problems in the end of my pipeline, when I want to save the model in the .pb format I'm using this high level estimator export_saved_model method: https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesRegressor#export_saved_model I have two numeric features, 'age' and 'time_spent' They're defined using tf.feature_column as such: After the model has been trained I turn the list of features into a dict using the method feature_column_make_parse_example_spec() and feed it to another method build_parsing_serving_input_receiver_fn() excactly as outlied on tensorflow's webpage, https://www.tensorflow.org/guide/saved_model under estimators. I then inspect the output using the CLI tools Resulting in the following: enter image description here Somehow Tensorflow squashes my two usefull numeric features into a useless string input crap called ""inputs"". In TF 1 this could be circumvented by creating a custom input_receiver_fn() function using some tf.placeholder method, and I'd get the correct output with two distinct numeric features. But tf.placeholder doesn't exist in TF 2, so now it's pretty useless. Sorry about the raging, but Tensorflow is horribly documented, and I'm really working with high level API's and it should just be straight out on the horse, but no. I'd really appreciate any help :)",https://stackoverflow.com/questions/59142986,12453038,Documentation Ambiguity
44949292,Is there a difference between activations functions in tensorflow? tf.nn.tanh vs tf.tanh,I want to setup a neuronal network and I am asking myself if there is a difference between those two functions? tf.nn.tanh vs tf.tanh,https://stackoverflow.com/questions/44949292,4355878,Documentation Replicability
41125183,Tensorflow: split_v with variable num_splits,"I am wondering if the same holds for tf.split_v() as tf.split(). According to the documentation split_v also accepts a Tensor as second argument. However, when I try this code The error is Is this possible or not?",https://stackoverflow.com/questions/41125183,987397,Documentation Replicability
61293983,How does a tf.train.Int64LIst hold dtype unint64?,"From the tensorflow docs for TFRecords and tf.Example, it states on the topic of tf.train.Feature: How is it possible to store a uint64 dtype into a Int64List?",https://stackoverflow.com/questions/61293983,11070463,Documentation Replicability
61305781,Using Tensorflow embedded columns raises All feature_columns must be _FeatureColumn instances error,"I am new to tensorflow and I was trying to follow the official documentation where I came across tf.feature_column.categorical_column_with_vocabulary_list The code I tested is: However , when I run this sample code I get the following error : ValueError: All feature_columns must be _FeatureColumn instances. Given: [EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='colors', vocabulary_list=('X', 'R', 'G', 'B', 'Y'), dtype=tf.string, default_value=0, num_oov_buckets=0), dimension=3, combiner='mean', initializer=, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)] What I am doing wrong?",https://stackoverflow.com/questions/61305781,11041539,Documentation Replication on Other Examples
42560143,Cannot find documentary for many things in tf.nn.seq2seq class in the tensorflow,tf.nn.seq2seq.embedding_attention_seq2seq I was searching the above function in tensorflow api. But I counln't find that by the exact name. But similar function was there in the class tf.contrib.legacy_seq2seq.embedding_attention_seq2seq . Why they moved few from nn class to contrib class ?,https://stackoverflow.com/questions/42560143,5915270,Documentation Replicability
47822996,How does tensorflow import work?,"When I do import tensorflow as tf, I can access different packages inside tf: tf.nn, tf.train, etc BUT accessing tensorflow.examples fails: I have to import it differently, like from tensorflow.examples.tutorials.mnist import input_data, and only after that tf.examples does resolve. Why?",https://stackoverflow.com/questions/47822996,9101095,Documentation Replicability
65794527,"Example of output_signature , output_types & output_shapes for complex object called by tf.data.Dataset.from_generator","I've a generator function that yields the following tuple: yield (transformed_input_array, set_y) transformed_input_array is a list of ndarrays with the following shape: (1024, 104), (1024, 142), (1024, 1), (1024, 1), (1024, 1), (1024, 1), (1024, 140) and the following types: tf.float64, tf.float64, tf.int8, tf.int16, tf.int8, tf.int8, tf.float64 set_y is a ndarray of shape 1024 and type of int64 I've wrapped my generator with tf.data.Dataset.from_generator function, here is the code: But when I run the training, I get the following error: If I try to run with output_signature param (commented out code), I get the following error: Can someone provide an example, of how I should treat complex type (list of ndarrays)? Couldn't find any example in TF documentation..",https://stackoverflow.com/questions/65794527,336558,Inadequate Examples
60678769,Who to do early stopping with the evaluation loss using tf.estimator.train_and_evaluate?,"I am using the Tensorflow estimator and explicitly the method tf.estimator.train_and_evaluate(). There is an early stopping hook for the training which is tf.contrib.estimator.stop_if_no_decrease_hook, but I do have the issue that the training loss is too jumpy to use for early stopping. Does anyone know how to do early stopping with tf.estimator based on the evaluation loss?",https://stackoverflow.com/questions/60678769,12227311,Documentation Replication on Other Examples
42127505,Tensorflow dense_to_sparse,I am trying to convert a uncompressed sparse array into a format accepted by tf.SparseTensor. There is an inbuilt function tf.sparse_to_dense that does exactly the opposite I am trying to do. So my question is there any inbuilt function in Tensorflow or Python to do this conversion?,https://stackoverflow.com/questions/42127505,7537870,Documentation Replicability
62086367,Does tf.data API make use of tf.placeholders under the hood?,"I am wondering if tf.data API is using tf.placeholders under the hood, and if creating tf.data.Dataset is like creating a separate computational graph in which we load preprocess data and then merge with computational graph that our model represents?",https://stackoverflow.com/questions/62086367,8560600,Documentation Replicability
42773379,tf.nn.relu vs. tf.contrib.layers.relu?,"I see this ""tf.nn.relu"" documented here: https://www.tensorflow.org/api_docs/python/tf/nn/relu But then I also see usage of tf.contrib.layers.relu on this page in ""model_fn"": https://www.tensorflow.org/extend/estimators It seems like the latter isn't described like the first one in an API-like fashion, but only presented in use. Why is this? Are the docs out of date? Why have two - is one old and no longer supported/going to be removed?",https://stackoverflow.com/questions/42773379,384137,Documentation Ambiguity
42787903,Feeding inputs to tf.contrib.learn estimators directly,"I am having trouble using the DNNRegressor estimator from TensorFlow's tf.contrib.learn. In the documentation page of the estimator two methods for providing inputs are presented. The first method uses the input_fn function, which, as described, should be used for pre-processing and feeding the input to the estimators, and the second method feeds the input directly. Examples: In this case feature_cols is a dict with: label is a single tf.constant column containing the labels. This worked. In this case I don't know what to feed in as X and y. I have tried the following: I also tried other combinations with dict and numpy arrays, but nothing worked. I would like to be able to make this work using the second method, since this is also useful for passing the objects to the evaluate and predict. Does anyone know the correct format for this? Also, is there a way to simply pass numpy arrays? Thank you! tl;dr What should the inputs to tf.contrib.learn estimator be in order to feed them directly using estimator.fit(x=X_train, y=y_train, steps=...)?",https://stackoverflow.com/questions/42787903,4330168,Documentation Replication on Other Examples
43261919,How to know if I should wrap operations in name_scope or variable_scope for Tensorboard with example,"When grouping together operations for visualisations there is a large difference between the outputs generated between the two programs below. The tf.name_scope gives double rnn outputs, one inside the name_scope and one outside while tf.variable_scope gives the clearer representation. How do I know if I must wrap something in variable vs name scopes (besides the obvious case with get_variable)? And",https://stackoverflow.com/questions/43261919,3139545,Documentation Replicability
41764199,Row-wise Histogram,"Given a 2-dimensional tensor t, what's the fastest way to compute a tensor h where I.e. where tf.histogram_fixed_width is called per row of the input tensor t? It seems that tf.histogram_fixed_width is missing an axis parameter that works like, e.g., tf.reduce_sum's axis parameter.",https://stackoverflow.com/questions/41764199,482601,Documentation Replication on Other Examples
50383462,how to randomly initialize weights in tensorflow?,"in tensorflow, I learned from the tutorial that one would initialize the variables with something like sess.run(tf.global_variables_initializer()) however I found that every time I run this with the same input dataset, the loss value starts with the same value. I presume this is due to the fact that the initialization is always setting up the variables with the same values. (probably zero) I wish to randomize the values of weights. I've tried searching for this but tensorflow docs doesn't give a clear answer if the initialization is done with zero values by default or random values. How can I specify the initializaing to setup random values? update my network is first a bunch of CNNs and pooling layers like below: ``` conv1 = tf.layers.conv2d(inputs=input_layer, filters=32, kernel_size=[3,3], padding=""same"", activation=tf.nn.relu, name=""conv_chad_1"") ``` AFAIK, the weights are defined inside these predefined layers. How do I specify these layers to initialize their weight variables randomly??",https://stackoverflow.com/questions/50383462,3044831,Documentation Replication on Other Examples
42897816,"Why does ""tf.Variable([.3], tf.float32)"" work in tensorflow?","The standard usage should be tf.Variable([.3], dtype=tf.float32), isn't it? I saw the tf.Variable([.3], tf.float32) in official documentation. The constructor function prototype of tf.Variable is __init__(self, initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None, expected_shape=None, import_scope=None). If we pass the parameter tf.float32 instead of dtype=tf.float32 (key parameter), how does it know the tf.float32 is employed for dtype. Does python interpreter check the parameter type?",https://stackoverflow.com/questions/42897816,4810225,Documentation Replicability
43567551,Tensorflow seed not working with LSTM model,"tf.set_random_seed() is not working and opt seed not found. For many parameters in the LSTM, it seems no opt seed found in the tf.nn.rnn_cell.BasicLSTMCell. Thus, for every time it produces different results. How to set the seed to produce the same results for running several times?",https://stackoverflow.com/questions/43567551,7907993,Documentation Replication on Other Examples
66980404,Tensorflow 2 - How to conditionally update values directly in tf.Variable,"Please advise how to conditionally update the original tf.Variable. This is a different question from conditional assignment of tf.variable in Tensorflow 2. tf.Variable is mutable and the assign method would update the same memory area. It looks the assign method does not have an option to incorporate condition when assigning values. Hence I suppose tf.where is to update tf.Variable conditionally. In numpy, indexing can be used to directly update the numpy array but there looks no such way in Tensorflow. As tf.Variable is mutable, expected that the tf.where will mutate the original Variable, however as below, the original Variable x has not been updated. Result: Please help me understand if there is a way to directly update x.",https://stackoverflow.com/questions/66980404,4281353,Documentation Replicability
67749813,TFRecord parsing for 3-D features,"I have similar question to this, but what if my feature shape is 3-D? Instead of prices (1,288), it is (1,288,3) for example. What should I put as the shape of tf.io.FixedLenFeature()? Is it tf.io.FixedLenFeature(shape=[288,3], tf.float32) or tf.io.FixedLenFeature(shape=[864], tf.float32) or anything else? Thanks!",https://stackoverflow.com/questions/67749813,11970084,Documentation Replicability
49686860,Side effect in tf.while_loop,"I am currently having a hard time trying to understand how tensorflow works, and I feel like the python interface is somehow obscure. I recently tried to run a simple print statement inside a tf.while_loop, and there are many things that remains unclear to me: Notice that if I initialize nb_iter with I got the following error: It get even worse when I try to use the 'i' index for indexing a tensor (example not shown here), I then get the following error Can someone point me to a documentation that explains how tf.while_loop works when used with tf.Variables, and if it possible to use side_effects (like print) inside the loop, as well as indexing tensor with the loop variable ? Thank you in advance for your help",https://stackoverflow.com/questions/49686860,2697831,Requesting (Additional) Documentation/Examples
49746064,What is the difference between tf.nn.ctc_beam_search_decoder and tf.contrib.seq2seq.BeamSearchDecoder mechanism?,I am building a seq2seq model with tensorflow. Can you explain the details of the two beam search functions? Thank you. tf.nn.ctc.beam_search_decoder https://www.tensorflow.org/api_docs/python/tf/nn/ctc_beam_search_decoder tf.contrib.seq2seq.BeamSearchDecoder https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/BeamSearchDecoder,https://stackoverflow.com/questions/49746064,9622974,Documentation Replicability
50259009,How to modify the return tensor from tf.nn.embedding_lookup()?,"I want to use scatter_nd_update to change the content of the tensor returned from tf.nn.embedding_lookup(). However, the returned tensor is not mutable, and the scatter_nd_update() require an mutable tensor as input. I spent a lot of time trying to find a solution, including using gen_state_ops._temporary_variable and using tf.sparse_to_dense, unfortunately all failed. I wonder is there a beautiful solution toward it?",https://stackoverflow.com/questions/50259009,4987560,Documentation Replicability
48051722,"The difference between tf.layers, tf.contrib, and tf.nn in Tensorflow","The variants of functions that perform the same functionalities in tensorflow is a very confusing (at least for me). I want to use the official tensorflow API (already gave up on the mess of the high-level API's). However, I trying to understand the difference between tf.layers, tf.contrib, and tf.nn For example, I am trying to use batch normalization in tensorflow. I google it and the first three links: tf.nn.batch_normalization tf.layers.batch_normalization tf.contrib.layers.batch_norm I understand they both have different parameters and eventually serve the same purpose. But the question is why do we have these three different classes? Is there a valid reason? I searched the source code and realized that tf.layers is a wrapper for tf.nn Update: This question is already answered here.",https://stackoverflow.com/questions/48051722,7361690,Documentation Ambiguity
49785239,TensorFlow.constant() throws TypeError with placeholder parameter (TF v1.4),"Python host language. TF v1.4. https://www.tensorflow.org/versions/r1.4/api_docs/python/tf/constant Nearly just like docs example (above), I need to make a constant 2-D tensor populated with scalar value, in my case some mean value, which is mean of r, but r is a placeholder, not a variable, NOT a numpy array. As such, feed_dict needs to be used to fill-in placeholder r in my application. Now a python ndarray or some python list was used in the docs example, which won't work in my application, so that's the difference. In my application, global_bias (below) needs to be a tf.constant not a tf.variable -- because what I'm saying is, the optimizer needs to not learn it. Just leave it be, please. Note: In my application, r is a placeholder not a variable. So, as I said, what TF code would be used to make a constant 2-D tensor populated with scalar value, in my case some mean value, which is mean of r, but r is a placeholder, not a variable, NOT a numpy array? Hey I tried .eval() but TF said you need a session to do that. So are you TF folks saying I need 2 sessions, run sequentially, just to do this little thing? Or else are you saying I should bypass feed_dict to feed in my r placeholder actual data via numpy arrays in through the side door (in app memory as a python list or ndarray) plus the front door (feed_dict), which would be janky? Thanks for clues! I need to get one.",https://stackoverflow.com/questions/49785239,39123,Documentation Replicability
49862069,Is there a way to dispose tensors in tf.Model object?,"I'm building an interactive app in which users can play around with the parameters of a model. After new parameters are specified, I no longer need the old tf.Model object and the associated tensors. Is there an equivalent of tf.Tensor.dispose() for tf.Model? Thanks!",https://stackoverflow.com/questions/49862069,9647899,Documentation Replicability
64776769,Why tf.contrib.layers.instance_norm layer contain StopGradient operation?,Why tf.contrib.layers.instance_norm layer contain StopGradient operation? i.e. why it's needed? Seems there is StopGradient even in simpler layer tf.nn.moments (that can be building block of tf.contrib.layers.instance_norm). Also I find a note on StopGradient in tf.nn.moments source code: So it's sort of optimisation because gradient is always zero?,https://stackoverflow.com/questions/64776769,1179925,Documentation Replicability
46658607,where is tf.nn.l2_loss defined?,According to this documentation https://www.tensorflow.org/api_docs/python/tf/nn/l2_loss it says But when I go to tensorflow/python/ops/gen_nn_ops.py there is no l2_loss defined. I'm trying to see what would be the difference between using tf.nn.l2_loss(W) or just using tf.reduce_sum(tf.square(W)).,https://stackoverflow.com/questions/46658607,3907250,Documentation Ambiguity
46659101,Using `softmax_cross_entropy_with_logits()` with `seq2seq.sequence_loss()`,"I have a working RNN using the default softmax loss function for tf.contrib.seq2seq.sequence_loss() (which I'm assuming is tf.nn.softmax()) but would instead like to use tf.nn.softmax_cross_entropy_with_logits(). According to the seq2seq.sequence_loss documentation, one may use softmax_loss_function= to override the default loss function: Here is my code that works: My attempt to change the loss function is as follows (I've only indicated the code that is different): The line cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks, softmax_loss_function=loss) is now giving me ""TypeError: 'Tensor' object is not callable."" This is one of the most opaque errors I've seen Tensorflow produce and I haven't found much of anything in the way of explanation on the internet. Any help would be appreciated.",https://stackoverflow.com/questions/46659101,852795,Documentation Replication on Other Examples
46976226,`tf.estimator.RunConfig` vs `tf.contrib.learn.RunConfig`,I am confused regarding whether I should be using tf.estimator.RunConfig or tf.contrib.learn.RunConfig to pass a RunConfig to an estimator. using tf.contrib.learn.RunConfig is straightforward: But tf.estimator.RunConfig has some odd syntax: Is there any reason to prefer one RunConfig over the other? The documentation is not clear on this.,https://stackoverflow.com/questions/46976226,592235,Lack of Alternative Solutions/Documentation
64826405,Tensorflow: 'axis' argument in dot product,"Can someone show me the way I should use the axis argument in tf.tensordot? I read the documentation but it was complicated and I'm still confused. I saw another question that asks about axis in tf.one_hot and in the answers were some good insights about the matter, but that didn't help me with tf.tensordot. I thought you can give me some insights on this too. For example, I know I can dot product a vector and a tensor like this: But when I batch them and add one dimension to them to be of the shape (b, n) and (b, m, n) to obtain a (b, m, 1), now I don't know how to dot product every batch.",https://stackoverflow.com/questions/64826405,7339624,Documentation Ambiguity
70735454,How to flatten a gradient (list of tensors) in tf.function (graph mode),"I want to do some linear algebra (e.g. tf.matmul) using the gradient. By default the gradient is returned as a list of tensors, where the tensors may have different shapes. My solution has been to reshape the gradient into a single vector. This works in eager mode, but now I want to compile my code using tf.function. It seems there is no way to write a function which can 'flatten' the gradient in graph mode (tf.function). I tried converting it like this, but it doesn't work with tf.function either. I tried TensorArrays, but it also does not work.",https://stackoverflow.com/questions/70735454,10565197,Documentation Replicability
46381790,How does TensorFlow handle none shape?,"I'm trying to implement a simple computational graph framework and test it with simple neural network, mainly by learning from TensorFlow. Now I would want to be clear how does TensorFlow handle none shape tensors. In this example, X has shape [None, n_input], weights['h1'] has shape [n_input, n_hidden_1], and biases['b1'] has shape [n_hidden_1]. When it tries to do this: layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']), tf.matmul(x, weights['h1']) should have shape [None, n_hidden_1], and how exactly does TensorFlow add it with biases['b1']? Based on the documentation, tf.add only works when the 2 operands have the same shape. If we run with a batch of size 10, tf.matmul(x, weights['h1']) will have shape [10, n_hidden_1], and it shouldn't be able to be added with biases['b1'].",https://stackoverflow.com/questions/46381790,7420679,Documentation Replication on Other Examples
37900780,In tensorflow what is the difference between tf.add and operator (+)?,"In tensorflow tutorials, I see both codes like tf.add(tf.matmul(X, W), b) and tf.matmul(X, W) + b, what is the difference between using the math function tf.add(), tf.assign(), etc and the operators + and =, etc, in precision or other aspects?",https://stackoverflow.com/questions/37900780,1429955,Documentation Replicability
67940962,How to convert this tensor flow code into pytorch code?,I am trying to implement an Image Denoising Gan which is written in tensorflow to pytorch and I am unable to understand what is tf.variable_scope and tf.Variable similar in pytorch. please help.,https://stackoverflow.com/questions/67940962,,Documentation Replicability
49997294,Moving away from tf.contrib.learn: distributed training with dedicated evaluator process,"In TF 1.8's upcoming release, tf.contrib.learn.* will be deprecated. The tf.contrib.learn.Experiment class recommends switching to tf.estimator.train_and_evaluate instead, so I'm trying to port my code to that framework. What I want to do is set up distributed training on two machines' GPUs, plus a third CPU-only process that does continuous evaluation on a small validation set. Following the examples in the documentation of train_and_evaluate and the Distributed Tensorflow guide, I managed to set up the training half of my desired architecture, but I can't find a way to set up an estimator. So far, what I have looks as follows: This code works, although my understanding of it is still pretty limited. I get what the PS and workers do, but from the specification of chief I understand this should be the ""master"" worker that also logs summaries and saves checkpoints. What I'm missing now is the periodic evaluation... and I'm at a loss. From the train_and_evaluate codebase I see there's some ""evaluator"" support but I don't understand how to set it up properly.",https://stackoverflow.com/questions/49997294,3214872,Documentation Replication on Other Examples
50457247,Tensorflow finding matching strings in tensor,"I'm trying to find variables that end in train_step from tf.report_uninitialized_variables(), but you can't iterate over tensors without eager execution. I get that you need to use tf.map_fn, but I do not understand it well enough. This is what I have:",https://stackoverflow.com/questions/50457247,3938049,Documentation Replicability
51882846,tensorflow: how to use tf.nn.leaky_relu with alpha as activation_fn for tf.contrib.layers.fully_connected?,"The default activation_fn for tf.contrib.layers.fully_connected is tf.nn.relu. If I want to change the activation_fn for a certain fully connected layer to tf.nn.tanh then I can call it as: tf.contrib.layers.fully_connected(inputs, num_outputs, activation_fn=tf.nn.tanh) Now if I want to use tf.nn.leaky_relu with alpha=0.01, I can't do that. I can only use tf.nn.leaky_relu with the default value of alpha. Is there any elegant way to do that ? Or should I make activation_fn=None and then manually call tf.nn.leaky_relu after that ?",https://stackoverflow.com/questions/51882846,3142049,Documentation Replicability
49294329,How to get the default session from a tf.estimator?,"I am trying the high API tf.estimator, but I find it hardly to get the session to debug some inter-result such as global step. The example from https://www.tensorflow.org/versions/master/get_started/custom_estimators I have try sess = tf.get_default_session and with tf.Session() as sess, but can't get the defut session.",https://stackoverflow.com/questions/49294329,8000475,Inadequate Examples
49564318,Issue with fine-tuning inceptionv3 in slim tensorflow and tf record batches,"I am trying to fine-tune inceptionv3 model using slim tensorflow library. I am unable to understand certain things while writing the code for it. I tried to read source code (no proper documentation) and figured out few things and I am able to fine-tune it and save the check point. Here are the steps I followed 1. I created a tf.record for my training data which is fine, now I am reading the data using the below code. Now I am finetuning the model using slim and this is the code. Now I have few questions about the code, which I am quite unable to figure out. Once, the code reaches slim.learning.train I don't see anything printing however, it's training, I can see in the log. Now, 1. How do I give the number of epochs to the code? Right now it's running step by step with each step has batch_size = 64. 2. How do I make sure that in the code tf.train.shuffle_batch I am not repeating my images and I am training over the whole dataset? 3. How can I print the loss values while it's training?",https://stackoverflow.com/questions/49564318,7776604,Documentation Replication on Other Examples
49605330,Example of tf.feature_column.indicator_column,"I’m reading tensorflow’s document about tf.feature_column.indicator_column. In this document, there is an example. My problem is the omitted(...) part of this code. I just want a complete, running, simple example. And I can’t find a kind example including tf.Example and so on. Can anyone make this complete? Thank you for advance.",https://stackoverflow.com/questions/49605330,766330,Documentation Completeness
51965094,What's the difference between tf.feature_column.input_layer and tf.layers.Input,I currently get stuck at distinguishing between tf.feature_column.input_layer and tf.layers.Input. Are the two exchangable in the actual constructed model? What is the use case of each of the two?,https://stackoverflow.com/questions/51965094,5729303,Documentation Replicability
52254253,How does tf.layers.dense() interact with inputs of higher dim?,"In tensorflow layers.dense(inputs, units, activation) implements a Multi-Layer Perceptron layer with arbitrary activation function. Output = activation(matmul(input, weights) + bias) Typically input has shape=[batch_size, input_size] and might look like this: (units = 128 and activation = tf.nn.relu are chosen arbitrarily) I have not found any documentation on what would happen, if i fed higher dimensional input, e.g. because one might have time_steps resulting in a tensor of shape=[time_step, batch_size, input_size]. What one would want here is that the layer is applied to each single input_vector for each timestep for each element of the batch. To put it a bit differently, the internal matmul of layers.dense() should simply use broadcasting in numpy style. Is the behaviour i expect here what actually happens? I.e. is: applying the dense layer to each input of size input_size for each time_step for each element in batch_size? This should then result in a tensor(in dense_layer above) of shape=[time_step, batch_size, 128] I'm asking, as e.g. tf.matmul does not support broadcasting in the numpy style, so i'm not sure, how tensorflow handles these cases. Edit: This post is related, but does not finally answer my question",https://stackoverflow.com/questions/52254253,6917400,Lack of Alternative Solutions/Documentation
54096360,How to properly use tf.scatter_update for N-Dimensional Updating?,"I've been trying to make an N-Dimensional update using tf.scatter_update (after tf.scatter_nd was failing due to shape mismatch). In general, these will be used to create masks for filtering slices of an incoming tensor. Presumption is that input Tensor A is of shape (batch, i, j, k(depth)). I am only interested in modifying i,j values for all k, and for all b. MWE: Resulting in: I have tried this via Python Script, Python Notebook, and with/without Eager Execution. No luck. Input absolutely must be a tensor, as the idea is to sparsely update this tensor midway through a series of operations. Is there something fundamental I'm missing regarding tf.scatter_update? Would tf.scatter_nd be more suited? If so, what are the differences, specifically with indices for the updates. When referencing tf.scatter_update documentation, the examples are basic and utilise constants; I'm having difficulty applying this to a more realistic situation and problem.",https://stackoverflow.com/questions/54096360,1587808,Inadequate Examples
56754293,TensorflowServing on a trained native Keras model with a preprocessing function for the input,"My final goal is to use the What-If-Tool on tensorboard. In order to do that, I need to serve my Keras model on TensorflowServing, and the data in a TFRecordFile. So the data has to be transformed into tf.Examples. The tool is supposed to grab the network to run inference on the data. however, the network cannot handle tf.Examples as an input. So the served model needs to have a preprocessing function. According to the tensorflow documentation, one way is to create a tensorflow Estimator, and to use ""serving_input_receiver_fn"" to preprocess the data. This would have been perfect except for the case that I can't make an already trained native Keras model into an Estimator. It seems that the only way it to create it from a tf.keras model (and not a native keras model like I have), and to train it directly with the estimator. Another way would be to use the tf.saved_model.simple_save function, and then use TensorflowServing, but I did not find a way to preprocess the tf.Examples to make a correct input for the network. Since this is not working, I have no clue on how to resolve this. Edit: I tried to transform my native keras into a tf.keras model. My model is really big, so I build this function: However, this is not working because of Lambda layer. In the config layer, the function is now written in the form of: Hence, I gave up this method hoping something else would allow to pre-process the input of my serving model.",https://stackoverflow.com/questions/56754293,10545585,Documentation Replication on Other Examples
52720886,Does tf.contrib.nn.sampled_sparse_softmax_loss allow for float16 training?,"Currently, tf.nn.sampled_softmax_loss does not allow for using float16. How to run define Tensorflow graph were all variables are in float16 instead instead of float32 I'm looking at tf.contrib.nn.sampled_sparse_softmax_loss from https://www.tensorflow.org/api_docs/python/tf/contrib/nn/sampled_sparse_softmax_loss And it seems that this may allow float16 values. This code for this function is here https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/nn/python/ops/sampling_ops.py Which seems to use tf.nn.sparse_softmax_cross_entropy_with_logits Which seems to have support for float16 https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits However, when I try to use it, I get an error My code My error message",https://stackoverflow.com/questions/52720886,3259896,Documentation Replication on Other Examples
52744559,How to freeze some part of the variable in tensorflow,"I've noticed that a variable can be frozen by setting trainable=False. I'm wondering how to freeze only part of it instead of splitting it into two variables. For example, I only want to freeze the first element in v = tf.Variable(tf.zeros([2]). It seems that grad_new = tf.multiply(grad, np.array([0,1])) doesn't work.",https://stackoverflow.com/questions/52744559,9475497,Documentation Replicability
52763539,Compute the mean for each row from a tf.SparseTensor in TensorFlow,I want to compute the mean for axis=0 for a tf.SparseTensor. I want something like tf.sparse_reduce_sum. TensorFlow doesn't provide a similar function for the mean calculation. Is there any way to count the values in each row in order to divide them with the sum?,https://stackoverflow.com/questions/52763539,9854132,Documentation Replication on Other Examples
52872239,Can tf.scatter_update or tf.scatter_nd_update be used to update column slices of a tensor?,"I want to implement a function that takes in a variable as input, mutate some of its rows or columns and replaces them back in the original variable. I am able to implement it for row slices using tf.gather and tf.scatter_update but unable to do so for column slices since apparently tf.scatter_update only updates the row slices and does not have an axis feature. I am not an expert in tensorflow therefore I may be missing something. Can someone help?",https://stackoverflow.com/questions/52872239,9936406,Documentation Replication on Other Examples
38213440,How can you get length of a TensorFlow string?,"Is there any way to get length of a TensorFlow string within TensorFlow? For example, is there any function that returns the length of a = tf.constant(""Hello everyone"", tf.string) as 14 without passing the string back to Python.",https://stackoverflow.com/questions/38213440,4193504,Documentation Replicability
53772787,tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) in tensorflow,What is purpose of tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)) in tensorflow? With more context:,https://stackoverflow.com/questions/53772787,1179925,Requesting (Additional) Documentation/Examples
75939760,Slow performance of tf.gather due to index validation on CPU,"I am using tensorflow to train Neural Radiancy Fields. I used tf.gather to implement a multiresolution hashtable encoding as described in this paper. The important part is, that I am trying to retrieve values from a tensor that is optimized during trainig according to hash values that I computed out of 3D positions. I need to potentially do this for hundreds of thousands of values to render a single image so performance is critical. I am using a GPU for training. The performance is not as good as I would expect it though which I suspect is due to the index validation for tf.gather being done on the CPU instead of the GPU. The GPU capacity is not fully utilized and I see significant CPU load. This is described in the documentation here. It says that the argument validate_indices is deprecated and that index validation is always done on the CPU. My question is, if there is potentially a way to either The values are updated during the training so precomputing values will not help in this case. This is the code for the hash embedding that is functional but not performant due to the index validation: Here are some dummy examples to test the code: I know that the tf.scatter_ functions do the index validation on the GPU however I can not figure out a way to utilize these functions to retrieve the values from the table.",https://stackoverflow.com/questions/75939760,13230142,Documentation Replication on Other Examples
57279754,"What are the Tensorflow qint8, quint8, qint32, qint16, and quint16 datatypes?","I'm looking at the Tensorflow tf.nn.quantized_conv2d function and I'm wondering what exactly the qint8, etc. dataypes are, particularly if they are the datatypes used for the ""fake quantization nodes"" in tf.contrib.quantize or are actually stored using 8 bits (for qint8) in memory. I know that they are defined in tf.dtypes.DType, but that doesn't have any information about what they actually are.",https://stackoverflow.com/questions/57279754,11860666,Documentation Replication on Other Examples
74736678,Migrate tf.contrib.lookup.index_table_from_tensor() from TF1.x to TF2.x,"I want to migrate a TF1.x implementation towards TF2.x and it used the method: which is not existing in TF2.x. in tf.lookup. Is there known way to migrate the function tf.contrib.lookup.index_table_from_tensor(tensor, dtype=tf.int32)? I tried to change: tf.contrib.lookup.index_table_from_tensor(tensor, dtype=tf.int32) towards tf.lookup.index_table_from_tensor(tensor, dtype=tf.int32) But the method doesn't exist in TF2.x. I can imagine that tf.lookup.StaticHashTable could help, but there is also no method llike index_table_from_tensor()",https://stackoverflow.com/questions/74736678,2240187,Documentation Replication on Other Examples
47935393,Tensorflow stft does not use gpu,"I have a very simple tensorflow program that computes the spectrogram of a wav file: The docs for tf.contrib.signal.stft say its implemented with GPU-compatible ops, and I see the device placement logs say every op in the program is being placed on my gpu. However, when I run this, my cpu is maxed out at 100%, and my gpu is completely idle. I've also tried adding an explicit with tf.device('/gpu:0'): above the stft line, but to no avail. What do I need to do to get this running on my gpu? Could it be a bug with the op? Other gpu ops, such as the convolution ops, seem to work great with my gpu and I can get full 100% usage from it when I use those. I am on Linux Ubuntu, Tensorflow 1.4.1 (compiled from source), and I am using CUDA 9.0 and cuDNN 7.0.",https://stackoverflow.com/questions/47935393,2382483,Documentation Replication on Other Examples
52878311,How to extract rows and columns from a 3D array in Tensorflow,"I wanted to do the following indexing operation on a TensorFlow tensor. What should be the equivalent operations in TensorFlow to get b and c as output? Although tf.gather_nd documentation has several examples but I could not generate equivalent indices tensor to get these results. I am not restricted to tf.gather_nd Any other suggestion to achieve the same operations on GPU will be helpful. old statement: c=a[:,idx], New statement: c=a[:,:,idx] What I wanted to achieve was re-ordering of columns as well.",https://stackoverflow.com/questions/52878311,4082304,Inadequate Examples
55711355,How to restore dangling tf.py_func within the tf.data.Dataset() with tf.saved_model API?,"After doing a research for restoring the tf.py_func() when using saved_model API in vain, I couldn't find other information than documented in tensorflow: Two save/load snippets help to illustrate the situation. Save part: Load part: Error: It's well known that the function wrapped by the tf.py_func() isn't saved with the model. Does anybody has a solution to restore this by using the small hint given by the tf doc applying tf.train.Server",https://stackoverflow.com/questions/55711355,9217178,Lack of Alternative Solutions/Documentation
74444314,Is there a good way to write 2d arrays or tensors to TFRecords in Tensorflow?,"I am currently working on a project using audio data. The first step of the project is to use another model to produce features for the audio example that are about [400 x 10_000] for each wav file and each wav file will have a label that I'm trying to predict. I will then build another model on top of this to produce my final result. I don't want to run preprocessing every time I run the model, so my plan was to have a preprocessing pipeline that runs the feature extraction model and saves it into a new folder and then I can just have the second model use the saved features directly. I was looking at using TFRecords, but the documentation is quite unhelpful. tf.io.serialize_tensor tfrecord This is what I've come up with to test it so far: But I'm getting this error: tl;dr: Getting the above error with TFRecords. Any recommendations to get this example working or another solution not using TFRecords?",https://stackoverflow.com/questions/74444314,11111806,Requesting (Additional) Documentation/Examples
57914611,Has tf.Estimator become obsolete in Tensorflow 2.0?,"Today I've set up a custom model using its tf.Estimator high-level API in Tensorflow 2.0. It was a pain in the *** to get it running, and there are very few complete examples online that implement custom Estimators in Tensorflow 2, which made me questioning the reasons for using this API. According to the docs, the main advantages of using the tf.Estimator API are: Advantage 2. clearly doesn't apply to Tensorflow 2.0 anymore, as it runs in eager mode by default, so you don't have to worry about sessions anyways. Advantage 1. also seems quite irrelevant in Tensorflow 2.0 - using tf.distribute.Strategy, you can now easily run even high level tf.Keras models in a distributed fashion and on CPUs/GPUs/TPUs. tf.Keras models are so much easier and faster to set up, so why did they even bother to keep the tf.Estimator API in Tensorflow 2.0? Are there other advantages of using this API?",https://stackoverflow.com/questions/57914611,8725045,Requesting (Additional) Documentation/Examples
59743351,Tensorflow 2.0.0: AttributeError: 'TensorSliceDataset' object has no attribute 'as_numpy_iterator',"I am testing tensorflow tf.data.Dataset method as_numpy_iterator using tensorflow 2.0.0. According to the official documentation https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#as_numpy_iterator, this function allows directly inspecting the content of a tensorflow dataset. But when I try the given example: There occurs an error: AttributeError: 'TensorSliceDataset' object has no attribute 'as_numpy_iteractor'. I am wondering if this method is just newly added, beyond the support of tensorflow 2.0.0. If so, is there an alternative to checking the dataset content as the as_numpy_iterator()?",https://stackoverflow.com/questions/59743351,6807211,Documentation Replicability
59870349,"tf.data for keras model with multiple inputs, one ctc output, ValueError","I'm using tf.version '2.1.0-rc2' My model has 2 inputs and 1 output and I am struggling to feed it data. Example https://keras.io/examples/image_ocr/ uses a generator that output a dictionary of numpy arrays. and fit_generator but https://www.tensorflow.org/api_docs/python/tf/keras/Model indicates I believe td.data is the recommended way to feed data into tf.keras. So I would like to figure this out. First, a simplified dummy model My generator based on tf.data Instantiate a train and validation generator from the above Call fit with the td.data generators The error I also tried the dictionary approach ... with model.fit calling as recommended But this gives me the following error:",https://stackoverflow.com/questions/59870349,4547188,Documentation Replicability
60384790,Difference - tf.gradients vs tf.keras.backend.gradients,"Being new to Tensorflow, I am trying to understand the difference between underlying functionality of tf.gradients and tf.keras.backend.gradients. The latter finds the gradient of input feature values w.r.t cost function. But I couldn't get a clear idea on the former whether it computes the gradient over cost function or output probabilities (For example, consider the case of binary classification using a simple feed forward network. Output probability here is referred to the Sigmoid activation outcome of final layer with single neuron. Cost is given by Binary cross entropy) I have referred the official documentation for tf.gradients, but it is short and vague (for me), and I did not get a clear picture - The documentation mentions it as just 'y' - is it cost or output probability? Why I need the gradients? To implement a basic gradient based feature attribution.",https://stackoverflow.com/questions/60384790,11606547,Requesting (Additional) Documentation/Examples
60516977,Difficulties in understanding higher order derivatives for tf.custom_gradient(),"Based on the example as quoted in tensorflow's website here: https://www.tensorflow.org/api_docs/python/tf/custom_gradient There is a lack of details on why second_order_and_transpose(ddy) returns two objects. Based on the documentation of tf.custom_gradient, the grad_fn (i.e. second_order_and_transpose()) should return a list of Tensors which are the derivatives of dy w.r.t. unused_x. It is also not even clear why did they name it unused_x. Anyone has any idea on this example or in general create custom gradients for higher order derivatives?",https://stackoverflow.com/questions/60516977,4723266,Documentation Completeness
60778828,higher order gradient through py_function,"I wonder how to calculate higher order gradients through tf.py_function in tf2.0. The following example (slightly modified from tensorflow doc) produces the correct dy_dx, and aa_x is None. Thank you.",https://stackoverflow.com/questions/60778828,13096010,Documentation Replication on Other Examples
38641887,How to save a trained tensorflow model for later use for application?,"I am a bit of a beginner with tensorflow so please excuse if this is a stupid question and the answer is obvious. I have created a Tensorflow graph where starting with placeholders for X and y I have optimized some tensors which represent my model. Part of the graph is something where a vector of predictions can be calculated, e.g. for linear regression something like After training has been completed I have acceptable values for w and d and now I want to save my model for later. Then, in a different python session I want to restore the model so that I can again run for some different data and get back the y-values. I want this to work in a way where the graph for calculating the y-values from the placeholders is also stored and restored - as long as the placeholders get fed the correct data, this should work transparently without the user (the one who applies the model) needing to know what the graph looks like). As far as I understand tf.train.Saver().save(..) only saves the variables but I also want to save the graph. I think that tf.train.export_meta_graph could be relevant here but I do not understand how to use it correctly, the documentation is a bit cryptic to me and the examples do not even use export_meta_graph anywhere.",https://stackoverflow.com/questions/38641887,1382437,Inadequate Examples
38958662,"Tensorflow: What are the ""output_node_names"" for freeze_graph.py in the model_with_buckets model?","I trained a tf.nn.seq2seq.model_with_buckets with seq2seq = tf.nn.seq2seq.embedding_attention_seq2seq very similar to the example in the Tensorflow Tutorial. Now I would like to freeze the graph using freeze_graph.py. How can I find the ""output_node_names"" in my model?",https://stackoverflow.com/questions/38958662,6718251,Inadequate Examples
47644412,TensorFlow Dataset API Parsing Error,"I'm using the TensorFlow Dataset API to parse a CSV file and run a logistic regression. I'm following the example from the TF documentation here. The following code snippet shows how I am setting up the model: When calling lr.train(input_fn = lambda: input_fn(data_path, 1, 100)) (note: batch size is 100) I'm getting the error So I'm assuming this means one of the tf.feature_column.numeric_column calls is getting a scalar value which it doesn't like. However, I cannot figure out why this is the case. I've set batch_size to a positive integer and according to the documentation the shape of the NDarray resulting from tf.feature_column.numeric_column should be 1Xbatch_size by default. Can anyone explain why TensorFlow is returning this error? I'm sure this question has a simple answer that will make me feel stupid for not figuring it out, but after spending some time on this I'm still stumped.",https://stackoverflow.com/questions/47644412,8152457,Documentation Ambiguity
57717004,Tensorflow: Modern way to load large data,"I want to train a convolutional neural network (using tf.keras from Tensorflow version 1.13) using numpy arrays as input data. The training data (which I currently store in a single &gt;30GB '.npz' file) does not fit in RAM all at once. What is the best way to save and load large data-sets into a neural network for training? Since I didn't manage to find a good answer to this (surely ubiquitous?) problem, I'm hoping to hear one here. Thank you very much in advance for any help! Similar questions seem to have been asked many times (e.g. training-classifier-from-tfrecords-in-tensorflow, tensorflow-synchronize-readings-from-tfrecord, how-to-load-data-parallelly-in-tensorflow) but are several years old and usually contain no conclusive answer. My current understanding is that using TFRecord files is a good way to approach this problem. The most promising tutorial I found so far explaining how to use TFRecord files with keras is medium.com. Other helpful sources were machinelearninguru.com and medium.com_source2 and sources therin. The official tensorflow documentation and tutorials (on tf.data.Dataset, Importing Data, tf_records etc.) did not help me. In particular, several of the examples given there didn't work for me even without modifications. I'm assuming TFRecords are a good way to solve my problem but I'm having a hard time using them. Here is an example I made based on the tutorial medium.com. I stripped down the code as much as I could. The code creates a TFRecord file and starts fitting, then just gets stuck with no output or error messages. I don't know what the problem is or how I could try to fix it.",https://stackoverflow.com/questions/57717004,9988487,Documentation Replication on Other Examples
57921463,How tf.data.experimental.group_by_window() operates in Tensorflow 2.0,I am trying to understand the tf.data.experimental.group_by_window() method in Tensorflow 2 but I have some difficulties. For a reproducible example I use the one presented in the documentation:,https://stackoverflow.com/questions/57921463,8270077,Documentation Ambiguity
58112355,"What, exactly, is eager execution from a programming point of view?","I am trying to understand eager execution. Pages returned by Google describe what it does for you, and I'm ok with that. I am trying to understand it from the point of view of program code. Here is an example from this article. The article says this statement does something different depending on whether you are in eager mode or not. Without eager mode, print(a) gives: With eager mode, print(a) gives: Please could someone explain what these two return values are. If they are two different object types, a Tensor and a tf.Tensor, what is the difference between these objects? I have searched the TensorFlow documentation and can't see anything that addresses this distinction. Any pointers gratefully received. Thanks, Julian",https://stackoverflow.com/questions/58112355,6691564,Inadequate Examples
39450992,Tensorflow slim how to specify batch size during training,"I'm trying to use slim interface to create and train a convolutional neural network, but I couldn't figure out how to specify the batch size for training. During the training my net crashes because of ""Out of Memory"" on my graphic card. So I think that should be a way to handle this condition... Do I have to split the data and the labels in batches and then explicitly loop or the slim.learning.train is taking care of it? In the code I paste train_data are all the data in my training set (numpy array)..and the model definition is not included here I had a quick loop to the sources but no luck so far... Any hints suggestions? Edit: I re-read the documentation...and I found this example But It's not clear at all how to feed image and label to be passed to tf.train.batch... as MyPascalVocDataLoader function is not specified... In my case my data set are loaded from a sqlite database and I have training data and labels as numpy array....still confused. Of course I tried to pass my numpy arrays (converted to constant tensor) to the tf.train.batch like this But seems not the right path to follow... it seems that the train.batch wants only one element from my data set...(how to pass this? it does not make sense to me to pass only train_data[0] and train_labels[0])",https://stackoverflow.com/questions/39450992,6695432,Documentation Ambiguity
69509388,TF BERT input packer on more than two inputs,"Some of the TensorFlow examples using BERT models show a use of the BERT preprocessor to ""pack"" inputs. E.g. in this example, text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok], tf.constant(20)) The documentation implies that this works equally well with more than two input sentences, such that (I would expect) one can do something like: text_preprocessed = bert_preprocess.bert_pack_inputs([tok, tok, tok], tf.constant(20)) However, so doing causes the error at the bottom[1] of this post. I get that there isn't a matching signature; if I read this correctly (and I may not!), there's a signature for a single input and one for two. But what's the recommended way to pack more than two sentences into input suitable for a classification task, as suggested in the above colab? 1.",https://stackoverflow.com/questions/69509388,211714,Inadequate Examples
51509549,Feature importance from tf.estimator.BoostedTreeRegression,"I am trying to extracted feature importance from a model built in python using tf.estimator.BoostedTreeRegressor. It looks like a standard way to achieve it is by iterating over all trees in the forest and from the importance of each tree's coefficients to calculate some statistics. Example in sklearn, xgboost. I have not found how to address this issue in tensorflow.",https://stackoverflow.com/questions/51509549,1090562,Documentation Replication on Other Examples
57570385,"How to generate custom mini-batches using Tensorflow 2.0, such as those in the paper ""In defense of the triplet loss""?","I want to implement a custom mini-batch generator in Tensorflow 2.0 using tf.data.Dataset API. Concretely, I have image data, 100 classes with ~200 examples each. For each mini-batch, I want to randomly sample P classes, and K images from each class, for a total of P*K examples in a mini-batch (as described in the paper In Defense of the Triplet Loss for Person Re-Identification]). I've been searching through documentation for tf.data.Dataset, but can't seem to find the right method. I've looked into the from_generator method, but it doesn't seem suitable for this, since it generates a whole dataset from scratch as I understood. It seems to me that one way to do it would be to make a new class similar to BatchDataset which can be found in tf.data.Dataset source code, where I would somehow implement the logic, but I'm hoping for an easier solution to be honest.",https://stackoverflow.com/questions/57570385,4301911,Requesting (Additional) Documentation/Examples
57719398,Unable to save model with tensorflow 2.0.0 beta1,"I have tried all the options described in the documentation but none of them allowed me to save my model in tensorflow 2.0.0 beta1. I've also tried to upgrade to the (also unstable) TF2-RC but that ruined even the code I had working in beta so I quickly rolled back for now to beta. See a minimal reproduction code below. What I have tried: 3. And this is where I am stuck now because it gives me no reasonable hint whatsoever. That's because I am NOT calling the save() function from a @tf.function, I'm already calling it from the outermost scope possible. In fact, I have no @tf.function at all in this minimal reproduction script below and still getting the same error. So I really have no idea how to save my model, I've tried every options and they all throw errors and provide no hints. The minimal reproduction example below works fine if you set save_model=False and it reproduces the error when save_model=True. It may seem unnecessary in this simplified auto-encoder code example to use a subclassed model but I have lots of custom functions added to it in my original VAE code that I need it for. Code:",https://stackoverflow.com/questions/57719398,4515762,Documentation Replicability
57970717,Using pretrained convolutional network as a GAN discriminator,"I've pulled some code from TF2.0 documentation to generate images from a custom dataset. The code is here Since the documentation uses Keras i figured i might change the discriminator network to a pretrained network e.g InceptionV3, and only train the top layers. I've found this code (Fine-tune InceptionV3 on a new set of classes). I cant seem to figure out how to replace the the one with the other. I understand that im trying to replace Sequential mode with the Functional API. But i guess they are somehow interconnected. However, im not a frequent Keras user. My questions is: How do i replace a custom CNN in Sequential mode with a pretrained one from the Functional API to use as a discriminator? EDIT: I would be happy if anyone has examples of doing it with the GANEstimator instead as im more used to TF. Use the generator to generate a random image The current discriminator and helpers (Outputs tf.Tensor([[-0.0003378]], shape=(1, 1), dtype=float32)) The desired discriminator All imports EDIT: This was the discriminator i ended up with! Thanks to @pandrey",https://stackoverflow.com/questions/57970717,11825110,Requesting (Additional) Documentation/Examples
72850120,"Keras - Specifying from_logits=False when using tf.keras.layers.Dense(1,activation='sigmoid')(x)","I am working on a binary classification problem, using transfer learning and image inputs and have a question regarding the I have been working through using the correct activation layers (e.g. Softmax or Sigmoid - sigmoid for binary softmax for multiclass) and noticed when I specify 'sigmoid' as part of the Dense() output layer, I no longer need to specify from_logits=True during model.compile(). This means when I am obtaining predictions, I don't use the tf.nn.sigmoid() function and instead simply check if the value is greater than 0.5, then 1, else 0. Is this correct? Here is my code: And then when I obtain predictions, I have the following: My intuition is that as I am specifying the sigmoid activation function during the Dense layer build, I no longer work with 'logits' and therefore do not need to apply the sigmoid function later on. In the documentation, I've seen both examples used but it's quite sparse on information when working with model.predict(), would appreciate any guidance.",https://stackoverflow.com/questions/72850120,6419012,Documentation Replicability
67563475,How to convert a tensorflow model and load as tfds,I need help convertaing my dataset from how I usually make it using tf.keras.preprocessing.image_dataset_from_directory To be used to replace this in an example How can I do so? I am unable to find related documentation so if you can link that it would be amazing. Thanks This is how the dataset is used in the example,https://stackoverflow.com/questions/67563475,15760012,Documentation Ambiguity
51392594,"`ValueError: Can not squeeze dim[1], expected a dimension of 1, got 10 for 'sparse_softmax_cross_entropy_loss' [Tensorflow]","I am rather new to Tensorflow, and has been trying to pick up the basics by reading through the guides and documentation on tensorflow.org I have learnt the basics of how to use the tf.data and tf.estimator APIs and is trying to get them to work together on a basic image model for MNIST. I am currently following these guides: https://www.tensorflow.org/tutorials/estimators/cnn https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py I've changed the original python script to be using Dataset.from_tensor_slices rather than numpy_input_fn but I am facing the error at the evaluation step. (though not at the training step) ValueError: Can not squeeze dim[1], expected a dimension of 1, got 10 for 'sparse_softmax_cross_entropy_loss/remove_squeezable_dimensions/Squeeze' (op: 'Squeeze') with input shapes: [1,10]. My code can be found in a python notebook here (only changed the input_fn): https://github.com/quanta0801/tf_scripts/blob/master/mnist/mnist_estimator_baseline.ipynb Thanks! PS: any additional links to excellent guides to using tf.data &amp; tf.estimators will be great too! Official documentation cycles between these, keras and the low level APIs which is not conducive.",https://stackoverflow.com/questions/51392594,10088125,Documentation Replication on Other Examples
61280184,Model with multiple outputs and custom loss function,"I'm trying to train a model that has multiple outputs and a custom loss function using keras, but I'm getting some error tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: iterating over ``tf.Tensor`` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function. It's hard to debug it because I'm doing model.compile and model.fit. I think it has something to do with how models are supposed to be defined when having multiple outputs, but I can't find good documentation on this. The guide specifies how to have models with multiple outputs suing the functional API, and has an example for this, but it doesn't clarify how custom loss functions should work when subclassing the Model API. My code is as follows: And where MLP is the following: I'm trying to minimize a custom loss function Finally, this is how I try to train it: Which results in the above error. Any pointers? In particular, the whole stack trace is So it's clearly related to the loss function. But the model's forward pass outputs a tuple, which I unpack in the loss function, so I don't know why is this an issue.",https://stackoverflow.com/questions/61280184,4934261,Lack of Alternative Solutions/Documentation
56229730,Trouble with zero-padding inputs for Steered Convolution Layer,"I'm using Tensorflow's new graphics library to apply a steered convolution to a series of meshes. In many cases, you will have a series of meshes that are not the same size and you must zero-pad the smaller ones. According to the documentation, the ""sizes"" argument of the graph_conv.feature_steered_convolution_layer function takes in an int tensor consisting of the number of non-padded elements of each mesh. For some reason, when this argument is set something other than ""None"", I get a warning telling me that the sparse array used in the ""neighbors"" argument is being converted to a dense matrix. This causes my program to run absurdly slowly. The issue seems to be tied to the way that it calculates gradients. If the optimizer is commented out, the error does not come up. I read about a similar problem (link below) where the solution to the problem was to use tf.dynamic_partition rather than tf.gather. However, the tf.gather functions, in this case are located within the graph_convolution library. I attempted to make some edits in my copy of the library, but to no avail. How to deal with UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape When the above code is run, I get the following warning: I am starting to think that this might have more to do with the library itself than with this piece of code. I have referenced this posed in GitHub in case it requires updates (or additional documentation) to the library. https://github.com/tensorflow/graphics/issues/13",https://stackoverflow.com/questions/56229730,11506976,Requesting (Additional) Documentation/Examples
56436701,tf.data.Dataset.window example from the documentation fails,I'm trying to use an example from the TF documentation for tf.data.Dataset.window and the example from the documentation is failing. Code derived from the documentation: Produces this error (trace removed): So iterator.get_next() is returning a VariantDataset rather than the usual tensor. TF Version: 1.13.1,https://stackoverflow.com/questions/56436701,4790871,Documentation Replication on Other Examples
56635027,Feeding array (shape with rank 1) to TensorFlow tf.case,"Following this example from the tf.case documentation: I want to do the same, but allow to use a feed_dict as input, illustrated by this snipped: So, basically I want to feed three int-arrays of equal length and receive an array of int-values containing either 17, 23, or -1. Unfortunately, there code above gives and error: I understand, that tf.case requires boolean scalar tensor input values but is there any way to achieve what I want? I also tried tf.cond without success.",https://stackoverflow.com/questions/56635027,11545782,Documentation Replicability
74060508,How to Save a Tensorflow Dataset,As the title says I'm trying to save a TensorSliceDataset object to file. Viewing tensorflow's website it seems that the tf.data.Dataset class has a save function but it is not implemented for TensorSliceDataset objects. Pickling also did not work for me. Example code returns error: AttributeError: 'TensorSliceDataset' object has no attribute 'save',https://stackoverflow.com/questions/74060508,7875444,Documentation Replicability
64119612,Join ragged character tensor,"I have a ragged tensor of characters (copy/pastable code to reproduce): I want to join the rows as strings, but I would like to do it in the graph. I can accomplish this by evaluating the tensor: But as this is the output of my network (trained in graph mode) I would like to be able to express this operation in tensorflow so that it can be evaluated in validation steps. Two things I've tried that do not work: Frustratingly, the documentation for tf.strings.join mentions ragged tensors but does not give an example of them. What am I missing? It seems there should be an obvious solution to this.",https://stackoverflow.com/questions/64119612,3015734,Inadequate Examples
61526556,Serializing a tensor and writing to tfrecord from within a graph,"I would like to write tensorflow example records to a TFRecordWriter from inside an AutoGraph generated graph. The documentation for tensorflow 2.0 states the following: However, tf.io.serialize_tensor returns a tensor of byte-string. Creating an Example proto requires a bytes list, not a tensor. How do I write a tf.train.Example to a tf record from inside a graph? Code to reproduce: and the error",https://stackoverflow.com/questions/61526556,1189782,Documentation Replication on Other Examples
44244763,TensorFlow tf.group ignoring dependencies?,"Following on from an earlier question, it seems tf.group is indeed ignoring dependencies. Here's a simple stand-alone example (I have run it on Python 2.7 with TensorFlow 1.1): Expected output: Actual output (different each time the code is run): There's nothing in the tf.group documentation to indicate why dependencies are ignored. Is there an alternative to tf.group that does consider dependencies? Switching to use tf.control_dependencies instead of tensorflow.python.ops.control_flow_ops.with_dependencies doesn't help:",https://stackoverflow.com/questions/44244763,127480,Documentation Ambiguity
44871248,Nonlinear input to output mapping (undefined range) using tensorflow,"I have an array of 1D input data (30,1). I m trying to map this to output data (30,1) (with noise). I have plotted the data and it is definitely non-linear and continuous. I want to train a neural network to reproduce this mapping. I am currently trying to complete this task using tensorflow. My problem right now is that the output data is in an undefined range (e.g. -2.74230671e+01, 1.00000000e+03, 6.34566772e+02 etc), and non-linear tensorflow activation functions seem to all between -1 and 1? https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/activation_functions_ I am rather new to tensorflow etc, so my question is, how do I approach this problem? I thought I could mean-normalize the data, but since I don't actually know the range of the output values (possibly unbounded). Is this possible using tensorflow functions or will I need to build my own? The approach I am using is below, where I tried different functions for tf.nn.relu:",https://stackoverflow.com/questions/44871248,5919010,Documentation Replication on Other Examples
64172765,Tensorflow custom gradients don't backpropagate,"I'm trying to understand custom gradients in tensorflow with tf.custom_gradient, so I was trying to reproduce a simple matrix multiplication with its gradients, where I'm only interested with the gradient with respect to the weight matrix. This gives the right gradient whenever I don't my custom gradient is not used in the backpropagation, but when I do multiple multiplications the gradient doesn't propagate back and I get None as a result for grad_w1. For example: The first snippet gives the right gradients, but the second snippet results in None.",https://stackoverflow.com/questions/64172765,12404907,Documentation Replication on Other Examples
58608838,"tf.layers.batch_normalization( training=is_train) is deprecated, what is the alternative?","this code works tf.layers.batch_normalizaton is depricated Are there any alternatives in tf.keras.layers.BatchNormalization()? I can't find a parameter for ""training"" in it",https://stackoverflow.com/questions/58608838,10342859,Documentation Ambiguity
34642595,Tensorflow Strides Argument,"I am trying to understand the strides argument in tf.nn.avg_pool, tf.nn.max_pool, tf.nn.conv2d. The documentation repeatedly says My questions are: Sadly the examples in the docs for reshape using -1 don't translate too well to this scenario.",https://stackoverflow.com/questions/34642595,3908247,Documentation Ambiguity
59131008,Most scalable way for using generators with tf.data ? tf.data guide says `from_generator` has limited scalability,"tf.data has a from_generator initializer, it doesn't seem like it's scalable. From the official guide https://www.tensorflow.org/guide/data#consuming_python_generators And in the official documentation https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator However, generators are the a fairly common method in training over very large amounts of data. So there must be some alternative best practice for this, but the official Tensorflow data guide doesn't not give any information on this.",https://stackoverflow.com/questions/59131008,3259896,Lack of Alternative Solutions/Documentation
59497372,Is there an alternative to tf.py_function() for custom Python code?,"I have started using TensorFlow 2.0 and have a little uncertainty with regard to one aspect. Suppose I have this use case: while ingesting data with the tf.data.Dataset I want to apply some specific augmentation operations upon some images. However, the external libraries that I am using require that the image is a numpy array, not a tensor. When using tf.data.Dataset.from_tensor_slices(), the flowing data needs to be of type Tensor. Concrete example: The code above does not work yielding an I have read the documentation on TensorFlow 2.0 stating that if one wants to use an arbitrary python logic, one should use tf.py_function or only TensorFlow primitives according to: How to convert ""tensor"" to ""numpy"" array in tensorflow? My question is the following: Is there another way to use arbitrary python code in a function with a custom decorator/an easier way than to use tf.py_function? To me honestly it seems that there must be a more elegant way than passing to a tf.py_function, transforming to a numpy array, perform operations A,B,C,D and then retransform to a tensor and yield the result.",https://stackoverflow.com/questions/59497372,6117017,Lack of Alternative Solutions/Documentation
59609732,Equivalent tensorflow.js function for tf.compat.v1.image.resize?,"I want to replicate the functionality of tf.compat.v1.image.resize(inputs, [height, width], align_corners=True) using tensorflow.js. What function or combination of functions in tensorflow.js would give the equivalent result as the above function?",https://stackoverflow.com/questions/59609732,4906660,Documentation Replicability
59729239,ConvLSTMCell in tensorflow 2,"After upgrade to tensorflow version 2 from 1, all modules from tf.contrib were depreciated. In order to apply attention method, I need every cell's state. Initially, what I did in tf version 1 was: Now, I am trying to conver this to code in tf version 2. However, as I mentioned above, both modules (tf.contrib and tf.compat) were depreciated. I found the alternative of tf.compat.v1.nn.dynamic_rnn which is tf.keras.layers.rnn but there's no such function that creates ConvLSTMCell. Any suggestion?",https://stackoverflow.com/questions/59729239,10106574,Documentation Completeness
66052849,Storing pickle object in Google Cloud Storage using Tensorflow.io.gfile,"I am trying to store a pickle object in a Google Cloud Storage bucket. This is a part of a machine learning pipeline [tutorial][1] provided by Google that I am following. I broke down the code to a minimal example that still throws the same error. In my actual code, the object is a class instance. When I run this, I get On the tutorial this step worked fine as it used Tensorflow 1 and tf.io.gfile.Open(), which was removed in Tensorflow 2 and replaced by the command above. Simply using open() also works, but of course that doesn't help me writing to a bucket. I also tried but it returns the same error. Please let me know know what I am doing wrong or if there is an alternative approach to store a pickled object directly to a bucket? Many thanks for your help! [1]: https://cloud.google.com/dataflow/docs/samples/molecules-walkthrough#overview",https://stackoverflow.com/questions/66052849,13127573,Documentation Replication on Other Examples
58818679,Why a model using tf.py_function can not be serialized?,According to the documentation of ty.py_function a model using it can't be serialized. Why is serialization not possible? I was looking for an explanation to why this is the case and alternatives to using tf.py_function but did not find helpful ones. In my specific case I want to use the Keras Tokenizer and its methods expect numpy arrays - so I am calling it using tf.py_function.,https://stackoverflow.com/questions/58818679,6092553,Lack of Alternative Solutions/Documentation
40451974,"Tensorflow, restore variables in a specific device","Maybe my question is a bit naive, but I really didn't find anything in the tensorflow documentation. I have a trained tensorflow model where the variables of it was placed in the GPU. Now I would like to restore this model and test it using the CPU. If I do this via 'tf.train.Saver.restore` as in the example: saver = tf.train.import_meta_graph(""/tmp/graph.meta"") saver.restore(session, ""/tmp/model.ckp"") I have the following excpetion: InvalidArgumentError: Cannot assign a device to node 'b_fc8/b_fc8/Adam_1': Could not satisfy explicit device specification '/device:GPU:0' because no devices matching that specification are registered in this process; available devices: /job:localhost/replica:0/task:0/cpu:0 How can I make restore these variables in the CPU? Thanks",https://stackoverflow.com/questions/40451974,5913101,Documentation Replication on Other Examples
41482823,What do the functions tf.squeeze and tf.nn.rnn do?,"What do the functions tf.squeeze and tf.nn.rnn do? I searched these API, but I can't find argument, examples etc. Also, what is the shape of p_inputs formed by the following code using tf.squeeze, and what is the meaning and case of using tf.nn.rnn?",https://stackoverflow.com/questions/41482823,7378465,Inadequate Examples
45229165,How can I serve the Faster RCNN with Resnet 101 model with tensorflow serving,"I am trying to serve the Faster RCNN with Resnet 101 model with tensorflow serving. I know I need to use tf.saved_model.builder.SavedModelBuilder to export the model definition as well as variables, then I need a script like inception_client.py provided by tensorflow_serving. while I am going through the examples and documentation and experimenting, I think someone may have done the same thing. So plase help if you have done the same or know how to get it done. Thanks in advance.",https://stackoverflow.com/questions/45229165,5566610,Documentation Replication on Other Examples
46534796,Properly concatenate feature maps in Tensorflow,"I am attempting to reproduce a Convolution Neural Network from a research paper using Tensorflow. There are many times in the diagram where the results of convolutions are concatenated. Currently I am using tf.concat(https://www.tensorflow.org/api_docs/python/tf/concat) along the last axis (representing channels) to concatenate these feature maps. I originally believed that I would want to concatenate along all axes, but this does not seem to be an option in tensorflow. Now I am facing the problem where the paper indicates that tensors(feature maps) of different sizes should be concatenated. tf.concat does not support concatenations of different sizes, so I am wondering if this was the correct command to use in the first place. In summary, what is the correct way to concatenate feature maps(sometimes of different sizes) in tensorflow? Thank you.",https://stackoverflow.com/questions/46534796,8015346,Documentation Replicability
65779087,How to use tf.gradients within a model and still use a custom training loop?,"I would like to make a TensorFlow model where the outputs respect a mathematical condition, namely that output 0 is a scalar function and all subsequent outputs are its partial derivatives w.r.t. the input. This is because my observations are the scalar function and its partials, and not using the partials for training would be a waste of information. For now, using simply tf.gradients works if I don't build a custom training loop, i.e. when I don't utilize eager execution. The model is built like this, and training works as expected: However, them problem now comes when I want to do online training (i.e. with an incremental dataset). In this case, I wouldn't compile my model at the very end. Instead, I write a loop as such (before calling model.compile): This however gives the following exception at prediction_Y = model(batches_X[i_batch]): As most examples, tutorials and documentation solely deal with using gradients to do training, and not within the model, I can't find any good resources how to deal with this. I tried to find how to use gradient tape, but I can't figure out how to use it in the model design phase. Any pointers would be appreciated! Versions used:",https://stackoverflow.com/questions/65779087,6848887,Inadequate Examples
42785026,tf.nn.conv2d vs tf.layers.conv2d,"Is there any advantage in using tf.nn.* over tf.layers.*? Most of the examples in the doc use tf.nn.conv2d, for instance, but it is not clear why they do so.",https://stackoverflow.com/questions/42785026,326849,Lack of Alternative Solutions/Documentation
41941940,TensorFlow: Understanding the `collections` argument in tf.summary.scalar,"I am working with TensorBoard, specifically tf.summary.scalar. In the documentation it has an arugment collections=None, which is described as: I don't understand this description, and what collections is used for. Can someone please explain this to me, and perhaps point me towards a good example use-case?",https://stackoverflow.com/questions/41941940,3747801,Documentation Ambiguity
61628845,Alternative to tf.compat.v1,"I was trying to migrate my tf1 codes into tf2. For this purpose, I changed the following functions as follows: tf.losses.softmax_cross_entropy()intotf.compat.v1.losses.softmax_cross_entropy() tf.train.MomentumOptimizer()intotf.compat.v1.train.MomentumOptimizer() tf.train.get_or_create_global_step() into tf.compat.v1.train.get_or_create_global_step() However, I want to rewrite my codes in tf2 directly without using tf.compat.v1 How is it?",https://stackoverflow.com/questions/61628845,4088201,Documentation Replication on Other Examples
44093698,How does Tensorflow Batch Normalization work?,"I'm using tensorflow batch normalization in my deep neural network successfully. I'm doing it the following way: And it works fine both for training and testing phases. However I encounter problems when I try to use the computed neural network parameters in my another project, where I need to compute all the matrix multiplications and stuff by myself. The problem is that I can't reproduce the behavior of the tf.nn.batch_normalization function: According to the formula on the page https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/nn/batch_normalization: But as we can see, Which differs from the value 0.30664611, computed by Tensorflow itself. So what am I doing wrong here and why I can't just calculate batch normalized value myself? Thanks in advance!",https://stackoverflow.com/questions/44093698,2078632,Documentation Replication on Other Examples
44415901,tensorflow using tf.train.string_input_producer,"I'm using tf.train.string_input_producer to read data from tfRecord file. I suppose it create a queue and pipeline and the data will automatically loaded and feed into my model. However, it stuck at the first batch, and show this exception: my tfrecord was made by tf.train.SequenceExample, instead of tf.train.Example, which don't have clear documentation in the official guide. here is code snapshot to reproduce my problem. (I believe my problem come from the queue initializing or sth. because it seems that the whole pipeline is hang up)",https://stackoverflow.com/questions/44415901,6861219,Lack of Alternative Solutions/Documentation
47984876,Tensorflow tf.map_fn parameters,"I'm attempting to structure my parameters so that they will work properly with tf.map_fn() but most of the example documentation only discusses arrays or tensors of the same shape as function arguments. Links include: Does tensorflow map_fn support taking more than one tensor? My specific example is this: I have some tensorflow function that expects [None, 2] and [x,y] as parameter tensor shapes. Tensor A is of shape [batch_size, x*y, 2] Tensor B is of shape [batch_size, x, y] From the tensorflow documentation: Since tensorsA and B only match in dimension 0, I cannot stack or concatenate them; I have also tried creating lambdaData as: All of the above result in varying dimension mismatch errors. I would follow the recommended use as per documentation of placing all of the data into a single tensor, but because of dimension mismatching between tensorA and tensorB I am unable to. Has anybody had any luck with tuples or lists of arguments for elems?",https://stackoverflow.com/questions/47984876,1519665,Lack of Alternative Solutions/Documentation
48021777,Tensorflow tf.concat,"Sorry for asking a totally elementary question, but I'm trying to use the tf.concat() function. Just to get going, I try to run the example code on their site: https://www.tensorflow.org/versions/r0.12/api_docs/python/array_ops/slicing_and_joining This should generate the output: [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]. However, I get the error message, saying that",https://stackoverflow.com/questions/48021777,8918908,Documentation Replication on Other Examples
50149953,Using tf.keras within Tensorflow,What is the correct way of using the tf.keras API. Can tf.layers.* be directly replaced with tf.keras.layers(Similarly activations or loss functions)? Is it necessary to import tf.keras.backend and do set_learning_phase? This doesnt seem to be explained on the official TF docs but is mentioned in this relatively old blog post.,https://stackoverflow.com/questions/50149953,3656081,Lack of Alternative Solutions/Documentation
50210594,the function of 'bounding_boxes' and 'min_object_covered' in tf.image.sample_distorted_bounding_box?,"How parameters 'bounding_boxes' and 'min_object_covered' control the generation of a single randomly distorted bounding box for an image in tf.image.sample_distorted_bounding_box? I have read the function in tensorflow api, but I still can not understand the proplem. Maybe I need a intuitive example.",https://stackoverflow.com/questions/50210594,8307005,Requesting (Additional) Documentation/Examples
50226274,how to explain the output of tf.rank in tensorflow,"I am new in tensorflow and have a question about tf.rank method. In the doc https://www.tensorflow.org/api_docs/python/tf/rank there is a simple example about the tf.rank: But when I run the code below: I get output like: Why can I get the output of ""3""?",https://stackoverflow.com/questions/50226274,4710918,Documentation Replication on Other Examples
49830843,What's the alternative for TensorFlow VocabularyProcessor?,"It says that it is deprecated and will be removed in the future. Another line says ""Please use tensorflow/transform or tf.data."". I searched the internet but I couldn't find the answer. Here is the line that gives me the warning:",https://stackoverflow.com/questions/49830843,8433372,Documentation Ambiguity
64837612,Google AI platform can't write to Cloud Storage,"Running a tensorflow-cloud job on Google AI Platform, the entrypoint of the job is the following: The job completed successfully, in the logs it prints ""hello world"". The bucket and the job are both in the same region. But I can't find the file in Cloud Storage. It is not there. I ran some other tests, where I did tf.io.gfile.listdir then wrote a new file and again tf.io.gfile.listdir, I printed the before and after, it seems that a file was added but when I open cloud storage, I can't find it there. Also was able to read files from storage. I'm not getting any permissions errors, and as the official docs say, AI Platform already has the permission to read/write to Cloud Storage. Here is my main.py file: This is the most minimal version where I can reproduce the problem.",https://stackoverflow.com/questions/64837612,4465386,Documentation Replication on Other Examples
53527368,Translating tensorflows conv2d to numpy/scipy operations?,"This is the documentation for tf.nn.conv2d: Given an input tensor of shape [batch, in_height, in_width, in_channels] and a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels], this op performs the following In other words, it takes in a tensor of n images and does convolution with out_channel filters. I am trying to translate to code that uses only numpy operations and the code is the following: The problem is that code is extremely slow. Are there any numpy or scipy functions that can replicate what tensorflows' conv2d is doing that is of similar efficiency? I have looked at https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html and it does convolution ONCE, meaning I have to pass a 2d tensor alongside a 2d kernel (it does not do multiple filters). None of the previous stackoverflow questions helped much with this. Thanks Edit: did some testing and my code is about 44000% slower than doing tf.nn.conv2d!",https://stackoverflow.com/questions/53527368,4505889,Documentation Ambiguity
71619495,Image normalization by tf.image.convert_image_dtype function,"According to documentation tf.image.convert_image_dtype ""Images that are represented using floating point values are expected to have values in the range [0,1)."" But in the keras tutorial(https://keras.io/examples/vision/cutmix/) i have seen the following preprocessing function: My question is: why did they divide by 255, when tf.image.convert_image_dtype already did that job?",https://stackoverflow.com/questions/71619495,5094589,Documentation Ambiguity
53572533,What is the second argument of TensorFlow's tf.data.filter() that I find no documentation of?,"I recently had a TypeError when using in The exact error was TypeError: lie_filter() takes 1 positional argument but 2 were given. Simply changing the function signature to lie_filter(line, x) made the error go away and the filtering appears to work as intended. However, it left me wondering what is this mysterious second argument. TensorFlow manual for tf.data.filter() only specifies one argument. There are also numerous examples by TensorFlow where filtering is done as per my attempt above. Take a look at, e.g., imports85.py. Printing the x inside lie_filter yields Tensor(""arg12:0"", shape=(), dtype=float32). What is the second argument and where can I find documentation about it? Thank you!",https://stackoverflow.com/questions/53572533,7676920,Documentation Completeness
53583456,What problem does a reinitializable iterator solve?,From the tf.data documentation: the following example was given: It is unclear what the benefit of this complexity is. Why not simply create 2 different iterators?,https://stackoverflow.com/questions/53583456,2341218,Documentation Ambiguity
47119604,"In tensorflow, does tf.summary record average values over multiple steps?","By default, RunConfig.save_summary_steps is 100 in tf.estimator.Estimator, so it saves summaries every 100 steps. At each time it saves a summary, does it just save the current summary value computed from the current step/minibatch? Or it saves the average summary values computed from the recent 100 steps/minibatches? I cannot find a clear description for this in the official documentation.",https://stackoverflow.com/questions/47119604,3295829,Lack of Alternative Solutions/Documentation
54509752,How to translate deprecated tf.train.QueueRunners tensorflow approach to importing data to new tf.data.Dataset approach,"Altough tensorflow recommends very much to not use deprecated functions that are going to be replaced by tf.data objects, there seems to be no good documentation for cleanly replacing the deprecated for the modern approach. Moreover, Tensorflow tutorials still use the deprecated functionality to treat file processing (Reading data tutorial: https://www.tensorflow.org/api_guides/python/reading_data). On the other hand, though there is good documentation for using the 'modern' approach (Importing data tutorial: https://www.tensorflow.org/guide/datasets), there still exists the old tutorials which will probably lead many, as me, to use the deprecated one first. That is why one would like to cleanly translate the deprecated to the 'modern' approach, and an example for this translation would probably be very useful. This code runs perfectly well for me, printing to console: As can be seen, it uses deprecated functions and objects as tf.train.string_input_producer() and tf.WholeFileReader(). An equivalent implementation using the 'modern' tf.data.Dataset is needed. EDIT: Found already given example for importing CSV data: Replacing Queue-based input pipelines with tf.data. I would like to be as complete as possible here, and suppose that more examples are better, so I don't feel it as a repeated question.",https://stackoverflow.com/questions/54509752,7906266,Requesting (Additional) Documentation/Examples
66367312,Possible to use tf.distribute.Strategy.mirroredstrategy on parts of the graph rather than entire train_step for GAN custom training script?,"I'm writing CTGAN code and want it to train in a distributed way. Therefore I'm using tf.distribute.Strategy.mirroredstrategy() In the tensorflow docs tutorial I'm following, it is mentioned that you should call your train_step code from a function called distribute_trainstep(), and decorate that with a tf.function. like so: This is straightforward, but decorating all within train_step in a tf.function renders all numpy code within the train_step useless. What should I do? Is there an alternative, by only wrapping functions within train_step selectively? Or will I have to replace all numpy operations with tensorflow's?",https://stackoverflow.com/questions/66367312,7528024,Documentation Replicability
54615708,Exporting a Keras model as a TF Estimator: trained model cannot be found,"I encountered the following issue when trying to export a Keras model as a TensorFlow Estimator with the purpose of serving the model. Since the same problem also popped up in an answer to this question, I will illustrate what happens on a toy example and provide my workaround solution for documentation purposes. This behaviour occurs with Tensorflow 1.12.0 and Keras 2.2.4. This happens with actual Keras as well as with tf.keras. The problem occurs when trying to export an Estimator that was created from a Keras model with tf.keras.estimator.model_to_estimator. Upon calling estimator.export_savedmodel, either a NotFoundError or a ValueError is thrown. The below code reproduces this for a toy example. Create a Keras model and save it: Next, convert the model to an estimator with tf.keras.estimator.model_to_estimator, add an input receiver function and export it in the Savedmodel format with estimator.export_savedmodel: This will throw:",https://stackoverflow.com/questions/54615708,5495381,Documentation Replicability
48815906,Implement early stopping in tf.estimator.DNNRegressor using the available training hooks,I am new to tensorflow and want to implement early stopping in tf.estimator.DNNRegressor with available training hooksTraining Hooks for the MNIST dataset. The early stopping hook will stop training if the loss does not improve for some specified number of steps. Tensorflow documentaton only provides example for Logging hooks. Can someone write a code snippet for implementing it?,https://stackoverflow.com/questions/48815906,6533039,Requesting (Additional) Documentation/Examples
50442156,Loading a model from tensorflow SavedModel onto mutliple GPUs,"Let's say someone hands me a TF SavedModel and I would like to replicate this model on the 4 GPUs I have on my machine so I can run inference in parallel on batches of data. Are there any good examples of how to do this? I can load a saved model in this way: ..but this would require that I have a handle to the session. For models that I have written myself, I would have access to the inference function and I could just call it and wrap it using with tf.device(), but in this case, I'm not sure how to extract the inference function out of a Saved Model. Should I load 4 separate sessions or is there a better way? Couldn't find much documentation on this, but apologies in advance if I missed something. Thanks!",https://stackoverflow.com/questions/50442156,3953896,Inadequate Examples
48427269,What's the efficient way to feed elements from Iterator (from tf.data.Dataset) into TensorFlow model?,"I'm using TensrFlow's new API for importing data via tf.data.Dataset and iterators. It is working fine, but I'm not sure if what I do is efficient. What I'm doing at the moment is evaluating an iterator's get_next() method, which gives me a bunch of elements like the actual image, its label, filename, etc. I then feed the image into my model using the feed_dict. I know that feed_dict is very slow, so am I losing benefits of Dataset and Iterators and having serialised dataset in TFRecords by evaluating the entries and feeding them into the graph via feed_dict? I haven't found any examples in TF's documentation which shows how one's expected to use Iterator's get_next() to feed elements into the model. Is it better to unpack get_next() and use the result directly in my graph?",https://stackoverflow.com/questions/48427269,298209,Inadequate Examples
49472402,Tensorflow tf.nn.softmax() function performs much better than hand-written softmax,"I'm writing a simple logistic regression with tensorflow. I found out that when using tf.nn.softmax, the algorithm converges much quicker, and in the end the accuracy is higher. If switched to my own implementation of softmax, the network converges slower, and the end accuracy is not as good. Here's the code: Using my softmax: Using tensorflow's softmax: From the documentation, in theory tensorflow's softmax should be exact the same as I implemented, no? EDIT: I added a seed when initializing from normal distribution, now I can reproduce the accuracy results. When setting axis value in ""My softmax"" line, only axis=0 doesn't result in error. Setting axis=1 or axis=-1 both results in this error:",https://stackoverflow.com/questions/49472402,778800,Documentation Replicability
68319579,tfa.optimizers.MultiOptimizer - TypeError: 'Not JSON Serializable:',"I'm trying to use tfa.optimizers.MultiOptimizer(). I did everything according to the docs (https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/MultiOptimizer) yet I'm getting the following error: TypeError: ('Not JSON Serializable:', &lt;tf.Tensor 'gradient_tape/model_80/dense_3/Tensordot/MatMul/MatMul:0' shape=(1, 1) dtype=float32&gt;) Below is a minimal, working example that reproduces the error, just copy and paste it. The error occurs when the first epoch is finished and the callback trys to save the model.",https://stackoverflow.com/questions/68319579,16300082,Documentation Replicability
68367159,Adding inputs to a `tf.data.generator` in tensorflow.js,"I'm trying to create a data generator, which I verified was working by itself in pure js. TFJS documentation for it is here, with two examples: https://js.tensorflow.org/api/latest/#data.generator I'd like to use a tf.data.generator as this datasets requires elaborate preprocessing. A minimal example is as follows: The error is as follows: Iterating through our datagenerator in pure javascript works: My understanding is that we are only passing dataGenerator into tf.data.generator instead of the entire class. Then, how is it possible to input variables into tf.data.generator? Thanks.",https://stackoverflow.com/questions/68367159,13610744,Documentation Ambiguity
50029121,How to use tf.layers classes instead of functions,"It seems that tf.Layer modules come in two flavours: functions and classes. I normally use the functions directly (e.g, tf.layers.dense) but I'd like to know how to use classes directly (tf.layers.Dense). I've started experimenting with the new eager execution mode in tensorflow and I think using classes are going to be useful there as well but I haven't seen good examples in the documentation. Is there any part of TF documentation that shows how these are used? I guess it would make sense to use them in a class where these layers are instantiated in the __init__ and then they're linked in the __call__ method when the inputs and dimensions are known? Are these tf.layer classes related to tf.keras.Model? Is there an equivalent wrapper class for using tf.layers? Update: for eager execution there's tfe.Network that must be inherited. There's an example here",https://stackoverflow.com/questions/50029121,298209,Lack of Alternative Solutions/Documentation
73049510,How to dynamically set pool size for AveragePooling2D layer/ How to pass external value to an sequential layer,"Trying to understand listwise documentation while trying to replicate by mixing deep model to listwise I am stuck at point where I am not able to set the pool size inside the sequential layer in an dynamic manner. For example consider below code After we create movie model lets try to test it before we can use it on proper movie data below is the test code This now works perfect and I will get an output as below Now if you notice in the code above I have hardcoded the pool_size as 1,4 ( tf.keras.layers.AveragePooling2D(pool_size=(1,4),strides=1, padding='valid',),) because the test sample I had used above only have maximum 4 words, so the vectorization will produce vector of size 4, now problem is how to I ensure the right pool size when I pass the whole dataset (movies) to the model. How can I pass such external value (pool_size) to an sequential layer from outside? The above code was run on google colab using tensorflow version 2.9.1",https://stackoverflow.com/questions/73049510,12271381,Documentation Replication on Other Examples
55411824,tf.layers.Conv1D vs tf.keras.layers.Conv1D,I was using tf.layers.conv1d found in this tutorial but then realised it's been deprecated. Then I discovered tf.layers.Conv1D and tf.keras.layers.Conv1D. I understand the later one is the keras implementation of one dimensional convolutional layer. I'm however not sure which one to use and what's the difference in terms of functionality. It would be great of someone could point out examples of using any of these two where the input data comes from csv files.,https://stackoverflow.com/questions/55411824,4426009,Requesting (Additional) Documentation/Examples
55421386,"Tensorflow/Keras, How to convert tf.feature_column into input tensors?","I have the following code to average embeddings for list of item-ids. (Embedding is trained on review_meta_id_input, and used as look up for pirors_input and for getting average embedding) I also have some feature columns such as.. (I just thought feature_column looked cool, but not many documents to look for..) I'd like to define [review_meta_id_iput, priors_input, (tensors from feature_columns)] as an input to keras Model. something like: In order to get tensors from feature columns, the closest lead I have now is from https://github.com/tensorflow/tensorflow/issues/17170 However I'm not sure what the features are in the code. There's no clear example on https://www.tensorflow.org/api_docs/python/tf/feature_column/input_layer either. How should I construct the features variable for fc_to_tensor ? Or is there a way to use keras.layers.Input and feature_column at the same time? Or is there an alternative than tf.feature_column to do the bucketing as above? then I'll just drop the feature_column for now;",https://stackoverflow.com/questions/55421386,433570,Inadequate Examples
55425811,Implementing Intersection over Union Loss Using Tensorflow,"This may be more of a Tensorflow gradient question. I have been attempting to implement Intersection over Union (IoU) as losses and have been running into some problems. To the point, here is the snippet of my code that computes the IoU: It works as predicted. However, the issue that I am having is the losses do not decrease. The model does train, though the results are less than ideal so I am wondering if I am implementing it correctly. Do I have to compute the gradients myself? I can compute the gradients for this IoU loss derived by this paper using tf.gradients(), though I am not sure how to incorporate that with the tf.train.AdamOptimizer(). Reading the documentation, I feel like compute_gradients and apply_gradients are the commands that I need to use, but I can't find any examples on how to use them. My understanding is that the Tensorflow graph should be able to come up with the gradient itself via chain rule. So is a custom gradient even necessary in this problem? If the custom gradient is not necessary then I may just have an ill-posed problem and need to adjust some hyperparameters. Note: I have tried Tensorflow's implementation of the IoU, tf.metrics.mean_iou(), but it spits out inf every time so I have abandoned that.",https://stackoverflow.com/questions/55425811,9754177,Documentation Replication on Other Examples
51997426,TensorFlow: alternate between datasets of different output shapes,"I'm trying to use tf.Dataset for a 3D image CNN where the shape of the 3D image fed into it from the training set and the validation set are different (training: (64, 64, 64), validation: (176, 176, 160)). I didn't even know this was possible, but I'm recreating this network based on a paper, and using the classic feed_dict method the network indeed works. For performance reasons (and just to learn) I'm trying to switch the network to use tf.Dataset instead. I have two datasets and iterators built like the following: TensorFlow documentation has examples regarding switching between datasets using reinitializable_iterator or feedable_iterator, but they all switch between iterators of same output shape, which is not the case here. How should I switch between training set and validation set using tf.Dataset and tf.data.Iterator in my case then?",https://stackoverflow.com/questions/51997426,10190297,Lack of Alternative Solutions/Documentation
70319286,Question about Google Colab Transformer Tutorial,"I'm trying to follow the Tensorflow Transformer tutorial here: https://github.com/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb In the tutorial, they reproduce the image of the Transformer model from the original ""Attention is All You Need"" paper. In the image the final layers of the Transformer model are a Dense layer followed by Softmax Activation. However in the code I only see something like this: self.final_layer = tf.keras.layers.Dense(target_vocab_size) where the Dense layer is defined. But I cannot find the Softmax Activation applied anywhere in the tutorial. What am I missing? Thanks in advance for your assistance.",https://stackoverflow.com/questions/70319286,11470040,Lack of Alternative Solutions/Documentation
70363340,Question about tensorflow.tile with a tensor of 5 dimensions,"I'm trying to understand the following thing from an implementation of a paper I'm currently reading: In tensorflow, if I have a tensor x of shape (4,64,5,5) ending up with something of shape (4,64,5,5,5) Reading the documentation for tf.tile, I still don't understand what is it exactly doing in this case. Am I replicating the new dimension for 5 times? And if yes, what is exactly placed in the new dimension by tensorflow? What am I exactly replicating?",https://stackoverflow.com/questions/70363340,14824108,Documentation Ambiguity
52234780,Reading from .tfrecord files using tf.data.Dataset,"I want to read the dataset generated by this code with the tf.data.Dataset api. The repo shows it was written like this: with (encoded byte-string, b'png', 32, 32, label) as parameters. So, to read the .tfrecord file, the data format would have to be: But it doesn't work. The dataset is empty after reading and generating an iterator with it raises OutOfRangeError: End of sequence. A short python script for reproduction can be found here. I'm struggling to find exact documentation or examples for this problem.",https://stackoverflow.com/questions/52234780,4443082,Documentation Replication on Other Examples
52319765,Swap a TensorFlow Dataset input pipeline with a placeholder after training,"I'm working with the new tf.data.Dataset API and I can't seem to figure out how to perform inference. Ultimately, I want to convert my model to a TensorRT graph and run it on the TX2, and all of the examples I have found assume you have a tf.placeholder for the input. Here is pseudocode for how I am training. The [...] is just meant to be a placeholder since I didn't actually run the code. Let's not debate the model, as it is just suppose to give an example: My question is how do I get this into TensorRT without having the input be a tf.placeholder? All of the example I can find use a tf.placeholder as the input. This example suggests that I can replace the iterator with a placeholder using the SavedModel class, but I cannot seem to find any documentation on how to accomplish that. Thanks! EDIT: Here is my solution thanks to the help below that will write out a UFF file that TensorRT can utilize. The biggest issues that I encountered was:",https://stackoverflow.com/questions/52319765,8759276,Lack of Alternative Solutions/Documentation
71074606,tf.TensorArray as a FIFO?,"Here I was pointed to use tf.TensorArray instead of tf.Variable or tf.queue.FIFOQueue for making FIFO contained in custom layer. Is it an effective way? Exist any alternative here? If it's the most effective method how can I replace self.queue.assign(tf.concat([self.queue[timesteps:, :], inputs], axis=0)) with methods of tf.TensorArray?",https://stackoverflow.com/questions/71074606,14986336,Documentation Replication on Other Examples
71130645,Correct axes to use dot product to evaluate the final output of a listwise learning to rank model,"I'm not being able to find the correct configuration to pass to a tf.keras.layers.Dot to make a pairwise dot product when the entries each have lists of values, like from a listwise learning to rank model. For instance, suppose: Calling tf.keras.layers.Dot(axes=??)([repeated_query_vector, document_vectors]) I want the output to be like: All examples I found in the documentation have one dimension less than my use case. What would be the correct value of axes for this call?",https://stackoverflow.com/questions/71130645,13262684,Inadequate Examples
53922040,How does tf.keras.layers.Conv2DTranspose behave with stride and padding?,"While a convolution layer in TensorFlow has a complete description https://www.tensorflow.org/api_guides/python/nn#Convolution, transposed convolution does not have one. Although tf.keras.layers.Conv2DTranspose has a reference to https://arxiv.org/pdf/1603.07285.pdf, it is not complete. Is there any documentation that describes how tf.keras.layers.Conv2DTranspose behaves?",https://stackoverflow.com/questions/53922040,8384504,Documentation Completeness
54183967,Using tf.map_fn with multiple GPUs,"I'm trying to extend my single-GPU TensorFlow code to multi-GPU. I have to work on 3 degrees of freedom and unfortunately I need to use tf.map_fn to parallelize over the 3rd one. I tried to use device placement as shown in the official documentation, but it looks like it is impossible to do it with tf.map_fn. Is there a way to run tf.map_fn on multiple GPUs? Here the error output: Here a simple code example to reproduce it:",https://stackoverflow.com/questions/54183967,6044435,Documentation Replicability
72749893,Optimizer.apply_gradients creating variables in tf.function,"I have created a neural style transfer with Eager Execution, but it does not work when I try to turn it into a tf.function. The error message says: However, no variable is being created inside the function. Here is a simplified version of the code, which is just a neural style transfer with one image (the goal is to make the generated image look exactly like the content image): The error message points to the line I have tried to change the input of optimizer.apply_gradients to zip([grad], [generated_image]), and every combination of lists and tuples I can think of, but the error still remains. I have also looked through https://www.tensorflow.org/guide/function#creating_tfvariables and https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer, but neither of them shows examples where the variable is not explicitly defined. The only conclusion that I can come to is that one of my commands (most likely optimizer.apply_gradients) creates a variable because of an issue in my earlier code. Is that correct?",https://stackoverflow.com/questions/72749893,17819542,Documentation Replication on Other Examples
53690602,Why tensorflow.contrib.framework.arg_scope not applicable to tf.keras.layers?,"I am trying to move from tf.contrib.slim to tf.keras packages. I feel comfortable with arg_scope syntax because it reduces many redundant arguments. I found the related solution with 'Alternative to arg_scope when using tf.layers' However, for keras layers it is not applicable and raises following error. This experimental code results I am considering to make a wrapper function for tf.keras.layers that is decorated with the add_arg_scope, but is it a right way of dealing with layers in tf.keras?, I am also wondering why keras does not support for arg_scope sugar syntax.",https://stackoverflow.com/questions/53690602,5011042,Documentation Replicability
38033079,Tensorflow understanding tf.train.shuffle_batch,"I have a single file of training data, about 100K rows, and I'm running a straightforward tf.train.GradientDescentOptimizer on each training step. The setup is essentially taken directly from Tensorflow's MNIST example. Code reproduced below: Given that I'm reading training data from a file, I'm using tf.train.string_input_producer and tf.decode_csv to read rows from the csv, and then tf.train.shuffle_batch to create batches that I then train on. I'm confused as to what my parameters should be for tf.train.shuffle_batch. I read Tensorflow's documentation, and yet I'm still not sure what the ""optimal"" batch_size, capacity, and min_after_dequeue values are. Can anyone help shed some light on how I go about choosing proper values for these parameters, or link me to a resource where I can learn more? Thanks-- Here's the API link: https://www.tensorflow.org/versions/r0.9/api_docs/python/io_ops.html#shuffle_batch",https://stackoverflow.com/questions/38033079,831938,Documentation Ambiguity
53787587,"AWS, Cuda, Tensorflow","When I'm running my Python code on the most powerfull AWS GPU instances (with 1 or 8 x Tesla v100 16mb aka. P3.x2large or P3.16xlarge) they are both only 2-3 times faster than my DELL XPS Geforce 1050-Ti laptop? I'm using Windows, Keras, Cuda 9, Tensorflow 1.12 and the newest Nvidia drivers. When I check the GPU load via GZU the GPU max. run at 43% load for a very short period - each time. The controller runs at max. 100%... The dataset I use is matrices in JSON format and the files are located on a Nitro drive at 10TB with MAX 64.000 IOPS. No matter if the folder contains 10TB, 1TB or 100mb...the training is still very very slow per iteration? All advises are more than welcome! UPDATE 1: From the Tensorflow docs: ""To start an input pipeline, you must define a source. For example, to construct a Dataset from some tensors in memory, you can use tf.data.Dataset.from_tensors() or tf.data.Dataset.from_tensor_slices(). Alternatively, if your input data are on disk in the recommended TFRecord format, you can construct a tf.data.TFRecordDataset."" Before I had matrices stored in JSON format (Made by Node). My TF runs in Python. I will now only save the coordinates in Node and save it in JSON format. The question is now: In Python what is the best solution to load data? Can TF use the coordinates only or do I have to make the coordinates back to matrices again or what?",https://stackoverflow.com/questions/53787587,5800182,Documentation Replication on Other Examples
75639137,TF1 to TF2 migration,"Hello I am new to tensorflow and I am working on a code that I would like to migrate from tensorflow 1 to 2. I have this line of code: As mentioned in https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder, I should use keras.Input. But even when specifying the shape, I can't have the same tensor as with compat.v1: To check the shape I use tf.shape(x1) or tf.shape(x2), but the shapes are not the same. Could anyone explain to me how to have, in TF2, the same shape as in TF1 ? Thanks and regards",https://stackoverflow.com/questions/75639137,15822972,Documentation Replicability
73179836,tensorflow.py_function fails to temporarily switch to eager execution while in graph mode,"I'm not sure if this is a Tensorflow bug or my misunderstanding about what this function is supposed to do, but I can't get tf.py_function to return an EagerTensor while in graph mode. Consequently, calling .numpy() on the output of this function fails. The issue can be reproduced using the exact example given in the official documentation (https://www.tensorflow.org/api_docs/python/tf/py_function): This generates the following error: I am running Python 3.8 and Tensorflow v2.9.1. Any help would be greatly appreciated!",https://stackoverflow.com/questions/73179836,10453038,Documentation Replicability
76012810,Unable to extract output probability array using Tensorflow for JS,"New to Javascript/Typescript + ML libs. Created a quick TS code snippet to test out the TensorFlow lib. I am stuck at a point where I am not able to extract the probability array and then choose the max from the output. In the last iteration I have here, I am using data() function but it does not compile giving this error: Even though input is of the type tf.Tensor and according to the docs here, it should work. I am definitely missing some thing here. I am tried going through other examples, but it seems like TensorFlow has a lot to offer and I did not come across anything that would be useful in my case. I feel I need help with these 3 lines here: Code snippet",https://stackoverflow.com/questions/76012810,4463330,Lack of Alternative Solutions/Documentation
73461248,Is there an alternative to tf.keras.utils.image_dataset_from_directory if images are not organized in class-folders?,"I want to train an image classification network. I have all images in one folder and a .json file with labels and a lot of meta data. I wrote a couple of functions to extract the images which correspond to the classes I want to train for, shuffle them and randomly split them into a train- and a val-list. So currently I have something like this: And now I want to convert those lists to a train- and a val-dataset to use in Tensorflow. The thing is that I can't use tf.keras.utils.image_dataset_from_directory because alle images, independent of their label, are in the same folder and it seems a bit pointless to me to move them around every time I start a new training. tf.keras.preprocessing.image.ImageDataGenerator is deprecated (https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) and now I am not sure which function to use to convert the lists into the needed dataset. This is all very new to me, so any input or hint is very welcome!",https://stackoverflow.com/questions/73461248,19399312,Documentation Replicability
74330500,Keras GradCam implementation that can process batches of images instead of a single image at a time,I'm following the GradCam example from the Keras documentation https://keras.io/examples/vision/grad_cam/ and want to modify it so that it can process a batch of images instead of only a single image at a time. I was already able to accomplish this but had to use a call to tf.map_fn which I would like to get rid of in the hopes of a performance improvement. My progress so far (whole code at Google Coolab): Is there any way to rewrite this code in such a way that it doesn't use tf.map_fn?,https://stackoverflow.com/questions/74330500,2422125,Documentation Replication on Other Examples
56939282,How do you feed a tf.data.Dataset dynamically in eager execution mode where initializable_iterator isn't available?,"What is the new approach (under eager execution) to feeding data through a dataset pipeline in a dynamic fashion, when we need to feed it sample by sample? I have a tf.data.Dataset which performs some preprocessing steps and reads data from a generator, drawing from a large dataset during training. Let's say that dataset is represented as: After training I want to produce various visualizations which require that I feed one sample at a time through the network for inference. I've now got this dataset preprocessing pipeline that I need to feed my raw sample through to be sized and shaped appropriately for the network input. This seems like a use case for the initializable iterator: This doesn't work with eager execution, you can't use the placeholder. The documentation examples all seem to assume a static input such as in the first example here. The only way I can think of doing this is with a queue and tf.data.Dataset.from_generator(...) which reads from the queue that I push to before predicting on the data. But this feels both hacky, and appears prone to deadlocks that I've yet to solve. TF 1.14.0",https://stackoverflow.com/questions/56939282,4790871,Documentation Replication on Other Examples
57175343,Multiple inputs of keras model with tf.data.Dataset.from_generator in Tensorflow 2,"I am trying to implement a model in keras that will have multiple inputs: To feed that model, I want to write a generator to use with tf.data.Dataset.from_generator. From the docs of from_generator, its not clear to me how I should provide its parameters output_types, output_shapes. Can anyone help me with this?",https://stackoverflow.com/questions/57175343,1113765,Inadequate Examples
57481282,How to use multiple GPU for a DCGAN using Tensorflow 2.0 - RuntimeError: Replica-local variables may only be assigned in a replica context,"I would like to develop a DCGAN with resolution of 256x256. To do so I need to use multiple GPU since only one it is not enough and it will probably take too much time. I followed the procedure explained in the documentation at this link https://www.tensorflow.org/beta/guide/distribute_strategy At the top of the script I used strategy = tf.distribute.MirroredStrategy() Then inside the Generator, Discriminator, and Loss functions I used with strategy.scope(): The error I get is: RuntimeError: Replica-local variables may only be assigned in a replica context. The error is the following",https://stackoverflow.com/questions/57481282,10763156,Documentation Replicability
35958139,TensorFlow: how can I sum a list of tf.Variables?,"I've got a 3D array that is of tf.Variable type. tf.reduce_sum only works on individual tensors. I've tried doing: ...but tf.reduce_sum() expects a tensor and not a list. Can I convert it to a tensor somehow, or is there another, better way to do it? I haven't found anything in the API.",https://stackoverflow.com/questions/35958139,4829166,Lack of Alternative Solutions/Documentation
55904359,TypeError computing gradients with GradientTape.gradient,"Hello, I'm currently trying to compute gradients in Tensorflow 1.13.1 and using the GradientTape class as explained in the official documentation , but I am getting a TypeError: Fetch argument None has invalid type &lt;class 'NoneType'&gt;. Below, I will include two simple cases where I get this error, using only out-of-the-box Tensorflow function, the first one being the simpler minimal working example, and the second one that I actually need to solve/get a work-around. For completeness, I am using Python 3.6.8. In this code, f1 and f2 are computed in different ways, but give the same array. However, when trying to compute the gradients associated with them, the first line one gives the following error, whereas the second line works flawlessly. I report below the stack trace of the error Please note that I also tried computing only one gradient at a time, i.e with persistent=False, and got the same results. Below, I will include also the minimal working example to reproduce the same error I got, but trying to resolve the problem I am actually working on. In this code, I'm using a RNN to compute an output w.r.t some inputs, and I need to compute the jacobian of this output w.r.t the inputs. Regarding the stack trace, I get the same error but with less lines (there are one for_fetch, &lt;listcomp&gt; and __init less in this stack trace). For completeness, I still include it below I feel like there is a bug with some Tensorflow function that gets me the error, however I am not sure. At the end, what interest me is getting a tensor containing the jacobian of the output of my network w.r.t to the inputs. How can I achieve that using other tools, or correcting my code ? EDIT: Ok, so I took into account the comments by danyfang, and tried to look into the issue raised on Github he quoted about tf.gradients returning None instead of 0 due to some implementation design in low-level Tensorflow. Therefore, I tried to create a simple case where I am sure that gradient are different from 0, by computing tf.matmul(tf.transpose(x), x). I am posting below a MWE. This shows (at least in my opinion) that the error arises not because of the behavior reported by this issue, but another thing due to lower level implementation.",https://stackoverflow.com/questions/55904359,9092979,Documentation Replication on Other Examples
36620995,How to use L2 pooling in Tensorflow?,"I am trying to implement one CNN architecture that uses L2 pooling. The reference paper particularly argues that L2 pooling was better than max pooling, so I would like to try L2 pooling after the activation function. However, Tensorflow seems to provide only tf.nn.avg_pool / tf.nn.max_pooling / tf.nn.max_pool_with_argmax. Is there a way to implement L2 pooling in Tensorflow? Will this be equivalent? Will this work well in terms of backpropagation?",https://stackoverflow.com/questions/36620995,3251207,Documentation Replicability
34877523,"In TensorFlow, what is tf.identity used for?","I've seen tf.identity used in a few places, such as the official CIFAR-10 tutorial and the batch-normalization implementation on stackoverflow, but I don't see why it's necessary. What's it used for? Can anyone give a use case or two? One proposed answer is that it can be used for transfer between the CPU and GPU. This is not clear to me. Extension to the question, based on this: loss = tower_loss(scope) is under the GPU block, which suggests to me that all operators defined in tower_loss are mapped to the GPU. Then, at the end of tower_loss, we see total_loss = tf.identity(total_loss) before it's returned. Why? What would be the flaw with not using tf.identity here?",https://stackoverflow.com/questions/34877523,1441121,Lack of Alternative Solutions/Documentation
51499154,"Mnist with Tensorflow Premade Estimator, input dimension mismatch on evaluate","I am rather new to Tensorflow, and has been trying to pick up the basics by reading through the guides and documentation on tensorflow.org I have learnt the basics of how to use the tf.data and tf.estimator APIs and is trying to get them to work together on a basic classification model for MNIST. I am using this script to load MNIST: https://github.com/tensorflow/models/blob/master/official/mnist/dataset.py I made modifications to the dataset function to return a feature dictionary rather than vector: My MNIST classifier script using the premade estimator in tf is as follows: The classifier doesn't crash on training, but on evaluate, I faced the following traceback: What could be causing the issue here? I have tried printing the output shapes and types of both train and test datasets, and they are exactly the same. I have also tried viewing the model on tensorboard, and only the projector tab is available, no scalars or graphs tab. Thanks! PS: Any links to TF tutorials using the Datasets and Estimators APIs will be great too.",https://stackoverflow.com/questions/51499154,10088125,Documentation Replication on Other Examples
76244268,Tensorflow: Build new model from input and middle layers of another model,"I'm trying to build new_model from another model layers for class activation mapping purposes. And with this code i get the following error: Already tried functional model API, providing Input() layer inside vgg_sequential() with the same error that my Input layer is disconected from the rest of my model. Beside this when using tf.keras.applications.efficientnet_v2 that provides input layers for rescaling and resizing images i don't have any problem. Any help, information, tips or links to docs that getas me to a solution will be very much appreciated. Thanks in advance.",https://stackoverflow.com/questions/76244268,2103321,Inadequate Examples
76447508,How to retrain a model that was saved using the tf.saved_model.save() function in Tensorflow,"I am building a Neural Machine Translator for English to Konkani (a local language) language using the Transformer architecture proposed by (Vaswani et, al. 2017). I am following the tutorial code from https://www.tensorflow.org/text/tutorials/transformer. I have trained the model and used the tf.saved_model.save() method to save the model files locally. I now want to retrain that saved model on a new dataset that I have gathered recently, but I've realised that after loading the model using the tf.saved_model.load() method, I am not able to train it again as the loaded model now lacks the necessary method model.fit() . Here is a part of the model training code: Here's the error I get when I try to retrain the model using the following code: The error: After reading the documentation I've realised that when saving the model in the above method, the model.fit() and other methods are not saved hence they are not callable. I need help in finding a way to retrain my saved model, It is not feasible for me to train a new model on a combined dataset as It will take up lot of time and I have very limited resources. I have been looking up on the web for days but couldn't find a solution. Any help in this regards will be appreciated!",https://stackoverflow.com/questions/76447508,16851318,Inadequate Examples
58802573,Pre processing keras dataset using keras tokenizer,"I am trying to do some pre processing using the keras tokenizer on data I read using the following code: Now that I have the parsed example (output of _parse_example map function) I want to do some pre-processing on the text using tf.keras.preprocessing.text.Tokenizer method texts_to_sequences. However, texts_to_sequences expects an input of python strings and I get Tensors in the parsed_example. I can work around it by using py_func to wrap my code (see 'emb': tf.py_func.. in the code below), but then I will not be able to serialize my model (according to the py_func documentation). Looking for a way to do that (or a link to some similar example)",https://stackoverflow.com/questions/58802573,6092553,Lack of Alternative Solutions/Documentation
60616507,Difference between tf.GradientTape and backprop.GradientTape,"When looking at the OptimizerV2 code in Tensorflow 1.15 I noticed that they use backprop.GradientTape to compute the gradient. I can't find any online reference to this class, only to tf.GradientTape. What is the difference between the two?",https://stackoverflow.com/questions/60616507,3936294,Lack of Alternative Solutions/Documentation
46900332,How to create custom metrics for use in Tensorflow's Estimator class?,"In Tensorflow's Creating Estimators in tf.estimator guide, the example used a metric that is already predefined in the tf.metrics module. Are there any resources that describe how to define a custom metric that can be used to evaluate an Estimator? I'd like to implement the F1 metric.",https://stackoverflow.com/questions/46900332,3775778,Documentation Replication on Other Examples
42695305,Accessing row in a 2-D tensor,"I have the following code of an incredibly simple neural network (this code is actually an adaptation for an easy question): Basically, given an input x, compare it to the weights w and choose the node BMU with the minimum differences. I have several problems with that code: 1. sometimes it works without errors sometimes it raises an exception. When it DOES NOT work, the output is this: The full stack follows: When it works bmu is wrong, it should be 1, but the slice is correct. Sometimes I get this: bmu is 1, but slice is empty. 2. When I switch to the GPU, I have an exception telling me I cannot use bmu for indexing. Starting with with tf.device(""gpu:0""):, I get this: The full stack trace follows: I cannot understand what's happening: I have an idea, but cannot find any reference in the documentation or anywhere else. May be I use the wrong keywords. Is there anyone who can help me?",https://stackoverflow.com/questions/42695305,774133,Lack of Alternative Solutions/Documentation
43367697,Batching and shuffling padded tf.train.SequenceExample,"I have some training example of a sequence-to-sequence scenario which are stored as tf.train.SequenceExample in one (or more) file(s) written TFRecordWriter. I would like to read, decode them and feed shuffled batches of them into my network. I have been struggling with the documentation and some tutorials found here and there but I could not make anything out of such stuff. I am working on a self-contained example, here below. Can anyone suggest me how to proceed? Thanks in advance! P.S. as a side request: any pointer about resources to better understand the input pipeline APIs of TensorFlow is appreciated.",https://stackoverflow.com/questions/43367697,1861627,Requesting (Additional) Documentation/Examples
54524992,Tensorflow serving trained model saved with saved_model,"I find tf.saved_model documentation not clear, is there any valuable resources how to read trained model within other session?",https://stackoverflow.com/questions/54524992,9914396,Documentation Replication on Other Examples
68217076,How to parse a tfds.features.Sequence object ? It is not compatible with tf.io.FixedLenSequenceFeature,"Recently I was trying to train a model on the Wider-Face Dataset. I found it is prebuilt into tfds (https://www.tensorflow.org/datasets/catalog/wider_face). However I am having difficulty parsing it. It's feature map is of the following form - So I passed the following nested dictionary to tf.io.parse_single_example But it gives me a value error of ValueError: Unsupported dict. Later I also learnt that Sequence does not support features which are of type tf.io.FixedLenSequenceFeature. Please let me know how can I parse this type of TFRecords. I didn't find much documentation of how to use the object detection datasets that are build into Tensorflow, so providing some links with examples will also be helpful. Thanks",https://stackoverflow.com/questions/68217076,16361344,Inadequate Examples
49201832,How to use TensorBoard and summary operations with the tf.layers module,"I have followed the TensorFlow Layers tutorial to create a CNN for MNIST digit classification using TensorFlow's tf.layers module. Now I'm trying to learn how to use TensorBoard from TensorBoard: Visualizing Learning. Perhaps this tutorial hasn't been updated recently, because it says its example code is a modification of that tutorial's and links to it, but the code is completely different: it manually defines a single-hidden-layer fully-connected network. The TensorBoard tutorial shows how to use tf.summary to attach summaries to a layer by creating operations on the layer's weights tensor, which is directly accessible because we manually defined the layer, and attaching tf.summary objects to those operations. To do this if I'm using tf.layers and its tutorial code, I believe I'd have to: Is that the best way to use TensorBoard with tf.layers, or is there a way that's more directly compatible with tf.layers and the functional interface? If so, is there an updated official TensorBoard tutorial? It would be nice if the documentation and tutorials were more unified.",https://stackoverflow.com/questions/49201832,2328207,Requesting (Additional) Documentation/Examples
70399567,How to type a `tensorflow.data.Dataset`,"I have some functions that accept instances of tensorflow.data.Dataset. I would like to type those functions so that the caller knows what is expected. However, doing data: Dataset, is not good enough, because people wouldn't know what kind of structure is produced by the object (shape of tensors, whether is tuple or dictionary, etc). There is another object, tensorflow.data.DatasetSpec, which can be used to document the objects that will be produces by the tensorflow.data.Dataset. However, I am not sure how to put those things together for a typing annotation, and I am left to describe the structure in docstring, which I do not think is ideal. I would love to see something like a generic, where you pass the type it accepts, like Dataset[Spec](like in lists we do List[int]), but I am happy to learn some other ways to achieve more descriptive typing than just data: tf.data.Dataset. Thanks a lot!",https://stackoverflow.com/questions/70399567,4833773,Lack of Alternative Solutions/Documentation
51396366,TensorFlow with keras: Where is the ReLU layer in tf version 1.8?,"Update: Found it: The class is tf.keras.layers.Activation; needs to be called with argument activation='relu'.... Trying to access tf.keras.layers.ReLU gives the error: In the docs, version master has such a layer. Version 1.8 (and 1.9) only seems to have leaky relu, PReLU, and other derivatives. Right now I'm using ThresholdedReLU with theta of 0.0, I hope this results in a standard ReLU. But there must be a simple 'ReLU' layer as well? Where can I find keras' ReLU layer in tensorflow 1.8? I want a keras layer class, i.e., not tf.keras.backend.relu. It feels as if I'm overlooking something completely obvious. I haven't used keras before, so, sorry if this is a super stupid question.",https://stackoverflow.com/questions/51396366,4726173,Documentation Replication on Other Examples
59772316,Multioutput model with custom losses containing dependencies with other outputs and other data,"I have a model with multiple outputs, the losses for each output can have dependencies with one of the other outputs, as well as some masks computed from the data. The overall loss of the model is a weighted sum over the losses. My model is subclassing tf.keras.Model and I am trying to write clean code that I can use with compile and fit. I would like the weights of the losses to be given during the compile. One way I have found addressing the loss dependencies issue (after reading some documentation and this answer) is to feed the masks type of data as input of the model and, in the implementation of call, to add the loss of each output with Model.add_loss. Can someone confirm me this? How do I get y_true from there? If this is a good solution, how do I specify that the overall model loss is a weighted sum of those losses during the compile, how can I access them? Also would it be better to use add_loss on each layer in the implementation of the model's call? Same question, how do I access them during the compile? If this was not a good solution, what is a good one? Thank you",https://stackoverflow.com/questions/59772316,7483509,Documentation Replication on Other Examples
43755609,TensorFlow Estimator : model_fn has following not expected args: ['self'],"I'm using TensorFlow (""TOKEN"") high-level API Estimators to create my neural net. But I'm using it into a class and I have to call an instance of my class to generate the model of the neural network. (Here ""TOKEN"") But I get the error : ""TOKEN"". I tried to remove the ""TOKEN"" for the args of my model but got another error TypeError: ‚Ä¶ got multiple values for keyword argument. Is there any way to use these EstimatorSpec into a class ?",https://stackoverflow.com/questions/43755609,7900457,Documentation Replicability
44415901,tensorflow using tf.train.string_input_producer,"I'm using ""TOKEN"" to read data from tfRecord file. I suppose it create a queue and pipeline and the data will automatically loaded and feed into my model. However, it stuck at the first batch, and show this exception. my tfrecord was made by ""TOKEN"", instead of ""TOKEN"", which don't have clear documentation in the official guide. here is code snapshot to reproduce my problem. (I believe my problem come from the queue initializing or sth. because it seems that the whole pipeline is hang up)",https://stackoverflow.com/questions/44415901,6861219,Documentation Replicability
45067801,tensorflow: how to calculate the zero-mean and of the rgb values and uni-variance,"I want to calculate the zero mean and univariance of an image. I have already read in a pair of images in a list as tensors with the dimensions (m, n, 3) The zero-mean is calculated by taking the mean of all red, green, blue values of all images in the list and substract the per image. For this task, can I use the moments method? if yes, which axes are correct?",https://stackoverflow.com/questions/45067801,4355878,Documentation Replication on Other Examples
46315774,Why should one use pure TensorFlow over Keras?,"Recently I was watching an introduction to TensorFlow and during that course Keras came up as an High Level API that could use either TensorFlow or Theano in the back end. I played around with both of them and did a MNIST LeNet-5 implementation with first pure TensorFlow and after that with Keras. At least on my Machine I wasn't able to see any significant boost in performance, when using TensorFlow directly. So my question now is: What are the advantages, of using TensorFlow directly over Keras? Maybe it's a matter of scale, will I see significant boost in perfomance on a larger scale project?",https://stackoverflow.com/questions/46315774,3874585,Documentation Replication on Other Examples
47748803,.h file not found when build tensorflow android demo with bazel,"error occurred(.h file not found,and different .h file every time I build) when I build tf android demo with bazel.I've tried some ways proposed in this website,but not solved my problem.please help me,very thanks! And sometimes another .h file not found enter image description here",https://stackoverflow.com/questions/47748803,7288357,Documentation Replicability
49335184,Reuse variables and model encapsulated in class,"I want to train a model in tensorflow and only define the graph and variables once. So I encapsulated that in a class as follows in this functionally non-sense minimum example. I expect the predict function to reuse the trained variables ""TOKEN"" and ""TOKEN"" but all I get is ""TOKEN"" in the return line of the ""TOKEN"" function. This sounds simple to me, but somewhere I must have a wrong assumption. The official Tensorflow tutorial just states ""TOKEN"" as one way of sharing variables. Defining variables as class variables should do that in my understanding. I have googled extensively but cannot find a simple example of tensorflow with classes like this. In my actual project I fiddled around with variable scope (""TOKEN"") which still raises me an uninitialized variables warning, which I can only fix by initializing but then the trained variables are reset, of course. Is the whole class approach like this wrong? How can I simply reuse my variables without a saver or other more complicated structure? And: Where is my understanding of tensorflow variables, graphs and so on, wrong?",https://stackoverflow.com/questions/49335184,5240546,Inadequate Examples
40409793,Tensorflow: Elementwise-inversion of multiple matrices of different shape,"I have a set of differently-shaped matrices ""TOKEN"". For efficiency purposes, I can store all of ""TOKEN"" into a single tensor of size ""TOKEN"". This works fine for doing things like batch matrix multiplication and elementwise additions. But what if I want to do elementwise divisions where the zero elements are ignored? The best version of this I've come up with is. Is this the fastest way? Does ""TOKEN"" still get accelerated well via a GPU?",https://stackoverflow.com/questions/40409793,330561,Documentation Ambiguity
40443951,Binary mask in Tensorflow,I would like to mask every other value along a particular dimension of a Tensor but don't see a good way to generate such a mask. For example Is there an easy way to generate such a mask? Ideally I would like to do something like the following. But slice assigning doesn't seem to be as simple as in Numpy.,https://stackoverflow.com/questions/40443951,7120564,Documentation Replicability
40534098,Can I apply calculated gradient in tensorflow?,"What I want to do is to simulate the back-propagation process on different machines, from one machine, I get the gradient from layer3 ""TOKEN"" as a numpy array, how am I able to get ""TOKEN"" efficiently given the gradient I received and passed to the previous layer?",https://stackoverflow.com/questions/40534098,5257450,Documentation Replication on Other Examples
40569670,Inserting data into regression network in keras,"I am currently struggling to understand how i should train my regression network using keras. I am not sure how I should pass my input data to the network. Both the input data and the output data is stored as a list of numpy arrays. Each input numpy array is a matrix which has (400 rows, x columns) Each output numpy array is a matrix which has (x number of rows, 13 columns) So input dimension is 400 and output is 13. But how do I pass each of these sets within the list to the training? Just by parsing data into gives me this error message .",https://stackoverflow.com/questions/40569670,6317366,Documentation Replicability
40819430,"Input pipeline based on tfrecord, the queue are always empty","I have a problem with tensorflow and a tfrecord based input pipeline. Each of my records contains. This is the code sample that I use to read the data from the tfrecord and create a minibatch. However when I try to use it in a training routine the performance are really bad (6/7 example for second with a relatively small net on a Titan X) and neither the cpu nor the gpu seems to be under high work load. I am using as training set 26 tfrecord file with 2500 example each (~""TOKEN""GB each) and a batch size of 32. I think that the slow performance are caused by the input queue always empty as can be seen from this graph. Can anybody spot where the problem in my input pipeline is? Or can anybody give me some guidance on why I have such bad performance?",https://stackoverflow.com/questions/40819430,5650844,Documentation Replication on Other Examples
40966850,Difference between tf.clip_by_average_norm and tf.clip_by_norm in tensorflow,"I'm not completely sure of the difference between the two gradients clipping operator ""TOKEN"" and ""TOKEN"". From the documentation, the difference seems to be that ""TOKEN"" uses ""TOKEN"" instead of ""TOKEN"". I understand what the L2-norm of a gradient is but what does ""TOKEN"" correspond too ? Documentation reference",https://stackoverflow.com/questions/40966850,4172685,Documentation Ambiguity
40977431,TensorFlow Dynamic RNN - How to take mean over all LSTM states ignoring zero vectors?,"I understand that ""TOKEN"" handles variable lengths by copying the last valid state to the end of the output vector and pads the LSTM output with zero vectors. In this case, i would like to do a mean over all LSTM states with ignoring the zero vectors. How can I do this in TensorFlow? Alternatively, how do you take a mean over a list of vectors while masking the zeros?",https://stackoverflow.com/questions/40977431,5718092,Documentation Replicability
41176347,"python , Passing array as parameter","I want to run another python file in a python file , passing array as parameter, not a string as parameter. I could pass a string like this. By the way , the reason why I could not ""TOKEN"" is that the ""TOKEN"" of tensorflow may conflict.",https://stackoverflow.com/questions/41176347,2455061,Documentation Ambiguity
41273756,TensorFlow while_loop parallelization TensorArray,"I don't exactly understand how the while_loop parallelization works. Suppose I have a TensorArray having 10 Tensors all of same shape. Now suppose the computations in the loop body for the first 5 Tensors are independent of the computations in the remaining 5 Tensors. Would TensorFlow run these two in parallel? Also if I use a Tensor instead of a TensorArray and made the updates to it using scatter_update, would it pass the gradients properly during backprop?",https://stackoverflow.com/questions/41273756,3150181,Documentation Ambiguity
41308515,Force copy of tensor when enqueuing,"first, I'm not sure if the title is very good, but it was the best I could come up with given my understanding of the situation. The background is that I'm trying to understand how queues work in tensorflow and ran into the following issue which puzzled me. I have a variable n, which I enqueue to a ""TOKEN"", and then I increment the variable. This is repeated several times, and one would expect a result similar to 0, 1, 2, ... However, when emptying the queue all values are the same. More precisely, the code is as follows. Which I expect would print. Instead I get the following result. It seems like I'm pushing some pointer to n to the queue, instead of the actual value, which is what I want. However, I don't really have any actual understanding of tensorflow internals, so maybe something else is going on? I tried changing since answers to How can I copy a variable in tensorflow and In TensorFlow, what is ""TOKEN"" used for? gives me the impression that it might help, but it does not change the result. I also tried adding a ""TOKEN""(), but again, all values are the same when dequeueing. Edit: The output above is from running the code on a computer with a single CPU, when trying to see if there was some difference between different versions of tensorflow, I noticed if I run the code on a computer with CPU and GPU I get the ""TOKEN"" result. Indeed, if I run with CUDA_VISIBLE_DEVICES=""TOKEN"" I get the result above, and with CUDA_VISIBLE_DEVICES=""TOKEN"" I get the ""TOKEN"" result.",https://stackoverflow.com/questions/41308515,6850023,Documentation Ambiguity
41900775,"TensorFlow: Value Error Shape and Rank Do Not Match: ValueError: Shape (?, 128, 128, 2) must have rank 2",I'm getting this exception when running a prediction using a deconv neural network. The rank and shape seem to be the same so I'm not sure what the issue is.,https://stackoverflow.com/questions/41900775,1628635,Documentation Replicability
41937874,how to make tensorflow placeholder to work properly,"I wrote simple code using tensorflow. I thought the code below should work. The goal of this code is simple. Just calculating simple addition. But, the interpreter says something like this; I just copied what is essential to the error. I am spending lots of hours for this simple code, but I don't see any problem. Any tip would be greatly appreciated.",https://stackoverflow.com/questions/41937874,5621202,Documentation Replication on Other Examples
42159830,How to use tf.compute_gradient_error,"I want to use ""TOKEN"" to check if all gradients are reasonable (almost equal zero). I did a small test, It seems this function accept placeholder. Compute gradient error without feed placeholder data is ok But when I start check, It shows error message lossL2 is weight l2 loss If I use parameter ""TOKEN"" to feed placeholder data, the pycharm debuger show ""TOKEN"" I think timeout is a acceptable result, because my model use almost 2 mins for each iteraction, I need to find some way to speed up Edit 02/11",https://stackoverflow.com/questions/42159830,6306884,Documentation Ambiguity
42169830,How to save two different checkpoints during training in Tensorflow,"Assume we have test/val/train splits. During training, we want to save some model checkpoints [save_1] that can be used to restart the training later. In addition, we want to save another model during training that shows the best performance on the validation sets [save_2]. After done with training, we use save_2 to report the performance on the test data. My question is that how we can have two different ""TOKEN"" during training in TensorFlow? whatever examples that I have seen, only save [save_1]. Pointer to any codes would be appreciated.",https://stackoverflow.com/questions/42169830,1214630,Documentation Replication on Other Examples
42211485,How to collect performance metrics from running google cloud ml training instances?,"I'm running a model on google cloud ml training, and it's taking about 10 hours with some naive guesses at the shapes of the machine. I'd like to optimize it a bit to cut down on running time and overall cost. What's the best way to determine if I'm using the resources effectively? I'd like cpu measurements, memory pressure, and GPU usage (whenever they are available). I suspect I'd need to either 1) log these or 2) install a monitoring agent like stack driver, and assume things like nvidia-smi are locatable, but I'm curious if any one has tried.",https://stackoverflow.com/questions/42211485,54858,Documentation Replication on Other Examples
42381548,Tensor Flow: Simple Network Surgery (dropping first layers),"Assume you have a trained network that is comprised by five layers represented as L1-&gt;L2-&gt;L3-&gt;L4-&gt;L5. Here, L1, is the input layer, comprised by a ""TOKEN"". How can you fix the output of a middle layer, like L3, to a value specified by the user and run a forward pass to see the output values of L5? In other words in this scenario we would like to treat L3 as the starting input layer and ignore L1 and L2 altogether. Finally, assume that there is no need for a backward pass: i.e., we only want to evaluate and not train further the model.",https://stackoverflow.com/questions/42381548,986040,Documentation Replication on Other Examples
42618350,Tensorflow consecutive slice_input_producer,"I am confused with how does ""TOKEN"" works, I know it returns a dequeue op, but if I apply another ""TOKEN"" on the previous dequeued tensor, it seems the second ""TOKEN"" triggered the first ones dequeue op without finishing slicing. Here is an example code. I am expecting 0,1,2,3,4,5 each time I eval ""TOKEN"". But it gives 0 and 4, the second ""TOKEN"" didn't finishing slicing the first row. How can I fix this? Many thanks in advance.",https://stackoverflow.com/questions/42618350,4405877,Documentation Ambiguity
42708109,I get a CUDA_ERROR_OUT_OF_MEMORY when using images with Estimator API r1.0,"I am trying to use the Estimator API from ""TOKEN"" to build, fit, and evaluate CNN image classifiers. My code below is based on abalone.py from the tutorial on Creating Estimators. In addition, I am importing the code from the cifar10 tutorial to provide both the model and the input feeds. The code is as follows. The error messages I get are as follows. The error messages continue to count down memory size and end with the following three lines.",https://stackoverflow.com/questions/42708109,7674512,Documentation Ambiguity
42777388,optimizer.applyGradient doesnt work in tensorflow,I am trying to change gradients in tensorflow and after that trying to update with applyGradient() function. This is my code and it doesnt work it returns error function. Can you please help me?,https://stackoverflow.com/questions/42777388,3104352,Documentation Replication on Other Examples
43057816,Tensorflow - Am I restoring the model correctly?,"I have the following code which is working (no errors). My question is just am I restoring the model correctly? Especially that I cannot see any output for the statement ""TOKEN"". So, I'm trying to know if I'm doing the following correct. EDIT 1 Would restoring this way work? EDIT 2 This is how I save my model.",https://stackoverflow.com/questions/43057816,588855,Documentation Replication on Other Examples