{"cells":[{"cell_type":"markdown","metadata":{"id":"-dFhj3JBqRY6"},"source":["![Methodology Phase 2.png](data:image/png;base64,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)"]},{"cell_type":"markdown","metadata":{"id":"hrzmUeFaqeAJ"},"source":["### **Questions Issue Types**\n","\n","1. **Documentation Replication on Other Examples**\n","\n","  - Issues related to replicating the documentation examples on other different examples.\n","  - **Prompt Task:** `Provide examples replicating the documentation on other examples along with explanation with respect to the question asked by the user.`\n","\n","2. **Documentation Ambiguity**\n","\n","  - Issues related to not understanding the content properly.\n","  - **Prompt Task:** `Provide step by step explanations and examples to understand the content of the documentation with respect to the question asked by the user.`\n","\n","3. **Documentation Completeness**\n","\n","  - Issues that mention that the documentation is incomplete or missing information.\n","  - **Prompt Task:** `Complete the documentation by adding the information with respect to the question asked by the user.`\n","\n","4. **Documentation Replicability**\n","\n","  - Issues related to replicating the documentation examples\n","  - **Prompt Task:** `Provide complete examples replicating the examples provided within the documentation with respect to the question asked by the user.`\n","\n","5. **Inadequate Examples**\n","\n","  - Issues that mention the documentation has insufficient examples.\n","  - **Prompt Task:** `Provide multiple complete examples to showcase its usability with respect to the question asked by the user.`\n","\n","6. **Lack of Alternative Solutions**\n","\n","  - Issues that mention the unavailability of alternative solutions or documentation.\n","  - **Prompt Task:** `Provide alternative solutions with respect to the question asked by the user.`\n","\n","7. **Requesting (Additional) Documentation/ Examples**\n","\n","  - Questions that request additional examples or documentations as a support.\n","  - **Prompt Task:** `Provide links to reliable additional documentation or examples with respect to the question asked by the user.`"]},{"cell_type":"markdown","metadata":{"id":"b5pHxuFkCmIu"},"source":["### Potention GPT Models Information\n","Prices are per 1,000 tokens, where 1,000 tokens is about 750 words. (**`GPT-4 Turbo` is a preview version**)\n","\n","\n","| GPT Model   | Context Window  | Training Data   | Input          | Output    |\n","| --------    | --------        | --------        | --------       | --------  |\n","| GPT-4 Turbo | 128,000 Tokens  | Up to Apr 2023  | \\$0.01/1K tokens | \\$0.03/1K tokens|\n","| GPT-4 8K    |   8,192 Tokens  | Up to Sep 2021  | \\$0.03/1K tokens | \\$0.06/1K tokens|\n","| GPT-4 32K   |  32,768 Tokens  | Up to Sep 2021  | \\$0.06/1K tokens | \\$0.12/1K tokens|\n","| GPT-3.5     |  16,385 Tokens  | Up to Sep 2021  | \\$0.0005/1K tokens | \\$0.0020/1K tokens|\n"]},{"cell_type":"markdown","metadata":{"id":"3RinGqGDPIR7"},"source":["# Code"]},{"cell_type":"markdown","metadata":{"id":"_7qTvEDnPL00"},"source":["**Import Libraries**"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"jA1mphgPoVVC"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n","\n","import os\n","import json\n","import pandas as pd\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","from setfit import SetFitModel\n","\n","# LangChain packages\n","from langchain_openai import ChatOpenAI\n","from langchain.tools import Tool\n","from langchain.agents import load_tools\n","from langchain.agents import initialize_agent\n","from langchain.vectorstores import Chroma\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.schema import Document\n","from langchain.prompts import PromptTemplate\n","from langchain.chains import LLMChain\n","from langchain import hub\n","from langchain.agents import create_openai_functions_agent\n","from langchain.agents import AgentExecutor\n","from langchain.document_loaders import DataFrameLoader, DirectoryLoader, YoutubeLoader, TextLoader\n","\n","\n","from langchain_openai import OpenAIEmbeddings\n","embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n","\n","\n","# Include custom python modules\n","import lib.helper_funcs as hf\n","import lib.global_settings as s\n","import lib.api as api\n","import lib.utils as utils\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["<Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/782057818488460', creation_time=1707424687928, experiment_id='782057818488460', last_update_time=1707927555086, lifecycle_stage='active', name='/Users/sharukat@gmail.com/DocumentCustomizer', tags={'mlflow.experiment.sourceName': '/Users/sharukat@gmail.com/DocumentCustomizer',\n"," 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n"," 'mlflow.ownerEmail': 'sharukat@gmail.com',\n"," 'mlflow.ownerId': '7145086246729944'}>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import mlflow\n","mlflow.set_tracking_uri(\"databricks\")\n","mlflow.set_experiment(\"/Users/sharukat@gmail.com/DocumentCustomizer\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Classify Stack Overflow Questions (Documentation-related)"]},{"cell_type":"markdown","metadata":{},"source":["SQL query to extract questions based on the question IDs\n","\n","```mysql\n","SELECT\n","    p.Id AS QuestionId,\n","    p.Title,\n","    p.Body,\n","    CONCAT('https://stackoverflow.com/questions/', p.Id) AS QuestionURL,\n","    p.OwnerUserId AS UserId\n","FROM Posts p\n","WHERE p.Id IN (your_list_of_question_ids_here);\n","```\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>QuestionId</th>\n","      <th>Title</th>\n","      <th>Body</th>\n","      <th>QuestionURL</th>\n","      <th>UserId</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>33617638</td>\n","      <td>TensorFlow (Mac OS X): can't determine number ...</td>\n","      <td>There must be a simple setting for Mac OS X, t...</td>\n","      <td>https://stackoverflow.com/questions/33617638</td>\n","      <td>904032.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>33624048</td>\n","      <td>Fail to run word embedding example in tensorfl...</td>\n","      <td>I am trying to run the word embedding example ...</td>\n","      <td>https://stackoverflow.com/questions/33624048</td>\n","      <td>1230772.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>33633370</td>\n","      <td>How to print the value of a Tensor object in T...</td>\n","      <td>I have been using the introductory example of ...</td>\n","      <td>https://stackoverflow.com/questions/33633370</td>\n","      <td>4993513.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>33641922</td>\n","      <td>conditional graph in tensorflow and for loop t...</td>\n","      <td>First the broad questions: My actual use case ...</td>\n","      <td>https://stackoverflow.com/questions/33641922</td>\n","      <td>3688217.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>33648167</td>\n","      <td>Why do we name variables in Tensorflow?</td>\n","      <td>In some of the places, I saw the syntax, where...</td>\n","      <td>https://stackoverflow.com/questions/33648167</td>\n","      <td>4165313.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>34542</th>\n","      <td>76444585</td>\n","      <td>Create Multivariate Normal Distribution with L...</td>\n","      <td>recently I am dealing with state space model. ...</td>\n","      <td>https://stackoverflow.com/questions/76444585</td>\n","      <td>6657003.0</td>\n","    </tr>\n","    <tr>\n","      <th>34543</th>\n","      <td>76447111</td>\n","      <td>where is the documentation of keras.engine.seq...</td>\n","      <td>I got &amp;lt;class 'keras.engine.sequential.Seque...</td>\n","      <td>https://stackoverflow.com/questions/76447111</td>\n","      <td>3646484.0</td>\n","    </tr>\n","    <tr>\n","      <th>34544</th>\n","      <td>76447508</td>\n","      <td>How to retrain a model that was saved using th...</td>\n","      <td>I am building a Neural Machine Translator for ...</td>\n","      <td>https://stackoverflow.com/questions/76447508</td>\n","      <td>16851318.0</td>\n","    </tr>\n","    <tr>\n","      <th>34545</th>\n","      <td>76447653</td>\n","      <td>The output sequence is almost uniform at each ...</td>\n","      <td>I'm using LSTM (python/tensorflow) to build a ...</td>\n","      <td>https://stackoverflow.com/questions/76447653</td>\n","      <td>11472474.0</td>\n","    </tr>\n","    <tr>\n","      <th>34546</th>\n","      <td>76449922</td>\n","      <td>Tensorflow training loss extremely low from th...</td>\n","      <td>I am working on the kaggle galaxy zoo classifi...</td>\n","      <td>https://stackoverflow.com/questions/76449922</td>\n","      <td>19575571.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>34547 rows × 5 columns</p>\n","</div>"],"text/plain":["       QuestionId                                              Title  \\\n","0        33617638  TensorFlow (Mac OS X): can't determine number ...   \n","1        33624048  Fail to run word embedding example in tensorfl...   \n","2        33633370  How to print the value of a Tensor object in T...   \n","3        33641922  conditional graph in tensorflow and for loop t...   \n","4        33648167            Why do we name variables in Tensorflow?   \n","...           ...                                                ...   \n","34542    76444585  Create Multivariate Normal Distribution with L...   \n","34543    76447111  where is the documentation of keras.engine.seq...   \n","34544    76447508  How to retrain a model that was saved using th...   \n","34545    76447653  The output sequence is almost uniform at each ...   \n","34546    76449922  Tensorflow training loss extremely low from th...   \n","\n","                                                    Body  \\\n","0      There must be a simple setting for Mac OS X, t...   \n","1      I am trying to run the word embedding example ...   \n","2      I have been using the introductory example of ...   \n","3      First the broad questions: My actual use case ...   \n","4      In some of the places, I saw the syntax, where...   \n","...                                                  ...   \n","34542  recently I am dealing with state space model. ...   \n","34543  I got &lt;class 'keras.engine.sequential.Seque...   \n","34544  I am building a Neural Machine Translator for ...   \n","34545  I'm using LSTM (python/tensorflow) to build a ...   \n","34546  I am working on the kaggle galaxy zoo classifi...   \n","\n","                                        QuestionURL      UserId  \n","0      https://stackoverflow.com/questions/33617638    904032.0  \n","1      https://stackoverflow.com/questions/33624048   1230772.0  \n","2      https://stackoverflow.com/questions/33633370   4993513.0  \n","3      https://stackoverflow.com/questions/33641922   3688217.0  \n","4      https://stackoverflow.com/questions/33648167   4165313.0  \n","...                                             ...         ...  \n","34542  https://stackoverflow.com/questions/76444585   6657003.0  \n","34543  https://stackoverflow.com/questions/76447111   3646484.0  \n","34544  https://stackoverflow.com/questions/76447508  16851318.0  \n","34545  https://stackoverflow.com/questions/76447653  11472474.0  \n","34546  https://stackoverflow.com/questions/76449922  19575571.0  \n","\n","[34547 rows x 5 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["tf_api_ques = pd.read_csv(os.path.join(s.DATA_PATH, \"tf_api_questions_v1.csv\"))\n","tf_api_ques['Body'] = tf_api_ques['Body'].apply(utils.text_preprocessor)\n","tf_api_ques"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba4a90a567f84e6887fb6459045b205a","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bfdfc06f63164c5d9120162dc141b6f6","version_major":2,"version_minor":0},"text/plain":["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"936563f099f840e39a1c4bc45075640f","version_major":2,"version_minor":0},"text/plain":["config_sentence_transformers.json:   0%|          | 0.00/164 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4e4d60011bab4b1daa7ed8ea511ff395","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/22.8k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8d928c9f272477e8255e48d075fe395","version_major":2,"version_minor":0},"text/plain":["sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"42f0d97090764d5c8dbc865a70d0c3cb","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5c600514cd7d4c61a9f423d3af39ded7","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89f0a28be4734b32af5d46f18b5965ed","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"55085c62a3864913916d4df194102241","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d99fbfeb7173460d8a07d932f8d0ed0e","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae56c3b5b7334c2997ec05fe0450b4bf","version_major":2,"version_minor":0},"text/plain":["1_Pooling/config.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4d3b0ddf7a484af9a80e977af2c9dc46","version_major":2,"version_minor":0},"text/plain":["config_setfit.json:   0%|          | 0.00/67.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d88014e8cc7046b1b006b6ad5175e35a","version_major":2,"version_minor":0},"text/plain":["model_head.pkl:   0%|          | 0.00/7.01k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["100%|██████████| 34547/34547 [1:38:30<00:00,  5.85it/s]  \n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>QuestionId</th>\n","      <th>Title</th>\n","      <th>Body</th>\n","      <th>QuestionURL</th>\n","      <th>UserId</th>\n","      <th>DocRelated</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>33617638</td>\n","      <td>TensorFlow (Mac OS X): can't determine number ...</td>\n","      <td>There must be a simple setting for Mac OS X, t...</td>\n","      <td>https://stackoverflow.com/questions/33617638</td>\n","      <td>904032.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>33624048</td>\n","      <td>Fail to run word embedding example in tensorfl...</td>\n","      <td>I am trying to run the word embedding example ...</td>\n","      <td>https://stackoverflow.com/questions/33624048</td>\n","      <td>1230772.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>33633370</td>\n","      <td>How to print the value of a Tensor object in T...</td>\n","      <td>I have been using the introductory example of ...</td>\n","      <td>https://stackoverflow.com/questions/33633370</td>\n","      <td>4993513.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>33641922</td>\n","      <td>conditional graph in tensorflow and for loop t...</td>\n","      <td>First the broad questions: My actual use case ...</td>\n","      <td>https://stackoverflow.com/questions/33641922</td>\n","      <td>3688217.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>33648167</td>\n","      <td>Why do we name variables in Tensorflow?</td>\n","      <td>In some of the places, I saw the syntax, where...</td>\n","      <td>https://stackoverflow.com/questions/33648167</td>\n","      <td>4165313.0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   QuestionId                                              Title  \\\n","0    33617638  TensorFlow (Mac OS X): can't determine number ...   \n","1    33624048  Fail to run word embedding example in tensorfl...   \n","2    33633370  How to print the value of a Tensor object in T...   \n","3    33641922  conditional graph in tensorflow and for loop t...   \n","4    33648167            Why do we name variables in Tensorflow?   \n","\n","                                                Body  \\\n","0  There must be a simple setting for Mac OS X, t...   \n","1  I am trying to run the word embedding example ...   \n","2  I have been using the introductory example of ...   \n","3  First the broad questions: My actual use case ...   \n","4  In some of the places, I saw the syntax, where...   \n","\n","                                    QuestionURL     UserId  DocRelated  \n","0  https://stackoverflow.com/questions/33617638   904032.0           0  \n","1  https://stackoverflow.com/questions/33624048  1230772.0           0  \n","2  https://stackoverflow.com/questions/33633370  4993513.0           0  \n","3  https://stackoverflow.com/questions/33641922  3688217.0           0  \n","4  https://stackoverflow.com/questions/33648167  4165313.0           0  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["doc_related = []\n","model = SetFitModel.from_pretrained(\"sharukat/so_mpnet-base_question_classifier\")\n","\n","#  Run inference\n","for index, row in tqdm(tf_api_ques.iterrows(), total=tf_api_ques.shape[0]):\n","    preds = model(row['Body'])\n","    doc_related.append(preds)\n","    \n","\n","tf_api_ques['DocRelated'] = doc_related\n","tf_api_ques.head()\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["DocRelated\n","0    32542\n","1     2005\n","Name: count, dtype: int64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["tf_api_ques.to_csv(os.path.join(s.DATA_PATH, \"tf_api_questions_v2.csv\"))\n","tf_doc_related_ques = tf_api_ques[tf_api_ques['DocRelated'] == 1]\n","tf_doc_related_ques.to_csv(os.path.join(s.DATA_PATH, \"tf_doc_api_questions_v0.csv\"), index=False)\n","\n","tf_api_ques['DocRelated'].value_counts()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 300x500 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["df = pd.read_csv(os.path.join(s.DATA_PATH, \"tf_api_questions_v2.csv\"))\n","\n","class_counts = df[\"DocRelated\"].value_counts()\n","\n","# Extract data for plotting \n","class_labels = [\"Not Doc-related\", \"Doc-related\"]\n","class_values = class_counts.to_numpy()\n","\n","\n","custom_colors = {\n","   \"Not Doc-related\": \"#FBDB65\",  # Example color for \"Not Doc-related\"\n","   \"Doc-related\": \"#7393B3\"   # Example color for \"Doc-related\"\n","}\n","plt.figure(figsize=(3, 5))  # Adjust figure size as needed\n","plt.bar(class_labels, class_values, color=[custom_colors[label] for label in class_labels])\n","plt.xticks(rotation=25, ha=\"center\", fontsize=12,)  # Rotate x-axis labels for better readability\n","plt.ylabel(\"Question Count\", fontdict={\"fontsize\":12})\n","# plt.title(\"Prevelance of TensorFlow API doc-related questions\", fontdict={\"fontsize\":12, \"fontweight\":\"bold\"})\n","plt.savefig(\"./figures/doc-related_questions.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n","# Display the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>QuestionId</th>\n","      <th>Title</th>\n","      <th>Body</th>\n","      <th>QuestionURL</th>\n","      <th>UserId</th>\n","      <th>DocRelated</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>24</th>\n","      <td>33720645</td>\n","      <td>Why is this TensorFlow implementation vastly l...</td>\n","      <td>As a toy example I'm trying to fit a function ...</td>\n","      <td>https://stackoverflow.com/questions/33720645</td>\n","      <td>1715157.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>33725159</td>\n","      <td>Understanding Variable scope example in Tensor...</td>\n","      <td>I was looking at the mechanics section for Ten...</td>\n","      <td>https://stackoverflow.com/questions/33725159</td>\n","      <td>5557105.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>33727935</td>\n","      <td>How to use stop_gradient in Tensorflow</td>\n","      <td>I'm wondering how to use stop_gradient in tens...</td>\n","      <td>https://stackoverflow.com/questions/33727935</td>\n","      <td>5565980.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>33932901</td>\n","      <td>What's the purpose of tf.app.flags in TensorFlow?</td>\n","      <td>I am reading some example codes in Tensorflow,...</td>\n","      <td>https://stackoverflow.com/questions/33932901</td>\n","      <td>5607347.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>33943992</td>\n","      <td>In Tensorflow, how do I generate a scalar summ...</td>\n","      <td>Does anyone have a minimal example of using a ...</td>\n","      <td>https://stackoverflow.com/questions/33943992</td>\n","      <td>5609344.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>34533</th>\n","      <td>76433726</td>\n","      <td>reading and decoding a .tfrecord file, and sav...</td>\n","      <td>I'm currently working on implementing a pytorc...</td>\n","      <td>https://stackoverflow.com/questions/76433726</td>\n","      <td>22042838.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>34535</th>\n","      <td>76440119</td>\n","      <td>Can't fold BatchNorm with Conv2D in Keras QAT ...</td>\n","      <td>I'm currently trying to use Keras' Quantizatio...</td>\n","      <td>https://stackoverflow.com/questions/76440119</td>\n","      <td>21807405.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>34540</th>\n","      <td>76444107</td>\n","      <td>Are 'validation_steps' used if the validation_...</td>\n","      <td>I was trying to use the Keras API to train a g...</td>\n","      <td>https://stackoverflow.com/questions/76444107</td>\n","      <td>11875606.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>34543</th>\n","      <td>76447111</td>\n","      <td>where is the documentation of keras.engine.seq...</td>\n","      <td>I got &amp;lt;class 'keras.engine.sequential.Seque...</td>\n","      <td>https://stackoverflow.com/questions/76447111</td>\n","      <td>3646484.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>34544</th>\n","      <td>76447508</td>\n","      <td>How to retrain a model that was saved using th...</td>\n","      <td>I am building a Neural Machine Translator for ...</td>\n","      <td>https://stackoverflow.com/questions/76447508</td>\n","      <td>16851318.0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2005 rows × 6 columns</p>\n","</div>"],"text/plain":["       QuestionId                                              Title  \\\n","24       33720645  Why is this TensorFlow implementation vastly l...   \n","26       33725159  Understanding Variable scope example in Tensor...   \n","28       33727935             How to use stop_gradient in Tensorflow   \n","56       33932901  What's the purpose of tf.app.flags in TensorFlow?   \n","58       33943992  In Tensorflow, how do I generate a scalar summ...   \n","...           ...                                                ...   \n","34533    76433726  reading and decoding a .tfrecord file, and sav...   \n","34535    76440119  Can't fold BatchNorm with Conv2D in Keras QAT ...   \n","34540    76444107  Are 'validation_steps' used if the validation_...   \n","34543    76447111  where is the documentation of keras.engine.seq...   \n","34544    76447508  How to retrain a model that was saved using th...   \n","\n","                                                    Body  \\\n","24     As a toy example I'm trying to fit a function ...   \n","26     I was looking at the mechanics section for Ten...   \n","28     I'm wondering how to use stop_gradient in tens...   \n","56     I am reading some example codes in Tensorflow,...   \n","58     Does anyone have a minimal example of using a ...   \n","...                                                  ...   \n","34533  I'm currently working on implementing a pytorc...   \n","34535  I'm currently trying to use Keras' Quantizatio...   \n","34540  I was trying to use the Keras API to train a g...   \n","34543  I got &lt;class 'keras.engine.sequential.Seque...   \n","34544  I am building a Neural Machine Translator for ...   \n","\n","                                        QuestionURL      UserId  DocRelated  \n","24     https://stackoverflow.com/questions/33720645   1715157.0           1  \n","26     https://stackoverflow.com/questions/33725159   5557105.0           1  \n","28     https://stackoverflow.com/questions/33727935   5565980.0           1  \n","56     https://stackoverflow.com/questions/33932901   5607347.0           1  \n","58     https://stackoverflow.com/questions/33943992   5609344.0           1  \n","...                                             ...         ...         ...  \n","34533  https://stackoverflow.com/questions/76433726  22042838.0           1  \n","34535  https://stackoverflow.com/questions/76440119  21807405.0           1  \n","34540  https://stackoverflow.com/questions/76444107  11875606.0           1  \n","34543  https://stackoverflow.com/questions/76447111   3646484.0           1  \n","34544  https://stackoverflow.com/questions/76447508  16851318.0           1  \n","\n","[2005 rows x 6 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["tf_doc_related_ques"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>QuestionId</th>\n","      <th>Title</th>\n","      <th>Body</th>\n","      <th>CleanedBody</th>\n","      <th>QuestionURL</th>\n","      <th>UserId</th>\n","      <th>DocRelated</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>33720645</td>\n","      <td>Why is this TensorFlow implementation vastly l...</td>\n","      <td>&lt;p&gt;As a toy example I'm trying to fit a functi...</td>\n","      <td>As a toy example I'm trying to fit a function ...</td>\n","      <td>https://stackoverflow.com/questions/33720645</td>\n","      <td>1715157.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>33725159</td>\n","      <td>Understanding Variable scope example in Tensor...</td>\n","      <td>&lt;p&gt;I was looking at the mechanics section for ...</td>\n","      <td>I was looking at the mechanics section for Ten...</td>\n","      <td>https://stackoverflow.com/questions/33725159</td>\n","      <td>5557105.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>33727935</td>\n","      <td>How to use stop_gradient in Tensorflow</td>\n","      <td>&lt;p&gt;I'm wondering how to use &lt;code&gt;stop_gradien...</td>\n","      <td>I'm wondering how to use stop_gradient in tens...</td>\n","      <td>https://stackoverflow.com/questions/33727935</td>\n","      <td>5565980.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>33932901</td>\n","      <td>What's the purpose of tf.app.flags in TensorFlow?</td>\n","      <td>&lt;p&gt;I am reading some example codes in Tensorfl...</td>\n","      <td>I am reading some example codes in Tensorflow,...</td>\n","      <td>https://stackoverflow.com/questions/33932901</td>\n","      <td>5607347.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>33943992</td>\n","      <td>In Tensorflow, how do I generate a scalar summ...</td>\n","      <td>&lt;p&gt;Does anyone have a minimal example of using...</td>\n","      <td>Does anyone have a minimal example of using a ...</td>\n","      <td>https://stackoverflow.com/questions/33943992</td>\n","      <td>5609344.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2000</th>\n","      <td>76433726</td>\n","      <td>reading and decoding a .tfrecord file, and sav...</td>\n","      <td>&lt;p&gt;I'm currently working on implementing a pyt...</td>\n","      <td>I'm currently working on implementing a pytorc...</td>\n","      <td>https://stackoverflow.com/questions/76433726</td>\n","      <td>22042838.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2001</th>\n","      <td>76440119</td>\n","      <td>Can't fold BatchNorm with Conv2D in Keras QAT ...</td>\n","      <td>&lt;p&gt;I'm currently trying to use Keras' Quantiza...</td>\n","      <td>I'm currently trying to use Keras' Quantizatio...</td>\n","      <td>https://stackoverflow.com/questions/76440119</td>\n","      <td>21807405.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2002</th>\n","      <td>76444107</td>\n","      <td>Are 'validation_steps' used if the validation_...</td>\n","      <td>&lt;p&gt;I was trying to use the Keras API to train ...</td>\n","      <td>I was trying to use the Keras API to train a g...</td>\n","      <td>https://stackoverflow.com/questions/76444107</td>\n","      <td>11875606.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2003</th>\n","      <td>76447111</td>\n","      <td>where is the documentation of keras.engine.seq...</td>\n","      <td>&lt;pre&gt;&lt;code&gt;import tensorflow as tf\\nmodel = tf...</td>\n","      <td>I got &amp;lt;class 'keras.engine.sequential.Seque...</td>\n","      <td>https://stackoverflow.com/questions/76447111</td>\n","      <td>3646484.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2004</th>\n","      <td>76447508</td>\n","      <td>How to retrain a model that was saved using th...</td>\n","      <td>&lt;p&gt;I am building a Neural Machine Translator f...</td>\n","      <td>I am building a Neural Machine Translator for ...</td>\n","      <td>https://stackoverflow.com/questions/76447508</td>\n","      <td>16851318.0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2005 rows × 7 columns</p>\n","</div>"],"text/plain":["      QuestionId                                              Title  \\\n","0       33720645  Why is this TensorFlow implementation vastly l...   \n","1       33725159  Understanding Variable scope example in Tensor...   \n","2       33727935             How to use stop_gradient in Tensorflow   \n","3       33932901  What's the purpose of tf.app.flags in TensorFlow?   \n","4       33943992  In Tensorflow, how do I generate a scalar summ...   \n","...          ...                                                ...   \n","2000    76433726  reading and decoding a .tfrecord file, and sav...   \n","2001    76440119  Can't fold BatchNorm with Conv2D in Keras QAT ...   \n","2002    76444107  Are 'validation_steps' used if the validation_...   \n","2003    76447111  where is the documentation of keras.engine.seq...   \n","2004    76447508  How to retrain a model that was saved using th...   \n","\n","                                                   Body  \\\n","0     <p>As a toy example I'm trying to fit a functi...   \n","1     <p>I was looking at the mechanics section for ...   \n","2     <p>I'm wondering how to use <code>stop_gradien...   \n","3     <p>I am reading some example codes in Tensorfl...   \n","4     <p>Does anyone have a minimal example of using...   \n","...                                                 ...   \n","2000  <p>I'm currently working on implementing a pyt...   \n","2001  <p>I'm currently trying to use Keras' Quantiza...   \n","2002  <p>I was trying to use the Keras API to train ...   \n","2003  <pre><code>import tensorflow as tf\\nmodel = tf...   \n","2004  <p>I am building a Neural Machine Translator f...   \n","\n","                                            CleanedBody  \\\n","0     As a toy example I'm trying to fit a function ...   \n","1     I was looking at the mechanics section for Ten...   \n","2     I'm wondering how to use stop_gradient in tens...   \n","3     I am reading some example codes in Tensorflow,...   \n","4     Does anyone have a minimal example of using a ...   \n","...                                                 ...   \n","2000  I'm currently working on implementing a pytorc...   \n","2001  I'm currently trying to use Keras' Quantizatio...   \n","2002  I was trying to use the Keras API to train a g...   \n","2003  I got &lt;class 'keras.engine.sequential.Seque...   \n","2004  I am building a Neural Machine Translator for ...   \n","\n","                                       QuestionURL      UserId  DocRelated  \n","0     https://stackoverflow.com/questions/33720645   1715157.0           1  \n","1     https://stackoverflow.com/questions/33725159   5557105.0           1  \n","2     https://stackoverflow.com/questions/33727935   5565980.0           1  \n","3     https://stackoverflow.com/questions/33932901   5607347.0           1  \n","4     https://stackoverflow.com/questions/33943992   5609344.0           1  \n","...                                            ...         ...         ...  \n","2000  https://stackoverflow.com/questions/76433726  22042838.0           1  \n","2001  https://stackoverflow.com/questions/76440119  21807405.0           1  \n","2002  https://stackoverflow.com/questions/76444107  11875606.0           1  \n","2003  https://stackoverflow.com/questions/76447111   3646484.0           1  \n","2004  https://stackoverflow.com/questions/76447508  16851318.0           1  \n","\n","[2005 rows x 7 columns]"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["# Merge the preprocessed text with raw question body\n","df1 = pd.read_csv(os.path.join(s.DATA_PATH, \"tf_doc_api_questions_v0.csv\"))\n","df1 = df1.rename(columns={'Body': 'CleanedBody'})\n","\n","df2 = pd.read_csv(os.path.join(s.DATA_PATH, \"tf_api_questions_v1.csv\"))\n","df = pd.merge(df1, df2[['QuestionId', 'Body']], on='QuestionId', how='inner')\n","\n","df = df[['QuestionId', 'Title', 'Body', 'CleanedBody', 'QuestionURL', 'UserId', 'DocRelated']]\n","df\n"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>QuestionId</th>\n","      <th>Title</th>\n","      <th>Body</th>\n","      <th>CleanedBody</th>\n","      <th>QuestionURL</th>\n","      <th>UserId</th>\n","      <th>DocRelated</th>\n","      <th>APIs_from_CleanBody</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>33720645</td>\n","      <td>Why is this TensorFlow implementation vastly l...</td>\n","      <td>&lt;p&gt;As a toy example I'm trying to fit a functi...</td>\n","      <td>As a toy example I'm trying to fit a function ...</td>\n","      <td>https://stackoverflow.com/questions/33720645</td>\n","      <td>1715157.0</td>\n","      <td>1</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>33725159</td>\n","      <td>Understanding Variable scope example in Tensor...</td>\n","      <td>&lt;p&gt;I was looking at the mechanics section for ...</td>\n","      <td>I was looking at the mechanics section for Ten...</td>\n","      <td>https://stackoverflow.com/questions/33725159</td>\n","      <td>5557105.0</td>\n","      <td>1</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>33727935</td>\n","      <td>How to use stop_gradient in Tensorflow</td>\n","      <td>&lt;p&gt;I'm wondering how to use &lt;code&gt;stop_gradien...</td>\n","      <td>I'm wondering how to use stop_gradient in tens...</td>\n","      <td>https://stackoverflow.com/questions/33727935</td>\n","      <td>5565980.0</td>\n","      <td>1</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>33932901</td>\n","      <td>What's the purpose of tf.app.flags in TensorFlow?</td>\n","      <td>&lt;p&gt;I am reading some example codes in Tensorfl...</td>\n","      <td>I am reading some example codes in Tensorflow,...</td>\n","      <td>https://stackoverflow.com/questions/33932901</td>\n","      <td>5607347.0</td>\n","      <td>1</td>\n","      <td>[tf.app.flags]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>33943992</td>\n","      <td>In Tensorflow, how do I generate a scalar summ...</td>\n","      <td>&lt;p&gt;Does anyone have a minimal example of using...</td>\n","      <td>Does anyone have a minimal example of using a ...</td>\n","      <td>https://stackoverflow.com/questions/33943992</td>\n","      <td>5609344.0</td>\n","      <td>1</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2000</th>\n","      <td>76433726</td>\n","      <td>reading and decoding a .tfrecord file, and sav...</td>\n","      <td>&lt;p&gt;I'm currently working on implementing a pyt...</td>\n","      <td>I'm currently working on implementing a pytorc...</td>\n","      <td>https://stackoverflow.com/questions/76433726</td>\n","      <td>22042838.0</td>\n","      <td>1</td>\n","      <td>[tf.data.TFRecordDataset, tf]</td>\n","    </tr>\n","    <tr>\n","      <th>2001</th>\n","      <td>76440119</td>\n","      <td>Can't fold BatchNorm with Conv2D in Keras QAT ...</td>\n","      <td>&lt;p&gt;I'm currently trying to use Keras' Quantiza...</td>\n","      <td>I'm currently trying to use Keras' Quantizatio...</td>\n","      <td>https://stackoverflow.com/questions/76440119</td>\n","      <td>21807405.0</td>\n","      <td>1</td>\n","      <td>[tf]</td>\n","    </tr>\n","    <tr>\n","      <th>2002</th>\n","      <td>76444107</td>\n","      <td>Are 'validation_steps' used if the validation_...</td>\n","      <td>&lt;p&gt;I was trying to use the Keras API to train ...</td>\n","      <td>I was trying to use the Keras API to train a g...</td>\n","      <td>https://stackoverflow.com/questions/76444107</td>\n","      <td>11875606.0</td>\n","      <td>1</td>\n","      <td>[tf.data]</td>\n","    </tr>\n","    <tr>\n","      <th>2003</th>\n","      <td>76447111</td>\n","      <td>where is the documentation of keras.engine.seq...</td>\n","      <td>&lt;pre&gt;&lt;code&gt;import tensorflow as tf\\nmodel = tf...</td>\n","      <td>I got &amp;lt;class 'keras.engine.sequential.Seque...</td>\n","      <td>https://stackoverflow.com/questions/76447111</td>\n","      <td>3646484.0</td>\n","      <td>1</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>2004</th>\n","      <td>76447508</td>\n","      <td>How to retrain a model that was saved using th...</td>\n","      <td>&lt;p&gt;I am building a Neural Machine Translator f...</td>\n","      <td>I am building a Neural Machine Translator for ...</td>\n","      <td>https://stackoverflow.com/questions/76447508</td>\n","      <td>16851318.0</td>\n","      <td>1</td>\n","      <td>[tf.saved_model.load, tf.saved_model.save]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2005 rows × 8 columns</p>\n","</div>"],"text/plain":["      QuestionId                                              Title  \\\n","0       33720645  Why is this TensorFlow implementation vastly l...   \n","1       33725159  Understanding Variable scope example in Tensor...   \n","2       33727935             How to use stop_gradient in Tensorflow   \n","3       33932901  What's the purpose of tf.app.flags in TensorFlow?   \n","4       33943992  In Tensorflow, how do I generate a scalar summ...   \n","...          ...                                                ...   \n","2000    76433726  reading and decoding a .tfrecord file, and sav...   \n","2001    76440119  Can't fold BatchNorm with Conv2D in Keras QAT ...   \n","2002    76444107  Are 'validation_steps' used if the validation_...   \n","2003    76447111  where is the documentation of keras.engine.seq...   \n","2004    76447508  How to retrain a model that was saved using th...   \n","\n","                                                   Body  \\\n","0     <p>As a toy example I'm trying to fit a functi...   \n","1     <p>I was looking at the mechanics section for ...   \n","2     <p>I'm wondering how to use <code>stop_gradien...   \n","3     <p>I am reading some example codes in Tensorfl...   \n","4     <p>Does anyone have a minimal example of using...   \n","...                                                 ...   \n","2000  <p>I'm currently working on implementing a pyt...   \n","2001  <p>I'm currently trying to use Keras' Quantiza...   \n","2002  <p>I was trying to use the Keras API to train ...   \n","2003  <pre><code>import tensorflow as tf\\nmodel = tf...   \n","2004  <p>I am building a Neural Machine Translator f...   \n","\n","                                            CleanedBody  \\\n","0     As a toy example I'm trying to fit a function ...   \n","1     I was looking at the mechanics section for Ten...   \n","2     I'm wondering how to use stop_gradient in tens...   \n","3     I am reading some example codes in Tensorflow,...   \n","4     Does anyone have a minimal example of using a ...   \n","...                                                 ...   \n","2000  I'm currently working on implementing a pytorc...   \n","2001  I'm currently trying to use Keras' Quantizatio...   \n","2002  I was trying to use the Keras API to train a g...   \n","2003  I got &lt;class 'keras.engine.sequential.Seque...   \n","2004  I am building a Neural Machine Translator for ...   \n","\n","                                       QuestionURL      UserId  DocRelated  \\\n","0     https://stackoverflow.com/questions/33720645   1715157.0           1   \n","1     https://stackoverflow.com/questions/33725159   5557105.0           1   \n","2     https://stackoverflow.com/questions/33727935   5565980.0           1   \n","3     https://stackoverflow.com/questions/33932901   5607347.0           1   \n","4     https://stackoverflow.com/questions/33943992   5609344.0           1   \n","...                                            ...         ...         ...   \n","2000  https://stackoverflow.com/questions/76433726  22042838.0           1   \n","2001  https://stackoverflow.com/questions/76440119  21807405.0           1   \n","2002  https://stackoverflow.com/questions/76444107  11875606.0           1   \n","2003  https://stackoverflow.com/questions/76447111   3646484.0           1   \n","2004  https://stackoverflow.com/questions/76447508  16851318.0           1   \n","\n","                             APIs_from_CleanBody  \n","0                                             []  \n","1                                             []  \n","2                                             []  \n","3                                 [tf.app.flags]  \n","4                                             []  \n","...                                          ...  \n","2000               [tf.data.TFRecordDataset, tf]  \n","2001                                        [tf]  \n","2002                                   [tf.data]  \n","2003                                          []  \n","2004  [tf.saved_model.load, tf.saved_model.save]  \n","\n","[2005 rows x 8 columns]"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["df['APIs_from_CleanBody'] = df['CleanedBody'].str.findall(r'\\btf(?:\\.\\w+)*').apply(set).apply(list)\n","df.to_csv(os.path.join(s.DATA_PATH, \"tf_doc_questions_with_apis_v0.csv\"), index=False)\n","df"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>QuestionId</th>\n","      <th>Title</th>\n","      <th>Body</th>\n","      <th>CleanedBody</th>\n","      <th>QuestionURL</th>\n","      <th>UserId</th>\n","      <th>DocRelated</th>\n","      <th>APIs_from_CleanBody</th>\n","      <th>Manual_Check</th>\n","      <th>Unnamed: 9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>49868782</td>\n","      <td>How to use tf.argmax</td>\n","      <td>&lt;p&gt;I want to test the function of tf.argmax(),...</td>\n","      <td>I want to test the function of tf.argmax(),but...</td>\n","      <td>https://stackoverflow.com/questions/49868782</td>\n","      <td>9639109.0</td>\n","      <td>1</td>\n","      <td>tf.argmax</td>\n","      <td>tf.argmax</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>71933464</td>\n","      <td>How to make true_fn of tf.cond skip a for loop...</td>\n","      <td>&lt;p&gt;I want to use &lt;code&gt;tf.cond&lt;/code&gt; to mimic...</td>\n","      <td>I want to use tf.cond to mimic the python if-e...</td>\n","      <td>https://stackoverflow.com/questions/71933464</td>\n","      <td>4982651.0</td>\n","      <td>1</td>\n","      <td>tf.cond</td>\n","      <td>tf.cond</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>60516977</td>\n","      <td>Difficulties in understanding higher order der...</td>\n","      <td>&lt;p&gt;Based on the example as quoted in tensorflo...</td>\n","      <td>Based on the example as quoted in tensorflow's...</td>\n","      <td>https://stackoverflow.com/questions/60516977</td>\n","      <td>4723266.0</td>\n","      <td>1</td>\n","      <td>tf.custom_gradient</td>\n","      <td>tf.custom_gradient</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>38810424</td>\n","      <td>How does one debug NaN values in TensorFlow?</td>\n","      <td>&lt;p&gt;I was running TensorFlow and I happen to ha...</td>\n","      <td>I was running TensorFlow and I happen to have ...</td>\n","      <td>https://stackoverflow.com/questions/38810424</td>\n","      <td>1601580.0</td>\n","      <td>1</td>\n","      <td>tf.add_check_numerics_ops', 'tf.Print</td>\n","      <td>tf.add_check_numerics_ops</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>41604616</td>\n","      <td>Save and load Tensorflow model</td>\n","      <td>&lt;p&gt;I want to save a Tensorflow (0.12.0) model,...</td>\n","      <td>I want to save a Tensorflow (0.12.0) model, in...</td>\n","      <td>https://stackoverflow.com/questions/41604616</td>\n","      <td>3444294.0</td>\n","      <td>1</td>\n","      <td>tf.add_to_collection', 'tf.get_collection</td>\n","      <td>tf.add_to_collection</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1999</th>\n","      <td>76225743</td>\n","      <td>TypeError: 'AutoTrackable' object is not calla...</td>\n","      <td>&lt;p&gt;In my code I wish to create a tokenizer fro...</td>\n","      <td>In my code I wish to create a tokenizer from a...</td>\n","      <td>https://stackoverflow.com/questions/76225743</td>\n","      <td>13871286.0</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2000</th>\n","      <td>76262758</td>\n","      <td>TF2.3 - More model outputs than targets</td>\n","      <td>&lt;p&gt;I am trying to write a model in which there...</td>\n","      <td>I am trying to write a model in which there ar...</td>\n","      <td>https://stackoverflow.com/questions/76262758</td>\n","      <td>588959.0</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2001</th>\n","      <td>76402835</td>\n","      <td>ValueError: tf.function only supports singleto...</td>\n","      <td>&lt;p&gt;I have the following code:&lt;/p&gt;\\n&lt;pre&gt;&lt;code&gt;...</td>\n","      <td>I have the following code: I get the error in ...</td>\n","      <td>https://stackoverflow.com/questions/76402835</td>\n","      <td>14114819.0</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2002</th>\n","      <td>76412332</td>\n","      <td>Save load and retrain a tensorflow model for m...</td>\n","      <td>&lt;p&gt;I've been trying train a model for machine ...</td>\n","      <td>I've been trying train a model for machine tra...</td>\n","      <td>https://stackoverflow.com/questions/76412332</td>\n","      <td>21164761.0</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2003</th>\n","      <td>76447111</td>\n","      <td>where is the documentation of keras.engine.seq...</td>\n","      <td>&lt;pre&gt;&lt;code&gt;import tensorflow as tf\\nmodel = tf...</td>\n","      <td>I got &amp;lt;class 'keras.engine.sequential.Seque...</td>\n","      <td>https://stackoverflow.com/questions/76447111</td>\n","      <td>3646484.0</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2004 rows × 10 columns</p>\n","</div>"],"text/plain":["      QuestionId                                              Title  \\\n","0       49868782                               How to use tf.argmax   \n","1       71933464  How to make true_fn of tf.cond skip a for loop...   \n","2       60516977  Difficulties in understanding higher order der...   \n","3       38810424       How does one debug NaN values in TensorFlow?   \n","4       41604616                     Save and load Tensorflow model   \n","...          ...                                                ...   \n","1999    76225743  TypeError: 'AutoTrackable' object is not calla...   \n","2000    76262758            TF2.3 - More model outputs than targets   \n","2001    76402835  ValueError: tf.function only supports singleto...   \n","2002    76412332  Save load and retrain a tensorflow model for m...   \n","2003    76447111  where is the documentation of keras.engine.seq...   \n","\n","                                                   Body  \\\n","0     <p>I want to test the function of tf.argmax(),...   \n","1     <p>I want to use <code>tf.cond</code> to mimic...   \n","2     <p>Based on the example as quoted in tensorflo...   \n","3     <p>I was running TensorFlow and I happen to ha...   \n","4     <p>I want to save a Tensorflow (0.12.0) model,...   \n","...                                                 ...   \n","1999  <p>In my code I wish to create a tokenizer fro...   \n","2000  <p>I am trying to write a model in which there...   \n","2001  <p>I have the following code:</p>\\n<pre><code>...   \n","2002  <p>I've been trying train a model for machine ...   \n","2003  <pre><code>import tensorflow as tf\\nmodel = tf...   \n","\n","                                            CleanedBody  \\\n","0     I want to test the function of tf.argmax(),but...   \n","1     I want to use tf.cond to mimic the python if-e...   \n","2     Based on the example as quoted in tensorflow's...   \n","3     I was running TensorFlow and I happen to have ...   \n","4     I want to save a Tensorflow (0.12.0) model, in...   \n","...                                                 ...   \n","1999  In my code I wish to create a tokenizer from a...   \n","2000  I am trying to write a model in which there ar...   \n","2001  I have the following code: I get the error in ...   \n","2002  I've been trying train a model for machine tra...   \n","2003  I got &lt;class 'keras.engine.sequential.Seque...   \n","\n","                                       QuestionURL      UserId  DocRelated  \\\n","0     https://stackoverflow.com/questions/49868782   9639109.0           1   \n","1     https://stackoverflow.com/questions/71933464   4982651.0           1   \n","2     https://stackoverflow.com/questions/60516977   4723266.0           1   \n","3     https://stackoverflow.com/questions/38810424   1601580.0           1   \n","4     https://stackoverflow.com/questions/41604616   3444294.0           1   \n","...                                            ...         ...         ...   \n","1999  https://stackoverflow.com/questions/76225743  13871286.0           1   \n","2000  https://stackoverflow.com/questions/76262758    588959.0           1   \n","2001  https://stackoverflow.com/questions/76402835  14114819.0           1   \n","2002  https://stackoverflow.com/questions/76412332  21164761.0           1   \n","2003  https://stackoverflow.com/questions/76447111   3646484.0           1   \n","\n","                            APIs_from_CleanBody               Manual_Check  \\\n","0                                     tf.argmax                  tf.argmax   \n","1                                       tf.cond                    tf.cond   \n","2                            tf.custom_gradient         tf.custom_gradient   \n","3         tf.add_check_numerics_ops', 'tf.Print  tf.add_check_numerics_ops   \n","4     tf.add_to_collection', 'tf.get_collection       tf.add_to_collection   \n","...                                         ...                        ...   \n","1999                                        NaN                        NaN   \n","2000                                        NaN                        NaN   \n","2001                                        NaN                        NaN   \n","2002                                        NaN                        NaN   \n","2003                                        NaN                        NaN   \n","\n","     Unnamed: 9  \n","0           NaN  \n","1           NaN  \n","2           NaN  \n","3           NaN  \n","4           NaN  \n","...         ...  \n","1999        NaN  \n","2000        NaN  \n","2001        NaN  \n","2002        NaN  \n","2003        NaN  \n","\n","[2004 rows x 10 columns]"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["xxx = pd.read_csv(os.path.join(s.DATA_PATH, \"tf_doc_questions_with_apis_v1.csv\"))\n","xxx.to_csv(os.path.join(s.DATA_PATH, \"tf_doc_questions_with_apis_v0.csv\"), index=False)\n","xxx"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>QuestionId</th>\n","      <th>Title</th>\n","      <th>Body</th>\n","      <th>DocRelated</th>\n","      <th>Issue Type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>34589335</td>\n","      <td>How does the distorted_inputs() function in th...</td>\n","      <td>I was going through the CIFAR-10 example at Te...</td>\n","      <td>1</td>\n","      <td>Documentation Replication on Other Examples</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>34619177</td>\n","      <td>What does tf.nn.conv2d do in tensorflow?</td>\n","      <td>I was looking at the docs of tensorflow about ...</td>\n","      <td>1</td>\n","      <td>Documentation Replication on Other Examples</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>34642595</td>\n","      <td>Tensorflow Strides Argument</td>\n","      <td>I am trying to understand the strides argument...</td>\n","      <td>1</td>\n","      <td>Documentation Ambiguity</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>34931121</td>\n","      <td>Can cond support TF ops with side effects?</td>\n","      <td>The (source code) documentation for tf.cond is...</td>\n","      <td>1</td>\n","      <td>Documentation Replicability</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>35689547</td>\n","      <td>How to process single training file in parallel</td>\n","      <td>I have a file train.csv that contains paths to...</td>\n","      <td>1</td>\n","      <td>Documentation Replication on Other Examples</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>537</th>\n","      <td>76380927</td>\n","      <td>Tensorflow decode image</td>\n","      <td>I am a beginner in tensorflow and I am trainin...</td>\n","      <td>1</td>\n","      <td>Requesting (Additional) Documentation/Examples</td>\n","    </tr>\n","    <tr>\n","      <th>538</th>\n","      <td>76391276</td>\n","      <td>Custom gradient for broadcasting operation</td>\n","      <td>I have an operation for which I want to define...</td>\n","      <td>1</td>\n","      <td>Documentation Ambiguity</td>\n","    </tr>\n","    <tr>\n","      <th>539</th>\n","      <td>76396532</td>\n","      <td>Ragged tensors in dataset, tensorflow, how do ...</td>\n","      <td>I have in my model, for fitting, and my datase...</td>\n","      <td>1</td>\n","      <td>Documentation Replicability</td>\n","    </tr>\n","    <tr>\n","      <th>540</th>\n","      <td>76444107</td>\n","      <td>Are 'validation_steps' used if the validation_...</td>\n","      <td>I was trying to use the Keras API to train a g...</td>\n","      <td>1</td>\n","      <td>Documentation Replication on Other Examples</td>\n","    </tr>\n","    <tr>\n","      <th>541</th>\n","      <td>76447508</td>\n","      <td>How to retrain a model that was saved using th...</td>\n","      <td>I am building a Neural Machine Translator for ...</td>\n","      <td>1</td>\n","      <td>Inadequate Examples</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>542 rows × 5 columns</p>\n","</div>"],"text/plain":["     QuestionId                                              Title  \\\n","0      34589335  How does the distorted_inputs() function in th...   \n","1      34619177           What does tf.nn.conv2d do in tensorflow?   \n","2      34642595                        Tensorflow Strides Argument   \n","3      34931121         Can cond support TF ops with side effects?   \n","4      35689547    How to process single training file in parallel   \n","..          ...                                                ...   \n","537    76380927                            Tensorflow decode image   \n","538    76391276         Custom gradient for broadcasting operation   \n","539    76396532  Ragged tensors in dataset, tensorflow, how do ...   \n","540    76444107  Are 'validation_steps' used if the validation_...   \n","541    76447508  How to retrain a model that was saved using th...   \n","\n","                                                  Body  DocRelated  \\\n","0    I was going through the CIFAR-10 example at Te...           1   \n","1    I was looking at the docs of tensorflow about ...           1   \n","2    I am trying to understand the strides argument...           1   \n","3    The (source code) documentation for tf.cond is...           1   \n","4    I have a file train.csv that contains paths to...           1   \n","..                                                 ...         ...   \n","537  I am a beginner in tensorflow and I am trainin...           1   \n","538  I have an operation for which I want to define...           1   \n","539  I have in my model, for fitting, and my datase...           1   \n","540  I was trying to use the Keras API to train a g...           1   \n","541  I am building a Neural Machine Translator for ...           1   \n","\n","                                         Issue Type  \n","0       Documentation Replication on Other Examples  \n","1       Documentation Replication on Other Examples  \n","2                           Documentation Ambiguity  \n","3                       Documentation Replicability  \n","4       Documentation Replication on Other Examples  \n","..                                              ...  \n","537  Requesting (Additional) Documentation/Examples  \n","538                         Documentation Ambiguity  \n","539                     Documentation Replicability  \n","540     Documentation Replication on Other Examples  \n","541                             Inadequate Examples  \n","\n","[542 rows x 5 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["doc_related_issue_types = pd.read_csv(os.path.join(s.DATA_PATH, \"doc_related_annotated_issue_types_v0.csv\"))\n","doc_related_issue_types = doc_related_issue_types[['QuestionId', 'Issue Type']]\n","\n","common_data = pd.merge(tf_doc_related_ques, doc_related_issue_types, how='inner', on='QuestionId')\n","annotated_data = common_data[['QuestionId', 'Title', 'Body', 'DocRelated', 'Issue Type']]\n","annotated_data.to_csv(os.path.join(s.DATA_PATH, \"annotated_data_v0.csv\"))\n","annotated_data\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["Issue Type\n","Documentation Replication on Other Examples       198\n","Documentation Replicability                        94\n","Lack of Alternative Solutions/Documentation        86\n","Documentation Ambiguity                            82\n","Inadequate Examples                                43\n","Requesting (Additional) Documentation/Examples     24\n","Documentation Completeness                         15\n","Name: count, dtype: int64"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["annotated_data['Issue Type'].value_counts()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Doc-related issue type unannotated questions\n","ids_to_remove = annotated_data['QuestionId'].tolist()\n","df_filtered = tf_doc_related_ques.drop(tf_doc_related_ques[tf_doc_related_ques['QuestionId'].isin(ids_to_remove)].index)\n","\n","# Since we already have 542 annotated ones, randomly select 1000 to annotate.\n","sampled_df =  df_filtered.sample(n=1000, random_state=42)\n","sampled_df.to_csv(os.path.join(s.DATA_PATH, \"sampled_issuetypes_data_v0.csv\"), index=False)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>QuestionId</th>\n","      <th>Title</th>\n","      <th>Body</th>\n","      <th>QuestionURL</th>\n","      <th>UserId</th>\n","      <th>DocRelated</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>70163993</td>\n","      <td>Replace tf.const with tf.variable in frozen gr...</td>\n","      <td>I got trouble to re-train frozen graph Due to ...</td>\n","      <td>https://stackoverflow.com/questions/70163993</td>\n","      <td>4457567.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>42865818</td>\n","      <td>Tensorflow Serving - \"No versions of servable\"...</td>\n","      <td>I've trained a model using the Getting Started...</td>\n","      <td>https://stackoverflow.com/questions/42865818</td>\n","      <td>3202362.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>44770980</td>\n","      <td>Tensorflow - Retrieve each character in a stri...</td>\n","      <td>I'm trying to retrieve the characters in a str...</td>\n","      <td>https://stackoverflow.com/questions/44770980</td>\n","      <td>5470522.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>49930682</td>\n","      <td>Getting InvalidArgumentError in softmax_cross_...</td>\n","      <td>I'm pretty new to tensorflow and trying to do ...</td>\n","      <td>https://stackoverflow.com/questions/49930682</td>\n","      <td>8444976.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>60974077</td>\n","      <td>How to save Keras model as frozen graph?</td>\n","      <td>I am working with Tensorflow 2.0 and want to s...</td>\n","      <td>https://stackoverflow.com/questions/60974077</td>\n","      <td>3861775.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>53367063</td>\n","      <td>tensorflow python expected dense_input to have...</td>\n","      <td>I am a complete newbie to tensorflow, trying t...</td>\n","      <td>https://stackoverflow.com/questions/53367063</td>\n","      <td>10672298.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>54703128</td>\n","      <td>No gradients provided for any variable, check ...</td>\n","      <td>I am trying to build a Bayesian Softmax Regres...</td>\n","      <td>https://stackoverflow.com/questions/54703128</td>\n","      <td>11055886.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>43284047</td>\n","      <td>What is the default kernel initializer in tf.l...</td>\n","      <td>The official Tensorflow API doc claims that th...</td>\n","      <td>https://stackoverflow.com/questions/43284047</td>\n","      <td>7833924.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>58947679</td>\n","      <td>No gradients provided for any variable in tens...</td>\n","      <td>I met a problem when I tried to use tensorflow...</td>\n","      <td>https://stackoverflow.com/questions/58947679</td>\n","      <td>10796214.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>48495699</td>\n","      <td>How to understand the effect of local_step in ...</td>\n","      <td>I want to implement the function which conduct...</td>\n","      <td>https://stackoverflow.com/questions/48495699</td>\n","      <td>4202137.0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 6 columns</p>\n","</div>"],"text/plain":["     QuestionId                                              Title  \\\n","0      70163993  Replace tf.const with tf.variable in frozen gr...   \n","1      42865818  Tensorflow Serving - \"No versions of servable\"...   \n","2      44770980  Tensorflow - Retrieve each character in a stri...   \n","3      49930682  Getting InvalidArgumentError in softmax_cross_...   \n","4      60974077           How to save Keras model as frozen graph?   \n","..          ...                                                ...   \n","995    53367063  tensorflow python expected dense_input to have...   \n","996    54703128  No gradients provided for any variable, check ...   \n","997    43284047  What is the default kernel initializer in tf.l...   \n","998    58947679  No gradients provided for any variable in tens...   \n","999    48495699  How to understand the effect of local_step in ...   \n","\n","                                                  Body  \\\n","0    I got trouble to re-train frozen graph Due to ...   \n","1    I've trained a model using the Getting Started...   \n","2    I'm trying to retrieve the characters in a str...   \n","3    I'm pretty new to tensorflow and trying to do ...   \n","4    I am working with Tensorflow 2.0 and want to s...   \n","..                                                 ...   \n","995  I am a complete newbie to tensorflow, trying t...   \n","996  I am trying to build a Bayesian Softmax Regres...   \n","997  The official Tensorflow API doc claims that th...   \n","998  I met a problem when I tried to use tensorflow...   \n","999  I want to implement the function which conduct...   \n","\n","                                      QuestionURL      UserId  DocRelated  \n","0    https://stackoverflow.com/questions/70163993   4457567.0           1  \n","1    https://stackoverflow.com/questions/42865818   3202362.0           1  \n","2    https://stackoverflow.com/questions/44770980   5470522.0           1  \n","3    https://stackoverflow.com/questions/49930682   8444976.0           1  \n","4    https://stackoverflow.com/questions/60974077   3861775.0           1  \n","..                                            ...         ...         ...  \n","995  https://stackoverflow.com/questions/53367063  10672298.0           1  \n","996  https://stackoverflow.com/questions/54703128  11055886.0           1  \n","997  https://stackoverflow.com/questions/43284047   7833924.0           1  \n","998  https://stackoverflow.com/questions/58947679  10796214.0           1  \n","999  https://stackoverflow.com/questions/48495699   4202137.0           1  \n","\n","[1000 rows x 6 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Preprocess only to select text\n","sampled_df = pd.read_csv(os.path.join(s.DATA_PATH, \"sampled_issuetypes_data_v0.csv\"))\n","# sampled_df['Body'] = sampled_df['Body'].apply(utils.text_preprocessor)\n","# sampled_df.to_csv(os.path.join(s.DATA_PATH, \"sampled_issuetypes_data_v2.csv\"))\n","sampled_df"]},{"cell_type":"markdown","metadata":{},"source":["#### Classify TensorFlow API documentation related question to identify issue types"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### API Documentation Smells"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>api_name</th>\n","      <th>url</th>\n","      <th>content</th>\n","      <th>url_count</th>\n","      <th>flesch_ease_score</th>\n","      <th>api_method_count</th>\n","      <th>total_words</th>\n","      <th>Bloated_avg</th>\n","      <th>Tangled_avg</th>\n","      <th>Excessive_Structured_avg</th>\n","      <th>Fragmented_avg</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>tf.AggregationMethod</td>\n","      <td>https://www.tensorflow.org/api_docs/python/tf/...</td>\n","      <td>&lt;p&gt;&lt;/p&gt;\\n&lt;p&gt;A class listing aggregation method...</td>\n","      <td>0</td>\n","      <td>26.67</td>\n","      <td>0</td>\n","      <td>71.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>tf.Assert</td>\n","      <td>https://www.tensorflow.org/api_docs/python/tf/...</td>\n","      <td>&lt;p&gt;&lt;/p&gt;\\n&lt;p&gt;Asserts that the given condition i...</td>\n","      <td>2</td>\n","      <td>66.94</td>\n","      <td>2</td>\n","      <td>63.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>tf.CriticalSection</td>\n","      <td>https://www.tensorflow.org/api_docs/python/tf/...</td>\n","      <td>&lt;p&gt;&lt;/p&gt;\\n&lt;p&gt;Critical section.&lt;/p&gt;\\n&lt;p&gt;A &lt;code ...</td>\n","      <td>1</td>\n","      <td>46.47</td>\n","      <td>0</td>\n","      <td>289.0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>tf.DType</td>\n","      <td>https://www.tensorflow.org/api_docs/python/tf/...</td>\n","      <td>&lt;p&gt;&lt;/p&gt;\\n&lt;p&gt;Represents the type of the element...</td>\n","      <td>10</td>\n","      <td>70.39</td>\n","      <td>1</td>\n","      <td>139.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>tf.DeviceSpec</td>\n","      <td>https://www.tensorflow.org/api_docs/python/tf/...</td>\n","      <td>&lt;p&gt;&lt;/p&gt;\\n&lt;p&gt;Represents a (possibly partial) sp...</td>\n","      <td>9</td>\n","      <td>51.24</td>\n","      <td>0</td>\n","      <td>289.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>4514</th>\n","      <td>tf.compat.v1.while_loop</td>\n","      <td>https://www.tensorflow.org/api_docs/python/tf/...</td>\n","      <td>&lt;p&gt;&lt;/p&gt;\\n&lt;p&gt;Repeat &lt;code dir=\"ltr\" translate=\"...</td>\n","      <td>2</td>\n","      <td>60.85</td>\n","      <td>2</td>\n","      <td>890.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4515</th>\n","      <td>tf.compat.v1.wrap_function</td>\n","      <td>https://www.tensorflow.org/api_docs/python/tf/...</td>\n","      <td>&lt;p&gt;&lt;/p&gt;\\n&lt;p&gt;Wraps the TF 1.x function fn into ...</td>\n","      <td>5</td>\n","      <td>56.55</td>\n","      <td>3</td>\n","      <td>214.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4516</th>\n","      <td>tf.compat.v1.xla</td>\n","      <td>https://www.tensorflow.org/api_docs/python/tf/...</td>\n","      <td>&lt;p&gt;&lt;/p&gt;\\n&lt;p&gt;Public API for tf.xla namespace.&lt;/...</td>\n","      <td>1</td>\n","      <td>56.93</td>\n","      <td>0</td>\n","      <td>17.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4517</th>\n","      <td>tf.compat.v1.xla.experimental</td>\n","      <td>https://www.tensorflow.org/api_docs/python/tf/...</td>\n","      <td>&lt;p&gt;&lt;/p&gt;\\n&lt;p&gt;Public API for tf.xla.experimental...</td>\n","      <td>2</td>\n","      <td>45.72</td>\n","      <td>0</td>\n","      <td>39.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4518</th>\n","      <td>tf.compat.v1.zeros_like</td>\n","      <td>https://www.tensorflow.org/api_docs/python/tf/...</td>\n","      <td>&lt;p&gt;&lt;/p&gt;\\n&lt;p&gt;Creates a tensor with all elements...</td>\n","      <td>1</td>\n","      <td>58.79</td>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4519 rows × 11 columns</p>\n","</div>"],"text/plain":["                           api_name  \\\n","0              tf.AggregationMethod   \n","1                         tf.Assert   \n","2                tf.CriticalSection   \n","3                          tf.DType   \n","4                     tf.DeviceSpec   \n","...                             ...   \n","4514        tf.compat.v1.while_loop   \n","4515     tf.compat.v1.wrap_function   \n","4516               tf.compat.v1.xla   \n","4517  tf.compat.v1.xla.experimental   \n","4518        tf.compat.v1.zeros_like   \n","\n","                                                    url  \\\n","0     https://www.tensorflow.org/api_docs/python/tf/...   \n","1     https://www.tensorflow.org/api_docs/python/tf/...   \n","2     https://www.tensorflow.org/api_docs/python/tf/...   \n","3     https://www.tensorflow.org/api_docs/python/tf/...   \n","4     https://www.tensorflow.org/api_docs/python/tf/...   \n","...                                                 ...   \n","4514  https://www.tensorflow.org/api_docs/python/tf/...   \n","4515  https://www.tensorflow.org/api_docs/python/tf/...   \n","4516  https://www.tensorflow.org/api_docs/python/tf/...   \n","4517  https://www.tensorflow.org/api_docs/python/tf/...   \n","4518  https://www.tensorflow.org/api_docs/python/tf/...   \n","\n","                                                content  url_count  \\\n","0     <p></p>\\n<p>A class listing aggregation method...          0   \n","1     <p></p>\\n<p>Asserts that the given condition i...          2   \n","2     <p></p>\\n<p>Critical section.</p>\\n<p>A <code ...          1   \n","3     <p></p>\\n<p>Represents the type of the element...         10   \n","4     <p></p>\\n<p>Represents a (possibly partial) sp...          9   \n","...                                                 ...        ...   \n","4514  <p></p>\\n<p>Repeat <code dir=\"ltr\" translate=\"...          2   \n","4515  <p></p>\\n<p>Wraps the TF 1.x function fn into ...          5   \n","4516  <p></p>\\n<p>Public API for tf.xla namespace.</...          1   \n","4517  <p></p>\\n<p>Public API for tf.xla.experimental...          2   \n","4518  <p></p>\\n<p>Creates a tensor with all elements...          1   \n","\n","      flesch_ease_score  api_method_count  total_words  Bloated_avg  \\\n","0                 26.67                 0         71.0            0   \n","1                 66.94                 2         63.0            0   \n","2                 46.47                 0        289.0            1   \n","3                 70.39                 1        139.0            0   \n","4                 51.24                 0        289.0            1   \n","...                 ...               ...          ...          ...   \n","4514              60.85                 2        890.0            1   \n","4515              56.55                 3        214.0            1   \n","4516              56.93                 0         17.0            0   \n","4517              45.72                 0         39.0            0   \n","4518              58.79                 1          NaN            0   \n","\n","      Tangled_avg  Excessive_Structured_avg  Fragmented_avg  \n","0               1                         0               0  \n","1               0                         1               0  \n","2               1                         0               0  \n","3               0                         1               1  \n","4               0                         0               1  \n","...           ...                       ...             ...  \n","4514            0                         1               0  \n","4515            0                         1               1  \n","4516            0                         0               0  \n","4517            1                         0               0  \n","4518            0                         1               0  \n","\n","[4519 rows x 11 columns]"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["doc_smells = pd.read_csv(os.path.join(s.DATA_PATH, 'tf_doc_smells.csv'))\n","doc_smells = doc_smells[[\n","    'api_name', 'url', 'content', 'url_count', 'flesch_ease_score', 'api_method_count', 'total_words', \n","    'Bloated_avg', 'Tangled_avg', 'Excessive_Structured_avg', 'Fragmented_avg']]\n","doc_smells\n"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Bloated Threshold: 141.75896414342628\n","Fragmented Threshold: 4.851073246293428\n","Excessive Structure Threshold: 0.9805266651914141\n","Tangled Threshold: 48.197373312679794\n"]}],"source":["print(f\"Bloated Threshold: {doc_smells['total_words'].mean()}\")\n","print(f\"Fragmented Threshold: {doc_smells['url_count'].mean()}\")\n","print(f\"Excessive Structure Threshold: {doc_smells['api_method_count'].mean()}\")\n","print(f\"Tangled Threshold: {doc_smells['flesch_ease_score'].mean()}\")\n","\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Bloated: 932\n","Total Fragmented: 813\n","Total Excessive Structure: 1685\n","Total Tangled: 1760\n"]}],"source":["print(f\"Total Bloated: {doc_smells['Bloated_avg'].sum(axis=0)}\")\n","print(f\"Total Fragmented: {doc_smells['Fragmented_avg'].sum(axis=0)}\")\n","print(f\"Total Excessive Structure: {doc_smells['Excessive_Structured_avg'].sum(axis=0)}\")\n","print(f\"Total Tangled: {doc_smells['Tangled_avg'].sum(axis=0)}\")\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Smell Counts: {'all_zeros': 1285, 'only_one': 1867, 'two_ones': 844, 'three_ones': 457, 'all_ones': 66}\n"]}],"source":["smell_counts = {\"all_zeros\": 0, \"only_one\": 0, \"two_ones\": 0, \"three_ones\": 0, \"all_ones\": 0}\n","\n","for index, row in doc_smells.iterrows():\n","  # Calculate the total smell score\n","  total_smell_score = row[\"Bloated_avg\"] + row[\"Fragmented_avg\"] + row[\"Excessive_Structured_avg\"] + row[\"Tangled_avg\"]\n","  \n","  # Increment count based on smell score\n","  smell_counts[f\"all_zeros\"] += 1 if total_smell_score == 0 else 0\n","  smell_counts[f\"only_one\"] += 1 if total_smell_score == 1 else 0\n","  smell_counts[f\"two_ones\"] += 1 if total_smell_score == 2 else 0\n","  smell_counts[f\"three_ones\"] += 1 if total_smell_score == 3 else 0\n","  smell_counts[f\"all_ones\"] += 1 if total_smell_score == 4 else 0\n","\n","print(f\"Smell Counts: {smell_counts}\")\n"]},{"cell_type":"markdown","metadata":{"id":"6-677bizh7Ey"},"source":["#### Persist a ChromaDB instance using the embeddings collections."]},{"cell_type":"markdown","metadata":{"id":"j2OMajSAfMc2"},"source":["* The original document is too lengthy to seamlessly fit into the context window of the LLM. Therefore, it is necessary to divide it into smaller sections."]},{"cell_type":"markdown","metadata":{"id":"YSCdcC5BVAmf"},"source":["**Stack Overflow question and accepted answer vector database.**"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":310},"executionInfo":{"elapsed":212,"status":"ok","timestamp":1707402947792,"user":{"displayName":"Sharuka Promodya Thirimanne","userId":"12088648339981588470"},"user_tz":300},"id":"wL85AY5A0NtP","outputId":"d583ab5e-d577-4045-b274-e2d47f3c28d1"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>QuestionId</th>\n","      <th>Title</th>\n","      <th>Body</th>\n","      <th>QuestionAPI</th>\n","      <th>CurrentAPI</th>\n","      <th>CreationDate</th>\n","      <th>AcceptedAnswer</th>\n","      <th>UserID</th>\n","      <th>User Reputation</th>\n","      <th>QuestionURL</th>\n","      <th>IssueType</th>\n","      <th>context</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>76324368</td>\n","      <td>Understanding tf.keras.layers.Dense()</td>\n","      <td>&lt;p&gt;I am trying to understand why there is a di...</td>\n","      <td>tf.keras.layers.Dense</td>\n","      <td>tf.keras.layers.Dense</td>\n","      <td>2023-05-24 14:00:25</td>\n","      <td>&lt;p&gt;After some experimentation I realized that ...</td>\n","      <td>18338104</td>\n","      <td>5</td>\n","      <td>https://stackoverflow.com/questions/76324368</td>\n","      <td>Documentation Replication on Other Examples</td>\n","      <td>Title: Understanding tf.keras.layers.Dense()\\n...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>74088086</td>\n","      <td>Seed in tensorflow initializer (tf.keras.initi...</td>\n","      <td>&lt;p&gt;looking at tensorflow documentation (see, e...</td>\n","      <td>tf.keras.initializers.GlorotNormal</td>\n","      <td>tf.keras.initializers.GlorotNormal</td>\n","      <td>2022-10-16 14:53:17</td>\n","      <td>&lt;p&gt;If you go to the link you send you can read...</td>\n","      <td>17788510</td>\n","      <td>168</td>\n","      <td>https://stackoverflow.com/questions/74088086</td>\n","      <td>Documentation Replication on Other Examples</td>\n","      <td>Title: Seed in tensorflow initializer (tf.kera...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>74029376</td>\n","      <td>Tensorflow custom reduction function with axis...</td>\n","      <td>&lt;p&gt;I would like to get the value with the maxi...</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>2022-10-11 13:57:31</td>\n","      <td>&lt;p&gt;It really depends on how many dimensions yo...</td>\n","      <td>18159603</td>\n","      <td>955</td>\n","      <td>https://stackoverflow.com/questions/74029376</td>\n","      <td>Lack of Alternative Solutions/Documentation</td>\n","      <td>Title: Tensorflow custom reduction function wi...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>74005009</td>\n","      <td>How to create output_signature for tensorflow....</td>\n","      <td>&lt;p&gt;I have a generator yielding data and labels...</td>\n","      <td>tf.data.Dataset</td>\n","      <td>tf.data.Dataset</td>\n","      <td>2022-10-09 13:04:41</td>\n","      <td>&lt;p&gt;if your datagen_row() function yields input...</td>\n","      <td>2300622</td>\n","      <td>1104</td>\n","      <td>https://stackoverflow.com/questions/74005009</td>\n","      <td>Documentation Replicability</td>\n","      <td>Title: How to create output_signature for tens...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>73165980</td>\n","      <td>Tensorflow: how to feed a variable-time-step i...</td>\n","      <td>&lt;p&gt;I have a simple X_train and Y_train data:&lt;/...</td>\n","      <td>tf.data.Dataset</td>\n","      <td>tf.data.Dataset</td>\n","      <td>2022-07-29 11:26:49</td>\n","      <td>&lt;p&gt;Short answer is, you can define &lt;code&gt;outpu...</td>\n","      <td>13454852</td>\n","      <td>151</td>\n","      <td>https://stackoverflow.com/questions/73165980</td>\n","      <td>Documentation Ambiguity</td>\n","      <td>Title: Tensorflow: how to feed a variable-time...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   QuestionId                                              Title  \\\n","0    76324368              Understanding tf.keras.layers.Dense()   \n","1    74088086  Seed in tensorflow initializer (tf.keras.initi...   \n","2    74029376  Tensorflow custom reduction function with axis...   \n","3    74005009  How to create output_signature for tensorflow....   \n","4    73165980  Tensorflow: how to feed a variable-time-step i...   \n","\n","                                                Body  \\\n","0  <p>I am trying to understand why there is a di...   \n","1  <p>looking at tensorflow documentation (see, e...   \n","2  <p>I would like to get the value with the maxi...   \n","3  <p>I have a generator yielding data and labels...   \n","4  <p>I have a simple X_train and Y_train data:</...   \n","\n","                          QuestionAPI                          CurrentAPI  \\\n","0               tf.keras.layers.Dense               tf.keras.layers.Dense   \n","1  tf.keras.initializers.GlorotNormal  tf.keras.initializers.GlorotNormal   \n","2                                   -                                   -   \n","3                     tf.data.Dataset                     tf.data.Dataset   \n","4                     tf.data.Dataset                     tf.data.Dataset   \n","\n","          CreationDate                                     AcceptedAnswer  \\\n","0  2023-05-24 14:00:25  <p>After some experimentation I realized that ...   \n","1  2022-10-16 14:53:17  <p>If you go to the link you send you can read...   \n","2  2022-10-11 13:57:31  <p>It really depends on how many dimensions yo...   \n","3  2022-10-09 13:04:41  <p>if your datagen_row() function yields input...   \n","4  2022-07-29 11:26:49  <p>Short answer is, you can define <code>outpu...   \n","\n","     UserID  User Reputation                                   QuestionURL  \\\n","0  18338104                5  https://stackoverflow.com/questions/76324368   \n","1  17788510              168  https://stackoverflow.com/questions/74088086   \n","2  18159603              955  https://stackoverflow.com/questions/74029376   \n","3   2300622             1104  https://stackoverflow.com/questions/74005009   \n","4  13454852              151  https://stackoverflow.com/questions/73165980   \n","\n","                                     IssueType  \\\n","0  Documentation Replication on Other Examples   \n","1  Documentation Replication on Other Examples   \n","2  Lack of Alternative Solutions/Documentation   \n","3                  Documentation Replicability   \n","4                      Documentation Ambiguity   \n","\n","                                             context  \n","0  Title: Understanding tf.keras.layers.Dense()\\n...  \n","1  Title: Seed in tensorflow initializer (tf.kera...  \n","2  Title: Tensorflow custom reduction function wi...  \n","3  Title: How to create output_signature for tens...  \n","4  Title: Tensorflow: how to feed a variable-time...  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Load CSV file that contains TensorFlow API documentation related Stack Overflow questions with accepted answers.\n","df = pd.read_csv(os.path.join(s.DATA_PATH, \"DocQues_AcceptedAns_Issues_v2_2018_2023.csv\"))\n","df[\"context\"] = \"\"\n","\n","for index, row in df.iterrows():\n","  context = f\"Title: {row['Title']}\\n\\n\"\n","  context += f\"Body: {row['Body']}\\n\\n\"\n","  context += f\"AcceptedAnswer: {row['AcceptedAnswer']}\"\n","  df.loc[index, \"context\"] = context\n","\n","df.head()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":130540,"status":"ok","timestamp":1707290304472,"user":{"displayName":"Sharuka Promodya Thirimanne","userId":"12088648339981588470"},"user_tz":300},"id":"Jcx9uCpH8xjq","outputId":"92e6d911-ad6f-4ff1-f37e-43d9378e685e"},"outputs":[{"name":"stderr","output_type":"stream","text":["Warning: model not found. Using cl100k_base encoding.\n"]}],"source":["persist_df = df[[\"QuestionId\", \"context\", \"QuestionURL\"]]\n","loader = DataFrameLoader(persist_df, page_content_column=\"context\")\n","so_questions_answers = loader.load()\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n","chunks = text_splitter.split_documents(so_questions_answers)\n","\n","db1 = Chroma.from_documents(chunks, embedding_function, persist_directory=os.path.join(s.VECTORDB_PATH, \"dataframe\"))"]},{"cell_type":"markdown","metadata":{"id":"KmlCdjNTVG8v"},"source":["**TensorFlow API documentation markdown files vector database.**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":717647,"status":"error","timestamp":1707402012945,"user":{"displayName":"Sharuka Promodya Thirimanne","userId":"12088648339981588470"},"user_tz":300},"id":"LdFHyDvEXTj-","outputId":"cd4536bb-db39-485e-cb51-290988088488"},"outputs":[],"source":["markdown_loader = DirectoryLoader(s.DOCUMENTATION_PATH, glob=\"**/*.md\", loader_cls=TextLoader)\n","markdown_docs = markdown_loader.load()\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n","chunks = text_splitter.split_documents(markdown_docs)\n","\n","db2 = Chroma.from_documents(chunks, embedding_function, persist_directory=os.path.join(s.VECTORDB_PATH, \"markdown\"))"]},{"cell_type":"markdown","metadata":{"id":"ZmS2oGQYVScv"},"source":["**TensorFlow-related YouTube video transcripts vector database.**"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"3rTg-uT7PiCn"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_id</th>\n","      <th>source</th>\n","      <th>title</th>\n","      <th>transcript</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>KNAWp2S3w94</td>\n","      <td>https://www.youtube.com/watch?v=KNAWp2S3w94&amp;li...</td>\n","      <td>Intro to Machine Learning (ML Zero to Hero - P...</td>\n","      <td>♪ (music) ♪ You've probably heard a lot\\nabout...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>bemDFpNooA8</td>\n","      <td>https://www.youtube.com/watch?v=bemDFpNooA8&amp;li...</td>\n","      <td>Basic Computer Vision with ML (ML Zero to Hero...</td>\n","      <td>♪ (music) ♪ Hi, everyone, and welcome to episo...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>x_VrgWTKkiM</td>\n","      <td>https://www.youtube.com/watch?v=x_VrgWTKkiM&amp;li...</td>\n","      <td>Introducing convolutional neural networks (ML ...</td>\n","      <td>♪ (music) ♪ Hi, and welcome to episode three\\n...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>u2TjZzNuly8</td>\n","      <td>https://www.youtube.com/watch?v=u2TjZzNuly8&amp;li...</td>\n","      <td>Build an image classifier (ML Zero to Hero - P...</td>\n","      <td>♪ (intro music) ♪ Hi, everybody, and welcome t...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>fNxaJsNG3-s</td>\n","      <td>https://www.youtube.com/watch?v=fNxaJsNG3-s&amp;li...</td>\n","      <td>Natural Language Processing - Tokenization (NL...</td>\n","      <td>LAURENCE MORONEY: Hi, and\\nwelcome to this ser...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      video_id                                             source  \\\n","0  KNAWp2S3w94  https://www.youtube.com/watch?v=KNAWp2S3w94&li...   \n","1  bemDFpNooA8  https://www.youtube.com/watch?v=bemDFpNooA8&li...   \n","2  x_VrgWTKkiM  https://www.youtube.com/watch?v=x_VrgWTKkiM&li...   \n","3  u2TjZzNuly8  https://www.youtube.com/watch?v=u2TjZzNuly8&li...   \n","4  fNxaJsNG3-s  https://www.youtube.com/watch?v=fNxaJsNG3-s&li...   \n","\n","                                               title  \\\n","0  Intro to Machine Learning (ML Zero to Hero - P...   \n","1  Basic Computer Vision with ML (ML Zero to Hero...   \n","2  Introducing convolutional neural networks (ML ...   \n","3  Build an image classifier (ML Zero to Hero - P...   \n","4  Natural Language Processing - Tokenization (NL...   \n","\n","                                          transcript  \n","0  ♪ (music) ♪ You've probably heard a lot\\nabout...  \n","1  ♪ (music) ♪ Hi, everyone, and welcome to episo...  \n","2  ♪ (music) ♪ Hi, and welcome to episode three\\n...  \n","3  ♪ (intro music) ♪ Hi, everybody, and welcome t...  \n","4  LAURENCE MORONEY: Hi, and\\nwelcome to this ser...  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["with open (os.path.join(s.ASSETS_PATH, 'youtube_videos.json'), 'r') as file:\n","  urls = json.load(file)\n","\n","data = []\n","for url in urls:\n","  try:\n","    youtube_loader = YoutubeLoader.from_youtube_url(url, add_video_info=True)\n","    yt_transcript = youtube_loader.load()\n","\n","    details = {\n","      'video_id': yt_transcript[0].metadata['source'],\n","      'source': url,\n","      'title': yt_transcript[0].metadata['title'],\n","      'transcript': yt_transcript[0].page_content\n","      }\n","    data.append(details)\n","  except Exception as e:\n","    print(f\"An errror occured with URL {url}: {e}\")\n","\n","transcript_df = pd.DataFrame(data)\n","transcript_df.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6886,"status":"ok","timestamp":1707373611634,"user":{"displayName":"Sharuka Promodya Thirimanne","userId":"12088648339981588470"},"user_tz":300},"id":"oMITPb8hV9Lk","outputId":"6bec2b9c-edc6-4975-b2cf-ae563279d97d"},"outputs":[{"name":"stderr","output_type":"stream","text":["Warning: model not found. Using cl100k_base encoding.\n"]}],"source":["transcript_loader = DataFrameLoader(transcript_df, page_content_column=\"transcript\")\n","yt_transcripts = transcript_loader.load()\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n","chunks = text_splitter.split_documents(yt_transcripts)\n","\n","db3 = Chroma.from_documents(chunks, embedding_function, persist_directory=os.path.join(s.VECTORDB_PATH, \"transcripts\"))"]},{"cell_type":"markdown","metadata":{},"source":["#### Query Stack Overflow using the API\n","We can easily get the responses we need when we include many responses, but this doesn't work well with LLMs. The recall performance for LLMs decreases as we add more into the context windows - we call this excessive filling of thhe context widnows \"context stuffing\".\n","\n","Fortunately, reranking offers us a solution to this by helping to find the most relevant responses (e.g top-3), and pull then into a smaller set of results to be given to the LLM."]},{"cell_type":"markdown","metadata":{"id":"5BGIJPSz821s"},"source":["## Pipeline Elements"]},{"cell_type":"markdown","metadata":{"id":"ag97eqiefK-q"},"source":["#### 6. Pipeline Element: Prompt Template"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"cgMn3sflt89i"},"outputs":[],"source":["template = \"\"\"\n","Use the following pieces of context as a support to answer the question at the end.  If cannot find relevant context, please think rationally and answer from your own knowledge base.\n","\n","{context}\n","\n","Below is a Stack Overflow question posted by a user related to documentation replication on other examples.\n","\n","Question Title: {title}\n","Question Body: {body}\n","\n","The above question is related to {issue_type} issue type. And below is the definition of that issue type.\n","\n","{issue_type} definition:\n","{definition}\n","\n","\n","Moreover, the question is related to the TensorFlow {api_name} API documentation. Below is the up-to-date TensorFlow API documentation in markdown format:\n","\n","{documentation}\n","\n","\n","{task}. Moreover, provide the response in markdown format in order to add that into the original documentation as a new section \"Alternative Resources\". This customization should avoid any questions like this in future.\n","\"\"\""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"116iDdQPq6_e"},"outputs":[],"source":["PROMPT = PromptTemplate(\n","    template=template,\n","    input_variables=['context', 'title','body', 'issue_type','definition', 'documentation', 'task'])"]},{"cell_type":"markdown","metadata":{},"source":["#### 7. Pipeline Element: LangChain Agent"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["agent_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0)\n","agent_prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n","search = TavilySearchResults()\n","tools = [search]\n","agent = create_openai_functions_agent(agent_llm, tools, agent_prompt)\n","agent_executor = AgentExecutor(agent=agent, tools=tools)"]},{"cell_type":"markdown","metadata":{"id":"orPVjzCRfC_Y"},"source":["#### 8. Pipeline Element: LangChain Pipeline"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"_jwqrJ6ufCn-"},"outputs":[],"source":["mlflow_callback = MlflowCallbackHandler(experiment=\"/Users/sharukat@gmail.com/DocumentCustomizer\",tracking_uri=\"databricks\")\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0, callbacks=[mlflow_callback], verbose=True,)\n","synopsis_chain = LLMChain(llm=llm, prompt=PROMPT, callbacks=[mlflow_callback])"]},{"cell_type":"markdown","metadata":{"id":"0qUBPmN9jZkE"},"source":["## Experiments"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":143,"status":"ok","timestamp":1707458754897,"user":{"displayName":"Sharuka Promodya Thirimanne","userId":"12088648339981588470"},"user_tz":300},"id":"E7VxSqdAsK0Z","outputId":"5c099687-80e6-4ba5-a5a4-54bbb033400c"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>QuestionId</th>\n","      <th>Title</th>\n","      <th>Body</th>\n","      <th>QuestionsAPI</th>\n","      <th>CurrentAPI</th>\n","      <th>CreationDate</th>\n","      <th>UserId</th>\n","      <th>UserReputation</th>\n","      <th>QuestionURL</th>\n","      <th>IssueType</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>76447508</td>\n","      <td>How to retrain a model that was saved using th...</td>\n","      <td>&lt;p&gt;I am building a Neural Machine Translator f...</td>\n","      <td>tf.saved_model.load</td>\n","      <td>tf.saved_model.load</td>\n","      <td>2023-04-17 23:39:34</td>\n","      <td>16851318</td>\n","      <td>41</td>\n","      <td>https://stackoverflow.com/questions/76447508</td>\n","      <td>Inadequate Examples</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>76396532</td>\n","      <td>Ragged tensors in dataset, tensorflow, how do ...</td>\n","      <td>&lt;p&gt;I have&lt;/p&gt;\\n&lt;pre&gt;&lt;code&gt;def call (self, inpu...</td>\n","      <td>tf.data.Dataset</td>\n","      <td>tf.data.Dataset</td>\n","      <td>2023-04-12 14:26:12</td>\n","      <td>13154958</td>\n","      <td>1</td>\n","      <td>https://stackoverflow.com/questions/76396532</td>\n","      <td>Documentation Replicability</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>76391276</td>\n","      <td>Custom gradient for broadcasting operation</td>\n","      <td>&lt;p&gt;I have an operation for which I want to def...</td>\n","      <td>tf.custom_gradient</td>\n","      <td>tf.custom_gradient</td>\n","      <td>2023-04-05 13:00:20</td>\n","      <td>1782792</td>\n","      <td>58906</td>\n","      <td>https://stackoverflow.com/questions/76391276</td>\n","      <td>Documentation Ambiguity</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>76380927</td>\n","      <td>Tensorflow decode image</td>\n","      <td>&lt;p&gt;I am a beginner in tensorflow and I am trai...</td>\n","      <td>tf.io.decode_image</td>\n","      <td>tf.io.decode_image</td>\n","      <td>2023-03-05 8:02:03</td>\n","      <td>15460221</td>\n","      <td>11</td>\n","      <td>https://stackoverflow.com/questions/76380927</td>\n","      <td>Requesting (Additional) Documentation/Examples</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>76244268</td>\n","      <td>Tensorflow: Build new model from input and mid...</td>\n","      <td>&lt;p&gt;I'm trying to build &lt;code&gt;new_model&lt;/code&gt; ...</td>\n","      <td>tf.keras.applications.efficientnet_v2</td>\n","      <td>tf.keras.applications.efficientnet_v2</td>\n","      <td>2023-03-04 23:59:16</td>\n","      <td>2103321</td>\n","      <td>75</td>\n","      <td>https://stackoverflow.com/questions/76244268</td>\n","      <td>Inadequate Examples</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   QuestionId                                              Title  \\\n","0    76447508  How to retrain a model that was saved using th...   \n","1    76396532  Ragged tensors in dataset, tensorflow, how do ...   \n","2    76391276         Custom gradient for broadcasting operation   \n","3    76380927                            Tensorflow decode image   \n","4    76244268  Tensorflow: Build new model from input and mid...   \n","\n","                                                Body  \\\n","0  <p>I am building a Neural Machine Translator f...   \n","1  <p>I have</p>\\n<pre><code>def call (self, inpu...   \n","2  <p>I have an operation for which I want to def...   \n","3  <p>I am a beginner in tensorflow and I am trai...   \n","4  <p>I'm trying to build <code>new_model</code> ...   \n","\n","                            QuestionsAPI  \\\n","0                    tf.saved_model.load   \n","1                        tf.data.Dataset   \n","2                     tf.custom_gradient   \n","3                     tf.io.decode_image   \n","4  tf.keras.applications.efficientnet_v2   \n","\n","                              CurrentAPI         CreationDate    UserId  \\\n","0                    tf.saved_model.load  2023-04-17 23:39:34  16851318   \n","1                        tf.data.Dataset  2023-04-12 14:26:12  13154958   \n","2                     tf.custom_gradient  2023-04-05 13:00:20   1782792   \n","3                     tf.io.decode_image   2023-03-05 8:02:03  15460221   \n","4  tf.keras.applications.efficientnet_v2  2023-03-04 23:59:16   2103321   \n","\n","   UserReputation                                   QuestionURL  \\\n","0              41  https://stackoverflow.com/questions/76447508   \n","1               1  https://stackoverflow.com/questions/76396532   \n","2           58906  https://stackoverflow.com/questions/76391276   \n","3              11  https://stackoverflow.com/questions/76380927   \n","4              75  https://stackoverflow.com/questions/76244268   \n","\n","                                        IssueType  \n","0                             Inadequate Examples  \n","1                     Documentation Replicability  \n","2                         Documentation Ambiguity  \n","3  Requesting (Additional) Documentation/Examples  \n","4                             Inadequate Examples  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["question_queries = pd.read_csv(os.path.join(s.DATA_PATH, \"DocQues_NoAns_Issues_v3_2020_2023.csv\"))\n","question_queries.head()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/x7/c7r5kc051js_fytcq8jbvt040000gp/T/ipykernel_73229/3603017155.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  result_df = question_queries.groupby('IssueType').apply(select_rows).reset_index(drop=True)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>QuestionId</th>\n","      <th>Title</th>\n","      <th>Body</th>\n","      <th>QuestionsAPI</th>\n","      <th>CurrentAPI</th>\n","      <th>CreationDate</th>\n","      <th>UserId</th>\n","      <th>UserReputation</th>\n","      <th>QuestionURL</th>\n","      <th>IssueType</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>76391276</td>\n","      <td>Custom gradient for broadcasting operation</td>\n","      <td>&lt;p&gt;I have an operation for which I want to def...</td>\n","      <td>tf.custom_gradient</td>\n","      <td>tf.custom_gradient</td>\n","      <td>2023-04-05 13:00:20</td>\n","      <td>1782792</td>\n","      <td>58906</td>\n","      <td>https://stackoverflow.com/questions/76391276</td>\n","      <td>Documentation Ambiguity</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>76153107</td>\n","      <td>Difference between tf.Module and tf.keras.Model</td>\n","      <td>&lt;p&gt;I know both &lt;code&gt;tf.Module&lt;/code&gt; and &lt;cod...</td>\n","      <td>tf.Module</td>\n","      <td>tf.Module</td>\n","      <td>2023-02-16 21:44:50</td>\n","      <td>21760922</td>\n","      <td>33</td>\n","      <td>https://stackoverflow.com/questions/76153107</td>\n","      <td>Documentation Ambiguity</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>75996642</td>\n","      <td>Is there a good equivalent of pandas' `apply` ...</td>\n","      <td>&lt;p&gt;&lt;strong&gt;BACKGROUND&lt;/strong&gt;&lt;/p&gt;\\n&lt;p&gt;The use...</td>\n","      <td>tf.py_function</td>\n","      <td>tf.py_function</td>\n","      <td>2022-11-14 15:44:35</td>\n","      <td>5640161</td>\n","      <td>791</td>\n","      <td>https://stackoverflow.com/questions/75996642</td>\n","      <td>Documentation Ambiguity</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>74434308</td>\n","      <td>Setting only global level seed gives same outp...</td>\n","      <td>&lt;p&gt;I am testing out the &lt;code&gt;tf.random.set_se...</td>\n","      <td>tf.random.set_seed</td>\n","      <td>tf.random.set_seed</td>\n","      <td>2021-06-28 11:48:57</td>\n","      <td>7422352</td>\n","      <td>5021</td>\n","      <td>https://stackoverflow.com/questions/74434308</td>\n","      <td>Documentation Ambiguity</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>71619495</td>\n","      <td>Image normalization by tf.image.convert_image_...</td>\n","      <td>&lt;p&gt;According to documentation &lt;code&gt;tf.image.c...</td>\n","      <td>tf.image.convert_image_dtype</td>\n","      <td>tf.image.convert_image_dtype</td>\n","      <td>2020-12-28 17:31:25</td>\n","      <td>5094589</td>\n","      <td>915</td>\n","      <td>https://stackoverflow.com/questions/71619495</td>\n","      <td>Documentation Ambiguity</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>73794766</td>\n","      <td>what is the meaning of axis=-1 in tf.keras.lay...</td>\n","      <td>&lt;p&gt;I'm trying to learn deep learning using ker...</td>\n","      <td>tf.keras.layers.Normalization</td>\n","      <td>tf.keras.layers.Normalization</td>\n","      <td>2021-04-22 10:07:35</td>\n","      <td>19986715</td>\n","      <td>1</td>\n","      <td>https://stackoverflow.com/questions/73794766</td>\n","      <td>Documentation Completeness</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>76396532</td>\n","      <td>Ragged tensors in dataset, tensorflow, how do ...</td>\n","      <td>&lt;p&gt;I have&lt;/p&gt;\\n&lt;pre&gt;&lt;code&gt;def call (self, inpu...</td>\n","      <td>tf.data.Dataset</td>\n","      <td>tf.data.Dataset</td>\n","      <td>2023-04-12 14:26:12</td>\n","      <td>13154958</td>\n","      <td>1</td>\n","      <td>https://stackoverflow.com/questions/76396532</td>\n","      <td>Documentation Replicability</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>75640862</td>\n","      <td>tf.py_function is only for Eager Mode?</td>\n","      <td>&lt;p&gt;Is &lt;code&gt;tf.py_function&lt;/code&gt; only for Eag...</td>\n","      <td>tf.py_function</td>\n","      <td>tf.py_function</td>\n","      <td>2022-06-27 12:56:46</td>\n","      <td>4281353</td>\n","      <td>20088</td>\n","      <td>https://stackoverflow.com/questions/75640862</td>\n","      <td>Documentation Replicability</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>75639137</td>\n","      <td>TF1 to TF2 migration</td>\n","      <td>&lt;p&gt;Hello I am new to tensorflow and I am worki...</td>\n","      <td>tf.compat.v1.placeholder</td>\n","      <td>tf.compat.v1.placeholder</td>\n","      <td>2022-05-21 11:25:40</td>\n","      <td>15822972</td>\n","      <td>101</td>\n","      <td>https://stackoverflow.com/questions/75639137</td>\n","      <td>Documentation Replicability</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>75478235</td>\n","      <td>tf.image.ssim() not accepting 'return_index_ma...</td>\n","      <td>&lt;p&gt;The documentation for Tensorflow's &lt;a href=...</td>\n","      <td>tf.image.ssim</td>\n","      <td>tf.image.ssim</td>\n","      <td>2022-02-28 8:05:44</td>\n","      <td>20324823</td>\n","      <td>1</td>\n","      <td>https://stackoverflow.com/questions/75478235</td>\n","      <td>Documentation Replicability</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>74060508</td>\n","      <td>How to Save a Tensorflow Dataset</td>\n","      <td>&lt;p&gt;As the title says I'm trying to save a &lt;cod...</td>\n","      <td>tf.data.Dataset</td>\n","      <td>tf.data.Dataset</td>\n","      <td>2021-05-13 17:54:55</td>\n","      <td>7875444</td>\n","      <td>298</td>\n","      <td>https://stackoverflow.com/questions/74060508</td>\n","      <td>Documentation Replicability</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>76040030</td>\n","      <td>Problem using Huggingface imagenet-1k dataset ...</td>\n","      <td>&lt;p&gt;I'm having a problem using the imagenet-1k ...</td>\n","      <td>tf.Tensor</td>\n","      <td>tf.Tensor</td>\n","      <td>2023-01-16 16:19:12</td>\n","      <td>21668078</td>\n","      <td>1</td>\n","      <td>https://stackoverflow.com/questions/76040030</td>\n","      <td>Documentation Replication on Other Examples</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>75939760</td>\n","      <td>Slow performance of tf.gather due to index val...</td>\n","      <td>&lt;p&gt;I am using tensorflow to train &lt;a href=\"htt...</td>\n","      <td>tf.gather</td>\n","      <td>tf.gather</td>\n","      <td>2022-08-21 18:58:29</td>\n","      <td>13230142</td>\n","      <td>1</td>\n","      <td>https://stackoverflow.com/questions/75939760</td>\n","      <td>Documentation Replication on Other Examples</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>75572543</td>\n","      <td>What to look out for when passing a generator ...</td>\n","      <td>&lt;p&gt;I want to replace the x and y training data...</td>\n","      <td>tf.keras.Model</td>\n","      <td>tf.keras.Model</td>\n","      <td>2022-04-01 18:22:14</td>\n","      <td>8536211</td>\n","      <td>360</td>\n","      <td>https://stackoverflow.com/questions/75572543</td>\n","      <td>Documentation Replication on Other Examples</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>72749893</td>\n","      <td>Optimizer.apply_gradients creating variables i...</td>\n","      <td>&lt;p&gt;I have created a neural style transfer with...</td>\n","      <td>tf.keras.optimizers.Optimizer</td>\n","      <td>tf.keras.optimizers.Optimizer</td>\n","      <td>2021-02-03 1:28:57</td>\n","      <td>17819542</td>\n","      <td>13</td>\n","      <td>https://stackoverflow.com/questions/72749893</td>\n","      <td>Documentation Replication on Other Examples</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>71933464</td>\n","      <td>How to make true_fn of tf.cond skip a for loop...</td>\n","      <td>&lt;p&gt;I want to use &lt;code&gt;tf.cond&lt;/code&gt; to mimic...</td>\n","      <td>tf.cond</td>\n","      <td>tf.cond</td>\n","      <td>2021-01-06 12:45:56</td>\n","      <td>4982651</td>\n","      <td>117</td>\n","      <td>https://stackoverflow.com/questions/71933464</td>\n","      <td>Documentation Replication on Other Examples</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>76447508</td>\n","      <td>How to retrain a model that was saved using th...</td>\n","      <td>&lt;p&gt;I am building a Neural Machine Translator f...</td>\n","      <td>tf.saved_model.load</td>\n","      <td>tf.saved_model.load</td>\n","      <td>2023-04-17 23:39:34</td>\n","      <td>16851318</td>\n","      <td>41</td>\n","      <td>https://stackoverflow.com/questions/76447508</td>\n","      <td>Inadequate Examples</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>76244268</td>\n","      <td>Tensorflow: Build new model from input and mid...</td>\n","      <td>&lt;p&gt;I'm trying to build &lt;code&gt;new_model&lt;/code&gt; ...</td>\n","      <td>tf.keras.applications.efficientnet_v2</td>\n","      <td>tf.keras.applications.efficientnet_v2</td>\n","      <td>2023-03-04 23:59:16</td>\n","      <td>2103321</td>\n","      <td>75</td>\n","      <td>https://stackoverflow.com/questions/76244268</td>\n","      <td>Inadequate Examples</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>72329108</td>\n","      <td>Is there a simple way to know which Tensorflow...</td>\n","      <td>&lt;p&gt;I have been trying to optimize some Tensorf...</td>\n","      <td>tf.cast</td>\n","      <td>tf.cast</td>\n","      <td>2021-01-16 21:00:36</td>\n","      <td>19167343</td>\n","      <td>11</td>\n","      <td>https://stackoverflow.com/questions/72329108</td>\n","      <td>Inadequate Examples</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>71130645</td>\n","      <td>Correct axes to use dot product to evaluate th...</td>\n","      <td>&lt;p&gt;I'm not being able to find the correct conf...</td>\n","      <td>tf.keras.layers</td>\n","      <td>tf.keras.layers</td>\n","      <td>2020-12-08 14:55:46</td>\n","      <td>13262684</td>\n","      <td>35</td>\n","      <td>https://stackoverflow.com/questions/71130645</td>\n","      <td>Inadequate Examples</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>66038861</td>\n","      <td>Why are both branches in tf.cond being execute...</td>\n","      <td>&lt;p&gt;I am using keras for a while now, but usual...</td>\n","      <td>tf.while_loop</td>\n","      <td>tf.while_loop</td>\n","      <td>2020-05-15 22:22:30</td>\n","      <td>15141021</td>\n","      <td>1</td>\n","      <td>https://stackoverflow.com/questions/66038861</td>\n","      <td>Inadequate Examples</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>76012810</td>\n","      <td>Unable to extract output probability array usi...</td>\n","      <td>&lt;p&gt;New to Javascript/Typescript + ML libs. Cre...</td>\n","      <td>tf.Tensor</td>\n","      <td>tf.Tensor</td>\n","      <td>2022-11-16 6:37:02</td>\n","      <td>4463330</td>\n","      <td>555</td>\n","      <td>https://stackoverflow.com/questions/76012810</td>\n","      <td>Lack of Alternative Solutions/Documentation</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>70328363</td>\n","      <td>Extra dimension to MaxPool1D layer from Conv1D...</td>\n","      <td>&lt;p&gt;I'm very new to Tensorflow (this is my firs...</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>2020-11-10 12:44:46</td>\n","      <td>8163401</td>\n","      <td>289</td>\n","      <td>https://stackoverflow.com/questions/70328363</td>\n","      <td>Lack of Alternative Solutions/Documentation</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>69792031</td>\n","      <td>Explanation of tf.keras.layers.CategoryEncodin...</td>\n","      <td>&lt;h1&gt;Question&lt;/h1&gt;\\n&lt;p&gt;Please help understand t...</td>\n","      <td>tf.keras.layers.CategoryEncoding</td>\n","      <td>tf.keras.layers.CategoryEncoding</td>\n","      <td>2020-10-14 15:04:24</td>\n","      <td>4281353</td>\n","      <td>20088</td>\n","      <td>https://stackoverflow.com/questions/69792031</td>\n","      <td>Lack of Alternative Solutions/Documentation</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>68354367</td>\n","      <td>Getting an error when using tf.keras.metrics.M...</td>\n","      <td>&lt;p&gt;I'm trying to add a Mean metric to a Keras ...</td>\n","      <td>tf.keras.metrics.Mean</td>\n","      <td>tf.keras.metrics.Mean</td>\n","      <td>2020-09-03 18:59:47</td>\n","      <td>6133787</td>\n","      <td>361</td>\n","      <td>https://stackoverflow.com/questions/68354367</td>\n","      <td>Lack of Alternative Solutions/Documentation</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>66049816</td>\n","      <td>Custom layer in sequential model tensorflow</td>\n","      <td>&lt;p&gt;I'm trying to create a custom layer for my ...</td>\n","      <td>tf.keras.layers.Layer</td>\n","      <td>tf.keras.layers.Layer</td>\n","      <td>2020-05-18 21:11:02</td>\n","      <td>12338521</td>\n","      <td>181</td>\n","      <td>https://stackoverflow.com/questions/66049816</td>\n","      <td>Lack of Alternative Solutions/Documentation</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>76380927</td>\n","      <td>Tensorflow decode image</td>\n","      <td>&lt;p&gt;I am a beginner in tensorflow and I am trai...</td>\n","      <td>tf.io.decode_image</td>\n","      <td>tf.io.decode_image</td>\n","      <td>2023-03-05 8:02:03</td>\n","      <td>15460221</td>\n","      <td>11</td>\n","      <td>https://stackoverflow.com/questions/76380927</td>\n","      <td>Requesting (Additional) Documentation/Examples</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    QuestionId                                              Title  \\\n","0     76391276         Custom gradient for broadcasting operation   \n","1     76153107    Difference between tf.Module and tf.keras.Model   \n","2     75996642  Is there a good equivalent of pandas' `apply` ...   \n","3     74434308  Setting only global level seed gives same outp...   \n","4     71619495  Image normalization by tf.image.convert_image_...   \n","5     73794766  what is the meaning of axis=-1 in tf.keras.lay...   \n","6     76396532  Ragged tensors in dataset, tensorflow, how do ...   \n","7     75640862             tf.py_function is only for Eager Mode?   \n","8     75639137                               TF1 to TF2 migration   \n","9     75478235  tf.image.ssim() not accepting 'return_index_ma...   \n","10    74060508                   How to Save a Tensorflow Dataset   \n","11    76040030  Problem using Huggingface imagenet-1k dataset ...   \n","12    75939760  Slow performance of tf.gather due to index val...   \n","13    75572543  What to look out for when passing a generator ...   \n","14    72749893  Optimizer.apply_gradients creating variables i...   \n","15    71933464  How to make true_fn of tf.cond skip a for loop...   \n","16    76447508  How to retrain a model that was saved using th...   \n","17    76244268  Tensorflow: Build new model from input and mid...   \n","18    72329108  Is there a simple way to know which Tensorflow...   \n","19    71130645  Correct axes to use dot product to evaluate th...   \n","20    66038861  Why are both branches in tf.cond being execute...   \n","21    76012810  Unable to extract output probability array usi...   \n","22    70328363  Extra dimension to MaxPool1D layer from Conv1D...   \n","23    69792031  Explanation of tf.keras.layers.CategoryEncodin...   \n","24    68354367  Getting an error when using tf.keras.metrics.M...   \n","25    66049816        Custom layer in sequential model tensorflow   \n","26    76380927                            Tensorflow decode image   \n","\n","                                                 Body  \\\n","0   <p>I have an operation for which I want to def...   \n","1   <p>I know both <code>tf.Module</code> and <cod...   \n","2   <p><strong>BACKGROUND</strong></p>\\n<p>The use...   \n","3   <p>I am testing out the <code>tf.random.set_se...   \n","4   <p>According to documentation <code>tf.image.c...   \n","5   <p>I'm trying to learn deep learning using ker...   \n","6   <p>I have</p>\\n<pre><code>def call (self, inpu...   \n","7   <p>Is <code>tf.py_function</code> only for Eag...   \n","8   <p>Hello I am new to tensorflow and I am worki...   \n","9   <p>The documentation for Tensorflow's <a href=...   \n","10  <p>As the title says I'm trying to save a <cod...   \n","11  <p>I'm having a problem using the imagenet-1k ...   \n","12  <p>I am using tensorflow to train <a href=\"htt...   \n","13  <p>I want to replace the x and y training data...   \n","14  <p>I have created a neural style transfer with...   \n","15  <p>I want to use <code>tf.cond</code> to mimic...   \n","16  <p>I am building a Neural Machine Translator f...   \n","17  <p>I'm trying to build <code>new_model</code> ...   \n","18  <p>I have been trying to optimize some Tensorf...   \n","19  <p>I'm not being able to find the correct conf...   \n","20  <p>I am using keras for a while now, but usual...   \n","21  <p>New to Javascript/Typescript + ML libs. Cre...   \n","22  <p>I'm very new to Tensorflow (this is my firs...   \n","23  <h1>Question</h1>\\n<p>Please help understand t...   \n","24  <p>I'm trying to add a Mean metric to a Keras ...   \n","25  <p>I'm trying to create a custom layer for my ...   \n","26  <p>I am a beginner in tensorflow and I am trai...   \n","\n","                             QuestionsAPI  \\\n","0                      tf.custom_gradient   \n","1                               tf.Module   \n","2                          tf.py_function   \n","3                      tf.random.set_seed   \n","4            tf.image.convert_image_dtype   \n","5           tf.keras.layers.Normalization   \n","6                         tf.data.Dataset   \n","7                          tf.py_function   \n","8                tf.compat.v1.placeholder   \n","9                           tf.image.ssim   \n","10                        tf.data.Dataset   \n","11                              tf.Tensor   \n","12                              tf.gather   \n","13                         tf.keras.Model   \n","14          tf.keras.optimizers.Optimizer   \n","15                                tf.cond   \n","16                    tf.saved_model.load   \n","17  tf.keras.applications.efficientnet_v2   \n","18                                tf.cast   \n","19                        tf.keras.layers   \n","20                          tf.while_loop   \n","21                              tf.Tensor   \n","22                                     -    \n","23       tf.keras.layers.CategoryEncoding   \n","24                  tf.keras.metrics.Mean   \n","25                  tf.keras.layers.Layer   \n","26                     tf.io.decode_image   \n","\n","                               CurrentAPI         CreationDate    UserId  \\\n","0                      tf.custom_gradient  2023-04-05 13:00:20   1782792   \n","1                               tf.Module  2023-02-16 21:44:50  21760922   \n","2                          tf.py_function  2022-11-14 15:44:35   5640161   \n","3                      tf.random.set_seed  2021-06-28 11:48:57   7422352   \n","4            tf.image.convert_image_dtype  2020-12-28 17:31:25   5094589   \n","5           tf.keras.layers.Normalization  2021-04-22 10:07:35  19986715   \n","6                         tf.data.Dataset  2023-04-12 14:26:12  13154958   \n","7                          tf.py_function  2022-06-27 12:56:46   4281353   \n","8                tf.compat.v1.placeholder  2022-05-21 11:25:40  15822972   \n","9                           tf.image.ssim   2022-02-28 8:05:44  20324823   \n","10                        tf.data.Dataset  2021-05-13 17:54:55   7875444   \n","11                              tf.Tensor  2023-01-16 16:19:12  21668078   \n","12                              tf.gather  2022-08-21 18:58:29  13230142   \n","13                         tf.keras.Model  2022-04-01 18:22:14   8536211   \n","14          tf.keras.optimizers.Optimizer   2021-02-03 1:28:57  17819542   \n","15                                tf.cond  2021-01-06 12:45:56   4982651   \n","16                    tf.saved_model.load  2023-04-17 23:39:34  16851318   \n","17  tf.keras.applications.efficientnet_v2  2023-03-04 23:59:16   2103321   \n","18                                tf.cast  2021-01-16 21:00:36  19167343   \n","19                        tf.keras.layers  2020-12-08 14:55:46  13262684   \n","20                          tf.while_loop  2020-05-15 22:22:30  15141021   \n","21                              tf.Tensor   2022-11-16 6:37:02   4463330   \n","22                                      -  2020-11-10 12:44:46   8163401   \n","23       tf.keras.layers.CategoryEncoding  2020-10-14 15:04:24   4281353   \n","24                  tf.keras.metrics.Mean  2020-09-03 18:59:47   6133787   \n","25                  tf.keras.layers.Layer  2020-05-18 21:11:02  12338521   \n","26                     tf.io.decode_image   2023-03-05 8:02:03  15460221   \n","\n","    UserReputation                                   QuestionURL  \\\n","0            58906  https://stackoverflow.com/questions/76391276   \n","1               33  https://stackoverflow.com/questions/76153107   \n","2              791  https://stackoverflow.com/questions/75996642   \n","3             5021  https://stackoverflow.com/questions/74434308   \n","4              915  https://stackoverflow.com/questions/71619495   \n","5                1  https://stackoverflow.com/questions/73794766   \n","6                1  https://stackoverflow.com/questions/76396532   \n","7            20088  https://stackoverflow.com/questions/75640862   \n","8              101  https://stackoverflow.com/questions/75639137   \n","9                1  https://stackoverflow.com/questions/75478235   \n","10             298  https://stackoverflow.com/questions/74060508   \n","11               1  https://stackoverflow.com/questions/76040030   \n","12               1  https://stackoverflow.com/questions/75939760   \n","13             360  https://stackoverflow.com/questions/75572543   \n","14              13  https://stackoverflow.com/questions/72749893   \n","15             117  https://stackoverflow.com/questions/71933464   \n","16              41  https://stackoverflow.com/questions/76447508   \n","17              75  https://stackoverflow.com/questions/76244268   \n","18              11  https://stackoverflow.com/questions/72329108   \n","19              35  https://stackoverflow.com/questions/71130645   \n","20               1  https://stackoverflow.com/questions/66038861   \n","21             555  https://stackoverflow.com/questions/76012810   \n","22             289  https://stackoverflow.com/questions/70328363   \n","23           20088  https://stackoverflow.com/questions/69792031   \n","24             361  https://stackoverflow.com/questions/68354367   \n","25             181  https://stackoverflow.com/questions/66049816   \n","26              11  https://stackoverflow.com/questions/76380927   \n","\n","                                         IssueType  \n","0                          Documentation Ambiguity  \n","1                          Documentation Ambiguity  \n","2                          Documentation Ambiguity  \n","3                          Documentation Ambiguity  \n","4                          Documentation Ambiguity  \n","5                       Documentation Completeness  \n","6                      Documentation Replicability  \n","7                      Documentation Replicability  \n","8                      Documentation Replicability  \n","9                      Documentation Replicability  \n","10                     Documentation Replicability  \n","11     Documentation Replication on Other Examples  \n","12     Documentation Replication on Other Examples  \n","13     Documentation Replication on Other Examples  \n","14     Documentation Replication on Other Examples  \n","15     Documentation Replication on Other Examples  \n","16                             Inadequate Examples  \n","17                             Inadequate Examples  \n","18                             Inadequate Examples  \n","19                             Inadequate Examples  \n","20                             Inadequate Examples  \n","21     Lack of Alternative Solutions/Documentation  \n","22     Lack of Alternative Solutions/Documentation  \n","23     Lack of Alternative Solutions/Documentation  \n","24     Lack of Alternative Solutions/Documentation  \n","25     Lack of Alternative Solutions/Documentation  \n","26  Requesting (Additional) Documentation/Examples  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["def select_rows(group):\n","    return group.head(5)\n","\n","result_df = question_queries.groupby('IssueType').apply(select_rows).reset_index(drop=True)\n","result_df"]},{"cell_type":"markdown","metadata":{"id":"o2DPBZqyerrs"},"source":["**Load the vector database**"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"IPG2Gz2sLhGt"},"outputs":[],"source":["so_vector_db = Chroma(persist_directory=os.path.join(s.VECTORDB_PATH,\"dataframe\"), embedding_function=embedding_function)\n","# tf_vector_db = Chroma(persist_directory=os.path.join(s.VECTORDB_PATH,\"markdown\"), embedding_function=embedding_function)\n","yt_vector_db = Chroma(persist_directory=os.path.join(s.VECTORDB_PATH,\"transcripts\"), embedding_function=embedding_function)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 'context', 'title','body', 'issue_type','definition', 'documentation', 'task'\n","for index, row in result_df.iterrows():\n","    try:\n","        so_with_score = so_vector_db.similarity_search_with_relevance_scores(row['Body'], k=3)\n","        yt_with_score = yt_vector_db.similarity_search_with_relevance_scores(row['Body'], k=3)\n","\n","        docs = []\n","        video_urls = []\n","        # Obtain semantic similarity with the score with respect to the query (SO question)\n","        for doc, score in so_with_score:\n","            if score > 0.6:\n","                docs.append(doc.page_content)\n","\n","        # Execute langchain agent to call Tavily search API using question title as the query\n","        res = agent_executor.invoke({\"input\": row['Title']})\n","        docs.append(res[\"output\"])\n","        \n","        for script, score in yt_with_score:\n","            if score > 0.6:\n","                video_urls.append(doc.page_content)\n","\n","        issue_type = row['IssueType']\n","        definition, task = hf.prompt_task(issue_type)\n","        documentation = hf.read_markdown_file(row['CurrentAPI'])\n","\n","        prompts = [\n","            {\n","                \"context\" : '\\n'.join(docs),\n","                \"title\" : row['Title'],\n","                \"body\" : row['Body'],\n","                \"issue_type\" : issue_type,\n","                \"definition\" : definition,\n","                \"api_name\":row['CurrentAPI'],\n","                \"documentation\": documentation,\n","                \"task\": task,\n","            },\n","        ]\n","\n","        synopsis_chain.apply(prompts)\n","        mlflow_callback.flush_tracker(synopsis_chain)\n","    except:\n","        pass"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["from langchain import hub\n","prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n","prompt"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Answer the following questions as best you can. You have access to the following tools:\n","\n","{tools}\n","\n","Use the following format:\n","\n","Question: the input question you must answer\n","Thought: you should always think about what to do\n","Action: the action to take, should be one of [{tool_names}]\n","Action Input: the input to the action\n","Observation: the result of the action\n","... (this Thought/Action/Action Input/Observation can repeat N times)\n","Thought: I now know the final answer\n","Final Answer: the final answer to the original input question\n","\n","Begin!\n","\n","Question: {input}\n","Thought:{agent_scratchpad}\n"]}],"source":["val = 'Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}'\n","print(val)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["PromptTemplate(input_variables=['agent_scratchpad', 'input', 'tools'], template=\"You are an expert at scraping websites. Your task is to web scrape a given URL and extract all the code \\npresent on that page, as well as extract relevant context from the description of the webpage.\\n\\nYou have access to the following tools:\\n\\n{tools}\\n\\nIn order to use a tool, you can use <tool></tool> and <tool_input></tool_input> tags. You will then get back a response in the form <observation></observation>\\nFor example, if you have a tool called 'search' that could run a google search, in order to search for the weather in SF you would respond:\\n\\n<tool>search</tool><tool_input>weather in SF</tool_input>\\n<observation>64 degrees</observation>\\n\\nWhen you are done, respond with a final answer between <final_answer></final_answer>. For example:\\n\\n<final_answer>The weather in SF is 64 degrees</final_answer>\\n\\nBegin!\\n\\nURL: {input}\\n{agent_scratchpad}\")"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["from langchain_core.prompts import PromptTemplate\n","\n","template = '''You are an expert at scraping websites. Your task is to web scrape a given URL and extract all the code \n","present on that page, as well as extract relevant context from the description of the webpage.\n","\n","You have access to the following tools:\n","\n","{tools}\n","\n","In order to use a tool, you can use <tool></tool> and <tool_input></tool_input> tags. You will then get back a response in the form <observation></observation>\n","For example, if you have a tool called 'search' that could run a google search, in order to search for the weather in SF you would respond:\n","\n","<tool>search</tool><tool_input>weather in SF</tool_input>\n","<observation>64 degrees</observation>\n","\n","When you are done, respond with a final answer between <final_answer></final_answer>. For example:\n","\n","<final_answer>The weather in SF is 64 degrees</final_answer>\n","\n","Begin!\n","\n","URL: {input}\n","{agent_scratchpad}'''\n","prompt = PromptTemplate.from_template(template)\n","prompt"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Identifying intent of the Stack Overflow question based on the question body......\n","Completed Successfully\n","\n","Retrieving relevant context from web using Cohere RAG Retriever....\n","Completed Successfully\n","\n"]}],"source":["from components.AIWebSearch.cohere_rag_retriever import relevant_context_retriever\n","from components.QuestionSummarizer.claude_summarizer import question_summarizer\n","\n","title = \"TF1 to TF2 migration\"\n","body = \"\"\"\"\n","    <p>Hello I am new to tensorflow and I am working on a code that I would like to migrate from tensorflow 1 to 2. I have this line of code:</p>\n","    <pre><code>x1 = tf.compat.v1.placeholder(tf.float32, [], name=&quot;x1&quot;)\n","    </code></pre>\n","    <p>As mentioned in <a href=\"https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder</a>, I should use <code>keras.Input</code>. But even when specifying the shape, I can't have the same tensor as with compat.v1:</p>\n","    <pre><code>x2 = tf.keras.Input(shape=[], dtype=tf.float32, name=&quot;x2&quot;)\n","    </code></pre>\n","    <p>To check the shape I use <code>tf.shape(x1)</code> or <code>tf.shape(x2)</code>, but the shapes are not the same. Could anyone explain to me how to have, in TF2, the same shape as in TF1 ?\n","    Thanks and regards</p>\n","\"\"\"\n","api_name = \"tf.compat.v1.placeholder\"\n","\n","question_result = question_summarizer(title, body)\n","intent = question_result[0]['text']\n","model_response, docs, urls = relevant_context_retriever(intent)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["'The user is seeking guidance on how to correctly migrate TensorFlow 1 code to TensorFlow 2, specifically on how to replicate the functionality and output of the `tf.compat.v1.placeholder` with `tf.keras.Input` in terms of tensor shape. They are looking for an explanation or solution that would allow them to achieve the same tensor shape in TensorFlow 2 as they had in TensorFlow 1, despite having already attempted to follow the official TensorFlow documentation.'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["intent"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["\"To migrate TensorFlow 1 code to TensorFlow 2, follow these steps:\\n1. Run an automated script to convert your TF1.x API usage to `tf.compat.v1`.\\n2. Remove old `tf.contrib.layers` and replace them with TF Slim symbols.\\n3. Rewrite your TF1.x model forward passes to run in TF2 with eager execution enabled.\\n4. Validate the accuracy and numerical correctness of your migrated code.\\n5. Upgrade your training, evaluation and model saving code to TF2 equivalents. \\n6. (Optional) Migrate your TF2-compatible `tf.compat.v1` APIs including TF Slim usage to idiomatic TF2 APIs.\\n\\nIn TensorFlow 2, the `tf.keras.Input` produces a symbolic tensor-like object, similar to a placeholder. Here's an example of how to convert this:\\n```\\ntf.placeholder(tf.float32, [None, n, p])\\n```\\n\\n becomes\\n\\n```\\ntf.keras.Input(shape=(32,))\\n```\\nThe shape parameter is required and should be set to match the shape of your tensor.\\n\\nTensorFlow 2 also introduces `tf.function`, which allows you to convert data-dependent control flow into graph-mode equivalents. You can use this to replicate the functionality of `tf.compat.v1.placeholder`.\\n\\nAdditionally, some default learning rates have changed in TensorFlow 2, which may affect your code's convergence behaviour. Check the default learning rates of the optimizers you're using.\""]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["model_response"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["[Document(page_content='Copyright 2018 The TensorFlow Authors.\\n\\n#@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.\\n\\nMigrate your TensorFlow 1 code to TensorFlow 2\\n\\nView on TensorFlow.org\\n\\nView source on GitHub\\n\\nThis doc for users of low level TensorFlow APIs. If you are using the high level APIs (tf.keras) there may be little or no action you need to take to make your code fully TensorFlow 2.0 compatible:\\n\\nCheck your optimizer\\'s default learning rate.\\n\\nNote that the \"name\" that metrics are logged to may have changed.\\n\\nIt is still possible to run 1.X code, unmodified (except for contrib), in TensorFlow 2.0:\\n\\nimport tensorflow.compat.v1 as tf tf.disable_v2_behavior()\\n\\nHowever, this does not let you take advantage of many of the improvements made in TensorFlow 2.0. This guide will help you upgrade your code, making it simpler, more performant, and easier to maintain.\\n\\nAutomatic conversion script\\n\\nThe first step, before attempting to implement the changes described in this doc, is to try running the upgrade script.\\n\\nThis will do an initial pass at upgrading your code to TensorFlow 2.0. But it can\\'t make your code idiomatic to 2.0. Your code may still make use of tf.compat.v1 endpoints to access placeholders, sessions, collections, and other 1.x-style functionality.\\n\\nTop-level behavioral changes\\n\\nIf your code works in TensorFlow 2.0 using tf.compat.v1.disable_v2_behavior(), there are still global behavioral changes you may need to address. The major changes are:\\n\\nEager execution, v1.enable_eager_execution() : Any code that implicitly uses a tf.Graph will fail. Be sure to wrap this code in a with tf.Graph().as_default() context.\\n\\nResource variables, v1.enable_resource_variables(): Some code may depends on non-deterministic behaviors enabled by TF reference variables. Resource variables are locked while being written to, and so provide more intuitive consistency guarantees.\\n\\nThis may change behavior in edge cases.\\n\\nThis may create extra copies and can have higher memory usage.\\n\\nThis can be disabled by passing use_resource=False to the tf.Variable constructor.\\n\\nTensor shapes, v1.enable_v2_tensorshape(): TF 2.0 simplifies the behavior of tensor shapes. Instead of t.shape[0].value you can say t.shape[0]. These changes should be small, and it makes sense to fix them right away. See TensorShape for examples.\\n\\nControl flow, v1.enable_control_flow_v2(): The TF 2.0 control flow implementation has been simplified, and so produces different graph representations. Please file bugs for any issues.\\n\\nMake the code 2.0-native\\n\\nThis guide will walk through several examples of converting TensorFlow 1.x code to TensorFlow 2.0. These changes will let your code take advantage of performance optimizations and simplified API calls.\\n\\nIn each case, the pattern is:\\n\\n1. Replace v1.Session.run calls\\n\\nEvery v1.Session.run call should be replaced by a Python function.\\n\\nThe feed_dict and v1.placeholders become function arguments.\\n\\nThe fetches become the function\\'s return value.\\n\\nDuring conversion eager execution allows easy debugging with standard Python tools like pdb.\\n\\nAfter that add a tf.function decorator to make it run efficiently in graph. See the Autograph Guide for more on how this works.\\n\\nUnlike v1.Session.run a tf.function has a fixed return signature, and always returns all outputs. If this causes performance problems, create two separate functions.\\n\\nThere is no need for a tf.control_dependencies or similar operations: A tf.function behaves as if it were run in the order written. tf.Variable assignments and tf.asserts, for example, are executed automatically.\\n\\n2. Use Python objects to track variables and losses\\n\\nAll name-based variable tracking is strongly discouraged in TF 2.0. Use Python objects to to track variables.\\n\\nUse tf.Variable instead of v1.get_variable.\\n\\nEvery v1.variable_scope should be converted to a Python object. Typically this will be one of:\\n\\ntf.keras.layers.Layer\\n\\nIf you need to aggregate lists of variables (like tf.Graph.get_collection(tf.GraphKeys.VARIABLES)), use the .variables and .trainable_variables attributes of the Layer and Model objects.\\n\\nThese Layer and Model classes implement several other properties that remove the need for global collections. Their .losses property can be a replacement for using the tf.GraphKeys.LOSSES collection.\\n\\nSee the keras guides for details.\\n\\nWarning: Many tf.compat.v1 symbols use the global collections implicitly.\\n\\n3. Upgrade your training loops\\n\\nUse the highest level API that works for your use case. Prefer tf.keras.Model.fit over building your own training loops.\\n\\nThese high level functions manage a lot of the low-level details that might be easy to miss if you write your own training loop. For example, they automatically collect the regularization losses, and set the training=True argument when calling the model.\\n\\n4. Upgrade your data input pipelines\\n\\nUse tf.data datasets for data input. These objects are efficient, expressive, and integrate well with tensorflow.\\n\\nThey can be passed directly to the tf.keras.Model.fit method.\\n\\nmodel.fit(dataset, epochs=5)\\n\\nThey can be iterated over directly standard Python:\\n\\nfor example_batch, label_batch in dataset: break\\n\\n5. Migrate off compat.v1 symbols\\n\\nThe tf.compat.v1 module contains the complete TensorFlow 1.x API, with its original semantics.\\n\\nThe TF2 upgrade script will convert symbols to their 2.0 equivalents if such a conversion is safe, i.e., if it can determine that the behavior of the 2.0 version is exactly equivalent (for instance, it will rename v1.arg_max to tf.argmax, since those are the same function).\\n\\nAfter the upgrade script is done with a piece of code, it is likely there are many mentions of compat.v1. It is worth going through the code and converting these manually to the 2.0 equivalent (it should be mentioned in the log if there is one).\\n\\nimport tensorflow as tf import tensorflow_datasets as tfds\\n\\nLow-level variables & operator execution\\n\\nExamples of low-level API use include:\\n\\nusing variable scopes to control reuse\\n\\ncreating variables with v1.get_variable.\\n\\naccessing collections explicitly\\n\\naccessing collections implicitly with methods like :\\n\\nv1.losses.get_regularization_loss\\n\\nusing v1.placeholder to set up graph inputs\\n\\nexecuting graphs with Session.run\\n\\ninitializing variables manually\\n\\nHere is what these patterns may look like in code using TensorFlow 1.x.\\n\\nin_a = tf.placeholder(dtype=tf.float32, shape=(2)) in_b = tf.placeholder(dtype=tf.float32, shape=(2)) def forward(x): with tf.variable_scope(\"matmul\", reuse=tf.AUTO_REUSE): W = tf.get_variable(\"W\", initializer=tf.ones(shape=(2,2)), regularizer=tf.contrib.layers.l2_regularizer(0.04)) b = tf.get_variable(\"b\", initializer=tf.zeros(shape=(2))) return W * x + b out_a = forward(in_a) out_b = forward(in_b) reg_loss=tf.losses.get_regularization_loss(scope=\"matmul\") with tf.Session() as sess: sess.run(tf.global_variables_initializer()) outs = sess.run([out_a, out_b, reg_loss], feed_dict={in_a: [1, 0], in_b: [0, 1]})\\n\\nIn the converted code:\\n\\nThe variables are local Python objects.\\n\\nThe forward function still defines the calculation.\\n\\nThe Session.run call is replaced with a call to forward\\n\\nThe optional tf.function decorator can be added for performance.\\n\\nThe regularizations are calculated manually, without referring to any global collection.\\n\\nNo sessions or placeholders.\\n\\nW = tf.Variable(tf.ones(shape=(2,2)), name=\"W\") b = tf.Variable(tf.zeros(shape=(2)), name=\"b\") @tf.function def forward(x): return W * x + b out_a = forward([1,0]) print(out_a)\\n\\nout_b = forward([0,1]) regularizer = tf.keras.regularizers.l2(0.04) reg_loss=regularizer(W)\\n\\nModels based on tf.layers\\n\\nThe v1.layers module is used to contain layer-functions that relied on v1.variable_scope to define and reuse variables.\\n\\ndef model(x, training, scope=\\'model\\'): with tf.variable_scope(scope, reuse=tf.AUTO_REUSE): x = tf.layers.conv2d(x, 32, 3, activation=tf.nn.relu, kernel_regularizer=tf.contrib.layers.l2_regularizer(0.04)) x = tf.layers.max_pooling2d(x, (2, 2), 1) x = tf.layers.flatten(x) x = tf.layers.dropout(x, 0.1, training=training) x = tf.layers.dense(x, 64, activation=tf.nn.relu) x = tf.layers.batch_normalization(x, training=training) x = tf.layers.dense(x, 10) return x train_out = model(train_data, training=True) test_out = model(test_data, training=False)\\n\\nThe simple stack of layers fits neatly into tf.keras.Sequential. (For more complex models see custom layers and models, and the functional API.)\\n\\nThe model tracks the variables, and regularization losses.\\n\\nThe conversion was one-to-one because there is a direct mapping from v1.layers to tf.keras.layers.\\n\\nMost arguments stayed the same. But notice the differences:\\n\\nThe training argument is passed to each layer by the model when it runs.\\n\\nThe first argument to the original model function (the input x) is gone. This is because object layers separate building the model from calling the model.\\n\\nIf you were using regularizers of initializers from tf.contrib, these have more argument changes than others.\\n\\nThe code no longer writes to collections, so functions like v1.losses.get_regularization_loss will no longer return these values, potentially breaking your training loops.\\n\\nmodel = tf.keras.Sequential([ tf.keras.layers.Conv2D(32, 3, activation=\\'relu\\', kernel_regularizer=tf.keras.regularizers.l2(0.04), input_shape=(28, 28, 1)), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Flatten(), tf.keras.layers.Dropout(0.1), tf.keras.layers.Dense(64, activation=\\'relu\\'), tf.keras.layers.BatchNormalization(), tf.keras.layers.Dense(10) ]) train_data = tf.ones(shape=(1, 28, 28, 1)) test_data = tf.ones(shape=(1, 28, 28, 1))\\n\\ntrain_out = model(train_data, training=True) print(train_out)\\n\\ntest_out = model(test_data, training=False) print(test_out)\\n\\n# Here are all the trainable variables. len(model.trainable_variables)\\n\\n# Here is the regularization loss. model.losses\\n\\nMixed variables & v1.layers\\n\\nExisting code often mixes lower-level TF 1.x variables and operations with higher-level v1.layers.\\n\\ndef model(x, training, scope=\\'model\\'): with tf.variable_scope(scope, reuse=tf.AUTO_REUSE): W = tf.get_variable( \"W\", dtype=tf.float32, initializer=tf.ones(shape=x.shape), regularizer=tf.contrib.layers.l2_regularizer(0.04), trainable=True) if training: x = x + W else: x = x + W * 0.5 x = tf.layers.conv2d(x, 32, 3, activation=tf.nn.relu) x = tf.layers.max_pooling2d(x, (2, 2), 1) x = tf.layers.flatten(x) return x train_out = model(train_data, training=True) test_out = model(test_data, training=False)\\n\\nTo convert this code, follow the pattern of mapping layers to layers as in the previous example.\\n\\nThe general pattern is:\\n\\nCollect layer parameters in __init__.\\n\\nBuild the variables in build.\\n\\nExecute the calculations in call, and return the result.\\n\\nThe v1.variable_scope is essentially a layer of its own. So rewrite it as a tf.keras.layers.Layer. See the guide for details.\\n\\n# Create a custom layer for part of the model class CustomLayer(tf.keras.layers.Layer): def __init__(self, *args, **kwargs): super(CustomLayer, self).__init__(*args, **kwargs) def build(self, input_shape): self.w = self.add_weight( shape=input_shape[1:], dtype=tf.float32, initializer=tf.keras.initializers.ones(), regularizer=tf.keras.regularizers.l2(0.02), trainable=True) # Call method will sometimes get used in graph mode, # training will get turned into a tensor @tf.function def call(self, inputs, training=None): if training: return inputs + self.w else: return inputs + self.w * 0.5\\n\\ncustom_layer = CustomLayer() print(custom_layer([1]).numpy()) print(custom_layer([1], training=True).numpy())\\n\\ntrain_data = tf.ones(shape=(1, 28, 28, 1)) test_data = tf.ones(shape=(1, 28, 28, 1)) # Build the model including the custom layer model = tf.keras.Sequential([ CustomLayer(input_shape=(28, 28, 1)), tf.keras.layers.Conv2D(32, 3, activation=\\'relu\\'), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Flatten(), ]) train_out = model(train_data, training=True) test_out = model(test_data, training=False)\\n\\nSome things to note:\\n\\nSubclassed Keras models & layers need to run in both v1 graphs (no automatic control dependencies) and in eager mode\\n\\nWrap the call() in a tf.function() to get autograph and automatic control dependencies\\n\\nDon\\'t forget to accept a training argument to call.\\n\\nSometimes it is a tf.Tensor\\n\\nSometimes it is a Python boolean.\\n\\nCreate model variables in constructor or Model.build using self.add_weight().\\n\\nIn Model.build you have access to the input shape, so can create weights with matching shape.\\n\\nUsing tf.keras.layers.Layer.add_weight allows Keras to track variables and regularization losses.\\n\\nDon\\'t keep tf.Tensors in your objects.\\n\\nThey might get created either in a tf.function or in the eager context, and these tensors behave differently.\\n\\nUse tf.Variables for state, they are always usable from both contexts\\n\\ntf.Tensors are only for intermediate values.\\n\\nA note on Slim & contrib.layers\\n\\nA large amount of older TensorFlow 1.x code uses the Slim library, which was packaged with TensorFlow 1.x as tf.contrib.layers. As a contrib module, this is no longer available in TensorFlow 2.0, even in tf.compat.v1. Converting code using Slim to TF 2.0 is more involved than converting repositories that use v1.layers. In fact, it may make sense to convert your Slim code to v1.layers first, then convert to Keras.\\n\\nRemove arg_scopes, all args need to be explicit\\n\\nIf you use them, split normalizer_fn and activation_fn into their own layers\\n\\nSeparable conv layers map to one or more different Keras layers (depthwise, pointwise, and separable Keras layers)\\n\\nSlim and v1.layers have different arg names & default values\\n\\nSome args have different scales\\n\\nIf you use Slim pre-trained models, try out Keras\\'s pre-traimed models from tf.keras.applications or TF Hub\\'s TF2 SavedModels exported from the original Slim code.\\n\\nSome tf.contrib layers might not have been moved to core TensorFlow but have instead been moved to the TF add-ons package.\\n\\nThere are many ways to feed data to a tf.keras model. They will accept Python generators and Numpy arrays as input.\\n\\nThe recommended way to feed data to a model is to use the tf.data package, which contains a collection of high performance classes for manipulating data.\\n\\nIf you are still using tf.queue, these are now only supported as data-structures, not as input pipelines.\\n\\nThe TensorFlow Datasets package (tfds) contains utilities for loading predefined datasets as tf.data.Dataset objects.\\n\\nFor this example, load the MNISTdataset, using tfds:\\n\\ndatasets, info = tfds.load(name=\\'mnist\\', with_info=True, as_supervised=True) mnist_train, mnist_test = datasets[\\'train\\'], datasets[\\'test\\']\\n\\nThen prepare the data for training:\\n\\nRe-scale each image.\\n\\nShuffle the order of the examples.\\n\\nCollect batches of images and labels.\\n\\nBUFFER_SIZE = 10 # Use a much larger value for real code. BATCH_SIZE = 64 NUM_EPOCHS = 5 def scale(image, label): image = tf.cast(image, tf.float32) image /= 255 return image, label\\n\\nTo keep the example short, trim the dataset to only return 5 batches:\\n\\ntrain_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE) test_data = mnist_test.map(scale).batch(BATCH_SIZE) STEPS_PER_EPOCH = 5 train_data = train_data.take(STEPS_PER_EPOCH) test_data = test_data.take(STEPS_PER_EPOCH)\\n\\nimage_batch, label_batch = next(iter(train_data))\\n\\nUse Keras training loops\\n\\nIf you don\\'t need low level control of your training process, using Keras\\'s built-in fit, evaluate, and predict methods is recommended. These methods provide a uniform interface to train the model regardless of the implementation (sequential, functional, or sub-classed).\\n\\nThe advantages of these methods include:\\n\\nThey accept Numpy arrays, Python generators and, tf.data.Datasets\\n\\nThey apply regularization, and activation losses automatically.\\n\\nThey support tf.distribute for multi-device training.\\n\\nThey support arbitrary callables as losses and metrics.\\n\\nThey support callbacks like tf.keras.callbacks.TensorBoard, and custom callbacks.\\n\\nThey are performant, automatically using TensorFlow graphs.\\n\\nHere is an example of training a model using a Dataset. (For details on how this works see tutorials.)\\n\\nmodel = tf.keras.Sequential([ tf.keras.layers.Conv2D(32, 3, activation=\\'relu\\', kernel_regularizer=tf.keras.regularizers.l2(0.02), input_shape=(28, 28, 1)), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Flatten(), tf.keras.layers.Dropout(0.1), tf.keras.layers.Dense(64, activation=\\'relu\\'), tf.keras.layers.BatchNormalization(), tf.keras.layers.Dense(10) ]) # Model is the full model w/o custom layers model.compile(optimizer=\\'adam\\', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\\'accuracy\\']) model.fit(train_data, epochs=NUM_EPOCHS) loss, acc = model.evaluate(test_data) print(\"Loss {}, Accuracy {}\".format(loss, acc))\\n\\nIf the Keras model\\'s training step works for you, but you need more control outside that step, consider using the tf.keras.Model.train_on_batch method, in your own data-iteration loop.\\n\\nRemember: Many things can be implemented as a tf.keras.callbacks.Callback.\\n\\nThis method has many of the advantages of the methods mentioned in the previous section, but gives the user control of the outer loop.\\n\\nYou can also use tf.keras.Model.test_on_batch or tf.keras.Model.evaluate to check performance during training.\\n\\nNote: train_on_batch and test_on_batch, by default return the loss and metrics for the single batch. If you pass reset_metrics=False they return accumulated metrics and you must remember to appropriately reset the metric accumulators. Also remember that some metrics like AUC require reset_metrics=False to be calculated correctly.\\n\\nTo continue training the above model:\\n\\n# Model is the full model w/o custom layers model.compile(optimizer=\\'adam\\', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\\'accuracy\\']) for epoch in range(NUM_EPOCHS): #Reset the metric accumulators model.reset_metrics() for image_batch, label_batch in train_data: result = model.train_on_batch(image_batch, label_batch) metrics_names = model.metrics_names print(\"train: \", \"{}: {:.3f}\".format(metrics_names[0], result[0]), \"{}: {:.3f}\".format(metrics_names[1], result[1])) for image_batch, label_batch in test_data: result = model.test_on_batch(image_batch, label_batch, # return accumulated metrics reset_metrics=False) metrics_names = model.metrics_names print(\"\\\\neval: \", \"{}: {:.3f}\".format(metrics_names[0], result[0]), \"{}: {:.3f}\".format(metrics_names[1], result[1]))\\n\\nCustomize the training step\\n\\nIf you need more flexibility and control, you can have it by implementing your own training loop. There are three steps:\\n\\nIterate over a Python generator or tf.data.Dataset to get batches of examples.\\n\\nUse tf.GradientTape to collect gradients.\\n\\nUse one of the tf.keras.optimizers to apply weight updates to the model\\'s variables.\\n\\nAlways include a training argument on the call method of subclassed layers and models.\\n\\nMake sure to call the model with the training argument set correctly.\\n\\nDepending on usage, model variables may not exist until the model is run on a batch of data.\\n\\nYou need to manually handle things like regularization losses for the model.\\n\\nNote the simplifications relative to v1:\\n\\nThere is no need to run variable initializers. Variables are initialized on creation.\\n\\nThere is no need to add manual control dependencies. Even in tf.function operations act as in eager mode.\\n\\nmodel = tf.keras.Sequential([ tf.keras.layers.Conv2D(32, 3, activation=\\'relu\\', kernel_regularizer=tf.keras.regularizers.l2(0.02), input_shape=(28, 28, 1)), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Flatten(), tf.keras.layers.Dropout(0.1), tf.keras.layers.Dense(64, activation=\\'relu\\'), tf.keras.layers.BatchNormalization(), tf.keras.layers.Dense(10) ]) optimizer = tf.keras.optimizers.Adam(0.001) loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) @tf.function def train_step(inputs, labels): with tf.GradientTape() as tape: predictions = model(inputs, training=True) regularization_loss=tf.math.add_n(model.losses) pred_loss=loss_fn(labels, predictions) total_loss=pred_loss + regularization_loss gradients = tape.gradient(total_loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) for epoch in range(NUM_EPOCHS): for inputs, labels in train_data: train_step(inputs, labels) print(\"Finished epoch\", epoch)\\n\\nNew-style metrics and losses\\n\\nIn TensorFlow 2.0, metrics and losses are objects. These work both eagerly and in tf.functions.\\n\\nA loss object is callable, and expects the (y_true, y_pred) as arguments:\\n\\ncce = tf.keras.losses.CategoricalCrossentropy(from_logits=True) cce([[1, 0]], [[-1.0,3.0]]).numpy()\\n\\nA metric object has the following methods:\\n\\nMetric.update_state() — add new observations\\n\\nMetric.result() —get the current result of the metric, given the observed values\\n\\nMetric.reset_states() — clear all observations.\\n\\nThe object itself is callable. Calling updates the state with new observations, as with update_state, and returns the new result of the metric.\\n\\nYou don\\'t have to manually initialize a metric\\'s variables, and because TensorFlow 2.0 has automatic control dependencies, you don\\'t need to worry about those either.\\n\\nThe code below uses a metric to keep track of the mean loss observed within a custom training loop.\\n\\n# Create the metrics loss_metric = tf.keras.metrics.Mean(name=\\'train_loss\\') accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy(name=\\'train_accuracy\\') @tf.function def train_step(inputs, labels): with tf.GradientTape() as tape: predictions = model(inputs, training=True) regularization_loss=tf.math.add_n(model.losses) pred_loss=loss_fn(labels, predictions) total_loss=pred_loss + regularization_loss gradients = tape.gradient(total_loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # Update the metrics loss_metric.update_state(total_loss) accuracy_metric.update_state(labels, predictions) for epoch in range(NUM_EPOCHS): # Reset the metrics loss_metric.reset_states() accuracy_metric.reset_states() for inputs, labels in train_data: train_step(inputs, labels) # Get the metric results mean_loss=loss_metric.result() mean_accuracy = accuracy_metric.result() print(\\'Epoch: \\', epoch) print(\\' loss: {:.3f}\\'.format(mean_loss)) print(\\' accuracy: {:.3f}\\'.format(mean_accuracy))\\n\\nIn TensorFlow 2.0 keras models are more consistent about handling metric names.\\n\\nNow when you pass a string in the list of metrics, that exact string is used as the metric\\'s name. These names are visible in the history object returned by model.fit, and in the logs passed to keras.callbacks. is set to the string you passed in the metric list.\\n\\nmodel.compile( optimizer = tf.keras.optimizers.Adam(0.001), loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics = [\\'acc\\', \\'accuracy\\', tf.keras.metrics.SparseCategoricalAccuracy(name=\"my_accuracy\")]) history = model.fit(train_data)\\n\\nhistory.history.keys()\\n\\nThis differs from previous versions where passing metrics=[\"accuracy\"] would result in dict_keys([\\'loss\\', \\'acc\\'])\\n\\nThe optimizers in v1.train, like v1.train.AdamOptimizer and v1.train.GradientDescentOptimizer, have equivalents in tf.keras.optimizers.\\n\\nConvert v1.train to keras.optimizers\\n\\nHere are things to keep in mind when converting your optimizers:\\n\\nUpgrading your optimizers may make old checkpoints incompatible.\\n\\nAll epsilons now default to 1e-7 instead of 1e-8 (which is negligible in most use cases).\\n\\nv1.train.GradientDescentOptimizer can be directly replaced by tf.keras.optimizers.SGD.\\n\\nv1.train.MomentumOptimizer can be directly replaced by the SGD optimizer using the momentum argument: tf.keras.optimizers.SGD(..., momentum=...).\\n\\nv1.train.AdamOptimizer can be converted to use tf.keras.optimizers.Adam. The beta1 and beta2 arguments have been renamed to beta_1 and beta_2.\\n\\nv1.train.RMSPropOptimizer can be converted to tf.keras.optimizers.RMSprop. The decay argument has been renamed to rho.\\n\\nv1.train.AdadeltaOptimizer can be converted directly to tf.keras.optimizers.Adadelta.\\n\\ntf.train.AdagradOptimizer can be converted directly to tf.keras.optimizers.Adagrad.\\n\\ntf.train.FtrlOptimizer can be converted directly to tf.keras.optimizers.Ftrl. The accum_name and linear_name arguments have been removed.\\n\\nThe tf.contrib.AdamaxOptimizer and tf.contrib.NadamOptimizer, can be converted directly to tf.keras.optimizers.Adamax and tf.keras.optimizers.Nadam. The beta1, and beta2 arguments have been renamed to beta_1 and beta_2.\\n\\nNew defaults for some tf.keras.optimizers\\n\\nWarning: If you see a change in convergence behavior for your models, check the default learning rates.\\n\\nThere are no changes for optimizers.SGD, optimizers.Adam, or optimizers.RMSprop.\\n\\nThe following default learning rates have changed:\\n\\noptimizers.Adagrad from 0.01 to 0.001\\n\\noptimizers.Adadelta from 1.0 to 0.001\\n\\noptimizers.Adamax from 0.002 to 0.001\\n\\noptimizers.Nadam from 0.002 to 0.001\\n\\nTensorFlow 2 includes significant changes to the tf.summary API used to write summary data for visualization in TensorBoard. For a general introduction to the new tf.summary, there are several tutorials available that use the TF 2 API. This includes a TensorBoard TF 2 Migration Guide\\n\\nCheckpoint compatibility\\n\\nTensorFlow 2.0 uses object-based checkpoints.\\n\\nOld-style name-based checkpoints can still be loaded, if you\\'re careful. The code conversion process may result in variable name changes, but there are workarounds.\\n\\nThe simplest approach it to line up the names of the new model with the names in the checkpoint:\\n\\nVariables still all have a name argument you can set.\\n\\nKeras models also take a name argument as which they set as the prefix for their variables.\\n\\nThe v1.name_scope function can be used to set variable name prefixes. This is very different from tf.variable_scope. It only affects names, and doesn\\'t track variables & reuse.\\n\\nIf that does not work for your use-case, try the v1.train.init_from_checkpoint function. It takes an assignment_map argument, which specifies the mapping from old names to new names.\\n\\nNote: Unlike object based checkpoints, which can defer loading, name-based checkpoints require that all variables be built when the function is called. Some models defer building variables until you call build or run the model on a batch of data.\\n\\nThe TensorFlow Estimator repository includes a conversion tool to upgrade the checkpoints for premade estimators from TensorFlow 1.X to 2.0. It may serve as an example of how to build a tool for a similar use-case.\\n\\nSaved models compatibility\\n\\nThere are no significant compatibility concerns for saved models.\\n\\nTensorFlow 1.x saved_models work in TensorFlow 2.x.\\n\\nTensorFlow 2.x saved_models work in TensorFlow 1.x—if all the ops are supported.\\n\\nA Graph.pb or Graph.pbtxt\\n\\nThere is no straightforward way to upgrade a raw Graph.pb file to TensorFlow 2.0. Your best bet is to upgrade the code that generated the file.\\n\\nBut, if you have a \"Frozen graph\" (a tf.Graph where the variables have been turned into constants), then it is possible to convert this to a concrete_function using v1.wrap_function:\\n\\ndef wrap_frozen_graph(graph_def, inputs, outputs): def _imports_graph_def(): tf.compat.v1.import_graph_def(graph_def, name=\"\") wrapped_import = tf.compat.v1.wrap_function(_imports_graph_def, []) import_graph = wrapped_import.graph return wrapped_import.prune( tf.nest.map_structure(import_graph.as_graph_element, inputs), tf.nest.map_structure(import_graph.as_graph_element, outputs))\\n\\nFor example, here is a frozed graph for Inception v1, from 2016:\\n\\npath = tf.keras.utils.get_file( \\'inception_v1_2016_08_28_frozen.pb\\', \\'http://storage.googleapis.com/download.tensorflow.org/models/inception_v1_2016_08_28_frozen.pb.tar.gz\\', untar=True)\\n\\nLoad the tf.GraphDef:\\n\\ngraph_def = tf.compat.v1.GraphDef() loaded = graph_def.ParseFromString(open(path,\\'rb\\').read())\\n\\nWrap it into a concrete_function:\\n\\ninception_func = wrap_frozen_graph( graph_def, inputs=\\'input:0\\', outputs=\\'InceptionV1/InceptionV1/Mixed_3b/Branch_1/Conv2d_0a_1x1/Relu:0\\')\\n\\nPass it a tensor as input:\\n\\ninput_img = tf.ones([1,224,224,3], dtype=tf.float32) inception_func(input_img).shape\\n\\nTraining with Estimators\\n\\nEstimators are supported in TensorFlow 2.0.\\n\\nWhen you use estimators, you can use input_fn(), tf.estimator.TrainSpec, and tf.estimator.EvalSpec from TensorFlow 1.x.\\n\\nHere is an example using input_fn with train and evaluate specs.\\n\\nCreating the input_fn and train/eval specs\\n\\n# Define the estimator\\'s input_fn def input_fn(): datasets, info = tfds.load(name=\\'mnist\\', with_info=True, as_supervised=True) mnist_train, mnist_test = datasets[\\'train\\'], datasets[\\'test\\'] BUFFER_SIZE = 10000 BATCH_SIZE = 64 def scale(image, label): image = tf.cast(image, tf.float32) image /= 255 return image, label[..., tf.newaxis] train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE) return train_data.repeat() # Define train & eval specs train_spec = tf.estimator.TrainSpec(input_fn=input_fn, max_steps=STEPS_PER_EPOCH * NUM_EPOCHS) eval_spec = tf.estimator.EvalSpec(input_fn=input_fn, steps=STEPS_PER_EPOCH)\\n\\nUsing a Keras model definition\\n\\nThere are some differences in how to construct your estimators in TensorFlow 2.0.\\n\\nWe recommend that you define your model using Keras, then use the tf.keras.estimator.model_to_estimator utility to turn your model into an estimator. The code below shows how to use this utility when creating and training an estimator.\\n\\ndef make_model(): return tf.keras.Sequential([ tf.keras.layers.Conv2D(32, 3, activation=\\'relu\\', kernel_regularizer=tf.keras.regularizers.l2(0.02), input_shape=(28, 28, 1)), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Flatten(), tf.keras.layers.Dropout(0.1), tf.keras.layers.Dense(64, activation=\\'relu\\'), tf.keras.layers.BatchNormalization(), tf.keras.layers.Dense(10) ])\\n\\nmodel = make_model() model.compile(optimizer=\\'adam\\', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\\'accuracy\\']) estimator = tf.keras.estimator.model_to_estimator( keras_model = model ) tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\\n\\nNote: We do not support creating weighted metrics in Keras and converting them to weighted metrics in the Estimator API using model_to_estimator You will have to create these metrics directly on the estimator spec using the add_metrics function.\\n\\nUsing a custom model_fn\\n\\nIf you have an existing custom estimator model_fn that you need to maintain, you can convert your model_fn to use a Keras model.\\n\\nHowever, for compatibility reasons, a custom model_fn will still run in 1.x-style graph mode. This means there is no eager execution and no automatic control dependencies.\\n\\nCustom model_fn with minimal changes\\n\\nTo make your custom model_fn work in TF 2.0, if you prefer minimal changes to the existing code, tf.compat.v1 symbols such as optimizers and metrics can be used.\\n\\nUsing a Keras models in a custom model_fn is similar to using it in a custom training loop:\\n\\nSet the training phase appropriately, based on the mode argument.\\n\\nExplicitly pass the model\\'s trainable_variables to the optimizer.\\n\\nBut there are important differences, relative to a custom loop:\\n\\nInstead of using Model.losses, extract the losses using Model.get_losses_for.\\n\\nExtract the model\\'s updates using Model.get_updates_for.\\n\\nNote: \"Updates\" are changes that need to be applied to a model after each batch. For example, the moving averages of the mean and variance in a layers.BatchNormalization layer.\\n\\nThe following code creates an estimator from a custom model_fn, illustrating all of these concerns.\\n\\ndef my_model_fn(features, labels, mode): model = make_model() optimizer = tf.compat.v1.train.AdamOptimizer() loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) training = (mode == tf.estimator.ModeKeys.TRAIN) predictions = model(features, training=training) if mode == tf.estimator.ModeKeys.PREDICT: return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions) reg_losses = model.get_losses_for(None) + model.get_losses_for(features) total_loss=loss_fn(labels, predictions) + tf.math.add_n(reg_losses) accuracy = tf.compat.v1.metrics.accuracy(labels=labels, predictions=tf.math.argmax(predictions, axis=1), name=\\'acc_op\\') update_ops = model.get_updates_for(None) + model.get_updates_for(features) minimize_op = optimizer.minimize( total_loss, var_list=model.trainable_variables, global_step=tf.compat.v1.train.get_or_create_global_step()) train_op = tf.group(minimize_op, update_ops) return tf.estimator.EstimatorSpec( mode=mode, predictions=predictions, loss=total_loss, train_op=train_op, eval_metric_ops={\\'accuracy\\': accuracy}) # Create the Estimator & Train estimator = tf.estimator.Estimator(model_fn=my_model_fn) tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\\n\\nCustom model_fn with TF 2.0 symbols\\n\\nIf you want to get rid of all TF 1.x symbols and upgrade your custom model_fn to native TF 2.0, you need to update the optimizer and metrics to tf.keras.optimizers and tf.keras.metrics.\\n\\nIn the custom model_fn, besides the above changes, more upgrades need to be made:\\n\\nUse tf.keras.optimizers instead of v1.train.Optimizer.\\n\\nExplicitly pass the model\\'s trainable_variables to the tf.keras.optimizers.\\n\\nTo compute the train_op/minimize_op,\\n\\nUse Optimizer.get_updates() if the loss is scalar loss Tensor(not a callable). The first element in the returned list is the desired train_op/minimize_op.\\n\\nIf the loss is a callable (such as a function), use Optimizer.minimize() to get the train_op/minimize_op.\\n\\nUse tf.keras.metrics instead of tf.compat.v1.metrics for evaluation.\\n\\nFor the above example of my_model_fn, the migrated code with 2.0 symbols is shown as:\\n\\ndef my_model_fn(features, labels, mode): model = make_model() training = (mode == tf.estimator.ModeKeys.TRAIN) loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) predictions = model(features, training=training) # Get both the unconditional losses (the None part) # and the input-conditional losses (the features part). reg_losses = model.get_losses_for(None) + model.get_losses_for(features) total_loss=loss_obj(labels, predictions) + tf.math.add_n(reg_losses) # Upgrade to tf.keras.metrics. accuracy_obj = tf.keras.metrics.Accuracy(name=\\'acc_obj\\') accuracy = accuracy_obj.update_state( y_true=labels, y_pred=tf.math.argmax(predictions, axis=1)) train_op = None if training: # Upgrade to tf.keras.optimizers. optimizer = tf.keras.optimizers.Adam() # Manually assign tf.compat.v1.global_step variable to optimizer.iterations # to make tf.compat.v1.train.global_step increased correctly. # This assignment is a must for any `tf.train.SessionRunHook` specified in # estimator, as SessionRunHooks rely on global step. optimizer.iterations = tf.compat.v1.train.get_or_create_global_step() # Get both the unconditional updates (the None part) # and the input-conditional updates (the features part). update_ops = model.get_updates_for(None) + model.get_updates_for(features) # Compute the minimize_op. minimize_op = optimizer.get_updates( total_loss, model.trainable_variables)[0] train_op = tf.group(minimize_op, *update_ops) return tf.estimator.EstimatorSpec( mode=mode, predictions=predictions, loss=total_loss, train_op=train_op, eval_metric_ops={\\'Accuracy\\': accuracy_obj}) # Create the Estimator & Train. estimator = tf.estimator.Estimator(model_fn=my_model_fn) tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\\n\\nPremade Estimators in the family of tf.estimator.DNN*, tf.estimator.Linear* and tf.estimator.DNNLinearCombined* are still supported in the TensorFlow 2.0 API, however, some arguments have changed:\\n\\ninput_layer_partitioner: Removed in 2.0.\\n\\nloss_reduction: Updated to tf.keras.losses.Reduction instead of tf.compat.v1.losses.Reduction. Its default value is also changed to tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE from tf.compat.v1.losses.Reduction.SUM.\\n\\noptimizer, dnn_optimizer and linear_optimizer: this arg has been updated to tf.keras.optimizers instead of the tf.compat.v1.train.Optimizer.\\n\\nTo migrate the above changes:\\n\\nNo migration is needed for input_layer_partitioner since Distribution Strategy will handle it automatically in TF 2.0.\\n\\nFor loss_reduction, check tf.keras.losses.Reduction for the supported options.\\n\\nFor optimizer args, if you do not pass in an optimizer, dnn_optimizer or linear_optimizer arg, or if you specify the optimizer arg as a string in your code, you don\\'t need to change anything. tf.keras.optimizers is used by default. Otherwise, you need to update it from tf.compat.v1.train.Optimizer to its corresponding tf.keras.optimizers\\n\\nCheckpoint Converter\\n\\nThe migration to keras.optimizers will break checkpoints saved using TF 1.x, as tf.keras.optimizers generates a different set of variables to be saved in checkpoints. To make old checkpoint reusable after your migration to TF 2.0, try the checkpoint converter tool.\\n\\n! curl -O https://raw.githubusercontent.com/tensorflow/estimator/master/tensorflow_estimator/python/estimator/tools/checkpoint_converter.py\\n\\nThe tool has builtin help:\\n\\n! python checkpoint_converter.py -h\\n\\nThis class was simplified to hold ints, instead of tf.compat.v1.Dimension objects. So there is no need to call .value() to get an int.\\n\\nIndividual tf.compat.v1.Dimension objects are still accessible from tf.TensorShape.dims.\\n\\nThe following demonstrate the differences between TensorFlow 1.x and TensorFlow 2.0.\\n\\n# Create a shape and choose an index i = 0 shape = tf.TensorShape([16, None, 256]) shape\\n\\nIf you had this in TF 1.x:\\n\\nvalue = shape[i].value\\n\\nThen do this in TF 2.0:\\n\\nvalue = shape[i] value\\n\\nIf you had this in TF 1.x:\\n\\nfor dim in shape: value = dim.value print(value)\\n\\nThen do this in TF 2.0:\\n\\nfor value in shape: print(value)\\n\\nIf you had this in TF 1.x (Or used any other dimension method):\\n\\ndim = shape[i] dim.assert_is_compatible_with(other_dim)\\n\\nThen do this in TF 2.0:\\n\\nother_dim = 16 Dimension = tf.compat.v1.Dimension if shape.rank is None: dim = Dimension(None) else: dim = shape.dims[i] dim.is_compatible_with(other_dim) # or any other dimension method\\n\\nshape = tf.TensorShape(None) if shape: dim = shape.dims[i] dim.is_compatible_with(other_dim) # or any other dimension method\\n\\nThe boolean value of a tf.TensorShape is True if the rank is known, False otherwise.\\n\\nprint(bool(tf.TensorShape([]))) # Scalar print(bool(tf.TensorShape([0]))) # 0-length vector print(bool(tf.TensorShape([1]))) # 1-length vector print(bool(tf.TensorShape([None]))) # Unknown-length vector print(bool(tf.TensorShape([1, 10, 100]))) # 3D tensor print(bool(tf.TensorShape([None, None, None]))) # 3D tensor with no known dimensions print() print(bool(tf.TensorShape(None))) # A tensor with unknown rank.\\n\\nRemove tf.colocate_with: TensorFlow\\'s device placement algorithms have improved significantly. This should no longer be necessary. If removing it causes a performance degredation please file a bug.\\n\\nReplace v1.ConfigProto usage with the equivalent functions from tf.config.\\n\\nThe overall process is:\\n\\nRun the upgrade script.\\n\\nRemove contrib symbols.\\n\\nSwitch your models to an object oriented style (Keras).\\n\\nUse tf.keras or tf.estimator training and evaluation loops where you can.\\n\\nOtherwise, use custom loops, but be sure to avoid sessions & collections.\\n\\nIt takes a little work to convert code to idiomatic TensorFlow 2.0, but every change results in:\\n\\nFewer lines of code.\\n\\nIncreased clarity and simplicity.\\n\\nContent source: tensorflow/docs-l10n\\n\\nTensorFlowLite_LSTM_Keras_Tutorial\\n\\nTensorFlowLite_LSTM_Keras_Tutorial\\n\\nmodel_maker_image_classification\\n\\ntensorflow_programming_concepts notebook.community | gallery | about', metadata={'id': 'web-search_7', 'snippet': 'Copyright 2018 The TensorFlow Authors.\\n\\n#@title Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # https://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License.\\n\\nMigrate your TensorFlow 1 code to TensorFlow 2\\n\\nView on TensorFlow.org\\n\\nView source on GitHub\\n\\nThis doc for users of low level TensorFlow APIs. If you are using the high level APIs (tf.keras) there may be little or no action you need to take to make your code fully TensorFlow 2.0 compatible:\\n\\nCheck your optimizer\\'s default learning rate.\\n\\nNote that the \"name\" that metrics are logged to may have changed.\\n\\nIt is still possible to run 1.X code, unmodified (except for contrib), in TensorFlow 2.0:\\n\\nimport tensorflow.compat.v1 as tf tf.disable_v2_behavior()\\n\\nHowever, this does not let you take advantage of many of the improvements made in TensorFlow 2.0. This guide will help you upgrade your code, making it simpler, more performant, and easier to maintain.\\n\\nAutomatic conversion script\\n\\nThe first step, before attempting to implement the changes described in this doc, is to try running the upgrade script.\\n\\nThis will do an initial pass at upgrading your code to TensorFlow 2.0. But it can\\'t make your code idiomatic to 2.0. Your code may still make use of tf.compat.v1 endpoints to access placeholders, sessions, collections, and other 1.x-style functionality.\\n\\nTop-level behavioral changes\\n\\nIf your code works in TensorFlow 2.0 using tf.compat.v1.disable_v2_behavior(), there are still global behavioral changes you may need to address. The major changes are:\\n\\nEager execution, v1.enable_eager_execution() : Any code that implicitly uses a tf.Graph will fail. Be sure to wrap this code in a with tf.Graph().as_default() context.\\n\\nResource variables, v1.enable_resource_variables(): Some code may depends on non-deterministic behaviors enabled by TF reference variables. Resource variables are locked while being written to, and so provide more intuitive consistency guarantees.\\n\\nThis may change behavior in edge cases.\\n\\nThis may create extra copies and can have higher memory usage.\\n\\nThis can be disabled by passing use_resource=False to the tf.Variable constructor.\\n\\nTensor shapes, v1.enable_v2_tensorshape(): TF 2.0 simplifies the behavior of tensor shapes. Instead of t.shape[0].value you can say t.shape[0]. These changes should be small, and it makes sense to fix them right away. See TensorShape for examples.\\n\\nControl flow, v1.enable_control_flow_v2(): The TF 2.0 control flow implementation has been simplified, and so produces different graph representations. Please file bugs for any issues.\\n\\nMake the code 2.0-native\\n\\nThis guide will walk through several examples of converting TensorFlow 1.x code to TensorFlow 2.0. These changes will let your code take advantage of performance optimizations and simplified API calls.\\n\\nIn each case, the pattern is:\\n\\n1. Replace v1.Session.run calls\\n\\nEvery v1.Session.run call should be replaced by a Python function.\\n\\nThe feed_dict and v1.placeholders become function arguments.\\n\\nThe fetches become the function\\'s return value.\\n\\nDuring conversion eager execution allows easy debugging with standard Python tools like pdb.\\n\\nAfter that add a tf.function decorator to make it run efficiently in graph. See the Autograph Guide for more on how this works.\\n\\nUnlike v1.Session.run a tf.function has a fixed return signature, and always returns all outputs. If this causes performance problems, create two separate functions.\\n\\nThere is no need for a tf.control_dependencies or similar operations: A tf.function behaves as if it were run in the order written. tf.Variable assignments and tf.asserts, for example, are executed automatically.\\n\\n2. Use Python objects to track variables and losses\\n\\nAll name-based variable tracking is strongly discouraged in TF 2.0. Use Python objects to to track variables.\\n\\nUse tf.Variable instead of v1.get_variable.\\n\\nEvery v1.variable_scope should be converted to a Python object. Typically this will be one of:\\n\\ntf.keras.layers.Layer\\n\\nIf you need to aggregate lists of variables (like tf.Graph.get_collection(tf.GraphKeys.VARIABLES)), use the .variables and .trainable_variables attributes of the Layer and Model objects.\\n\\nThese Layer and Model classes implement several other properties that remove the need for global collections. Their .losses property can be a replacement for using the tf.GraphKeys.LOSSES collection.\\n\\nSee the keras guides for details.\\n\\nWarning: Many tf.compat.v1 symbols use the global collections implicitly.\\n\\n3. Upgrade your training loops\\n\\nUse the highest level API that works for your use case. Prefer tf.keras.Model.fit over building your own training loops.\\n\\nThese high level functions manage a lot of the low-level details that might be easy to miss if you write your own training loop. For example, they automatically collect the regularization losses, and set the training=True argument when calling the model.\\n\\n4. Upgrade your data input pipelines\\n\\nUse tf.data datasets for data input. These objects are efficient, expressive, and integrate well with tensorflow.\\n\\nThey can be passed directly to the tf.keras.Model.fit method.\\n\\nmodel.fit(dataset, epochs=5)\\n\\nThey can be iterated over directly standard Python:\\n\\nfor example_batch, label_batch in dataset: break\\n\\n5. Migrate off compat.v1 symbols\\n\\nThe tf.compat.v1 module contains the complete TensorFlow 1.x API, with its original semantics.\\n\\nThe TF2 upgrade script will convert symbols to their 2.0 equivalents if such a conversion is safe, i.e., if it can determine that the behavior of the 2.0 version is exactly equivalent (for instance, it will rename v1.arg_max to tf.argmax, since those are the same function).\\n\\nAfter the upgrade script is done with a piece of code, it is likely there are many mentions of compat.v1. It is worth going through the code and converting these manually to the 2.0 equivalent (it should be mentioned in the log if there is one).\\n\\nimport tensorflow as tf import tensorflow_datasets as tfds\\n\\nLow-level variables & operator execution\\n\\nExamples of low-level API use include:\\n\\nusing variable scopes to control reuse\\n\\ncreating variables with v1.get_variable.\\n\\naccessing collections explicitly\\n\\naccessing collections implicitly with methods like :\\n\\nv1.losses.get_regularization_loss\\n\\nusing v1.placeholder to set up graph inputs\\n\\nexecuting graphs with Session.run\\n\\ninitializing variables manually\\n\\nHere is what these patterns may look like in code using TensorFlow 1.x.\\n\\nin_a = tf.placeholder(dtype=tf.float32, shape=(2)) in_b = tf.placeholder(dtype=tf.float32, shape=(2)) def forward(x): with tf.variable_scope(\"matmul\", reuse=tf.AUTO_REUSE): W = tf.get_variable(\"W\", initializer=tf.ones(shape=(2,2)), regularizer=tf.contrib.layers.l2_regularizer(0.04)) b = tf.get_variable(\"b\", initializer=tf.zeros(shape=(2))) return W * x + b out_a = forward(in_a) out_b = forward(in_b) reg_loss=tf.losses.get_regularization_loss(scope=\"matmul\") with tf.Session() as sess: sess.run(tf.global_variables_initializer()) outs = sess.run([out_a, out_b, reg_loss], feed_dict={in_a: [1, 0], in_b: [0, 1]})\\n\\nIn the converted code:\\n\\nThe variables are local Python objects.\\n\\nThe forward function still defines the calculation.\\n\\nThe Session.run call is replaced with a call to forward\\n\\nThe optional tf.function decorator can be added for performance.\\n\\nThe regularizations are calculated manually, without referring to any global collection.\\n\\nNo sessions or placeholders.\\n\\nW = tf.Variable(tf.ones(shape=(2,2)), name=\"W\") b = tf.Variable(tf.zeros(shape=(2)), name=\"b\") @tf.function def forward(x): return W * x + b out_a = forward([1,0]) print(out_a)\\n\\nout_b = forward([0,1]) regularizer = tf.keras.regularizers.l2(0.04) reg_loss=regularizer(W)\\n\\nModels based on tf.layers\\n\\nThe v1.layers module is used to contain layer-functions that relied on v1.variable_scope to define and reuse variables.\\n\\ndef model(x, training, scope=\\'model\\'): with tf.variable_scope(scope, reuse=tf.AUTO_REUSE): x = tf.layers.conv2d(x, 32, 3, activation=tf.nn.relu, kernel_regularizer=tf.contrib.layers.l2_regularizer(0.04)) x = tf.layers.max_pooling2d(x, (2, 2), 1) x = tf.layers.flatten(x) x = tf.layers.dropout(x, 0.1, training=training) x = tf.layers.dense(x, 64, activation=tf.nn.relu) x = tf.layers.batch_normalization(x, training=training) x = tf.layers.dense(x, 10) return x train_out = model(train_data, training=True) test_out = model(test_data, training=False)\\n\\nThe simple stack of layers fits neatly into tf.keras.Sequential. (For more complex models see custom layers and models, and the functional API.)\\n\\nThe model tracks the variables, and regularization losses.\\n\\nThe conversion was one-to-one because there is a direct mapping from v1.layers to tf.keras.layers.\\n\\nMost arguments stayed the same. But notice the differences:\\n\\nThe training argument is passed to each layer by the model when it runs.\\n\\nThe first argument to the original model function (the input x) is gone. This is because object layers separate building the model from calling the model.\\n\\nIf you were using regularizers of initializers from tf.contrib, these have more argument changes than others.\\n\\nThe code no longer writes to collections, so functions like v1.losses.get_regularization_loss will no longer return these values, potentially breaking your training loops.\\n\\nmodel = tf.keras.Sequential([ tf.keras.layers.Conv2D(32, 3, activation=\\'relu\\', kernel_regularizer=tf.keras.regularizers.l2(0.04), input_shape=(28, 28, 1)), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Flatten(), tf.keras.layers.Dropout(0.1), tf.keras.layers.Dense(64, activation=\\'relu\\'), tf.keras.layers.BatchNormalization(), tf.keras.layers.Dense(10) ]) train_data = tf.ones(shape=(1, 28, 28, 1)) test_data = tf.ones(shape=(1, 28, 28, 1))\\n\\ntrain_out = model(train_data, training=True) print(train_out)\\n\\ntest_out = model(test_data, training=False) print(test_out)\\n\\n# Here are all the trainable variables. len(model.trainable_variables)\\n\\n# Here is the regularization loss. model.losses\\n\\nMixed variables & v1.layers\\n\\nExisting code often mixes lower-level TF 1.x variables and operations with higher-level v1.layers.\\n\\ndef model(x, training, scope=\\'model\\'): with tf.variable_scope(scope, reuse=tf.AUTO_REUSE): W = tf.get_variable( \"W\", dtype=tf.float32, initializer=tf.ones(shape=x.shape), regularizer=tf.contrib.layers.l2_regularizer(0.04), trainable=True) if training: x = x + W else: x = x + W * 0.5 x = tf.layers.conv2d(x, 32, 3, activation=tf.nn.relu) x = tf.layers.max_pooling2d(x, (2, 2), 1) x = tf.layers.flatten(x) return x train_out = model(train_data, training=True) test_out = model(test_data, training=False)\\n\\nTo convert this code, follow the pattern of mapping layers to layers as in the previous example.\\n\\nThe general pattern is:\\n\\nCollect layer parameters in __init__.\\n\\nBuild the variables in build.\\n\\nExecute the calculations in call, and return the result.\\n\\nThe v1.variable_scope is essentially a layer of its own. So rewrite it as a tf.keras.layers.Layer. See the guide for details.\\n\\n# Create a custom layer for part of the model class CustomLayer(tf.keras.layers.Layer): def __init__(self, *args, **kwargs): super(CustomLayer, self).__init__(*args, **kwargs) def build(self, input_shape): self.w = self.add_weight( shape=input_shape[1:], dtype=tf.float32, initializer=tf.keras.initializers.ones(), regularizer=tf.keras.regularizers.l2(0.02), trainable=True) # Call method will sometimes get used in graph mode, # training will get turned into a tensor @tf.function def call(self, inputs, training=None): if training: return inputs + self.w else: return inputs + self.w * 0.5\\n\\ncustom_layer = CustomLayer() print(custom_layer([1]).numpy()) print(custom_layer([1], training=True).numpy())\\n\\ntrain_data = tf.ones(shape=(1, 28, 28, 1)) test_data = tf.ones(shape=(1, 28, 28, 1)) # Build the model including the custom layer model = tf.keras.Sequential([ CustomLayer(input_shape=(28, 28, 1)), tf.keras.layers.Conv2D(32, 3, activation=\\'relu\\'), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Flatten(), ]) train_out = model(train_data, training=True) test_out = model(test_data, training=False)\\n\\nSome things to note:\\n\\nSubclassed Keras models & layers need to run in both v1 graphs (no automatic control dependencies) and in eager mode\\n\\nWrap the call() in a tf.function() to get autograph and automatic control dependencies\\n\\nDon\\'t forget to accept a training argument to call.\\n\\nSometimes it is a tf.Tensor\\n\\nSometimes it is a Python boolean.\\n\\nCreate model variables in constructor or Model.build using self.add_weight().\\n\\nIn Model.build you have access to the input shape, so can create weights with matching shape.\\n\\nUsing tf.keras.layers.Layer.add_weight allows Keras to track variables and regularization losses.\\n\\nDon\\'t keep tf.Tensors in your objects.\\n\\nThey might get created either in a tf.function or in the eager context, and these tensors behave differently.\\n\\nUse tf.Variables for state, they are always usable from both contexts\\n\\ntf.Tensors are only for intermediate values.\\n\\nA note on Slim & contrib.layers\\n\\nA large amount of older TensorFlow 1.x code uses the Slim library, which was packaged with TensorFlow 1.x as tf.contrib.layers. As a contrib module, this is no longer available in TensorFlow 2.0, even in tf.compat.v1. Converting code using Slim to TF 2.0 is more involved than converting repositories that use v1.layers. In fact, it may make sense to convert your Slim code to v1.layers first, then convert to Keras.\\n\\nRemove arg_scopes, all args need to be explicit\\n\\nIf you use them, split normalizer_fn and activation_fn into their own layers\\n\\nSeparable conv layers map to one or more different Keras layers (depthwise, pointwise, and separable Keras layers)\\n\\nSlim and v1.layers have different arg names & default values\\n\\nSome args have different scales\\n\\nIf you use Slim pre-trained models, try out Keras\\'s pre-traimed models from tf.keras.applications or TF Hub\\'s TF2 SavedModels exported from the original Slim code.\\n\\nSome tf.contrib layers might not have been moved to core TensorFlow but have instead been moved to the TF add-ons package.\\n\\nThere are many ways to feed data to a tf.keras model. They will accept Python generators and Numpy arrays as input.\\n\\nThe recommended way to feed data to a model is to use the tf.data package, which contains a collection of high performance classes for manipulating data.\\n\\nIf you are still using tf.queue, these are now only supported as data-structures, not as input pipelines.\\n\\nThe TensorFlow Datasets package (tfds) contains utilities for loading predefined datasets as tf.data.Dataset objects.\\n\\nFor this example, load the MNISTdataset, using tfds:\\n\\ndatasets, info = tfds.load(name=\\'mnist\\', with_info=True, as_supervised=True) mnist_train, mnist_test = datasets[\\'train\\'], datasets[\\'test\\']\\n\\nThen prepare the data for training:\\n\\nRe-scale each image.\\n\\nShuffle the order of the examples.\\n\\nCollect batches of images and labels.\\n\\nBUFFER_SIZE = 10 # Use a much larger value for real code. BATCH_SIZE = 64 NUM_EPOCHS = 5 def scale(image, label): image = tf.cast(image, tf.float32) image /= 255 return image, label\\n\\nTo keep the example short, trim the dataset to only return 5 batches:\\n\\ntrain_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE) test_data = mnist_test.map(scale).batch(BATCH_SIZE) STEPS_PER_EPOCH = 5 train_data = train_data.take(STEPS_PER_EPOCH) test_data = test_data.take(STEPS_PER_EPOCH)\\n\\nimage_batch, label_batch = next(iter(train_data))\\n\\nUse Keras training loops\\n\\nIf you don\\'t need low level control of your training process, using Keras\\'s built-in fit, evaluate, and predict methods is recommended. These methods provide a uniform interface to train the model regardless of the implementation (sequential, functional, or sub-classed).\\n\\nThe advantages of these methods include:\\n\\nThey accept Numpy arrays, Python generators and, tf.data.Datasets\\n\\nThey apply regularization, and activation losses automatically.\\n\\nThey support tf.distribute for multi-device training.\\n\\nThey support arbitrary callables as losses and metrics.\\n\\nThey support callbacks like tf.keras.callbacks.TensorBoard, and custom callbacks.\\n\\nThey are performant, automatically using TensorFlow graphs.\\n\\nHere is an example of training a model using a Dataset. (For details on how this works see tutorials.)\\n\\nmodel = tf.keras.Sequential([ tf.keras.layers.Conv2D(32, 3, activation=\\'relu\\', kernel_regularizer=tf.keras.regularizers.l2(0.02), input_shape=(28, 28, 1)), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Flatten(), tf.keras.layers.Dropout(0.1), tf.keras.layers.Dense(64, activation=\\'relu\\'), tf.keras.layers.BatchNormalization(), tf.keras.layers.Dense(10) ]) # Model is the full model w/o custom layers model.compile(optimizer=\\'adam\\', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\\'accuracy\\']) model.fit(train_data, epochs=NUM_EPOCHS) loss, acc = model.evaluate(test_data) print(\"Loss {}, Accuracy {}\".format(loss, acc))\\n\\nIf the Keras model\\'s training step works for you, but you need more control outside that step, consider using the tf.keras.Model.train_on_batch method, in your own data-iteration loop.\\n\\nRemember: Many things can be implemented as a tf.keras.callbacks.Callback.\\n\\nThis method has many of the advantages of the methods mentioned in the previous section, but gives the user control of the outer loop.\\n\\nYou can also use tf.keras.Model.test_on_batch or tf.keras.Model.evaluate to check performance during training.\\n\\nNote: train_on_batch and test_on_batch, by default return the loss and metrics for the single batch. If you pass reset_metrics=False they return accumulated metrics and you must remember to appropriately reset the metric accumulators. Also remember that some metrics like AUC require reset_metrics=False to be calculated correctly.\\n\\nTo continue training the above model:\\n\\n# Model is the full model w/o custom layers model.compile(optimizer=\\'adam\\', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\\'accuracy\\']) for epoch in range(NUM_EPOCHS): #Reset the metric accumulators model.reset_metrics() for image_batch, label_batch in train_data: result = model.train_on_batch(image_batch, label_batch) metrics_names = model.metrics_names print(\"train: \", \"{}: {:.3f}\".format(metrics_names[0], result[0]), \"{}: {:.3f}\".format(metrics_names[1], result[1])) for image_batch, label_batch in test_data: result = model.test_on_batch(image_batch, label_batch, # return accumulated metrics reset_metrics=False) metrics_names = model.metrics_names print(\"\\\\neval: \", \"{}: {:.3f}\".format(metrics_names[0], result[0]), \"{}: {:.3f}\".format(metrics_names[1], result[1]))\\n\\nCustomize the training step\\n\\nIf you need more flexibility and control, you can have it by implementing your own training loop. There are three steps:\\n\\nIterate over a Python generator or tf.data.Dataset to get batches of examples.\\n\\nUse tf.GradientTape to collect gradients.\\n\\nUse one of the tf.keras.optimizers to apply weight updates to the model\\'s variables.\\n\\nAlways include a training argument on the call method of subclassed layers and models.\\n\\nMake sure to call the model with the training argument set correctly.\\n\\nDepending on usage, model variables may not exist until the model is run on a batch of data.\\n\\nYou need to manually handle things like regularization losses for the model.\\n\\nNote the simplifications relative to v1:\\n\\nThere is no need to run variable initializers. Variables are initialized on creation.\\n\\nThere is no need to add manual control dependencies. Even in tf.function operations act as in eager mode.\\n\\nmodel = tf.keras.Sequential([ tf.keras.layers.Conv2D(32, 3, activation=\\'relu\\', kernel_regularizer=tf.keras.regularizers.l2(0.02), input_shape=(28, 28, 1)), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Flatten(), tf.keras.layers.Dropout(0.1), tf.keras.layers.Dense(64, activation=\\'relu\\'), tf.keras.layers.BatchNormalization(), tf.keras.layers.Dense(10) ]) optimizer = tf.keras.optimizers.Adam(0.001) loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) @tf.function def train_step(inputs, labels): with tf.GradientTape() as tape: predictions = model(inputs, training=True) regularization_loss=tf.math.add_n(model.losses) pred_loss=loss_fn(labels, predictions) total_loss=pred_loss + regularization_loss gradients = tape.gradient(total_loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) for epoch in range(NUM_EPOCHS): for inputs, labels in train_data: train_step(inputs, labels) print(\"Finished epoch\", epoch)\\n\\nNew-style metrics and losses\\n\\nIn TensorFlow 2.0, metrics and losses are objects. These work both eagerly and in tf.functions.\\n\\nA loss object is callable, and expects the (y_true, y_pred) as arguments:\\n\\ncce = tf.keras.losses.CategoricalCrossentropy(from_logits=True) cce([[1, 0]], [[-1.0,3.0]]).numpy()\\n\\nA metric object has the following methods:\\n\\nMetric.update_state() — add new observations\\n\\nMetric.result() —get the current result of the metric, given the observed values\\n\\nMetric.reset_states() — clear all observations.\\n\\nThe object itself is callable. Calling updates the state with new observations, as with update_state, and returns the new result of the metric.\\n\\nYou don\\'t have to manually initialize a metric\\'s variables, and because TensorFlow 2.0 has automatic control dependencies, you don\\'t need to worry about those either.\\n\\nThe code below uses a metric to keep track of the mean loss observed within a custom training loop.\\n\\n# Create the metrics loss_metric = tf.keras.metrics.Mean(name=\\'train_loss\\') accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy(name=\\'train_accuracy\\') @tf.function def train_step(inputs, labels): with tf.GradientTape() as tape: predictions = model(inputs, training=True) regularization_loss=tf.math.add_n(model.losses) pred_loss=loss_fn(labels, predictions) total_loss=pred_loss + regularization_loss gradients = tape.gradient(total_loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # Update the metrics loss_metric.update_state(total_loss) accuracy_metric.update_state(labels, predictions) for epoch in range(NUM_EPOCHS): # Reset the metrics loss_metric.reset_states() accuracy_metric.reset_states() for inputs, labels in train_data: train_step(inputs, labels) # Get the metric results mean_loss=loss_metric.result() mean_accuracy = accuracy_metric.result() print(\\'Epoch: \\', epoch) print(\\' loss: {:.3f}\\'.format(mean_loss)) print(\\' accuracy: {:.3f}\\'.format(mean_accuracy))\\n\\nIn TensorFlow 2.0 keras models are more consistent about handling metric names.\\n\\nNow when you pass a string in the list of metrics, that exact string is used as the metric\\'s name. These names are visible in the history object returned by model.fit, and in the logs passed to keras.callbacks. is set to the string you passed in the metric list.\\n\\nmodel.compile( optimizer = tf.keras.optimizers.Adam(0.001), loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics = [\\'acc\\', \\'accuracy\\', tf.keras.metrics.SparseCategoricalAccuracy(name=\"my_accuracy\")]) history = model.fit(train_data)\\n\\nhistory.history.keys()\\n\\nThis differs from previous versions where passing metrics=[\"accuracy\"] would result in dict_keys([\\'loss\\', \\'acc\\'])\\n\\nThe optimizers in v1.train, like v1.train.AdamOptimizer and v1.train.GradientDescentOptimizer, have equivalents in tf.keras.optimizers.\\n\\nConvert v1.train to keras.optimizers\\n\\nHere are things to keep in mind when converting your optimizers:\\n\\nUpgrading your optimizers may make old checkpoints incompatible.\\n\\nAll epsilons now default to 1e-7 instead of 1e-8 (which is negligible in most use cases).\\n\\nv1.train.GradientDescentOptimizer can be directly replaced by tf.keras.optimizers.SGD.\\n\\nv1.train.MomentumOptimizer can be directly replaced by the SGD optimizer using the momentum argument: tf.keras.optimizers.SGD(..., momentum=...).\\n\\nv1.train.AdamOptimizer can be converted to use tf.keras.optimizers.Adam. The beta1 and beta2 arguments have been renamed to beta_1 and beta_2.\\n\\nv1.train.RMSPropOptimizer can be converted to tf.keras.optimizers.RMSprop. The decay argument has been renamed to rho.\\n\\nv1.train.AdadeltaOptimizer can be converted directly to tf.keras.optimizers.Adadelta.\\n\\ntf.train.AdagradOptimizer can be converted directly to tf.keras.optimizers.Adagrad.\\n\\ntf.train.FtrlOptimizer can be converted directly to tf.keras.optimizers.Ftrl. The accum_name and linear_name arguments have been removed.\\n\\nThe tf.contrib.AdamaxOptimizer and tf.contrib.NadamOptimizer, can be converted directly to tf.keras.optimizers.Adamax and tf.keras.optimizers.Nadam. The beta1, and beta2 arguments have been renamed to beta_1 and beta_2.\\n\\nNew defaults for some tf.keras.optimizers\\n\\nWarning: If you see a change in convergence behavior for your models, check the default learning rates.\\n\\nThere are no changes for optimizers.SGD, optimizers.Adam, or optimizers.RMSprop.\\n\\nThe following default learning rates have changed:\\n\\noptimizers.Adagrad from 0.01 to 0.001\\n\\noptimizers.Adadelta from 1.0 to 0.001\\n\\noptimizers.Adamax from 0.002 to 0.001\\n\\noptimizers.Nadam from 0.002 to 0.001\\n\\nTensorFlow 2 includes significant changes to the tf.summary API used to write summary data for visualization in TensorBoard. For a general introduction to the new tf.summary, there are several tutorials available that use the TF 2 API. This includes a TensorBoard TF 2 Migration Guide\\n\\nCheckpoint compatibility\\n\\nTensorFlow 2.0 uses object-based checkpoints.\\n\\nOld-style name-based checkpoints can still be loaded, if you\\'re careful. The code conversion process may result in variable name changes, but there are workarounds.\\n\\nThe simplest approach it to line up the names of the new model with the names in the checkpoint:\\n\\nVariables still all have a name argument you can set.\\n\\nKeras models also take a name argument as which they set as the prefix for their variables.\\n\\nThe v1.name_scope function can be used to set variable name prefixes. This is very different from tf.variable_scope. It only affects names, and doesn\\'t track variables & reuse.\\n\\nIf that does not work for your use-case, try the v1.train.init_from_checkpoint function. It takes an assignment_map argument, which specifies the mapping from old names to new names.\\n\\nNote: Unlike object based checkpoints, which can defer loading, name-based checkpoints require that all variables be built when the function is called. Some models defer building variables until you call build or run the model on a batch of data.\\n\\nThe TensorFlow Estimator repository includes a conversion tool to upgrade the checkpoints for premade estimators from TensorFlow 1.X to 2.0. It may serve as an example of how to build a tool for a similar use-case.\\n\\nSaved models compatibility\\n\\nThere are no significant compatibility concerns for saved models.\\n\\nTensorFlow 1.x saved_models work in TensorFlow 2.x.\\n\\nTensorFlow 2.x saved_models work in TensorFlow 1.x—if all the ops are supported.\\n\\nA Graph.pb or Graph.pbtxt\\n\\nThere is no straightforward way to upgrade a raw Graph.pb file to TensorFlow 2.0. Your best bet is to upgrade the code that generated the file.\\n\\nBut, if you have a \"Frozen graph\" (a tf.Graph where the variables have been turned into constants), then it is possible to convert this to a concrete_function using v1.wrap_function:\\n\\ndef wrap_frozen_graph(graph_def, inputs, outputs): def _imports_graph_def(): tf.compat.v1.import_graph_def(graph_def, name=\"\") wrapped_import = tf.compat.v1.wrap_function(_imports_graph_def, []) import_graph = wrapped_import.graph return wrapped_import.prune( tf.nest.map_structure(import_graph.as_graph_element, inputs), tf.nest.map_structure(import_graph.as_graph_element, outputs))\\n\\nFor example, here is a frozed graph for Inception v1, from 2016:\\n\\npath = tf.keras.utils.get_file( \\'inception_v1_2016_08_28_frozen.pb\\', \\'http://storage.googleapis.com/download.tensorflow.org/models/inception_v1_2016_08_28_frozen.pb.tar.gz\\', untar=True)\\n\\nLoad the tf.GraphDef:\\n\\ngraph_def = tf.compat.v1.GraphDef() loaded = graph_def.ParseFromString(open(path,\\'rb\\').read())\\n\\nWrap it into a concrete_function:\\n\\ninception_func = wrap_frozen_graph( graph_def, inputs=\\'input:0\\', outputs=\\'InceptionV1/InceptionV1/Mixed_3b/Branch_1/Conv2d_0a_1x1/Relu:0\\')\\n\\nPass it a tensor as input:\\n\\ninput_img = tf.ones([1,224,224,3], dtype=tf.float32) inception_func(input_img).shape\\n\\nTraining with Estimators\\n\\nEstimators are supported in TensorFlow 2.0.\\n\\nWhen you use estimators, you can use input_fn(), tf.estimator.TrainSpec, and tf.estimator.EvalSpec from TensorFlow 1.x.\\n\\nHere is an example using input_fn with train and evaluate specs.\\n\\nCreating the input_fn and train/eval specs\\n\\n# Define the estimator\\'s input_fn def input_fn(): datasets, info = tfds.load(name=\\'mnist\\', with_info=True, as_supervised=True) mnist_train, mnist_test = datasets[\\'train\\'], datasets[\\'test\\'] BUFFER_SIZE = 10000 BATCH_SIZE = 64 def scale(image, label): image = tf.cast(image, tf.float32) image /= 255 return image, label[..., tf.newaxis] train_data = mnist_train.map(scale).shuffle(BUFFER_SIZE).batch(BATCH_SIZE) return train_data.repeat() # Define train & eval specs train_spec = tf.estimator.TrainSpec(input_fn=input_fn, max_steps=STEPS_PER_EPOCH * NUM_EPOCHS) eval_spec = tf.estimator.EvalSpec(input_fn=input_fn, steps=STEPS_PER_EPOCH)\\n\\nUsing a Keras model definition\\n\\nThere are some differences in how to construct your estimators in TensorFlow 2.0.\\n\\nWe recommend that you define your model using Keras, then use the tf.keras.estimator.model_to_estimator utility to turn your model into an estimator. The code below shows how to use this utility when creating and training an estimator.\\n\\ndef make_model(): return tf.keras.Sequential([ tf.keras.layers.Conv2D(32, 3, activation=\\'relu\\', kernel_regularizer=tf.keras.regularizers.l2(0.02), input_shape=(28, 28, 1)), tf.keras.layers.MaxPooling2D(), tf.keras.layers.Flatten(), tf.keras.layers.Dropout(0.1), tf.keras.layers.Dense(64, activation=\\'relu\\'), tf.keras.layers.BatchNormalization(), tf.keras.layers.Dense(10) ])\\n\\nmodel = make_model() model.compile(optimizer=\\'adam\\', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\\'accuracy\\']) estimator = tf.keras.estimator.model_to_estimator( keras_model = model ) tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\\n\\nNote: We do not support creating weighted metrics in Keras and converting them to weighted metrics in the Estimator API using model_to_estimator You will have to create these metrics directly on the estimator spec using the add_metrics function.\\n\\nUsing a custom model_fn\\n\\nIf you have an existing custom estimator model_fn that you need to maintain, you can convert your model_fn to use a Keras model.\\n\\nHowever, for compatibility reasons, a custom model_fn will still run in 1.x-style graph mode. This means there is no eager execution and no automatic control dependencies.\\n\\nCustom model_fn with minimal changes\\n\\nTo make your custom model_fn work in TF 2.0, if you prefer minimal changes to the existing code, tf.compat.v1 symbols such as optimizers and metrics can be used.\\n\\nUsing a Keras models in a custom model_fn is similar to using it in a custom training loop:\\n\\nSet the training phase appropriately, based on the mode argument.\\n\\nExplicitly pass the model\\'s trainable_variables to the optimizer.\\n\\nBut there are important differences, relative to a custom loop:\\n\\nInstead of using Model.losses, extract the losses using Model.get_losses_for.\\n\\nExtract the model\\'s updates using Model.get_updates_for.\\n\\nNote: \"Updates\" are changes that need to be applied to a model after each batch. For example, the moving averages of the mean and variance in a layers.BatchNormalization layer.\\n\\nThe following code creates an estimator from a custom model_fn, illustrating all of these concerns.\\n\\ndef my_model_fn(features, labels, mode): model = make_model() optimizer = tf.compat.v1.train.AdamOptimizer() loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) training = (mode == tf.estimator.ModeKeys.TRAIN) predictions = model(features, training=training) if mode == tf.estimator.ModeKeys.PREDICT: return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions) reg_losses = model.get_losses_for(None) + model.get_losses_for(features) total_loss=loss_fn(labels, predictions) + tf.math.add_n(reg_losses) accuracy = tf.compat.v1.metrics.accuracy(labels=labels, predictions=tf.math.argmax(predictions, axis=1), name=\\'acc_op\\') update_ops = model.get_updates_for(None) + model.get_updates_for(features) minimize_op = optimizer.minimize( total_loss, var_list=model.trainable_variables, global_step=tf.compat.v1.train.get_or_create_global_step()) train_op = tf.group(minimize_op, update_ops) return tf.estimator.EstimatorSpec( mode=mode, predictions=predictions, loss=total_loss, train_op=train_op, eval_metric_ops={\\'accuracy\\': accuracy}) # Create the Estimator & Train estimator = tf.estimator.Estimator(model_fn=my_model_fn) tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\\n\\nCustom model_fn with TF 2.0 symbols\\n\\nIf you want to get rid of all TF 1.x symbols and upgrade your custom model_fn to native TF 2.0, you need to update the optimizer and metrics to tf.keras.optimizers and tf.keras.metrics.\\n\\nIn the custom model_fn, besides the above changes, more upgrades need to be made:\\n\\nUse tf.keras.optimizers instead of v1.train.Optimizer.\\n\\nExplicitly pass the model\\'s trainable_variables to the tf.keras.optimizers.\\n\\nTo compute the train_op/minimize_op,\\n\\nUse Optimizer.get_updates() if the loss is scalar loss Tensor(not a callable). The first element in the returned list is the desired train_op/minimize_op.\\n\\nIf the loss is a callable (such as a function), use Optimizer.minimize() to get the train_op/minimize_op.\\n\\nUse tf.keras.metrics instead of tf.compat.v1.metrics for evaluation.\\n\\nFor the above example of my_model_fn, the migrated code with 2.0 symbols is shown as:\\n\\ndef my_model_fn(features, labels, mode): model = make_model() training = (mode == tf.estimator.ModeKeys.TRAIN) loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) predictions = model(features, training=training) # Get both the unconditional losses (the None part) # and the input-conditional losses (the features part). reg_losses = model.get_losses_for(None) + model.get_losses_for(features) total_loss=loss_obj(labels, predictions) + tf.math.add_n(reg_losses) # Upgrade to tf.keras.metrics. accuracy_obj = tf.keras.metrics.Accuracy(name=\\'acc_obj\\') accuracy = accuracy_obj.update_state( y_true=labels, y_pred=tf.math.argmax(predictions, axis=1)) train_op = None if training: # Upgrade to tf.keras.optimizers. optimizer = tf.keras.optimizers.Adam() # Manually assign tf.compat.v1.global_step variable to optimizer.iterations # to make tf.compat.v1.train.global_step increased correctly. # This assignment is a must for any `tf.train.SessionRunHook` specified in # estimator, as SessionRunHooks rely on global step. optimizer.iterations = tf.compat.v1.train.get_or_create_global_step() # Get both the unconditional updates (the None part) # and the input-conditional updates (the features part). update_ops = model.get_updates_for(None) + model.get_updates_for(features) # Compute the minimize_op. minimize_op = optimizer.get_updates( total_loss, model.trainable_variables)[0] train_op = tf.group(minimize_op, *update_ops) return tf.estimator.EstimatorSpec( mode=mode, predictions=predictions, loss=total_loss, train_op=train_op, eval_metric_ops={\\'Accuracy\\': accuracy_obj}) # Create the Estimator & Train. estimator = tf.estimator.Estimator(model_fn=my_model_fn) tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\\n\\nPremade Estimators in the family of tf.estimator.DNN*, tf.estimator.Linear* and tf.estimator.DNNLinearCombined* are still supported in the TensorFlow 2.0 API, however, some arguments have changed:\\n\\ninput_layer_partitioner: Removed in 2.0.\\n\\nloss_reduction: Updated to tf.keras.losses.Reduction instead of tf.compat.v1.losses.Reduction. Its default value is also changed to tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE from tf.compat.v1.losses.Reduction.SUM.\\n\\noptimizer, dnn_optimizer and linear_optimizer: this arg has been updated to tf.keras.optimizers instead of the tf.compat.v1.train.Optimizer.\\n\\nTo migrate the above changes:\\n\\nNo migration is needed for input_layer_partitioner since Distribution Strategy will handle it automatically in TF 2.0.\\n\\nFor loss_reduction, check tf.keras.losses.Reduction for the supported options.\\n\\nFor optimizer args, if you do not pass in an optimizer, dnn_optimizer or linear_optimizer arg, or if you specify the optimizer arg as a string in your code, you don\\'t need to change anything. tf.keras.optimizers is used by default. Otherwise, you need to update it from tf.compat.v1.train.Optimizer to its corresponding tf.keras.optimizers\\n\\nCheckpoint Converter\\n\\nThe migration to keras.optimizers will break checkpoints saved using TF 1.x, as tf.keras.optimizers generates a different set of variables to be saved in checkpoints. To make old checkpoint reusable after your migration to TF 2.0, try the checkpoint converter tool.\\n\\n! curl -O https://raw.githubusercontent.com/tensorflow/estimator/master/tensorflow_estimator/python/estimator/tools/checkpoint_converter.py\\n\\nThe tool has builtin help:\\n\\n! python checkpoint_converter.py -h\\n\\nThis class was simplified to hold ints, instead of tf.compat.v1.Dimension objects. So there is no need to call .value() to get an int.\\n\\nIndividual tf.compat.v1.Dimension objects are still accessible from tf.TensorShape.dims.\\n\\nThe following demonstrate the differences between TensorFlow 1.x and TensorFlow 2.0.\\n\\n# Create a shape and choose an index i = 0 shape = tf.TensorShape([16, None, 256]) shape\\n\\nIf you had this in TF 1.x:\\n\\nvalue = shape[i].value\\n\\nThen do this in TF 2.0:\\n\\nvalue = shape[i] value\\n\\nIf you had this in TF 1.x:\\n\\nfor dim in shape: value = dim.value print(value)\\n\\nThen do this in TF 2.0:\\n\\nfor value in shape: print(value)\\n\\nIf you had this in TF 1.x (Or used any other dimension method):\\n\\ndim = shape[i] dim.assert_is_compatible_with(other_dim)\\n\\nThen do this in TF 2.0:\\n\\nother_dim = 16 Dimension = tf.compat.v1.Dimension if shape.rank is None: dim = Dimension(None) else: dim = shape.dims[i] dim.is_compatible_with(other_dim) # or any other dimension method\\n\\nshape = tf.TensorShape(None) if shape: dim = shape.dims[i] dim.is_compatible_with(other_dim) # or any other dimension method\\n\\nThe boolean value of a tf.TensorShape is True if the rank is known, False otherwise.\\n\\nprint(bool(tf.TensorShape([]))) # Scalar print(bool(tf.TensorShape([0]))) # 0-length vector print(bool(tf.TensorShape([1]))) # 1-length vector print(bool(tf.TensorShape([None]))) # Unknown-length vector print(bool(tf.TensorShape([1, 10, 100]))) # 3D tensor print(bool(tf.TensorShape([None, None, None]))) # 3D tensor with no known dimensions print() print(bool(tf.TensorShape(None))) # A tensor with unknown rank.\\n\\nRemove tf.colocate_with: TensorFlow\\'s device placement algorithms have improved significantly. This should no longer be necessary. If removing it causes a performance degredation please file a bug.\\n\\nReplace v1.ConfigProto usage with the equivalent functions from tf.config.\\n\\nThe overall process is:\\n\\nRun the upgrade script.\\n\\nRemove contrib symbols.\\n\\nSwitch your models to an object oriented style (Keras).\\n\\nUse tf.keras or tf.estimator training and evaluation loops where you can.\\n\\nOtherwise, use custom loops, but be sure to avoid sessions & collections.\\n\\nIt takes a little work to convert code to idiomatic TensorFlow 2.0, but every change results in:\\n\\nFewer lines of code.\\n\\nIncreased clarity and simplicity.\\n\\nContent source: tensorflow/docs-l10n\\n\\nTensorFlowLite_LSTM_Keras_Tutorial\\n\\nTensorFlowLite_LSTM_Keras_Tutorial\\n\\nmodel_maker_image_classification\\n\\ntensorflow_programming_concepts notebook.community | gallery | about', 'timestamp': '2024-02-01T19:45:14', 'title': '| notebook.community', 'url': 'https://notebook.community/tensorflow/docs-l10n/site/en-snapshot/guide/migrate', 'relevance_score': 0.9991704}),\n"," Document(page_content='Stack Overflow Public questions & answers\\n\\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\\n\\nTalent Build your employer brand\\n\\nAdvertising Reach developers & technologists worldwide\\n\\nLabs The future of collective knowledge sharing\\n\\nCollectives™ on Stack Overflow\\n\\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\\n\\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\\n\\nGet early access and see previews of new features. Learn more about Labs\\n\\nReplacing placeholder for tensorflow v2\\n\\nAsked 4 years, 3 months ago\\n\\nModified 3 years, 11 months ago\\n\\nFor my project, I need to convert a directed graph into a tensorflow implementation of the graph as if it was a neural network. In tensorflow version 1, I could just define all of my inputs as placeholders and then just generate the dataflow graph for the outputs using a breadthfirst search of the graph. Then I would just feed in my inputs using a feed_dict. However, in TensorFlow v2.0 they have decided to do away with placeholders entirely.\\n\\nHow would I make a tf.function for each graph that takes in a variable amount of inputs and returns a variable amount of outputs without using a placeholder?\\n\\nI want to generate a tf.function like this that works for an arbitrary acyclic directed graph so that I can take advantage of tensorflow GPU support to run the graph feed forward a few thousand times in a row after I have generated it.\\n\\nEdit for code example:\\n\\nMy graph is defined as a dictionary. Each key represents a node and has a corresponding value of another dictionary specifying incoming and outgoing links with weights.\\n\\n{ \"A\": { \"incoming\": [(\"B\", 2), (\"C\", -1)], \"outgoing\": [(\"D\", 3)] } }\\n\\nI have omitted the entries for B,C, and D for brevity. Here is how I would construct the code I want in tensorflow v1.0 where inputs is just a list of key values that are strictly inputs to the graph\\n\\ndef construct_graph(graph_dict, inputs, outputs): queue = inputs[:] make_dict = {} for key, val in graph_dict.items(): if key in inputs: make_dict[key] = tf.placeholder(tf.float32, name=key) else: make_dict[key] = None # Breadth-First search of graph starting from inputs while len(queue) != 0: cur = graph_dict[queue[0]] for outg in cur[\"outgoing\"]: if make_dict[outg[0]]: # If discovered node, do add/multiply operation make_dict[outg[0]] = tf.add(make_dict[outg[0]], tf.multiply(outg[1], make_dict[queue[0]])) else: # If undiscovered node, input is just coming in multiplied and add outgoing to queue make_dict[outg[0]] = tf.multiply(make_dict[queue[0]], outg[1]) for outgo in graph_dict[outg[0]][\"outgoing\"]: queue.append(outgo[0]) queue.pop(0) # Returns one data graph for each output return [make_dict[x] for x in outputs]\\n\\nI would then be able to run the outputs many times as they are simply graphs with placeholders that I would provide a feed_dict for.\\n\\nObviously, this is not the intended way in TensorFlow v2.0 as they seem to strongly discourage the use of placeholders in this new version.\\n\\nThe point is that I only have to do this preprocessing for a graph once, as it returns a datagraph which is independent of the graph_dict definition.\\n\\nImprove this question\\n\\nedited Nov 29, 2019 at 23:24\\n\\nasked Nov 22, 2019 at 1:22\\n\\n70622 gold badges88 silver badges2525 bronze badges 4\\n\\nWithout some example of what you are trying to do it is very hard to figure out a solution. Can you give an example of a function that \"takes in a variable amount of inputs and returns a variable amount of outputs\" that you wouldn\\'t be able to translate to a TF 2.0 tf.function?\\n\\n– jdehesa Nov 29, 2019 at 11:13\\n\\n@jdehesa Code example added.\\n\\n– Em Eldar Nov 29, 2019 at 23:24\\n\\n@OmerEldar, what\\'s in inputs and outputs?\\n\\n– thushv89 Dec 3, 2019 at 8:17\\n\\nAnd if you can include an example graph resulted for example graph_dict, inputs and outputs, would be much helpful.\\n\\n– thushv89 Dec 3, 2019 at 8:37\\n\\nSorted by: Reset to default\\n\\nHighest score (default)\\n\\nTrending (recent votes count more)\\n\\nDate modified (newest first)\\n\\nDate created (oldest first)\\n\\nMake your code work with TF 2.0\\n\\nBelow is a sample code which you can use with TF 2.0. It relies on the compatibility API that is accessible as tensorflow.compat.v1, and requires to disable v2 behaviors. I don\\'t know if it behaves as you expected. If not, then provide us more explanation of what you try to achieve.\\n\\nimport tensorflow.compat.v1 as tf tf.disable_v2_behavior() @tf.function def construct_graph(graph_dict, inputs, outputs): queue = inputs[:] make_dict = {} for key, val in graph_dict.items(): if key in inputs: make_dict[key] = tf.placeholder(tf.float32, name=key) else: make_dict[key] = None # Breadth-First search of graph starting from inputs while len(queue) != 0: cur = graph_dict[queue[0]] for outg in cur[\"outgoing\"]: if make_dict[outg[0]]: # If discovered node, do add/multiply operation make_dict[outg[0]] = tf.add(make_dict[outg[0]], tf.multiply(outg[1], make_dict[queue[0]])) else: # If undiscovered node, input is just coming in multiplied and add outgoing to queue make_dict[outg[0]] = tf.multiply(make_dict[queue[0]], outg[1]) for outgo in graph_dict[outg[0]][\"outgoing\"]: queue.append(outgo[0]) queue.pop(0) # Returns one data graph for each output return [make_dict[x] for x in outputs] def main(): graph_def = { \"B\": { \"incoming\": [], \"outgoing\": [(\"A\", 1.0)] }, \"C\": { \"incoming\": [], \"outgoing\": [(\"A\", 1.0)] }, \"A\": { \"incoming\": [(\"B\", 2.0), (\"C\", -1.0)], \"outgoing\": [(\"D\", 3.0)] }, \"D\": { \"incoming\": [(\"A\", 2.0)], \"outgoing\": [] } } outputs = construct_graph(graph_def, [\"B\", \"C\"], [\"A\"]) print(outputs) if __name__ == \"__main__\": main()\\n\\n[<tf.Tensor \\'PartitionedCall:0\\' shape=<unknown> dtype=float32>]\\n\\nMigrate your code to TF 2.0\\n\\nWhile the above snippet is valid, it is still tied to TF 1.0. To migrate it to TF 2.0 you have to refactor a little bit your code.\\n\\nInstead of returning a list of tensors, which were callables with TF 1.0, I advise you to return a list of keras.layers.Model.\\n\\nBelow is a working example:\\n\\nimport tensorflow as tf def construct_graph(graph_dict, inputs, outputs): queue = inputs[:] make_dict = {} for key, val in graph_dict.items(): if key in inputs: # Use keras.Input instead of placeholders make_dict[key] = tf.keras.Input(name=key, shape=(), dtype=tf.dtypes.float32) else: make_dict[key] = None # Breadth-First search of graph starting from inputs while len(queue) != 0: cur = graph_dict[queue[0]] for outg in cur[\"outgoing\"]: if make_dict[outg[0]] is not None: # If discovered node, do add/multiply operation make_dict[outg[0]] = tf.keras.layers.add([ make_dict[outg[0]], tf.keras.layers.multiply( [[outg[1]], make_dict[queue[0]]], )], ) else: # If undiscovered node, input is just coming in multiplied and add outgoing to queue make_dict[outg[0]] = tf.keras.layers.multiply( [make_dict[queue[0]], [outg[1]]] ) for outgo in graph_dict[outg[0]][\"outgoing\"]: queue.append(outgo[0]) queue.pop(0) # Returns one data graph for each output model_inputs = [make_dict[key] for key in inputs] model_outputs = [make_dict[key] for key in outputs] return [tf.keras.Model(inputs=model_inputs, outputs=o) for o in model_outputs] def main(): graph_def = { \"B\": { \"incoming\": [], \"outgoing\": [(\"A\", 1.0)] }, \"C\": { \"incoming\": [], \"outgoing\": [(\"A\", 1.0)] }, \"A\": { \"incoming\": [(\"B\", 2.0), (\"C\", -1.0)], \"outgoing\": [(\"D\", 3.0)] }, \"D\": { \"incoming\": [(\"A\", 2.0)], \"outgoing\": [] } } outputs = construct_graph(graph_def, [\"B\", \"C\"], [\"A\"]) print(\"Builded models:\", outputs) for o in outputs: o.summary(120) print(\"Output:\", o((1.0, 1.0))) if __name__ == \"__main__\": main()\\n\\nWhat to notice here?\\n\\nChange from placeholder to keras.Input, requiring to set the shape of the input.\\n\\nUse keras.layers.[add|multiply] for computation. This is probably not required, but stick to one interface. However, it requires to wrap factors inside a list (to handle batching)\\n\\nBuild keras.Model to return\\n\\nCall your model with a tuple of values (not a dictionary anymore)\\n\\nHere is the output of the code.\\n\\nBuilded models: [<tensorflow.python.keras.engine.training.Model object at 0x7fa0b49f0f50>] Model: \"model\" ________________________________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ======================================================================================================================== B (InputLayer) [(None,)] 0 ________________________________________________________________________________________________________________________ C (InputLayer) [(None,)] 0 ________________________________________________________________________________________________________________________ tf_op_layer_mul (TensorFlowOpLayer) [(None,)] 0 B[0][0] ________________________________________________________________________________________________________________________ tf_op_layer_mul_1 (TensorFlowOpLayer) [(None,)] 0 C[0][0] ________________________________________________________________________________________________________________________ add (Add) (None,) 0 tf_op_layer_mul[0][0] tf_op_layer_mul_1[0][0] ======================================================================================================================== Total params: 0 Trainable params: 0 Non-trainable params: 0 ________________________________________________________________________________________________________________________ Output: tf.Tensor([2.], shape=(1,), dtype=float32)\\n\\nedited Jun 20, 2020 at 9:12\\n\\nanswered Dec 3, 2019 at 14:05\\n\\nAlexisBRENONAlexisBRENON\\n\\n3,00922 gold badges1919 silver badges3030 bronze badges 5\\n\\nYour solution for migration is exactly what I was looking for, I didn\\'t know the keras api had such good definition for model construction. Great answer.\\n\\n– Em Eldar Dec 4, 2019 at 11:29\\n\\n@Alexis the script currently gives this error now ValueError: A merge layer should be called on a list of inputs.\\n\\n– Hirak Sarkar Mar 16, 2021 at 17:20\\n\\n@Hirak This worked with TF 2.0. Since then maybe some API changed. With just this single line of error, hard to say what is wrong, but I don\\'t think that it is related to the migration process.\\n\\n– AlexisBRENON Mar 16, 2021 at 17:27\\n\\n@AlexisBRENON thanks for the reply, Here is the link to ipynb with the errors colab.research.google.com/drive/…\\n\\n– Hirak Sarkar Mar 16, 2021 at 18:30\\n\\n@AlexisBRENON just for future reference, if other folks get this same error doing this correction: outg[1] to K.constant([outg[1]]) will fix the error.\\n\\n– Hirak Sarkar Mar 17, 2021 at 2:45\\n\\nNot the answer you\\'re looking for? Browse other questions tagged\\n\\ntensorflow2.0 or ask your own question.\\n\\nIn Rust we trust? White House Office urges memory safety\\n\\nChunking express: An expert breaks down how to build your RAG system\\n\\n2024 Community Moderator Election\\n\\nChanging how community leadership works on Stack Exchange: a proposal and...\\n\\nOur partnership with Google and commitment to socially responsible AI\\n\\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\\n\\n1 What is the replacement of Placeholder in Tensorflow 2.0\\n\\n11 How to change batch size dynamically in Tensorflow 2.0 Dataset?\\n\\n1 How to use TensorFlow v2 for dataflow programming?\\n\\n-1 How to upgrade TensorFlow script to TensorFlow 2.0\\n\\n1 ValueError: A merge layer should be called on a list of inputs for dynamically constructed graph\\n\\n0 What can replace tensorflow.placeholder()?\\n\\n1 Replacement of \"placeholder\" in TensorFlow 2\\n\\n2 Working without TensorFlow placeholder\\n\\n1 Tensorflow placeholder_2 error\\n\\n1 Tensorflow placeholder declaration\\n\\n0 Error: Tensor not recognized, when Feeding Placeholder in Tensorflow1.x any suggestions?\\n\\n2 Any new version of tf.placeholder?\\n\\n11 Why do I get AttributeError: module \\'tensorflow\\' has no attribute \\'placeholder\\'?\\n\\n1 Module \\'tensorflow\\' has no attribute \\'placeholder\\' but tfv1 is imported\\n\\n0 Tensorflow error: AttributeError: module \\'tensorflow\\' has no attribute \\'placeholder\\'\\n\\n1 Invalid placeholder in tensorflow\\n\\n1 What is the best way to replace tensorflows placeholder for version 2.5.0 in google colabs python?\\n\\nHot Network Questions\\n\\nDoes working 2 jobs in a company count as an experience?\\n\\nWhat happens when Congress violates the Constitution through inaction?\\n\\nfirst apartment ever need pet advice\\n\\nModeling timeseries with strong seasonality\\n\\nProbability of pairs\\n\\nCan I follow this recipe?\\n\\nWhere does it say it’s not about the job but rather it’s the merits?\\n\\nHow easily could a pre-scientific people purify a substance and hence grow a pure crystal?\\n\\nUsage of ese/esa/eso with implied gender\\n\\nWhat is the difference between \"within five to six days\" and \"within six days\"?\\n\\nCan I hire someone to help with the worldbuilding of realistic sci-fi mechanics?\\n\\nHow is Riddlemaster Sphinx\\'s identifier greater than the number of cards in the set?\\n\\nFind the B and tell me what you see\\n\\nContourPolarPlot or PolarContourPlot?\\n\\nRegular Curve on Parallel Lines in Illustrator\\n\\nInterfacing an LM2903B output to a microcontroller\\n\\nAtari SIO vs IEEE 488\\n\\nGreat battles in the history of mathematics\\n\\nWorld weavers or something, a double seven child without magic\\n\\nDo 1.5% of lithium-ion batteries overheat, explode, or catch fire each year?\\n\\nTravel Ban in the United Kingdom for Criminal History\\n\\nHow can a QFT field act on particle states in Fock space?\\n\\nTennenbaum\\'s Theorem and polynomials\\n\\nBlown out tyre on pushchair not sitting correctly more hot questions\\n\\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', metadata={'id': 'web-search_1', 'snippet': 'Stack Overflow Public questions & answers\\n\\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\\n\\nTalent Build your employer brand\\n\\nAdvertising Reach developers & technologists worldwide\\n\\nLabs The future of collective knowledge sharing\\n\\nCollectives™ on Stack Overflow\\n\\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\\n\\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\\n\\nGet early access and see previews of new features. Learn more about Labs\\n\\nReplacing placeholder for tensorflow v2\\n\\nAsked 4 years, 3 months ago\\n\\nModified 3 years, 11 months ago\\n\\nFor my project, I need to convert a directed graph into a tensorflow implementation of the graph as if it was a neural network. In tensorflow version 1, I could just define all of my inputs as placeholders and then just generate the dataflow graph for the outputs using a breadthfirst search of the graph. Then I would just feed in my inputs using a feed_dict. However, in TensorFlow v2.0 they have decided to do away with placeholders entirely.\\n\\nHow would I make a tf.function for each graph that takes in a variable amount of inputs and returns a variable amount of outputs without using a placeholder?\\n\\nI want to generate a tf.function like this that works for an arbitrary acyclic directed graph so that I can take advantage of tensorflow GPU support to run the graph feed forward a few thousand times in a row after I have generated it.\\n\\nEdit for code example:\\n\\nMy graph is defined as a dictionary. Each key represents a node and has a corresponding value of another dictionary specifying incoming and outgoing links with weights.\\n\\n{ \"A\": { \"incoming\": [(\"B\", 2), (\"C\", -1)], \"outgoing\": [(\"D\", 3)] } }\\n\\nI have omitted the entries for B,C, and D for brevity. Here is how I would construct the code I want in tensorflow v1.0 where inputs is just a list of key values that are strictly inputs to the graph\\n\\ndef construct_graph(graph_dict, inputs, outputs): queue = inputs[:] make_dict = {} for key, val in graph_dict.items(): if key in inputs: make_dict[key] = tf.placeholder(tf.float32, name=key) else: make_dict[key] = None # Breadth-First search of graph starting from inputs while len(queue) != 0: cur = graph_dict[queue[0]] for outg in cur[\"outgoing\"]: if make_dict[outg[0]]: # If discovered node, do add/multiply operation make_dict[outg[0]] = tf.add(make_dict[outg[0]], tf.multiply(outg[1], make_dict[queue[0]])) else: # If undiscovered node, input is just coming in multiplied and add outgoing to queue make_dict[outg[0]] = tf.multiply(make_dict[queue[0]], outg[1]) for outgo in graph_dict[outg[0]][\"outgoing\"]: queue.append(outgo[0]) queue.pop(0) # Returns one data graph for each output return [make_dict[x] for x in outputs]\\n\\nI would then be able to run the outputs many times as they are simply graphs with placeholders that I would provide a feed_dict for.\\n\\nObviously, this is not the intended way in TensorFlow v2.0 as they seem to strongly discourage the use of placeholders in this new version.\\n\\nThe point is that I only have to do this preprocessing for a graph once, as it returns a datagraph which is independent of the graph_dict definition.\\n\\nImprove this question\\n\\nedited Nov 29, 2019 at 23:24\\n\\nasked Nov 22, 2019 at 1:22\\n\\n70622 gold badges88 silver badges2525 bronze badges 4\\n\\nWithout some example of what you are trying to do it is very hard to figure out a solution. Can you give an example of a function that \"takes in a variable amount of inputs and returns a variable amount of outputs\" that you wouldn\\'t be able to translate to a TF 2.0 tf.function?\\n\\n– jdehesa Nov 29, 2019 at 11:13\\n\\n@jdehesa Code example added.\\n\\n– Em Eldar Nov 29, 2019 at 23:24\\n\\n@OmerEldar, what\\'s in inputs and outputs?\\n\\n– thushv89 Dec 3, 2019 at 8:17\\n\\nAnd if you can include an example graph resulted for example graph_dict, inputs and outputs, would be much helpful.\\n\\n– thushv89 Dec 3, 2019 at 8:37\\n\\nSorted by: Reset to default\\n\\nHighest score (default)\\n\\nTrending (recent votes count more)\\n\\nDate modified (newest first)\\n\\nDate created (oldest first)\\n\\nMake your code work with TF 2.0\\n\\nBelow is a sample code which you can use with TF 2.0. It relies on the compatibility API that is accessible as tensorflow.compat.v1, and requires to disable v2 behaviors. I don\\'t know if it behaves as you expected. If not, then provide us more explanation of what you try to achieve.\\n\\nimport tensorflow.compat.v1 as tf tf.disable_v2_behavior() @tf.function def construct_graph(graph_dict, inputs, outputs): queue = inputs[:] make_dict = {} for key, val in graph_dict.items(): if key in inputs: make_dict[key] = tf.placeholder(tf.float32, name=key) else: make_dict[key] = None # Breadth-First search of graph starting from inputs while len(queue) != 0: cur = graph_dict[queue[0]] for outg in cur[\"outgoing\"]: if make_dict[outg[0]]: # If discovered node, do add/multiply operation make_dict[outg[0]] = tf.add(make_dict[outg[0]], tf.multiply(outg[1], make_dict[queue[0]])) else: # If undiscovered node, input is just coming in multiplied and add outgoing to queue make_dict[outg[0]] = tf.multiply(make_dict[queue[0]], outg[1]) for outgo in graph_dict[outg[0]][\"outgoing\"]: queue.append(outgo[0]) queue.pop(0) # Returns one data graph for each output return [make_dict[x] for x in outputs] def main(): graph_def = { \"B\": { \"incoming\": [], \"outgoing\": [(\"A\", 1.0)] }, \"C\": { \"incoming\": [], \"outgoing\": [(\"A\", 1.0)] }, \"A\": { \"incoming\": [(\"B\", 2.0), (\"C\", -1.0)], \"outgoing\": [(\"D\", 3.0)] }, \"D\": { \"incoming\": [(\"A\", 2.0)], \"outgoing\": [] } } outputs = construct_graph(graph_def, [\"B\", \"C\"], [\"A\"]) print(outputs) if __name__ == \"__main__\": main()\\n\\n[<tf.Tensor \\'PartitionedCall:0\\' shape=<unknown> dtype=float32>]\\n\\nMigrate your code to TF 2.0\\n\\nWhile the above snippet is valid, it is still tied to TF 1.0. To migrate it to TF 2.0 you have to refactor a little bit your code.\\n\\nInstead of returning a list of tensors, which were callables with TF 1.0, I advise you to return a list of keras.layers.Model.\\n\\nBelow is a working example:\\n\\nimport tensorflow as tf def construct_graph(graph_dict, inputs, outputs): queue = inputs[:] make_dict = {} for key, val in graph_dict.items(): if key in inputs: # Use keras.Input instead of placeholders make_dict[key] = tf.keras.Input(name=key, shape=(), dtype=tf.dtypes.float32) else: make_dict[key] = None # Breadth-First search of graph starting from inputs while len(queue) != 0: cur = graph_dict[queue[0]] for outg in cur[\"outgoing\"]: if make_dict[outg[0]] is not None: # If discovered node, do add/multiply operation make_dict[outg[0]] = tf.keras.layers.add([ make_dict[outg[0]], tf.keras.layers.multiply( [[outg[1]], make_dict[queue[0]]], )], ) else: # If undiscovered node, input is just coming in multiplied and add outgoing to queue make_dict[outg[0]] = tf.keras.layers.multiply( [make_dict[queue[0]], [outg[1]]] ) for outgo in graph_dict[outg[0]][\"outgoing\"]: queue.append(outgo[0]) queue.pop(0) # Returns one data graph for each output model_inputs = [make_dict[key] for key in inputs] model_outputs = [make_dict[key] for key in outputs] return [tf.keras.Model(inputs=model_inputs, outputs=o) for o in model_outputs] def main(): graph_def = { \"B\": { \"incoming\": [], \"outgoing\": [(\"A\", 1.0)] }, \"C\": { \"incoming\": [], \"outgoing\": [(\"A\", 1.0)] }, \"A\": { \"incoming\": [(\"B\", 2.0), (\"C\", -1.0)], \"outgoing\": [(\"D\", 3.0)] }, \"D\": { \"incoming\": [(\"A\", 2.0)], \"outgoing\": [] } } outputs = construct_graph(graph_def, [\"B\", \"C\"], [\"A\"]) print(\"Builded models:\", outputs) for o in outputs: o.summary(120) print(\"Output:\", o((1.0, 1.0))) if __name__ == \"__main__\": main()\\n\\nWhat to notice here?\\n\\nChange from placeholder to keras.Input, requiring to set the shape of the input.\\n\\nUse keras.layers.[add|multiply] for computation. This is probably not required, but stick to one interface. However, it requires to wrap factors inside a list (to handle batching)\\n\\nBuild keras.Model to return\\n\\nCall your model with a tuple of values (not a dictionary anymore)\\n\\nHere is the output of the code.\\n\\nBuilded models: [<tensorflow.python.keras.engine.training.Model object at 0x7fa0b49f0f50>] Model: \"model\" ________________________________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ======================================================================================================================== B (InputLayer) [(None,)] 0 ________________________________________________________________________________________________________________________ C (InputLayer) [(None,)] 0 ________________________________________________________________________________________________________________________ tf_op_layer_mul (TensorFlowOpLayer) [(None,)] 0 B[0][0] ________________________________________________________________________________________________________________________ tf_op_layer_mul_1 (TensorFlowOpLayer) [(None,)] 0 C[0][0] ________________________________________________________________________________________________________________________ add (Add) (None,) 0 tf_op_layer_mul[0][0] tf_op_layer_mul_1[0][0] ======================================================================================================================== Total params: 0 Trainable params: 0 Non-trainable params: 0 ________________________________________________________________________________________________________________________ Output: tf.Tensor([2.], shape=(1,), dtype=float32)\\n\\nedited Jun 20, 2020 at 9:12\\n\\nanswered Dec 3, 2019 at 14:05\\n\\nAlexisBRENONAlexisBRENON\\n\\n3,00922 gold badges1919 silver badges3030 bronze badges 5\\n\\nYour solution for migration is exactly what I was looking for, I didn\\'t know the keras api had such good definition for model construction. Great answer.\\n\\n– Em Eldar Dec 4, 2019 at 11:29\\n\\n@Alexis the script currently gives this error now ValueError: A merge layer should be called on a list of inputs.\\n\\n– Hirak Sarkar Mar 16, 2021 at 17:20\\n\\n@Hirak This worked with TF 2.0. Since then maybe some API changed. With just this single line of error, hard to say what is wrong, but I don\\'t think that it is related to the migration process.\\n\\n– AlexisBRENON Mar 16, 2021 at 17:27\\n\\n@AlexisBRENON thanks for the reply, Here is the link to ipynb with the errors colab.research.google.com/drive/…\\n\\n– Hirak Sarkar Mar 16, 2021 at 18:30\\n\\n@AlexisBRENON just for future reference, if other folks get this same error doing this correction: outg[1] to K.constant([outg[1]]) will fix the error.\\n\\n– Hirak Sarkar Mar 17, 2021 at 2:45\\n\\nNot the answer you\\'re looking for? Browse other questions tagged\\n\\ntensorflow2.0 or ask your own question.\\n\\nIn Rust we trust? White House Office urges memory safety\\n\\nChunking express: An expert breaks down how to build your RAG system\\n\\n2024 Community Moderator Election\\n\\nChanging how community leadership works on Stack Exchange: a proposal and...\\n\\nOur partnership with Google and commitment to socially responsible AI\\n\\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\\n\\n1 What is the replacement of Placeholder in Tensorflow 2.0\\n\\n11 How to change batch size dynamically in Tensorflow 2.0 Dataset?\\n\\n1 How to use TensorFlow v2 for dataflow programming?\\n\\n-1 How to upgrade TensorFlow script to TensorFlow 2.0\\n\\n1 ValueError: A merge layer should be called on a list of inputs for dynamically constructed graph\\n\\n0 What can replace tensorflow.placeholder()?\\n\\n1 Replacement of \"placeholder\" in TensorFlow 2\\n\\n2 Working without TensorFlow placeholder\\n\\n1 Tensorflow placeholder_2 error\\n\\n1 Tensorflow placeholder declaration\\n\\n0 Error: Tensor not recognized, when Feeding Placeholder in Tensorflow1.x any suggestions?\\n\\n2 Any new version of tf.placeholder?\\n\\n11 Why do I get AttributeError: module \\'tensorflow\\' has no attribute \\'placeholder\\'?\\n\\n1 Module \\'tensorflow\\' has no attribute \\'placeholder\\' but tfv1 is imported\\n\\n0 Tensorflow error: AttributeError: module \\'tensorflow\\' has no attribute \\'placeholder\\'\\n\\n1 Invalid placeholder in tensorflow\\n\\n1 What is the best way to replace tensorflows placeholder for version 2.5.0 in google colabs python?\\n\\nHot Network Questions\\n\\nDoes working 2 jobs in a company count as an experience?\\n\\nWhat happens when Congress violates the Constitution through inaction?\\n\\nfirst apartment ever need pet advice\\n\\nModeling timeseries with strong seasonality\\n\\nProbability of pairs\\n\\nCan I follow this recipe?\\n\\nWhere does it say it’s not about the job but rather it’s the merits?\\n\\nHow easily could a pre-scientific people purify a substance and hence grow a pure crystal?\\n\\nUsage of ese/esa/eso with implied gender\\n\\nWhat is the difference between \"within five to six days\" and \"within six days\"?\\n\\nCan I hire someone to help with the worldbuilding of realistic sci-fi mechanics?\\n\\nHow is Riddlemaster Sphinx\\'s identifier greater than the number of cards in the set?\\n\\nFind the B and tell me what you see\\n\\nContourPolarPlot or PolarContourPlot?\\n\\nRegular Curve on Parallel Lines in Illustrator\\n\\nInterfacing an LM2903B output to a microcontroller\\n\\nAtari SIO vs IEEE 488\\n\\nGreat battles in the history of mathematics\\n\\nWorld weavers or something, a double seven child without magic\\n\\nDo 1.5% of lithium-ion batteries overheat, explode, or catch fire each year?\\n\\nTravel Ban in the United Kingdom for Criminal History\\n\\nHow can a QFT field act on particle states in Fock space?\\n\\nTennenbaum\\'s Theorem and polynomials\\n\\nBlown out tyre on pushchair not sitting correctly more hot questions\\n\\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.', 'timestamp': '2024-03-05T23:47:04', 'title': 'python - Replacing placeholder for tensorflow v2 - Stack Overflow', 'url': 'https://stackoverflow.com/questions/58986126/replacing-placeholder-for-tensorflow-v2', 'relevance_score': 0.99909604}),\n"," Document(page_content='Stack Overflow Public questions & answers\\n\\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\\n\\nTalent Build your employer brand\\n\\nAdvertising Reach developers & technologists worldwide\\n\\nLabs The future of collective knowledge sharing\\n\\nCollectives™ on Stack Overflow\\n\\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\\n\\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\\n\\nHow to convert placeholder from Tensorflow 1.0 to tf.keras.input in Tensorflow 2.0\\n\\nAsked 1 year, 10 months ago\\n\\nModified 1 year, 10 months ago\\n\\nI have this line of code (for Tensorflow 1.0):\\n\\ntf.placeholder(tf.float32, [None, n, p])\\n\\nn and p are just random numbers.\\n\\nHow to translate this line of code into tf.keras.input for Tensorflow 2.0?\\n\\nThanks a lot in advance!\\n\\nImprove this question\\n\\nedited Sep 26, 2021 at 11:49\\n\\n4,9941717 gold badges3333 silver badges4141 bronze badges\\n\\nasked Sep 3, 2021 at 11:11\\n\\n2755 bronze badges 1\\n\\nI found answer here stackoverflow.com/questions/59583308/…\\n\\n– lindo Sep 4, 2021 at 13:59\\n\\nSorted by: Reset to default\\n\\nHighest score (default)\\n\\nTrending (recent votes count more)\\n\\nDate modified (newest first)\\n\\nDate created (oldest first)\\n\\nIssue was resolved after creating two different virtual environments for TF 2.x and 1.x.\\n\\nFor more information you can refer detail answer mentioned by Denver here.\\n\\n(paraphrased from lindo)\\n\\nFrom Tensorflow documentation\\n\\nIn TF 2.x, you can just pass tensors directly into ops and layers. If you want to explicitly set up your inputs, you can see Keras functional API on how to use tf.keras.Input to replace tf.compat.v1.placeholder. tf.function arguments also do the job of tf.compat.v1.placeholder. For more details please read Better performance with tf.function.\\n\\nInput produces a symbolic tensor-like object (i.e. a placeholder). This can be used with lower-level TensorFlow ops that take tensors as inputs as shown below\\n\\nx = Input(shape=(32,)) y = tf.square(x) # This op will be treated like a layer model = Model(x, y)\\n\\nFor more details you can refer Migrate your TensorFlow 1 code to TensorFlow 2\\n\\nanswered Sep 14, 2021 at 14:07\\n\\nNot the answer you\\'re looking for? Browse other questions tagged\\n\\ntensorflow or ask your own question.\\n\\nImproving time to first byte: Q&A with Dana Lawson of Netlify\\n\\nWhat it’s like to be on the Python Steering Council (Ep. 592)\\n\\nColors update: A more detailed look\\n\\nStack Overflow at WeAreDevelopers World Congress in Berlin\\n\\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\\n\\nLaunching 2 new collectives: PHP and NLP\\n\\nConclusions from title-drafting and question-content assistance experiments...\\n\\n2 How switch tensorflow versions between 2.0 and 1.x?\\n\\n0 feeding tensorflow data into placeholder Tensorflow\\n\\n1 Tensorflow placeholder declaration\\n\\n0 TypeError when feeding tensorflow placeholder\\n\\n1 Can we feed a value without defining tf.placeholder?\\n\\n0 Error: Tensor not recognized, when Feeding Placeholder in Tensorflow1.x any suggestions?\\n\\n1 Tensorflow, Change placeholder and pass values\\n\\n1 How to correctly set the value of placeholder in tensorflow?\\n\\n3 Getting exception while using tf.placeholder in TensorFlow\\n\\n28 Replacing placeholder for tensorflow v2\\n\\n2 Assign tensor value to placeholder in tensorflow v1.13\\n\\nHot Network Questions\\n\\nRain Water is considered pure or impure\\n\\nHow are communist regimes nationalist?\\n\\nWere cartridge slots cheaper at the back?\\n\\nWhat is an English equivalent of \\'Colorín, Colorado, este cuento se ha acabado,\\' a phrase used at the end of stories?\\n\\nAdding a leading zero to single digits in a comma separated string of numbers\\n\\nWhy has it been so hard to come up with testable predictions for string theory?\\n\\nHow does the Buddhist pursue meaning?\\n\\nHow is this global temperature chart compiled?\\n\\nHow to remove \"0\" from numbers of paragraphs when they are not in subsubsection\\n\\nIs what my DM did impacting player agency in a negative way?\\n\\nWhich side is the capacitor negative on this board? (IO/10 marking)\\n\\nThe sum of the series by derivatives\\n\\nIs GFCI and surge suppressor needed on a grounded 3-prong plug?\\n\\nSystematic references on linearizing conditional / logical expressions\\n\\nMay I reveal my identity as an author during peer review?\\n\\nMains Plug Soldering Wires\\n\\nWhich \"Bar\" eviscerated US District Judge Aileen Cannon after granting the special master review of documents?\\n\\nHow to create a random zonogon?\\n\\nWent in our crawl space the other day and noticed one of the posts supporting the main joist of our house is just supporting air\\n\\nWhat are the differences between by the time, when, and before in perfect tenses?\\n\\nAre all free monoids residually finite?\\n\\nIs it possible for a group/clan of 10k people to start their own civilization away from other people in 2050? Like the Amish but with more technology?\\n\\nHow can kaiju exist in nature and not significantly alter civilization?\\n\\nCustom y-axis Compression more hot questions\\n\\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\\n\\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\\n\\nAccept all cookies Necessary cookies only', metadata={'id': 'web-search_4', 'snippet': 'Stack Overflow Public questions & answers\\n\\nStack Overflow for Teams Where developers & technologists share private knowledge with coworkers\\n\\nTalent Build your employer brand\\n\\nAdvertising Reach developers & technologists worldwide\\n\\nLabs The future of collective knowledge sharing\\n\\nCollectives™ on Stack Overflow\\n\\nFind centralized, trusted content and collaborate around the technologies you use most. Learn more about Collectives\\n\\nConnect and share knowledge within a single location that is structured and easy to search. Learn more about Teams\\n\\nHow to convert placeholder from Tensorflow 1.0 to tf.keras.input in Tensorflow 2.0\\n\\nAsked 1 year, 10 months ago\\n\\nModified 1 year, 10 months ago\\n\\nI have this line of code (for Tensorflow 1.0):\\n\\ntf.placeholder(tf.float32, [None, n, p])\\n\\nn and p are just random numbers.\\n\\nHow to translate this line of code into tf.keras.input for Tensorflow 2.0?\\n\\nThanks a lot in advance!\\n\\nImprove this question\\n\\nedited Sep 26, 2021 at 11:49\\n\\n4,9941717 gold badges3333 silver badges4141 bronze badges\\n\\nasked Sep 3, 2021 at 11:11\\n\\n2755 bronze badges 1\\n\\nI found answer here stackoverflow.com/questions/59583308/…\\n\\n– lindo Sep 4, 2021 at 13:59\\n\\nSorted by: Reset to default\\n\\nHighest score (default)\\n\\nTrending (recent votes count more)\\n\\nDate modified (newest first)\\n\\nDate created (oldest first)\\n\\nIssue was resolved after creating two different virtual environments for TF 2.x and 1.x.\\n\\nFor more information you can refer detail answer mentioned by Denver here.\\n\\n(paraphrased from lindo)\\n\\nFrom Tensorflow documentation\\n\\nIn TF 2.x, you can just pass tensors directly into ops and layers. If you want to explicitly set up your inputs, you can see Keras functional API on how to use tf.keras.Input to replace tf.compat.v1.placeholder. tf.function arguments also do the job of tf.compat.v1.placeholder. For more details please read Better performance with tf.function.\\n\\nInput produces a symbolic tensor-like object (i.e. a placeholder). This can be used with lower-level TensorFlow ops that take tensors as inputs as shown below\\n\\nx = Input(shape=(32,)) y = tf.square(x) # This op will be treated like a layer model = Model(x, y)\\n\\nFor more details you can refer Migrate your TensorFlow 1 code to TensorFlow 2\\n\\nanswered Sep 14, 2021 at 14:07\\n\\nNot the answer you\\'re looking for? Browse other questions tagged\\n\\ntensorflow or ask your own question.\\n\\nImproving time to first byte: Q&A with Dana Lawson of Netlify\\n\\nWhat it’s like to be on the Python Steering Council (Ep. 592)\\n\\nColors update: A more detailed look\\n\\nStack Overflow at WeAreDevelopers World Congress in Berlin\\n\\nTemporary policy: Generative AI (e.g., ChatGPT) is banned\\n\\nLaunching 2 new collectives: PHP and NLP\\n\\nConclusions from title-drafting and question-content assistance experiments...\\n\\n2 How switch tensorflow versions between 2.0 and 1.x?\\n\\n0 feeding tensorflow data into placeholder Tensorflow\\n\\n1 Tensorflow placeholder declaration\\n\\n0 TypeError when feeding tensorflow placeholder\\n\\n1 Can we feed a value without defining tf.placeholder?\\n\\n0 Error: Tensor not recognized, when Feeding Placeholder in Tensorflow1.x any suggestions?\\n\\n1 Tensorflow, Change placeholder and pass values\\n\\n1 How to correctly set the value of placeholder in tensorflow?\\n\\n3 Getting exception while using tf.placeholder in TensorFlow\\n\\n28 Replacing placeholder for tensorflow v2\\n\\n2 Assign tensor value to placeholder in tensorflow v1.13\\n\\nHot Network Questions\\n\\nRain Water is considered pure or impure\\n\\nHow are communist regimes nationalist?\\n\\nWere cartridge slots cheaper at the back?\\n\\nWhat is an English equivalent of \\'Colorín, Colorado, este cuento se ha acabado,\\' a phrase used at the end of stories?\\n\\nAdding a leading zero to single digits in a comma separated string of numbers\\n\\nWhy has it been so hard to come up with testable predictions for string theory?\\n\\nHow does the Buddhist pursue meaning?\\n\\nHow is this global temperature chart compiled?\\n\\nHow to remove \"0\" from numbers of paragraphs when they are not in subsubsection\\n\\nIs what my DM did impacting player agency in a negative way?\\n\\nWhich side is the capacitor negative on this board? (IO/10 marking)\\n\\nThe sum of the series by derivatives\\n\\nIs GFCI and surge suppressor needed on a grounded 3-prong plug?\\n\\nSystematic references on linearizing conditional / logical expressions\\n\\nMay I reveal my identity as an author during peer review?\\n\\nMains Plug Soldering Wires\\n\\nWhich \"Bar\" eviscerated US District Judge Aileen Cannon after granting the special master review of documents?\\n\\nHow to create a random zonogon?\\n\\nWent in our crawl space the other day and noticed one of the posts supporting the main joist of our house is just supporting air\\n\\nWhat are the differences between by the time, when, and before in perfect tenses?\\n\\nAre all free monoids residually finite?\\n\\nIs it possible for a group/clan of 10k people to start their own civilization away from other people in 2050? Like the Amish but with more technology?\\n\\nHow can kaiju exist in nature and not significantly alter civilization?\\n\\nCustom y-axis Compression more hot questions\\n\\nTo subscribe to this RSS feed, copy and paste this URL into your RSS reader.\\n\\nBy clicking “Accept all cookies”, you agree Stack Exchange can store cookies on your device and disclose information in accordance with our Cookie Policy.\\n\\nAccept all cookies Necessary cookies only', 'timestamp': '2023-07-23T15:49:00', 'title': 'python - How to convert placeholder from Tensorflow 1.0 to tf.keras.input in Tensorflow 2.0 - Stack Overflow', 'url': 'https://stackoverflow.com/questions/69043984/how-to-convert-placeholder-from-tensorflow-1-0-to-tf-keras-input-in-tensorflow-2', 'relevance_score': 0.99891406})]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["docs"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMGe3g3YEKsQvKxoHQdvy8C","mount_file_id":"1yp_ms-nPuQR4fHx_Twn-dzzMNzWStQ0w","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
